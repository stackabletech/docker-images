Entirely remove hadoop-streaming

From: Lars Francke <git@lars-francke.de>


---
 .../src/main/resources/assemblies/hadoop-tools.xml |   15 
 hadoop-project/pom.xml                             |    5 
 hadoop-project/src/site/site.xml                   |    1 
 .../dev-support/findbugs-exclude.xml               |   51 -
 hadoop-tools/hadoop-streaming/pom.xml              |  219 ----
 .../apache/hadoop/record/BinaryRecordInput.java    |  161 ---
 .../apache/hadoop/record/BinaryRecordOutput.java   |  143 ---
 .../main/java/org/apache/hadoop/record/Buffer.java |  258 -----
 .../org/apache/hadoop/record/CsvRecordOutput.java  |  161 ---
 .../main/java/org/apache/hadoop/record/Index.java  |   45 -
 .../main/java/org/apache/hadoop/record/Record.java |  103 --
 .../org/apache/hadoop/record/RecordComparator.java |   55 -
 .../java/org/apache/hadoop/record/RecordInput.java |  128 --
 .../org/apache/hadoop/record/RecordOutput.java     |  149 ---
 .../main/java/org/apache/hadoop/record/Utils.java  |  498 ---------
 .../java/org/apache/hadoop/record/package.html     |  806 ---------------
 .../apache/hadoop/streaming/AutoInputFormat.java   |   74 -
 .../apache/hadoop/streaming/DumpTypedBytes.java    |  145 ---
 .../org/apache/hadoop/streaming/Environment.java   |  132 --
 .../apache/hadoop/streaming/HadoopStreaming.java   |   71 -
 .../org/apache/hadoop/streaming/JarBuilder.java    |  208 ----
 .../apache/hadoop/streaming/LoadTypedBytes.java    |  104 --
 .../org/apache/hadoop/streaming/PathFinder.java    |  102 --
 .../org/apache/hadoop/streaming/PipeCombiner.java  |   42 -
 .../org/apache/hadoop/streaming/PipeMapRed.java    |  627 -----------
 .../org/apache/hadoop/streaming/PipeMapRunner.java |   36 -
 .../org/apache/hadoop/streaming/PipeMapper.java    |  157 ---
 .../org/apache/hadoop/streaming/PipeReducer.java   |  161 ---
 .../hadoop/streaming/StreamBaseRecordReader.java   |  148 ---
 .../apache/hadoop/streaming/StreamInputFormat.java |   82 -
 .../org/apache/hadoop/streaming/StreamJob.java     | 1105 --------------------
 .../apache/hadoop/streaming/StreamKeyValUtil.java  |  137 --
 .../org/apache/hadoop/streaming/StreamUtil.java    |  205 ----
 .../hadoop/streaming/StreamXmlRecordReader.java    |  302 -----
 .../hadoop/streaming/io/IdentifierResolver.java    |  132 --
 .../apache/hadoop/streaming/io/InputWriter.java    |   48 -
 .../streaming/io/KeyOnlyTextInputWriter.java       |   35 -
 .../streaming/io/KeyOnlyTextOutputReader.java      |   86 --
 .../apache/hadoop/streaming/io/OutputReader.java   |   59 -
 .../hadoop/streaming/io/RawBytesInputWriter.java   |   73 -
 .../hadoop/streaming/io/RawBytesOutputReader.java  |   92 --
 .../hadoop/streaming/io/TextInputWriter.java       |   76 -
 .../hadoop/streaming/io/TextOutputReader.java      |  115 --
 .../hadoop/streaming/io/TypedBytesInputWriter.java |   63 -
 .../streaming/io/TypedBytesOutputReader.java       |   80 -
 .../mapreduce/StreamBaseRecordReader.java          |  154 ---
 .../streaming/mapreduce/StreamInputFormat.java     |  104 --
 .../streaming/mapreduce/StreamXmlRecordReader.java |  341 ------
 .../java/org/apache/hadoop/streaming/package.html  |   27 
 .../java/org/apache/hadoop/typedbytes/Type.java    |   50 -
 .../apache/hadoop/typedbytes/TypedBytesInput.java  |  506 ---------
 .../apache/hadoop/typedbytes/TypedBytesOutput.java |  334 ------
 .../hadoop/typedbytes/TypedBytesRecordInput.java   |  161 ---
 .../hadoop/typedbytes/TypedBytesRecordOutput.java  |  139 ---
 .../hadoop/typedbytes/TypedBytesWritable.java      |   88 --
 .../hadoop/typedbytes/TypedBytesWritableInput.java |  385 -------
 .../typedbytes/TypedBytesWritableOutput.java       |  224 ----
 .../java/org/apache/hadoop/typedbytes/package.html |   67 -
 .../src/main/shellprofile.d/hadoop-streaming.sh    |   51 -
 .../src/site/markdown/HadoopStreaming.md.vm        |  576 ----------
 .../src/site/resources/css/site.css                |   30 -
 hadoop-tools/hadoop-streaming/src/test/bin/cat.cmd |   18 
 .../hadoop-streaming/src/test/bin/xargs_cat.cmd    |   18 
 .../src/test/java/ClassWithNoPackage.java          |   20 
 .../org/apache/hadoop/streaming/DelayEchoApp.java  |   57 -
 .../java/org/apache/hadoop/streaming/FailApp.java  |   58 -
 .../org/apache/hadoop/streaming/OutputOnlyApp.java |   38 -
 .../apache/hadoop/streaming/RawBytesMapApp.java    |   66 -
 .../apache/hadoop/streaming/RawBytesReduceApp.java |   75 -
 .../org/apache/hadoop/streaming/StderrApp.java     |   78 -
 .../apache/hadoop/streaming/StreamAggregate.java   |   57 -
 .../hadoop/streaming/TestAutoInputFormat.java      |  113 --
 .../hadoop/streaming/TestClassWithNoPackage.java   |   56 -
 .../hadoop/streaming/TestDumpTypedBytes.java       |   94 --
 .../org/apache/hadoop/streaming/TestFileArgs.java  |  118 --
 .../org/apache/hadoop/streaming/TestGzipInput.java |   44 -
 .../hadoop/streaming/TestLoadTypedBytes.java       |   91 --
 .../apache/hadoop/streaming/TestMRFramework.java   |   48 -
 .../hadoop/streaming/TestMultipleArchiveFiles.java |  140 ---
 .../hadoop/streaming/TestMultipleCachefiles.java   |  157 ---
 .../hadoop/streaming/TestRawBytesStreaming.java    |   97 --
 .../hadoop/streaming/TestStreamAggregate.java      |  104 --
 .../hadoop/streaming/TestStreamDataProtocol.java   |  117 --
 .../org/apache/hadoop/streaming/TestStreamJob.java |   85 --
 .../hadoop/streaming/TestStreamReduceNone.java     |  110 --
 .../streaming/TestStreamXmlMultipleRecords.java    |  184 ---
 .../streaming/TestStreamXmlRecordReader.java       |   61 -
 .../org/apache/hadoop/streaming/TestStreaming.java |  203 ----
 .../hadoop/streaming/TestStreamingBackground.java  |   87 --
 .../hadoop/streaming/TestStreamingBadRecords.java  |  329 ------
 .../hadoop/streaming/TestStreamingCombiner.java    |   54 -
 .../hadoop/streaming/TestStreamingCounters.java    |   53 -
 .../hadoop/streaming/TestStreamingExitStatus.java  |  109 --
 .../hadoop/streaming/TestStreamingFailure.java     |   55 -
 .../hadoop/streaming/TestStreamingKeyValue.java    |  139 ---
 .../TestStreamingOutputKeyValueTypes.java          |  198 ----
 .../streaming/TestStreamingOutputOnlyKeys.java     |   51 -
 .../hadoop/streaming/TestStreamingSeparator.java   |  122 --
 .../hadoop/streaming/TestStreamingStatus.java      |  336 ------
 .../hadoop/streaming/TestStreamingStderr.java      |  110 --
 .../org/apache/hadoop/streaming/TestSymLink.java   |  142 ---
 .../hadoop/streaming/TestTypedBytesStreaming.java  |   91 --
 .../hadoop/streaming/TestUnconsumedInput.java      |  104 --
 .../java/org/apache/hadoop/streaming/TrApp.java    |  116 --
 .../org/apache/hadoop/streaming/TrAppReduce.java   |  112 --
 .../apache/hadoop/streaming/TypedBytesMapApp.java  |   59 -
 .../hadoop/streaming/TypedBytesReduceApp.java      |   58 -
 .../java/org/apache/hadoop/streaming/UniqApp.java  |   57 -
 .../java/org/apache/hadoop/streaming/UtilTest.java |  145 ---
 .../apache/hadoop/streaming/ValueCountReduce.java  |   64 -
 .../streaming/io/TestKeyOnlyTextOutputReader.java  |   68 -
 .../mapreduce/TestStreamXmlRecordReader.java       |  145 ---
 .../hadoop/typedbytes/TestTypedBytesWritable.java  |   65 -
 hadoop-tools/hadoop-tools-dist/pom.xml             |   11 
 hadoop-tools/pom.xml                               |    2 
 115 files changed, 1 insertion(+), 15920 deletions(-)
 delete mode 100644 hadoop-tools/hadoop-streaming/dev-support/findbugs-exclude.xml
 delete mode 100644 hadoop-tools/hadoop-streaming/pom.xml
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordInput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordOutput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Buffer.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/CsvRecordOutput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Index.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Record.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordComparator.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordInput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordOutput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Utils.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/package.html
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/AutoInputFormat.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/DumpTypedBytes.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/Environment.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/HadoopStreaming.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/JarBuilder.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/LoadTypedBytes.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PathFinder.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeCombiner.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRunner.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapper.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeReducer.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamBaseRecordReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamInputFormat.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamKeyValUtil.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamUtil.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamXmlRecordReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/IdentifierResolver.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/InputWriter.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextInputWriter.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextOutputReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/OutputReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesInputWriter.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextInputWriter.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextOutputReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesInputWriter.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamBaseRecordReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamInputFormat.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamXmlRecordReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/package.html
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/Type.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesInput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesOutput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordInput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordOutput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritable.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableInput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableOutput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/package.html
 delete mode 100755 hadoop-tools/hadoop-streaming/src/main/shellprofile.d/hadoop-streaming.sh
 delete mode 100644 hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm
 delete mode 100644 hadoop-tools/hadoop-streaming/src/site/resources/css/site.css
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/bin/cat.cmd
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/bin/xargs_cat.cmd
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/ClassWithNoPackage.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/DelayEchoApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/FailApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/OutputOnlyApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/RawBytesMapApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/RawBytesReduceApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StderrApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StreamAggregate.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestAutoInputFormat.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestClassWithNoPackage.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestDumpTypedBytes.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestFileArgs.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestGzipInput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestLoadTypedBytes.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMRFramework.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleArchiveFiles.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleCachefiles.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestRawBytesStreaming.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamAggregate.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamDataProtocol.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamJob.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamReduceNone.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlMultipleRecords.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlRecordReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreaming.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBackground.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBadRecords.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCombiner.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCounters.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingExitStatus.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingFailure.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingKeyValue.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputKeyValueTypes.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputOnlyKeys.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingSeparator.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStatus.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStderr.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestSymLink.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestTypedBytesStreaming.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestUnconsumedInput.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrAppReduce.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TypedBytesMapApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TypedBytesReduceApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UniqApp.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UtilTest.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/ValueCountReduce.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/io/TestKeyOnlyTextOutputReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/mapreduce/TestStreamXmlRecordReader.java
 delete mode 100644 hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/typedbytes/TestTypedBytesWritable.java

diff --git a/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml b/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
index 4cde654d0fa..f8f4a6ea528 100644
--- a/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
+++ b/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
@@ -53,21 +53,6 @@
         <include>*-sources.jar</include>
       </includes>
     </fileSet>
-    <fileSet>
-      <directory>../hadoop-streaming/target</directory>
-      <outputDirectory>/share/hadoop/${hadoop.component}/sources</outputDirectory>
-      <includes>
-        <include>*-sources.jar</include>
-      </includes>
-    </fileSet>
-    <fileSet>
-      <directory>../hadoop-streaming/src/main/shellprofile.d</directory>
-      <includes>
-        <include>*</include>
-      </includes>
-      <outputDirectory>/libexec/shellprofile.d</outputDirectory>
-      <fileMode>0755</fileMode>
-    </fileSet>
     <fileSet>
       <directory>../hadoop-aws/src/main/bin</directory>
       <outputDirectory>/bin</outputDirectory>
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index 43ca02d5464..2e3eec4aeb8 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -631,11 +631,6 @@
         <version>${hadoop.version}</version>
       </dependency>
 
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-streaming</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
       <dependency>
         <groupId>org.apache.hadoop</groupId>
         <artifactId>hadoop-distcp</artifactId>
diff --git a/hadoop-project/src/site/site.xml b/hadoop-project/src/site/site.xml
index 5d9ad281ce6..728267ead6a 100644
--- a/hadoop-project/src/site/site.xml
+++ b/hadoop-project/src/site/site.xml
@@ -190,7 +190,6 @@
     </menu>
 
     <menu name="Tools" inherit="top">
-      <item name="Hadoop Streaming" href="hadoop-streaming/HadoopStreaming.html"/>
       <item name="DistCp" href="hadoop-distcp/DistCp.html"/>
       <item name="HDFS Federation Balance" href="hadoop-federation-balance/HDFSFederationBalance.html"/>
       <item name="Hadoop Benchmarking" href="hadoop-project-dist/hadoop-common/Benchmarking.html"/>
diff --git a/hadoop-tools/hadoop-streaming/dev-support/findbugs-exclude.xml b/hadoop-tools/hadoop-streaming/dev-support/findbugs-exclude.xml
deleted file mode 100644
index b886222b3cc..00000000000
--- a/hadoop-tools/hadoop-streaming/dev-support/findbugs-exclude.xml
+++ /dev/null
@@ -1,51 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-<FindBugsFilter>
-  <!-- Workaround bugs in findbugs 1.3.9. See http://sourceforge.net/p/findbugs/bugs/918 for more details. -->
-  <Match>
-    <Or>
-      <Package name="org.apache.hadoop.streaming" />
-    </Or>
-    <Bug pattern="NP_ALWAYS_NULL"/>
-  </Match>
-  <Match>
-    <Class name="org.apache.hadoop.streaming.JarBuilder" />
-    <Bug pattern="NP_NULL_PARAM_DEREF_ALL_TARGETS_DANGEROUS"/>
-  </Match>
-
-  <Match>
-    <Or>
-      <Class name="org.apache.hadoop.streaming.PipeMapper" />
-      <Class name="org.apache.hadoop.streaming.PipeReducer"/>
-    </Or>
-    <Or>
-      <Method name="getFieldSeparator"/>
-      <Method name="getInputSeparator"/>
-    </Or>
-    <Bug pattern="EI_EXPOSE_REP"/>
-  </Match>
-
-  <Match>
-    <Package name="org.apache.hadoop.record" />
-    <Or>
-      <Bug pattern="EI_EXPOSE_REP" />
-      <Bug pattern="EI_EXPOSE_REP2" />
-      <Bug pattern="MS_PKGPROTECT" />
-    </Or>
-  </Match>
-</FindBugsFilter>
diff --git a/hadoop-tools/hadoop-streaming/pom.xml b/hadoop-tools/hadoop-streaming/pom.xml
deleted file mode 100644
index 443d45c73d9..00000000000
--- a/hadoop-tools/hadoop-streaming/pom.xml
+++ /dev/null
@@ -1,219 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.hadoop</groupId>
-    <artifactId>hadoop-project</artifactId>
-    <version>3.4.0</version>
-    <relativePath>../../hadoop-project</relativePath>
-  </parent>
-  <artifactId>hadoop-streaming</artifactId>
-  <version>3.4.0</version>
-  <description>Apache Hadoop MapReduce Streaming</description>
-  <name>Apache Hadoop MapReduce Streaming</name>
-  <packaging>jar</packaging>
-
-  <properties>
-    <hadoop.log.dir>${project.build.directory}/log</hadoop.log.dir>
-    <test.exclude.pattern>%regex[.*(TestStreamingStatus).*]</test.exclude.pattern>
-  </properties>
-
-  <dependencies>
-    <dependency>
-      <groupId>junit</groupId>
-      <artifactId>junit</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.mockito</groupId>
-      <artifactId>mockito-core</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-annotations</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-app</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-hs</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-core</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
-      <scope>test</scope>
-      <type>test-jar</type>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs-client</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <scope>test</scope>
-      <type>test-jar</type>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs</artifactId>
-      <scope>test</scope>
-      <exclusions>
-        <exclusion>
-          <groupId>org.ow2.asm</groupId>
-          <artifactId>asm-commons</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs</artifactId>
-      <scope>test</scope>
-      <type>test-jar</type>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-tests</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>cglib</groupId>
-      <artifactId>cglib</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.bouncycastle</groupId>
-      <artifactId>bcprov-jdk15on</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.bouncycastle</groupId>
-      <artifactId>bcpkix-jdk15on</artifactId>
-      <scope>test</scope>
-    </dependency>
-  </dependencies>
-
-  <build>
-    <plugins>
-       <plugin>
-        <groupId>com.github.spotbugs</groupId>
-        <artifactId>spotbugs-maven-plugin</artifactId>
-         <configuration>
-          <xmlOutput>true</xmlOutput>
-          <excludeFilterFile>${basedir}/dev-support/findbugs-exclude.xml</excludeFilterFile>
-          <effort>Max</effort>
-        </configuration>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-antrun-plugin</artifactId>
-        <executions>
-          <execution>
-            <id>create-log-dir</id>
-            <phase>process-test-resources</phase>
-            <goals>
-              <goal>run</goal>
-            </goals>
-            <configuration>
-              <target>
-                <delete dir="${test.build.data}"/>
-                <mkdir dir="${test.build.data}"/>
-                <mkdir dir="${hadoop.log.dir}"/>
-              </target>
-            </configuration>
-          </execution>
-          <execution>
-            <id>copy-test-bin</id>
-            <phase>process-test-resources</phase>
-            <goals>
-              <goal>run</goal>
-            </goals>
-            <configuration>
-              <target>
-                <copy todir="target/bin">
-                  <fileset dir="src/test/bin" />
-                </copy>
-              </target>
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-jar-plugin</artifactId>
-         <configuration>
-          <archive>
-           <manifest>
-            <mainClass>org.apache.hadoop.streaming.HadoopStreaming</mainClass>
-           </manifest>
-         </archive>
-        </configuration>
-       </plugin>
-       <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-dependency-plugin</artifactId>
-        <executions>
-          <execution>
-            <id>deplist</id>
-            <phase>compile</phase>
-            <goals>
-              <goal>list</goal>
-            </goals>
-            <configuration>
-              <!-- referenced by a built-in command -->
-              <outputFile>${project.basedir}/target/hadoop-tools-deps/${project.artifactId}.tools-builtin.txt</outputFile>
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-    </plugins>
-    <testResources>
-      <testResource>
-        <directory>${basedir}/../../hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/conf</directory>
-        <includes>
-          <include>capacity-scheduler.xml</include>
-        </includes>
-        <filtering>false</filtering>
-      </testResource>
-    </testResources>
-  </build>
-</project>
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordInput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordInput.java
deleted file mode 100644
index f923a8065fd..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordInput.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import java.io.DataInput;
-import java.io.IOException;
-import java.io.DataInputStream;
-import java.io.InputStream;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class BinaryRecordInput implements RecordInput {
-    
-  private DataInput in;
-    
-  static private class BinaryIndex implements Index {
-    private int nelems;
-    private BinaryIndex(int nelems) {
-      this.nelems = nelems;
-    }
-    @Override
-    public boolean done() {
-      return (nelems <= 0);
-    }
-    @Override
-    public void incr() {
-      nelems--;
-    }
-  }
-    
-  private BinaryRecordInput() {}
-    
-  private void setDataInput(DataInput inp) {
-    this.in = inp;
-  }
-    
-  private static final ThreadLocal<BinaryRecordInput> B_IN =
-      new ThreadLocal<BinaryRecordInput>() {
-      @Override
-      protected BinaryRecordInput initialValue() {
-        return new BinaryRecordInput();
-      }
-    };
-    
-  /**
-   * Get a thread-local record input for the supplied DataInput.
-   * @param inp data input stream
-   * @return binary record input corresponding to the supplied DataInput.
-   */
-  public static BinaryRecordInput get(DataInput inp) {
-    BinaryRecordInput bin = B_IN.get();
-    bin.setDataInput(inp);
-    return bin;
-  }
-    
-  /** Creates a new instance of BinaryRecordInput */
-  public BinaryRecordInput(InputStream strm) {
-    this.in = new DataInputStream(strm);
-  }
-    
-  /** Creates a new instance of BinaryRecordInput */
-  public BinaryRecordInput(DataInput din) {
-    this.in = din;
-  }
-    
-  @Override
-  public byte readByte(final String tag) throws IOException {
-    return in.readByte();
-  }
-    
-  @Override
-  public boolean readBool(final String tag) throws IOException {
-    return in.readBoolean();
-  }
-    
-  @Override
-  public int readInt(final String tag) throws IOException {
-    return Utils.readVInt(in);
-  }
-    
-  @Override
-  public long readLong(final String tag) throws IOException {
-    return Utils.readVLong(in);
-  }
-    
-  @Override
-  public float readFloat(final String tag) throws IOException {
-    return in.readFloat();
-  }
-    
-  @Override
-  public double readDouble(final String tag) throws IOException {
-    return in.readDouble();
-  }
-    
-  @Override
-  public String readString(final String tag) throws IOException {
-    return Utils.fromBinaryString(in);
-  }
-    
-  @Override
-  public Buffer readBuffer(final String tag) throws IOException {
-    final int len = Utils.readVInt(in);
-    final byte[] barr = new byte[len];
-    in.readFully(barr);
-    return new Buffer(barr);
-  }
-    
-  @Override
-  public void startRecord(final String tag) throws IOException {
-    // no-op
-  }
-    
-  @Override
-  public void endRecord(final String tag) throws IOException {
-    // no-op
-  }
-    
-  @Override
-  public Index startVector(final String tag) throws IOException {
-    return new BinaryIndex(readInt(tag));
-  }
-    
-  @Override
-  public void endVector(final String tag) throws IOException {
-    // no-op
-  }
-    
-  @Override
-  public Index startMap(final String tag) throws IOException {
-    return new BinaryIndex(readInt(tag));
-  }
-    
-  @Override
-  public void endMap(final String tag) throws IOException {
-    // no-op
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordOutput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordOutput.java
deleted file mode 100644
index adc01f311f8..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/BinaryRecordOutput.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import java.io.IOException;
-import java.util.TreeMap;
-import java.util.ArrayList;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.OutputStream;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class BinaryRecordOutput implements RecordOutput {
-    
-  private DataOutput out;
-    
-  private BinaryRecordOutput() {}
-    
-  private void setDataOutput(DataOutput out) {
-    this.out = out;
-  }
-    
-  private static final ThreadLocal<BinaryRecordOutput> B_OUT =
-      new ThreadLocal<BinaryRecordOutput>() {
-    @Override
-    protected BinaryRecordOutput initialValue() {
-      return new BinaryRecordOutput();
-    }
-  };
-
-  /**
-   * Get a thread-local record output for the supplied DataOutput.
-   * @param out data output stream
-   * @return binary record output corresponding to the supplied DataOutput.
-   */
-  public static BinaryRecordOutput get(DataOutput out) {
-    BinaryRecordOutput bout = B_OUT.get();
-    bout.setDataOutput(out);
-    return bout;
-  }
-    
-  /** Creates a new instance of BinaryRecordOutput */
-  public BinaryRecordOutput(OutputStream out) {
-    this.out = new DataOutputStream(out);
-  }
-    
-  /** Creates a new instance of BinaryRecordOutput */
-  public BinaryRecordOutput(DataOutput out) {
-    this.out = out;
-  }
-    
-    
-  @Override
-  public void writeByte(byte b, String tag) throws IOException {
-    out.writeByte(b);
-  }
-    
-  @Override
-  public void writeBool(boolean b, String tag) throws IOException {
-    out.writeBoolean(b);
-  }
-    
-  @Override
-  public void writeInt(int i, String tag) throws IOException {
-    Utils.writeVInt(out, i);
-  }
-    
-  @Override
-  public void writeLong(long l, String tag) throws IOException {
-    Utils.writeVLong(out, l);
-  }
-    
-  @Override
-  public void writeFloat(float f, String tag) throws IOException {
-    out.writeFloat(f);
-  }
-    
-  @Override
-  public void writeDouble(double d, String tag) throws IOException {
-    out.writeDouble(d);
-  }
-    
-  @Override
-  public void writeString(String s, String tag) throws IOException {
-    Utils.toBinaryString(out, s);
-  }
-    
-  @Override
-  public void writeBuffer(Buffer buf, String tag)
-    throws IOException {
-    byte[] barr = buf.get();
-    int len = buf.getCount();
-    Utils.writeVInt(out, len);
-    out.write(barr, 0, len);
-  }
-    
-  @Override
-  public void startRecord(Record r, String tag) throws IOException {}
-    
-  @Override
-  public void endRecord(Record r, String tag) throws IOException {}
-    
-  @Override
-  public void startVector(ArrayList v, String tag) throws IOException {
-    writeInt(v.size(), tag);
-  }
-    
-  @Override
-  public void endVector(ArrayList v, String tag) throws IOException {}
-    
-  @Override
-  public void startMap(TreeMap v, String tag) throws IOException {
-    writeInt(v.size(), tag);
-  }
-    
-  @Override
-  public void endMap(TreeMap v, String tag) throws IOException {}
-    
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Buffer.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Buffer.java
deleted file mode 100644
index 737d63d11d6..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Buffer.java
+++ /dev/null
@@ -1,258 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import java.io.UnsupportedEncodingException;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * A byte sequence that is used as a Java native type for buffer.
- * It is resizable and distinguishes between the count of the sequence and
- * the current capacity.
- * 
- * @deprecated Replaced by <a href="http://avro.apache.org/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class Buffer implements Comparable, Cloneable {
-  /** Number of valid bytes in this.bytes. */
-  private int count;
-  /** Backing store for Buffer. */
-  private byte[] bytes = null;
-
-  /**
-   * Create a zero-count sequence.
-   */
-  public Buffer() {
-    this.count = 0;
-  }
-
-  /**
-   * Create a Buffer using the byte array as the initial value.
-   *
-   * @param bytes This array becomes the backing storage for the object.
-   */
-  public Buffer(byte[] bytes) {
-    this.bytes = bytes;
-    this.count = (bytes == null) ? 0 : bytes.length;
-  }
-  
-  /**
-   * Create a Buffer using the byte range as the initial value.
-   *
-   * @param bytes Copy of this array becomes the backing storage for the object.
-   * @param offset offset into byte array
-   * @param length length of data
-   */
-  public Buffer(byte[] bytes, int offset, int length) {
-    copy(bytes, offset, length);
-  }
-  
-  
-  /**
-   * Use the specified bytes array as underlying sequence.
-   *
-   * @param bytes byte sequence
-   */
-  public void set(byte[] bytes) {
-    this.count = (bytes == null) ? 0 : bytes.length;
-    this.bytes = bytes;
-  }
-  
-  /**
-   * Copy the specified byte array to the Buffer. Replaces the current buffer.
-   *
-   * @param bytes byte array to be assigned
-   * @param offset offset into byte array
-   * @param length length of data
-   */
-  public final void copy(byte[] bytes, int offset, int length) {
-    if (this.bytes == null || this.bytes.length < length) {
-      this.bytes = new byte[length];
-    }
-    System.arraycopy(bytes, offset, this.bytes, 0, length);
-    this.count = length;
-  }
-  
-  /**
-   * Get the data from the Buffer.
-   * 
-   * @return The data is only valid between 0 and getCount() - 1.
-   */
-  public byte[] get() {
-    if (bytes == null) {
-      bytes = new byte[0];
-    }
-    return bytes;
-  }
-  
-  /**
-   * Get the current count of the buffer.
-   */
-  public int getCount() {
-    return count;
-  }
-  
-  /**
-   * Get the capacity, which is the maximum count that could handled without
-   * resizing the backing storage.
-   * 
-   * @return The number of bytes
-   */
-  public int getCapacity() {
-    return this.get().length;
-  }
-  
-  /**
-   * Change the capacity of the backing storage.
-   * The data is preserved if newCapacity {@literal >=} getCount().
-   * @param newCapacity The new capacity in bytes.
-   */
-  public void setCapacity(int newCapacity) {
-    if (newCapacity < 0) {
-      throw new IllegalArgumentException("Invalid capacity argument "+newCapacity); 
-    }
-    if (newCapacity == 0) {
-      this.bytes = null;
-      this.count = 0;
-      return;
-    }
-    if (newCapacity != getCapacity()) {
-      byte[] data = new byte[newCapacity];
-      if (newCapacity < count) {
-        count = newCapacity;
-      }
-      if (count != 0) {
-        System.arraycopy(this.get(), 0, data, 0, count);
-      }
-      bytes = data;
-    }
-  }
-  
-  /**
-   * Reset the buffer to 0 size
-   */
-  public void reset() {
-    setCapacity(0);
-  }
-  
-  /**
-   * Change the capacity of the backing store to be the same as the current 
-   * count of buffer.
-   */
-  public void truncate() {
-    setCapacity(count);
-  }
-
-  /**
-   * Append specified bytes to the buffer.
-   *
-   * @param bytes byte array to be appended
-   * @param offset offset into byte array
-   * @param length length of data
-
-  */
-  public void append(byte[] bytes, int offset, int length) {
-    setCapacity(count+length);
-    System.arraycopy(bytes, offset, this.get(), count, length);
-    count = count + length;
-  }
-  
-  /**
-   * Append specified bytes to the buffer
-   *
-   * @param bytes byte array to be appended
-   */
-  public void append(byte[] bytes) {
-    append(bytes, 0, bytes.length);
-  }
-  
-  // inherit javadoc
-  @Override
-  public int hashCode() {
-    int hash = 1;
-    byte[] b = this.get();
-    for (int i = 0; i < count; i++)
-      hash = (31 * hash) + b[i];
-    return hash;
-  }
-  
-  /**
-   * Define the sort order of the Buffer.
-   * 
-   * @param other The other buffer
-   * @return Positive if this is bigger than other, 0 if they are equal, and
-   *         negative if this is smaller than other.
-   */
-  @Override
-  public int compareTo(Object other) {
-    Buffer right = ((Buffer) other);
-    byte[] lb = this.get();
-    byte[] rb = right.get();
-    for (int i = 0; i < count && i < right.count; i++) {
-      int a = (lb[i] & 0xff);
-      int b = (rb[i] & 0xff);
-      if (a != b) {
-        return a - b;
-      }
-    }
-    return count - right.count;
-  }
-  
-  // inherit javadoc
-  @Override
-  public boolean equals(Object other) {
-    if (other instanceof Buffer && this != other) {
-      return compareTo(other) == 0;
-    }
-    return (this == other);
-  }
-  
-  // inheric javadoc
-  @Override
-  public String toString() {
-    StringBuilder sb = new StringBuilder(2*count);
-    for(int idx = 0; idx < count; idx++) {
-      sb.append(Character.forDigit((bytes[idx] & 0xF0) >> 4, 16));
-      sb.append(Character.forDigit(bytes[idx] & 0x0F, 16));
-    }
-    return sb.toString();
-  }
-  
-  /**
-   * Convert the byte buffer to a string an specific character encoding
-   *
-   * @param charsetName Valid Java Character Set Name
-   */
-  public String toString(String charsetName)
-    throws UnsupportedEncodingException {
-    return new String(this.get(), 0, this.getCount(), charsetName);
-  }
-  
-  // inherit javadoc
-  @Override
-  public Object clone() throws CloneNotSupportedException {
-    Buffer result = (Buffer) super.clone();
-    result.copy(this.get(), 0, this.getCount());
-    return result;
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/CsvRecordOutput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/CsvRecordOutput.java
deleted file mode 100644
index 18cf23e78a4..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/CsvRecordOutput.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import java.io.IOException;
-import java.util.TreeMap;
-import java.util.ArrayList;
-import java.io.PrintStream;
-import java.io.OutputStream;
-import java.io.UnsupportedEncodingException;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class CsvRecordOutput implements RecordOutput {
-
-  private PrintStream stream;
-  private boolean isFirst = true;
-    
-  private void throwExceptionOnError(String tag) throws IOException {
-    if (stream.checkError()) {
-      throw new IOException("Error serializing "+tag);
-    }
-  }
- 
-  private void printCommaUnlessFirst() {
-    if (!isFirst) {
-      stream.print(",");
-    }
-    isFirst = false;
-  }
-    
-  /** Creates a new instance of CsvRecordOutput */
-  public CsvRecordOutput(OutputStream out) {
-    try {
-      stream = new PrintStream(out, true, "UTF-8");
-    } catch (UnsupportedEncodingException ex) {
-      throw new RuntimeException(ex);
-    }
-  }
-    
-  @Override
-  public void writeByte(byte b, String tag) throws IOException {
-    writeLong((long)b, tag);
-  }
-    
-  @Override
-  public void writeBool(boolean b, String tag) throws IOException {
-    printCommaUnlessFirst();
-    String val = b ? "T" : "F";
-    stream.print(val);
-    throwExceptionOnError(tag);
-  }
-    
-  @Override
-  public void writeInt(int i, String tag) throws IOException {
-    writeLong((long)i, tag);
-  }
-    
-  @Override
-  public void writeLong(long l, String tag) throws IOException {
-    printCommaUnlessFirst();
-    stream.print(l);
-    throwExceptionOnError(tag);
-  }
-    
-  @Override
-  public void writeFloat(float f, String tag) throws IOException {
-    writeDouble((double)f, tag);
-  }
-    
-  @Override
-  public void writeDouble(double d, String tag) throws IOException {
-    printCommaUnlessFirst();
-    stream.print(d);
-    throwExceptionOnError(tag);
-  }
-    
-  @Override
-  public void writeString(String s, String tag) throws IOException {
-    printCommaUnlessFirst();
-    stream.print(Utils.toCSVString(s));
-    throwExceptionOnError(tag);
-  }
-    
-  @Override
-  public void writeBuffer(Buffer buf, String tag)
-    throws IOException {
-    printCommaUnlessFirst();
-    stream.print(Utils.toCSVBuffer(buf));
-    throwExceptionOnError(tag);
-  }
-    
-  @Override
-  public void startRecord(Record r, String tag) throws IOException {
-    if (tag != null && ! tag.isEmpty()) {
-      printCommaUnlessFirst();
-      stream.print("s{");
-      isFirst = true;
-    }
-  }
-    
-  @Override
-  public void endRecord(Record r, String tag) throws IOException {
-    if (tag == null || tag.isEmpty()) {
-      stream.print("\n");
-      isFirst = true;
-    } else {
-      stream.print("}");
-      isFirst = false;
-    }
-  }
-    
-  @Override
-  public void startVector(ArrayList v, String tag) throws IOException {
-    printCommaUnlessFirst();
-    stream.print("v{");
-    isFirst = true;
-  }
-    
-  @Override
-  public void endVector(ArrayList v, String tag) throws IOException {
-    stream.print("}");
-    isFirst = false;
-  }
-    
-  @Override
-  public void startMap(TreeMap v, String tag) throws IOException {
-    printCommaUnlessFirst();
-    stream.print("m{");
-    isFirst = true;
-  }
-    
-  @Override
-  public void endMap(TreeMap v, String tag) throws IOException {
-    stream.print("}");
-    isFirst = false;
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Index.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Index.java
deleted file mode 100644
index 1d36c92bc0c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Index.java
+++ /dev/null
@@ -1,45 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Interface that acts as an iterator for deserializing maps.
- * The deserializer returns an instance that the record uses to
- * read vectors and maps. An example of usage is as follows:
- *
- * <code>
- * Index idx = startVector(...);
- * while (!idx.done()) {
- *   .... // read element of a vector
- *   idx.incr();
- * }
- * </code>
- * 
- * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public interface Index {
-  boolean done();
-  void incr();
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Record.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Record.java
deleted file mode 100644
index 84df8b8187e..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Record.java
+++ /dev/null
@@ -1,103 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.io.WritableComparable;
-
-/**
- * Abstract class that is extended by generated classes.
- * 
- * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public abstract class Record implements WritableComparable, Cloneable {
-  
-  /**
-   * Serialize a record with tag (ususally field name)
-   * @param rout Record output destination
-   * @param tag record tag (Used only in tagged serialization e.g. XML)
-   */
-  public abstract void serialize(RecordOutput rout, String tag)
-    throws IOException;
-  
-  /**
-   * Deserialize a record with a tag (usually field name)
-   * @param rin Record input source
-   * @param tag Record tag (Used only in tagged serialization e.g. XML)
-   */
-  public abstract void deserialize(RecordInput rin, String tag)
-    throws IOException;
-  
-  // inheric javadoc
-  @Override
-  public abstract int compareTo (final Object peer) throws ClassCastException;
-  
-  /**
-   * Serialize a record without a tag
-   * @param rout Record output destination
-   */
-  public void serialize(RecordOutput rout) throws IOException {
-    this.serialize(rout, "");
-  }
-  
-  /**
-   * Deserialize a record without a tag
-   * @param rin Record input source
-   */
-  public void deserialize(RecordInput rin) throws IOException {
-    this.deserialize(rin, "");
-  }
-  
-  // inherit javadoc
-  @Override
-  public void write(final DataOutput out) throws java.io.IOException {
-    BinaryRecordOutput bout = BinaryRecordOutput.get(out);
-    this.serialize(bout);
-  }
-  
-  // inherit javadoc
-  @Override
-  public void readFields(final DataInput din) throws java.io.IOException {
-    BinaryRecordInput rin = BinaryRecordInput.get(din);
-    this.deserialize(rin);
-  }
-
-  // inherit javadoc
-  @Override
-  public String toString() {
-    try {
-      ByteArrayOutputStream s = new ByteArrayOutputStream();
-      CsvRecordOutput a = new CsvRecordOutput(s);
-      this.serialize(a);
-      return new String(s.toByteArray(), StandardCharsets.UTF_8);
-    } catch (Exception ex) {
-      throw new RuntimeException(ex);
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordComparator.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordComparator.java
deleted file mode 100644
index 805d93160a8..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordComparator.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableComparator;
-
-/**
- * A raw record comparator base class
- * 
- * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public abstract class RecordComparator extends WritableComparator {
-  
-  /**
-   * Construct a raw {@link Record} comparison implementation. */
-  protected RecordComparator(Class<? extends WritableComparable> recordClass) {
-    super(recordClass);
-  }
-  
-  // inheric JavaDoc
-  @Override
-  public abstract int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2);
-  
-  /**
-   * Register an optimized comparator for a {@link Record} implementation.
-   *
-   * @param c record classs for which a raw comparator is provided
-   * @param comparator Raw comparator instance for class c 
-   */
-  public static synchronized void define(Class c, RecordComparator comparator) {
-    WritableComparator.define(c, comparator);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordInput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordInput.java
deleted file mode 100644
index 6495194a248..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordInput.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import java.io.IOException;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Interface that all the Deserializers have to implement.
- * 
- * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public interface RecordInput {
-  /**
-   * Read a byte from serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return value read from serialized record.
-   */
-  byte readByte(String tag) throws IOException;
-  
-  /**
-   * Read a boolean from serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return value read from serialized record.
-   */
-  boolean readBool(String tag) throws IOException;
-  
-  /**
-   * Read an integer from serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return value read from serialized record.
-   */
-  int readInt(String tag) throws IOException;
-  
-  /**
-   * Read a long integer from serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return value read from serialized record.
-   */
-  long readLong(String tag) throws IOException;
-  
-  /**
-   * Read a single-precision float from serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return value read from serialized record.
-   */
-  float readFloat(String tag) throws IOException;
-  
-  /**
-   * Read a double-precision number from serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return value read from serialized record.
-   */
-  double readDouble(String tag) throws IOException;
-  
-  /**
-   * Read a UTF-8 encoded string from serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return value read from serialized record.
-   */
-  String readString(String tag) throws IOException;
-  
-  /**
-   * Read byte array from serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return value read from serialized record.
-   */
-  Buffer readBuffer(String tag) throws IOException;
-  
-  /**
-   * Check the mark for start of the serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   */
-  void startRecord(String tag) throws IOException;
-  
-  /**
-   * Check the mark for end of the serialized record.
-   * @param tag Used by tagged serialization formats (such as XML)
-   */
-  void endRecord(String tag) throws IOException;
-  
-  /**
-   * Check the mark for start of the serialized vector.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return Index that is used to count the number of elements.
-   */
-  Index startVector(String tag) throws IOException;
-  
-  /**
-   * Check the mark for end of the serialized vector.
-   * @param tag Used by tagged serialization formats (such as XML)
-   */
-  void endVector(String tag) throws IOException;
-  
-  /**
-   * Check the mark for start of the serialized map.
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @return Index that is used to count the number of map entries.
-   */
-  Index startMap(String tag) throws IOException;
-  
-  /**
-   * Check the mark for end of the serialized map.
-   * @param tag Used by tagged serialization formats (such as XML)
-   */
-  void endMap(String tag) throws IOException;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordOutput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordOutput.java
deleted file mode 100644
index 503ea35f794..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/RecordOutput.java
+++ /dev/null
@@ -1,149 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import java.io.IOException;
-import java.util.TreeMap;
-import java.util.ArrayList;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Interface that all the serializers have to implement.
- * 
- * @deprecated Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public interface RecordOutput {
-  /**
-   * Write a byte to serialized record.
-   * @param b Byte to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void writeByte(byte b, String tag) throws IOException;
-  
-  /**
-   * Write a boolean to serialized record.
-   * @param b Boolean to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void writeBool(boolean b, String tag) throws IOException;
-  
-  /**
-   * Write an integer to serialized record.
-   * @param i Integer to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void writeInt(int i, String tag) throws IOException;
-  
-  /**
-   * Write a long integer to serialized record.
-   * @param l Long to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void writeLong(long l, String tag) throws IOException;
-  
-  /**
-   * Write a single-precision float to serialized record.
-   * @param f Float to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void writeFloat(float f, String tag) throws IOException;
-  
-  /**
-   * Write a double precision floating point number to serialized record.
-   * @param d Double to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void writeDouble(double d, String tag) throws IOException;
-  
-  /**
-   * Write a unicode string to serialized record.
-   * @param s String to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void writeString(String s, String tag) throws IOException;
-  
-  /**
-   * Write a buffer to serialized record.
-   * @param buf Buffer to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void writeBuffer(Buffer buf, String tag)
-    throws IOException;
-  
-  /**
-   * Mark the start of a record to be serialized.
-   * @param r Record to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void startRecord(Record r, String tag) throws IOException;
-  
-  /**
-   * Mark the end of a serialized record.
-   * @param r Record to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void endRecord(Record r, String tag) throws IOException;
-  
-  /**
-   * Mark the start of a vector to be serialized.
-   * @param v Vector to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void startVector(ArrayList v, String tag) throws IOException;
-  
-  /**
-   * Mark the end of a serialized vector.
-   * @param v Vector to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void endVector(ArrayList v, String tag) throws IOException;
-  
-  /**
-   * Mark the start of a map to be serialized.
-   * @param m Map to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void startMap(TreeMap m, String tag) throws IOException;
-  
-  /**
-   * Mark the end of a serialized map.
-   * @param m Map to be serialized
-   * @param tag Used by tagged serialization formats (such as XML)
-   * @throws IOException Indicates error in serialization
-   */
-  public void endMap(TreeMap m, String tag) throws IOException;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Utils.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Utils.java
deleted file mode 100644
index 59e2080c3ed..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/Utils.java
+++ /dev/null
@@ -1,498 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.record;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.io.WritableUtils;
-
-/**
- * Various utility functions for Hadoop record I/O runtime.
- * 
- * @deprecated Replaced by <a href="http://avro.apache.org/">Avro</a>.
- */
-@Deprecated
-@InterfaceAudience.Public
-@InterfaceStability.Stable
-public class Utils {
-  
-  /** Cannot create a new instance of Utils */
-  private Utils() {
-  }
-  
-  public static final char[] hexchars = { '0', '1', '2', '3', '4', '5',
-                                          '6', '7', '8', '9', 'A', 'B',
-                                          'C', 'D', 'E', 'F' };
-  /**
-   *
-   * @param s
-   * @return
-   */
-  static String toXMLString(String s) {
-    StringBuilder sb = new StringBuilder();
-    for (int idx = 0; idx < s.length(); idx++) {
-      char ch = s.charAt(idx);
-      if (ch == '<') {
-        sb.append("&lt;");
-      } else if (ch == '&') {
-        sb.append("&amp;");
-      } else if (ch == '%') {
-        sb.append("%0025");
-      } else if (ch < 0x20 ||
-                 (ch > 0xD7FF && ch < 0xE000) ||
-                 (ch > 0xFFFD)) {
-        sb.append("%");
-        sb.append(hexchars[(ch & 0xF000) >> 12]);
-        sb.append(hexchars[(ch & 0x0F00) >> 8]);
-        sb.append(hexchars[(ch & 0x00F0) >> 4]);
-        sb.append(hexchars[(ch & 0x000F)]);
-      } else {
-        sb.append(ch);
-      }
-    }
-    return sb.toString();
-  }
-  
-  static private int h2c(char ch) {
-    if (ch >= '0' && ch <= '9') {
-      return ch - '0';
-    } else if (ch >= 'A' && ch <= 'F') {
-      return ch - 'A' + 10;
-    } else if (ch >= 'a' && ch <= 'f') {
-      return ch - 'a' + 10;
-    }
-    return 0;
-  }
-  
-  /**
-   *
-   * @param s
-   * @return
-   */
-  static String fromXMLString(String s) {
-    StringBuilder sb = new StringBuilder();
-    for (int idx = 0; idx < s.length();) {
-      char ch = s.charAt(idx++);
-      if (ch == '%') {
-        int ch1 = h2c(s.charAt(idx++)) << 12;
-        int ch2 = h2c(s.charAt(idx++)) << 8;
-        int ch3 = h2c(s.charAt(idx++)) << 4;
-        int ch4 = h2c(s.charAt(idx++));
-        char res = (char)(ch1 | ch2 | ch3 | ch4);
-        sb.append(res);
-      } else {
-        sb.append(ch);
-      }
-    }
-    return sb.toString();
-  }
-  
-  /**
-   *
-   * @param s
-   * @return
-   */
-  static String toCSVString(String s) {
-    StringBuilder sb = new StringBuilder(s.length()+1);
-    sb.append('\'');
-    int len = s.length();
-    for (int i = 0; i < len; i++) {
-      char c = s.charAt(i);
-      switch(c) {
-      case '\0':
-        sb.append("%00");
-        break;
-      case '\n':
-        sb.append("%0A");
-        break;
-      case '\r':
-        sb.append("%0D");
-        break;
-      case ',':
-        sb.append("%2C");
-        break;
-      case '}':
-        sb.append("%7D");
-        break;
-      case '%':
-        sb.append("%25");
-        break;
-      default:
-        sb.append(c);
-      }
-    }
-    return sb.toString();
-  }
-  
-  /**
-   *
-   * @param s
-   * @throws java.io.IOException
-   * @return
-   */
-  static String fromCSVString(String s) throws IOException {
-    if (s.charAt(0) != '\'') {
-      throw new IOException("Error deserializing string.");
-    }
-    int len = s.length();
-    StringBuilder sb = new StringBuilder(len-1);
-    for (int i = 1; i < len; i++) {
-      char c = s.charAt(i);
-      if (c == '%') {
-        char ch1 = s.charAt(i+1);
-        char ch2 = s.charAt(i+2);
-        i += 2;
-        if (ch1 == '0' && ch2 == '0') {
-          sb.append('\0');
-        } else if (ch1 == '0' && ch2 == 'A') {
-          sb.append('\n');
-        } else if (ch1 == '0' && ch2 == 'D') {
-          sb.append('\r');
-        } else if (ch1 == '2' && ch2 == 'C') {
-          sb.append(',');
-        } else if (ch1 == '7' && ch2 == 'D') {
-          sb.append('}');
-        } else if (ch1 == '2' && ch2 == '5') {
-          sb.append('%');
-        } else {
-          throw new IOException("Error deserializing string.");
-        }
-      } else {
-        sb.append(c);
-      }
-    }
-    return sb.toString();
-  }
-  
-  /**
-   *
-   * @param s
-   * @return
-   */
-  static String toXMLBuffer(Buffer s) {
-    return s.toString();
-  }
-  
-  /**
-   *
-   * @param s
-   * @throws java.io.IOException
-   * @return
-   */
-  static Buffer fromXMLBuffer(String s)
-    throws IOException {
-    if (s.length() == 0) { return new Buffer(); }
-    int blen = s.length()/2;
-    byte[] barr = new byte[blen];
-    for (int idx = 0; idx < blen; idx++) {
-      char c1 = s.charAt(2*idx);
-      char c2 = s.charAt(2*idx+1);
-      barr[idx] = (byte)Integer.parseInt(""+c1+c2, 16);
-    }
-    return new Buffer(barr);
-  }
-  
-  /**
-   *
-   * @param buf
-   * @return
-   */
-  static String toCSVBuffer(Buffer buf) {
-    StringBuilder sb = new StringBuilder("#");
-    sb.append(buf.toString());
-    return sb.toString();
-  }
-  
-  /**
-   * Converts a CSV-serialized representation of buffer to a new
-   * Buffer
-   * @param s CSV-serialized representation of buffer
-   * @throws java.io.IOException
-   * @return Deserialized Buffer
-   */
-  static Buffer fromCSVBuffer(String s)
-    throws IOException {
-    if (s.charAt(0) != '#') {
-      throw new IOException("Error deserializing buffer.");
-    }
-    if (s.length() == 1) { return new Buffer(); }
-    int blen = (s.length()-1)/2;
-    byte[] barr = new byte[blen];
-    for (int idx = 0; idx < blen; idx++) {
-      char c1 = s.charAt(2*idx+1);
-      char c2 = s.charAt(2*idx+2);
-      barr[idx] = (byte)Integer.parseInt(""+c1+c2, 16);
-    }
-    return new Buffer(barr);
-  }
-  
-  private static int utf8LenForCodePoint(final int cpt) throws IOException {
-    if (cpt >=0 && cpt <= 0x7F) {
-      return 1;
-    }
-    if (cpt >= 0x80 && cpt <= 0x07FF) {
-      return 2;
-    }
-    if ((cpt >= 0x0800 && cpt < 0xD800) ||
-        (cpt > 0xDFFF && cpt <= 0xFFFD)) {
-      return 3;
-    }
-    if (cpt >= 0x10000 && cpt <= 0x10FFFF) {
-      return 4;
-    }
-    throw new IOException("Illegal Unicode Codepoint "+
-                          Integer.toHexString(cpt)+" in string.");
-  }
-  
-  private static final int B10 =    Integer.parseInt("10000000", 2);
-  private static final int B110 =   Integer.parseInt("11000000", 2);
-  private static final int B1110 =  Integer.parseInt("11100000", 2);
-  private static final int B11110 = Integer.parseInt("11110000", 2);
-  private static final int B11 =    Integer.parseInt("11000000", 2);
-  private static final int B111 =   Integer.parseInt("11100000", 2);
-  private static final int B1111 =  Integer.parseInt("11110000", 2);
-  private static final int B11111 = Integer.parseInt("11111000", 2);
-  
-  private static int writeUtf8(int cpt, final byte[] bytes, final int offset)
-    throws IOException {
-    if (cpt >=0 && cpt <= 0x7F) {
-      bytes[offset] = (byte) cpt;
-      return 1;
-    }
-    if (cpt >= 0x80 && cpt <= 0x07FF) {
-      bytes[offset+1] = (byte) (B10 | (cpt & 0x3F));
-      cpt = cpt >> 6;
-      bytes[offset] = (byte) (B110 | (cpt & 0x1F));
-      return 2;
-    }
-    if ((cpt >= 0x0800 && cpt < 0xD800) ||
-        (cpt > 0xDFFF && cpt <= 0xFFFD)) {
-      bytes[offset+2] = (byte) (B10 | (cpt & 0x3F));
-      cpt = cpt >> 6;
-      bytes[offset+1] = (byte) (B10 | (cpt & 0x3F));
-      cpt = cpt >> 6;
-      bytes[offset] = (byte) (B1110 | (cpt & 0x0F));
-      return 3;
-    }
-    if (cpt >= 0x10000 && cpt <= 0x10FFFF) {
-      bytes[offset+3] = (byte) (B10 | (cpt & 0x3F));
-      cpt = cpt >> 6;
-      bytes[offset+2] = (byte) (B10 | (cpt & 0x3F));
-      cpt = cpt >> 6;
-      bytes[offset+1] = (byte) (B10 | (cpt & 0x3F));
-      cpt = cpt >> 6;
-      bytes[offset] = (byte) (B11110 | (cpt & 0x07));
-      return 4;
-    }
-    throw new IOException("Illegal Unicode Codepoint "+
-                          Integer.toHexString(cpt)+" in string.");
-  }
-  
-  static void toBinaryString(final DataOutput out, final String str)
-    throws IOException {
-    final int strlen = str.length();
-    byte[] bytes = new byte[strlen*4]; // Codepoints expand to 4 bytes max
-    int utf8Len = 0;
-    int idx = 0;
-    while(idx < strlen) {
-      final int cpt = str.codePointAt(idx);
-      idx += Character.isSupplementaryCodePoint(cpt) ? 2 : 1;
-      utf8Len += writeUtf8(cpt, bytes, utf8Len);
-    }
-    writeVInt(out, utf8Len);
-    out.write(bytes, 0, utf8Len);
-  }
-  
-  static boolean isValidCodePoint(int cpt) {
-    return !((cpt > 0x10FFFF) ||
-             (cpt >= 0xD800 && cpt <= 0xDFFF) ||
-             (cpt >= 0xFFFE && cpt <=0xFFFF));
-  }
-  
-  private static int utf8ToCodePoint(int b1, int b2, int b3, int b4) {
-    int cpt = 0;
-    cpt = (((b1 & ~B11111) << 18) |
-           ((b2 & ~B11) << 12) |
-           ((b3 & ~B11) << 6) |
-           (b4 & ~B11));
-    return cpt;
-  }
-  
-  private static int utf8ToCodePoint(int b1, int b2, int b3) {
-    int cpt = 0;
-    cpt = (((b1 & ~B1111) << 12) | ((b2 & ~B11) << 6) | (b3 & ~B11));
-    return cpt;
-  }
-  
-  private static int utf8ToCodePoint(int b1, int b2) {
-    int cpt = 0;
-    cpt = (((b1 & ~B111) << 6) | (b2 & ~B11));
-    return cpt;
-  }
-  
-  private static void checkB10(int b) throws IOException {
-    if ((b & B11) != B10) {
-      throw new IOException("Invalid UTF-8 representation.");
-    }
-  }
-  
-  static String fromBinaryString(final DataInput din) throws IOException {
-    final int utf8Len = readVInt(din);
-    final byte[] bytes = new byte[utf8Len];
-    din.readFully(bytes);
-    int len = 0;
-    // For the most commmon case, i.e. ascii, numChars = utf8Len
-    StringBuilder sb = new StringBuilder(utf8Len);
-    while(len < utf8Len) {
-      int cpt = 0;
-      final int b1 = bytes[len++] & 0xFF;
-      if (b1 <= 0x7F) {
-        cpt = b1;
-      } else if ((b1 & B11111) == B11110) {
-        int b2 = bytes[len++] & 0xFF;
-        checkB10(b2);
-        int b3 = bytes[len++] & 0xFF;
-        checkB10(b3);
-        int b4 = bytes[len++] & 0xFF;
-        checkB10(b4);
-        cpt = utf8ToCodePoint(b1, b2, b3, b4);
-      } else if ((b1 & B1111) == B1110) {
-        int b2 = bytes[len++] & 0xFF;
-        checkB10(b2);
-        int b3 = bytes[len++] & 0xFF;
-        checkB10(b3);
-        cpt = utf8ToCodePoint(b1, b2, b3);
-      } else if ((b1 & B111) == B110) {
-        int b2 = bytes[len++] & 0xFF;
-        checkB10(b2);
-        cpt = utf8ToCodePoint(b1, b2);
-      } else {
-        throw new IOException("Invalid UTF-8 byte "+Integer.toHexString(b1)+
-                              " at offset "+(len-1)+" in length of "+utf8Len);
-      }
-      if (!isValidCodePoint(cpt)) {
-        throw new IOException("Illegal Unicode Codepoint "+
-                              Integer.toHexString(cpt)+" in stream.");
-      }
-      sb.appendCodePoint(cpt);
-    }
-    return sb.toString();
-  }
-  
-  /** Parse a float from a byte array. */
-  public static float readFloat(byte[] bytes, int start) {
-    return WritableComparator.readFloat(bytes, start);
-  }
-  
-  /** Parse a double from a byte array. */
-  public static double readDouble(byte[] bytes, int start) {
-    return WritableComparator.readDouble(bytes, start);
-  }
-  
-  /**
-   * Reads a zero-compressed encoded long from a byte array and returns it.
-   * @param bytes byte array with decode long
-   * @param start starting index
-   * @throws java.io.IOException
-   * @return deserialized long
-   */
-  public static long readVLong(byte[] bytes, int start) throws IOException {
-    return WritableComparator.readVLong(bytes, start);
-  }
-  
-  /**
-   * Reads a zero-compressed encoded integer from a byte array and returns it.
-   * @param bytes byte array with the encoded integer
-   * @param start start index
-   * @throws java.io.IOException
-   * @return deserialized integer
-   */
-  public static int readVInt(byte[] bytes, int start) throws IOException {
-    return WritableComparator.readVInt(bytes, start);
-  }
-  
-  /**
-   * Reads a zero-compressed encoded long from a stream and return it.
-   * @param in input stream
-   * @throws java.io.IOException
-   * @return deserialized long
-   */
-  public static long readVLong(DataInput in) throws IOException {
-    return WritableUtils.readVLong(in);
-  }
-  
-  /**
-   * Reads a zero-compressed encoded integer from a stream and returns it.
-   * @param in input stream
-   * @throws java.io.IOException
-   * @return deserialized integer
-   */
-  public static int readVInt(DataInput in) throws IOException {
-    return WritableUtils.readVInt(in);
-  }
-  
-  /**
-   * Get the encoded length if an integer is stored in a variable-length format
-   * @return the encoded length
-   */
-  public static int getVIntSize(long i) {
-    return WritableUtils.getVIntSize(i);
-  }
-  
-  /**
-   * Serializes a long to a binary stream with zero-compressed encoding.
-   * For {@literal -112 <= i <= 127}, only one byte is used with the actual
-   * value. For other values of i, the first byte value indicates whether the
-   * long is positive or negative, and the number of bytes that follow.
-   * If the first byte value v is between -113 and -120, the following long
-   * is positive, with number of bytes that follow are -(v+112).
-   * If the first byte value v is between -121 and -128, the following long
-   * is negative, with number of bytes that follow are -(v+120). Bytes are
-   * stored in the high-non-zero-byte-first order.
-   *
-   * @param stream Binary output stream
-   * @param i Long to be serialized
-   * @throws java.io.IOException
-   */
-  public static void writeVLong(DataOutput stream, long i) throws IOException {
-    WritableUtils.writeVLong(stream, i);
-  }
-  
-  /**
-   * Serializes an int to a binary stream with zero-compressed encoding.
-   *
-   * @param stream Binary output stream
-   * @param i int to be serialized
-   * @throws java.io.IOException
-   */
-  public static void writeVInt(DataOutput stream, int i) throws IOException {
-    WritableUtils.writeVInt(stream, i);
-  }
-  
-  /** Lexicographic order of binary data. */
-  public static int compareBytes(byte[] b1, int s1, int l1,
-                                 byte[] b2, int s2, int l2) {
-    return WritableComparator.compareBytes(b1, s1, l1, b2, s2, l2);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/package.html b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/package.html
deleted file mode 100644
index 2c1e94564e6..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/record/package.html
+++ /dev/null
@@ -1,806 +0,0 @@
-<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
-<html>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-  <head>
-    <title>Hadoop Record I/O</title>
-  </head>
-  <body>
-    <p>
-    (DEPRECATED) Hadoop record I/O contains classes and a record description language
-    translator for simplifying serialization and deserialization of records in a
-    language-neutral manner.
-    </p>
-    
-    <p>
-    DEPRECATED: Replaced by <a href="http://hadoop.apache.org/avro/">Avro</a>.
-    </p>
-  
-  <h2>Introduction</h2>
-  
-  Software systems of any significant complexity require mechanisms for data 
-interchange with the outside world. These interchanges typically involve the
-marshaling and unmarshaling of logical units of data to and from data streams
-(files, network connections, memory buffers etc.). Applications usually have
-some code for serializing and deserializing the data types that they manipulate
-embedded in them. The work of serialization has several features that make
-automatic code generation for it worthwhile. Given a particular output encoding
-(binary, XML, etc.), serialization of primitive types and simple compositions
-of primitives (structs, vectors etc.) is a very mechanical task. Manually
-written serialization code can be susceptible to bugs especially when records
-have a large number of fields or a record definition changes between software
-versions. Lastly, it can be very useful for applications written in different
-programming languages to be able to share and interchange data. This can be 
-made a lot easier by describing the data records manipulated by these
-applications in a language agnostic manner and using the descriptions to derive
-implementations of serialization in multiple target languages. 
-
-This document describes Hadoop Record I/O, a mechanism that is aimed 
-at
-<ul> 
-<li> enabling the specification of simple serializable data types (records) 
-<li> enabling the generation of code in multiple target languages for
-marshaling and unmarshaling such types
-<li> providing target language specific support that will enable application 
-programmers to incorporate generated code into their applications
-</ul>
-
-The goals of Hadoop Record I/O are similar to those of mechanisms such as XDR,
-ASN.1, PADS and ICE. While these systems all include a DDL that enables
-the specification of most record types, they differ widely in what else they
-focus on. The focus in Hadoop Record I/O is on data marshaling and
-multi-lingual support.  We take a translator-based approach to serialization.
-Hadoop users have to describe their data in a simple data description
-language. The Hadoop DDL translator rcc generates code that users
-can invoke in order to read/write their data from/to simple stream 
-abstractions. Next we list explicitly some of the goals and non-goals of
-Hadoop Record I/O.
-
-
-<h3>Goals</h3>
-
-<ul>
-<li> Support for commonly used primitive types. Hadoop should include as
-primitives commonly used builtin types from programming languages we intend to
-support.
-
-<li> Support for common data compositions (including recursive compositions).
-Hadoop should support widely used composite types such as structs and
-vectors.
-
-<li> Code generation in multiple target languages. Hadoop should be capable of
-generating serialization code in multiple target languages and should be
-easily extensible to new target languages. The initial target languages are
-C++ and Java.
-
-<li> Support for generated target languages. Hadooop should include support
-in the form of headers, libraries, packages for supported target languages 
-that enable easy inclusion and use of generated code in applications.
-
-<li> Support for multiple output encodings. Candidates include
-packed binary, comma-separated text, XML etc.
-
-<li> Support for specifying record types in a backwards/forwards compatible
-manner. This will probably be in the form of support for optional fields in
-records. This version of the document does not include a description of the
-planned mechanism, we intend to include it in the next iteration.
-
-</ul>
-
-<h3>Non-Goals</h3>
-
-<ul>
-  <li> Serializing existing arbitrary C++ classes.
-  <li> Serializing complex data structures such as trees, linked lists etc.
-  <li> Built-in indexing schemes, compression, or check-sums.
-  <li> Dynamic construction of objects from an XML schema.
-</ul>
-
-The remainder of this document describes the features of Hadoop record I/O
-in more detail. Section 2 describes the data types supported by the system.
-Section 3 lays out the DDL syntax with some examples of simple records. 
-Section 4 describes the process of code generation with rcc. Section 5
-describes target language mappings and support for Hadoop types. We include a
-fairly complete description of C++ mappings with intent to include Java and
-others in upcoming iterations of this document. The last section talks about
-supported output encodings.
-
-
-<h2>Data Types and Streams</h2>
-
-This section describes the primitive and composite types supported by Hadoop.
-We aim to support a set of types that can be used to simply and efficiently
-express a wide range of record types in different programming languages.
-
-<h3>Primitive Types</h3>
-
-For the most part, the primitive types of Hadoop map directly to primitive
-types in high level programming languages. Special cases are the
-ustring (a Unicode string) and buffer types, which we believe
-find wide use and which are usually implemented in library code and not
-available as language built-ins. Hadoop also supplies these via library code
-when a target language built-in is not present and there is no widely
-adopted "standard" implementation. The complete list of primitive types is:
-
-<ul>
-  <li> byte: An 8-bit unsigned integer.
-  <li> boolean: A boolean value.
-  <li> int: A 32-bit signed integer.
-  <li> long: A 64-bit signed integer.
-  <li> float: A single precision floating point number as described by
-    IEEE-754.
-  <li> double: A double precision floating point number as described by
-    IEEE-754.
-  <li> ustring: A string consisting of Unicode characters.
-  <li> buffer: An arbitrary sequence of bytes. 
-</ul>
-
-
-<h3>Composite Types</h3>
-Hadoop supports a small set of composite types that enable the description
-of simple aggregate types and containers. A composite type is serialized
-by sequentially serializing it constituent elements. The supported
-composite types are:
-
-<ul>
-
-  <li> record: An aggregate type like a C-struct. This is a list of
-typed fields that are together considered a single unit of data. A record
-is serialized by sequentially serializing its constituent fields. In addition
-to serialization a record has comparison operations (equality and less-than)
-implemented for it, these are defined as memberwise comparisons.
-
-  <li>vector: A sequence of entries of the same data type, primitive
-or composite.
-
-  <li> map: An associative container mapping instances of a key type to
-instances of a value type. The key and value types may themselves be primitive
-or composite types. 
-
-</ul>
-
-<h3>Streams</h3>
-
-Hadoop generates code for serializing and deserializing record types to
-abstract streams. For each target language Hadoop defines very simple input
-and output stream interfaces. Application writers can usually develop
-concrete implementations of these by putting a one method wrapper around
-an existing stream implementation.
-
-
-<h2>DDL Syntax and Examples</h2>
-
-We now describe the syntax of the Hadoop data description language. This is
-followed by a few examples of DDL usage.
- 
-<h3>Hadoop DDL Syntax</h3>
-
-<pre><code>
-recfile = *include module *record
-include = "include" path
-path = (relative-path / absolute-path)
-module = "module" module-name
-module-name = name *("." name)
-record := "class" name "{" 1*(field) "}"
-field := type name ";"
-name :=  ALPHA (ALPHA / DIGIT / "_" )*
-type := (ptype / ctype)
-ptype := ("byte" / "boolean" / "int" |
-          "long" / "float" / "double"
-          "ustring" / "buffer")
-ctype := (("vector" "&lt;" type "&gt;") /
-          ("map" "&lt;" type "," type "&gt;" ) ) / name)
-</code></pre>
-
-A DDL file describes one or more record types. It begins with zero or
-more include declarations, a single mandatory module declaration
-followed by zero or more class declarations. The semantics of each of
-these declarations are described below:
-
-<ul>
-
-<li>include: An include declaration specifies a DDL file to be
-referenced when generating code for types in the current DDL file. Record types
-in the current compilation unit may refer to types in all included files.
-File inclusion is recursive. An include does not trigger code
-generation for the referenced file.
-
-<li> module: Every Hadoop DDL file must have a single module
-declaration that follows the list of includes and precedes all record
-declarations. A module declaration identifies a scope within which
-the names of all types in the current file are visible. Module names are
-mapped to C++ namespaces, Java packages etc. in generated code.
-
-<li> class: Records types are specified through class
-declarations. A class declaration is like a Java class declaration.
-It specifies a named record type and a list of fields that constitute records
-of the type. Usage is illustrated in the following examples.
-
-</ul>
-
-<h3>Examples</h3>
-
-<ul>
-<li>A simple DDL file links.jr with just one record declaration. 
-<pre><code>
-module links {
-    class Link {
-        ustring URL;
-        boolean isRelative;
-        ustring anchorText;
-    };
-}
-</code></pre>
-
-<li> A DDL file outlinks.jr which includes another
-<pre><code>
-include "links.jr"
-
-module outlinks {
-    class OutLinks {
-        ustring baseURL;
-        vector&lt;links.Link&gt; outLinks;
-    };
-}
-</code></pre>
-</ul>
-
-<h2>Code Generation</h2>
-
-The Hadoop translator is written in Java. Invocation is done by executing a 
-wrapper shell script named named rcc. It takes a list of
-record description files as a mandatory argument and an
-optional language argument (the default is Java) --language or
--l. Thus a typical invocation would look like:
-<pre><code>
-$ rcc -l C++ &lt;filename&gt; ...
-</code></pre>
-
-
-<h2>Target Language Mappings and Support</h2>
-
-For all target languages, the unit of code generation is a record type. 
-For each record type, Hadoop generates code for serialization and
-deserialization, record comparison and access to record members.
-
-<h3>C++</h3>
-
-Support for including Hadoop generated C++ code in applications comes in the
-form of a header file recordio.hh which needs to be included in source
-that uses Hadoop types and a library librecordio.a which applications need
-to be linked with. The header declares the Hadoop C++ namespace which defines
-appropriate types for the various primitives, the basic interfaces for
-records and streams and enumerates the supported serialization encodings.
-Declarations of these interfaces and a description of their semantics follow:
-
-<pre><code>
-namespace hadoop {
-
-  enum RecFormat { kBinary, kXML, kCSV };
-
-  class InStream {
-  public:
-    virtual ssize_t read(void *buf, size_t n) = 0;
-  };
-
-  class OutStream {
-  public:
-    virtual ssize_t write(const void *buf, size_t n) = 0;
-  };
-
-  class IOError : public runtime_error {
-  public:
-    explicit IOError(const std::string&amp; msg);
-  };
-
-  class IArchive;
-  class OArchive;
-
-  class RecordReader {
-  public:
-    RecordReader(InStream&amp; in, RecFormat fmt);
-    virtual ~RecordReader(void);
-
-    virtual void read(Record&amp; rec);
-  };
-
-  class RecordWriter {
-  public:
-    RecordWriter(OutStream&amp; out, RecFormat fmt);
-    virtual ~RecordWriter(void);
-
-    virtual void write(Record&amp; rec);
-  };
-
-
-  class Record {
-  public:
-    virtual std::string type(void) const = 0;
-    virtual std::string signature(void) const = 0;
-  protected:
-    virtual bool validate(void) const = 0;
-
-    virtual void
-    serialize(OArchive&amp; oa, const std::string&amp; tag) const = 0;
-
-    virtual void
-    deserialize(IArchive&amp; ia, const std::string&amp; tag) = 0;
-  };
-}
-</code></pre>
-
-<ul>
-
-<li> RecFormat: An enumeration of the serialization encodings supported
-by this implementation of Hadoop.
-
-<li> InStream: A simple abstraction for an input stream. This has a 
-single public read method that reads n bytes from the stream into
-the buffer buf. Has the same semantics as a blocking read system
-call. Returns the number of bytes read or -1 if an error occurs.
-
-<li> OutStream: A simple abstraction for an output stream. This has a 
-single write method that writes n bytes to the stream from the
-buffer buf. Has the same semantics as a blocking write system
-call. Returns the number of bytes written or -1 if an error occurs.
-
-<li> RecordReader: A RecordReader reads records one at a time from
-an underlying stream in a specified record format. The reader is instantiated
-with a stream and a serialization format. It has a read method that
-takes an instance of a record and deserializes the record from the stream.
-
-<li> RecordWriter: A RecordWriter writes records one at a
-time to an underlying stream in a specified record format. The writer is
-instantiated with a stream and a serialization format. It has a
-write method that takes an instance of a record and serializes the
-record to the stream.
-
-<li> Record: The base class for all generated record types. This has two
-public methods type and signature that return the typename and the
-type signature of the record.
-
-</ul>
-
-Two files are generated for each record file (note: not for each record). If a
-record file is named "name.jr", the generated files are 
-"name.jr.cc" and "name.jr.hh" containing serialization 
-implementations and record type declarations respectively.
-
-For each record in the DDL file, the generated header file will contain a
-class definition corresponding to the record type, method definitions for the
-generated type will be present in the '.cc' file.  The generated class will
-inherit from the abstract class hadoop::Record. The DDL files
-module declaration determines the namespace the record belongs to.
-Each '.' delimited token in the module declaration results in the
-creation of a namespace. For instance, the declaration module docs.links
-results in the creation of a docs namespace and a nested 
-docs::links namespace. In the preceding examples, the Link class
-is placed in the links namespace. The header file corresponding to
-the links.jr file will contain:
-
-<pre><code>
-namespace links {
-  class Link : public hadoop::Record {
-    // ....
-  };
-};
-</code></pre>
-
-Each field within the record will cause the generation of a private member
-declaration of the appropriate type in the class declaration, and one or more
-acccessor methods. The generated class will implement the serialize and
-deserialize methods defined in hadoop::Record+. It will also 
-implement the inspection methods type and signature from
-hadoop::Record. A default constructor and virtual destructor will also
-be generated. Serialization code will read/write records into streams that
-implement the hadoop::InStream and the hadoop::OutStream interfaces.
-
-For each member of a record an accessor method is generated that returns 
-either the member or a reference to the member. For members that are returned 
-by value, a setter method is also generated. This is true for primitive 
-data members of the types byte, int, long, boolean, float and 
-double. For example, for a int field called MyField the folowing
-code is generated.
-
-<pre><code>
-...
-private:
-  int32_t mMyField;
-  ...
-public:
-  int32_t getMyField(void) const {
-    return mMyField;
-  };
-
-  void setMyField(int32_t m) {
-    mMyField = m;
-  };
-  ...
-</code></pre>
-
-For a ustring or buffer or composite field. The generated code
-only contains accessors that return a reference to the field. A const
-and a non-const accessor are generated. For example:
-
-<pre><code>
-...
-private:
-  std::string mMyBuf;
-  ...
-public:
-
-  std::string&amp; getMyBuf() {
-    return mMyBuf;
-  };
-
-  const std::string&amp; getMyBuf() const {
-    return mMyBuf;
-  };
-  ...
-</code></pre>
-
-<h4>Examples</h4>
-
-Suppose the inclrec.jr file contains:
-<pre><code>
-module inclrec {
-    class RI {
-        int      I32;
-        double   D;
-        ustring  S;
-    };
-}
-</code></pre>
-
-and the testrec.jr file contains:
-
-<pre><code>
-include "inclrec.jr"
-module testrec {
-    class R {
-        vector&lt;float&gt; VF;
-        RI            Rec;
-        buffer        Buf;
-    };
-}
-</code></pre>
-
-Then the invocation of rcc such as:
-<pre><code>
-$ rcc -l c++ inclrec.jr testrec.jr
-</code></pre>
-will result in generation of four files:
-inclrec.jr.{cc,hh} and testrec.jr.{cc,hh}.
-
-The inclrec.jr.hh will contain:
-
-<pre><code>
-#ifndef _INCLREC_JR_HH_
-#define _INCLREC_JR_HH_
-
-#include "recordio.hh"
-
-namespace inclrec {
-  
-  class RI : public hadoop::Record {
-
-  private:
-
-    int32_t      I32;
-    double       D;
-    std::string  S;
-
-  public:
-
-    RI(void);
-    virtual ~RI(void);
-
-    virtual bool operator==(const RI&amp; peer) const;
-    virtual bool operator&lt;(const RI&amp; peer) const;
-
-    virtual int32_t getI32(void) const { return I32; }
-    virtual void setI32(int32_t v) { I32 = v; }
-
-    virtual double getD(void) const { return D; }
-    virtual void setD(double v) { D = v; }
-
-    virtual std::string&amp; getS(void) const { return S; }
-    virtual const std::string&amp; getS(void) const { return S; }
-
-    virtual std::string type(void) const;
-    virtual std::string signature(void) const;
-
-  protected:
-
-    virtual void serialize(hadoop::OArchive&amp; a) const;
-    virtual void deserialize(hadoop::IArchive&amp; a);
-  };
-} // end namespace inclrec
-
-#endif /* _INCLREC_JR_HH_ */
-
-</code></pre>
-
-The testrec.jr.hh file will contain:
-
-
-<pre><code>
-
-#ifndef _TESTREC_JR_HH_
-#define _TESTREC_JR_HH_
-
-#include "inclrec.jr.hh"
-
-namespace testrec {
-  class R : public hadoop::Record {
-
-  private:
-
-    std::vector&lt;float&gt; VF;
-    inclrec::RI        Rec;
-    std::string        Buf;
-
-  public:
-
-    R(void);
-    virtual ~R(void);
-
-    virtual bool operator==(const R&amp; peer) const;
-    virtual bool operator&lt;(const R&amp; peer) const;
-
-    virtual std::vector&lt;float&gt;&amp; getVF(void) const;
-    virtual const std::vector&lt;float&gt;&amp; getVF(void) const;
-
-    virtual std::string&amp; getBuf(void) const ;
-    virtual const std::string&amp; getBuf(void) const;
-
-    virtual inclrec::RI&amp; getRec(void) const;
-    virtual const inclrec::RI&amp; getRec(void) const;
-    
-    virtual bool serialize(hadoop::OutArchive&amp; a) const;
-    virtual bool deserialize(hadoop::InArchive&amp; a);
-    
-    virtual std::string type(void) const;
-    virtual std::string signature(void) const;
-  };
-}; // end namespace testrec
-#endif /* _TESTREC_JR_HH_ */
-
-</code></pre>
-
-<h3>Java</h3>
-
-Code generation for Java is similar to that for C++. A Java class is generated
-for each record type with private members corresponding to the fields. Getters
-and setters for fields are also generated. Some differences arise in the
-way comparison is expressed and in the mapping of modules to packages and
-classes to files. For equality testing, an equals method is generated
-for each record type. As per Java requirements a hashCode method is also
-generated. For comparison a compareTo method is generated for each
-record type. This has the semantics as defined by the Java Comparable
-interface, that is, the method returns a negative integer, zero, or a positive
-integer as the invoked object is less than, equal to, or greater than the
-comparison parameter.
-
-A .java file is generated per record type as opposed to per DDL
-file as in C++. The module declaration translates to a Java
-package declaration. The module name maps to an identical Java package
-name. In addition to this mapping, the DDL compiler creates the appropriate
-directory hierarchy for the package and places the generated .java
-files in the correct directories.
-
-<h2>Mapping Summary</h2>
-
-<pre><code>
-DDL Type        C++ Type            Java Type 
-
-boolean         bool                boolean
-byte            int8_t              byte
-int             int32_t             int
-long            int64_t             long
-float           float               float
-double          double              double
-ustring         std::string         java.lang.String
-buffer          std::string         org.apache.hadoop.record.Buffer
-class type      class type          class type
-vector&lt;type&gt;    std::vector&lt;type&gt;   java.util.ArrayList&lt;type&gt;
-map&lt;type,type&gt;  std::map&lt;type,type&gt; java.util.TreeMap&lt;type,type&gt;
-</code></pre>
-
-<h2>Data encodings</h2>
-
-This section describes the format of the data encodings supported by Hadoop.
-Currently, three data encodings are supported, namely binary, CSV and XML.
-
-<h3>Binary Serialization Format</h3>
-
-The binary data encoding format is fairly dense. Serialization of composite
-types is simply defined as a concatenation of serializations of the constituent
-elements (lengths are included in vectors and maps).
-
-Composite types are serialized as follows:
-<ul>
-<li> class: Sequence of serialized members.
-<li> vector: The number of elements serialized as an int. Followed by a
-sequence of serialized elements.
-<li> map: The number of key value pairs serialized as an int. Followed
-by a sequence of serialized (key,value) pairs.
-</ul>
-
-Serialization of primitives is more interesting, with a zero compression
-optimization for integral types and normalization to UTF-8 for strings. 
-Primitive types are serialized as follows:
-
-<ul>
-<li> byte: Represented by 1 byte, as is.
-<li> boolean: Represented by 1-byte (0 or 1)
-<li> int/long: Integers and longs are serialized zero compressed.
-Represented as 1-byte if -120 &lt;= value &lt; 128. Otherwise, serialized as a
-sequence of 2-5 bytes for ints, 2-9 bytes for longs. The first byte represents
-the number of trailing bytes, N, as the negative number (-120-N). For example,
-the number 1024 (0x400) is represented by the byte sequence 'x86 x04 x00'.
-This doesn't help much for 4-byte integers but does a reasonably good job with
-longs without bit twiddling.
-<li> float/double: Serialized in IEEE 754 single and double precision
-format in network byte order. This is the format used by Java.
-<li> ustring: Serialized as 4-byte zero compressed length followed by
-data encoded as UTF-8. Strings are normalized to UTF-8 regardless of native
-language representation.
-<li> buffer: Serialized as a 4-byte zero compressed length followed by the
-raw bytes in the buffer.
-</ul>
-
-
-<h3>CSV Serialization Format</h3>
-
-The CSV serialization format has a lot more structure than the "standard"
-Excel CSV format, but we believe the additional structure is useful because
-
-<ul>
-<li> it makes parsing a lot easier without detracting too much from legibility
-<li> the delimiters around composites make it obvious when one is reading a
-sequence of Hadoop records
-</ul>
-
-Serialization formats for the various types are detailed in the grammar that
-follows. The notable feature of the formats is the use of delimiters for 
-indicating the certain field types.
-
-<ul>
-<li> A string field begins with a single quote (').
-<li> A buffer field begins with a sharp (#).
-<li> A class, vector or map begins with 's{', 'v{' or 'm{' respectively and
-ends with '}'.
-</ul>
-
-The CSV format can be described by the following grammar:
-
-<pre><code>
-record = primitive / struct / vector / map
-primitive = boolean / int / long / float / double / ustring / buffer
-
-boolean = "T" / "F"
-int = ["-"] 1*DIGIT
-long = ";" ["-"] 1*DIGIT
-float = ["-"] 1*DIGIT "." 1*DIGIT ["E" / "e" ["-"] 1*DIGIT]
-double = ";" ["-"] 1*DIGIT "." 1*DIGIT ["E" / "e" ["-"] 1*DIGIT]
-
-ustring = "'" *(UTF8 char except NULL, LF, % and , / "%00" / "%0a" / "%25" / "%2c" )
-
-buffer = "#" *(BYTE except NULL, LF, % and , / "%00" / "%0a" / "%25" / "%2c" )
-
-struct = "s{" record *("," record) "}"
-vector = "v{" [record *("," record)] "}"
-map = "m{" [*(record "," record)] "}"
-</code></pre>
-
-<h3>XML Serialization Format</h3>
-
-The XML serialization format is the same used by Apache XML-RPC
-(http://ws.apache.org/xmlrpc/types.html). This is an extension of the original
-XML-RPC format and adds some additional data types. All record I/O types are
-not directly expressible in this format, and access to a DDL is required in
-order to convert these to valid types. All types primitive or composite are
-represented by &lt;value&gt; elements. The particular XML-RPC type is
-indicated by a nested element in the &lt;value&gt; element. The encoding for
-records is always UTF-8. Primitive types are serialized as follows:
-
-<ul>
-<li> byte: XML tag &lt;ex:i1&gt;. Values: 1-byte unsigned 
-integers represented in US-ASCII
-<li> boolean: XML tag &lt;boolean&gt;. Values: "0" or "1"
-<li> int: XML tags &lt;i4&gt; or &lt;int&gt;. Values: 4-byte
-signed integers represented in US-ASCII.
-<li> long: XML tag &lt;ex:i8&gt;. Values: 8-byte signed integers
-represented in US-ASCII.
-<li> float: XML tag &lt;ex:float&gt;. Values: Single precision
-floating point numbers represented in US-ASCII.
-<li> double: XML tag &lt;double&gt;. Values: Double precision
-floating point numbers represented in US-ASCII.
-<li> ustring: XML tag &lt;;string&gt;. Values: String values
-represented as UTF-8. XML does not permit all Unicode characters in literal
-data. In particular, NULLs and control chars are not allowed. Additionally,
-XML processors are required to replace carriage returns with line feeds and to
-replace CRLF sequences with line feeds. Programming languages that we work
-with do not impose these restrictions on string types. To work around these
-restrictions, disallowed characters and CRs are percent escaped in strings.
-The '%' character is also percent escaped.
-<li> buffer: XML tag &lt;string&gt;. Values: Arbitrary binary
-data. Represented as hexBinary, each byte is replaced by its 2-byte
-hexadecimal representation.
-</ul>
-
-Composite types are serialized as follows:
-
-<ul>
-<li> class: XML tag &lt;struct&gt;. A struct is a sequence of
-&lt;member&gt; elements. Each &lt;member&gt; element has a &lt;name&gt;
-element and a &lt;value&gt; element. The &lt;name&gt; is a string that must
-match /[a-zA-Z][a-zA-Z0-9_]*/. The value of the member is represented
-by a &lt;value&gt; element.
-
-<li> vector: XML tag &lt;array&gt;. An &lt;array&gt; contains a
-single &lt;data&gt; element. The &lt;data&gt; element is a sequence of
-&lt;value&gt; elements each of which represents an element of the vector.
-
-<li> map: XML tag &lt;array&gt;. Same as vector.
-
-</ul>
-
-For example:
-
-<pre><code>
-class {
-  int           MY_INT;            // value 5
-  vector&lt;float&gt; MY_VEC;            // values 0.1, -0.89, 2.45e4
-  buffer        MY_BUF;            // value '\00\n\tabc%'
-}
-</code></pre>
-
-is serialized as
-
-<pre><code class="XML">
-&lt;value&gt;
-  &lt;struct&gt;
-    &lt;member&gt;
-      &lt;name&gt;MY_INT&lt;/name&gt;
-      &lt;value&gt;&lt;i4&gt;5&lt;/i4&gt;&lt;/value&gt;
-    &lt;/member&gt;
-    &lt;member&gt;
-      &lt;name&gt;MY_VEC&lt;/name&gt;
-      &lt;value&gt;
-        &lt;array&gt;
-          &lt;data&gt;
-            &lt;value&gt;&lt;ex:float&gt;0.1&lt;/ex:float&gt;&lt;/value&gt;
-            &lt;value&gt;&lt;ex:float&gt;-0.89&lt;/ex:float&gt;&lt;/value&gt;
-            &lt;value&gt;&lt;ex:float&gt;2.45e4&lt;/ex:float&gt;&lt;/value&gt;
-          &lt;/data&gt;
-        &lt;/array&gt;
-      &lt;/value&gt;
-    &lt;/member&gt;
-    &lt;member&gt;
-      &lt;name&gt;MY_BUF&lt;/name&gt;
-      &lt;value&gt;&lt;string&gt;%00\n\tabc%25&lt;/string&gt;&lt;/value&gt;
-    &lt;/member&gt;
-  &lt;/struct&gt;
-&lt;/value&gt; 
-</code></pre>
-
-  </body>
-</html>
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/AutoInputFormat.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/AutoInputFormat.java
deleted file mode 100644
index 10cf614b944..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/AutoInputFormat.java
+++ /dev/null
@@ -1,74 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.EOFException;
-import java.io.IOException;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.InputFormat;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.SequenceFileInputFormat;
-import org.apache.hadoop.mapred.TextInputFormat;
-
-/**
- * An {@link InputFormat} that tries to deduce the types of the input files
- * automatically. It can currently handle text and sequence files.
- */
-public class AutoInputFormat extends FileInputFormat {
-
-  private TextInputFormat textInputFormat = new TextInputFormat();
-
-  private SequenceFileInputFormat seqFileInputFormat = 
-    new SequenceFileInputFormat();
-
-  public void configure(JobConf job) {
-    textInputFormat.configure(job);
-    // SequenceFileInputFormat has no configure() method
-  }
-
-  public RecordReader getRecordReader(InputSplit split, JobConf job,
-    Reporter reporter) throws IOException {
-    FileSplit fileSplit = (FileSplit) split;
-    FileSystem fs = FileSystem.get(fileSplit.getPath().toUri(), job);
-    FSDataInputStream is = fs.open(fileSplit.getPath());
-    byte[] header = new byte[3];
-    RecordReader reader = null;
-    try {
-      is.readFully(header);
-    } catch (EOFException eof) {
-      reader = textInputFormat.getRecordReader(split, job, reporter);
-    } finally {
-      is.close();
-    }
-    if (header[0] == 'S' && header[1] == 'E' && header[2] == 'Q') {
-      reader = seqFileInputFormat.getRecordReader(split, job, reporter);
-    } else {
-      reader = textInputFormat.getRecordReader(split, job, reporter);
-    }
-    return reader;
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/DumpTypedBytes.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/DumpTypedBytes.java
deleted file mode 100644
index ffddc7cc111..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/DumpTypedBytes.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.typedbytes.TypedBytesOutput;
-import org.apache.hadoop.typedbytes.TypedBytesWritableOutput;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-
-/**
- * Utility program that fetches all files that match a given pattern and dumps
- * their content to stdout as typed bytes. This works for all files that can be
- * handled by {@link org.apache.hadoop.streaming.AutoInputFormat}.
- */
-public class DumpTypedBytes implements Tool {
-
-  private Configuration conf;
-
-  public DumpTypedBytes(Configuration conf) {
-    this.conf = conf;
-  }
-  
-  public DumpTypedBytes() {
-    this(new Configuration());
-  }
-
-  public Configuration getConf() {
-    return conf;
-  }
-
-  public void setConf(Configuration conf) {
-    this.conf = conf;
-  }
-
-  /**
-   * The main driver for <code>DumpTypedBytes</code>.
-   */
-  public int run(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.err.println("Too few arguments!");
-      printUsage();
-      return 1;
-    }
-    Path pattern = new Path(args[0]);
-    FileSystem fs = pattern.getFileSystem(getConf());
-    fs.setVerifyChecksum(true);
-    for (Path p : FileUtil.stat2Paths(fs.globStatus(pattern), pattern)) {
-      List<FileStatus> inputFiles = new ArrayList<FileStatus>();
-      FileStatus status = fs.getFileStatus(p);
-      if (status.isDirectory()) {
-        FileStatus[] files = fs.listStatus(p);
-        Collections.addAll(inputFiles, files);
-      } else {
-        inputFiles.add(status);
-      }
-      return dumpTypedBytes(inputFiles);
-    }
-    return -1;
-  }
-
-  private void printUsage() {
-    System.out.println("Usage: mapred streaming dumptb <glob-pattern>");
-    System.out.println("  Dumps all files that match the given pattern to " +
-        "standard output as typed bytes.");
-    System.out.println("  The files can be text or sequence files");
-  }
-
-  /**
-   * Dump given list of files to standard output as typed bytes.
-   */
-  @SuppressWarnings("unchecked")
-  private int dumpTypedBytes(List<FileStatus> files) throws IOException {
-    JobConf job = new JobConf(getConf()); 
-    DataOutputStream dout = new DataOutputStream(System.out);
-    AutoInputFormat autoInputFormat = new AutoInputFormat();
-    for (FileStatus fileStatus : files) {
-      FileSplit split = new FileSplit(fileStatus.getPath(), 0,
-        fileStatus.getLen() * fileStatus.getBlockSize(),
-        (String[]) null);
-      RecordReader recReader = null;
-      try {
-        recReader = autoInputFormat.getRecordReader(split, job, Reporter.NULL);
-        Object key = recReader.createKey();
-        Object value = recReader.createValue();
-        while (recReader.next(key, value)) {
-          if (key instanceof Writable) {
-            TypedBytesWritableOutput.get(dout).write((Writable) key);
-          } else {
-            TypedBytesOutput.get(dout).write(key);
-          }
-          if (value instanceof Writable) {
-            TypedBytesWritableOutput.get(dout).write((Writable) value);
-          } else {
-            TypedBytesOutput.get(dout).write(value);
-          }
-        }
-      } finally {
-        if (recReader != null) {
-          recReader.close();
-        }
-      }
-    }
-    dout.flush();
-    return 0;
-  }
-
-  public static void main(String[] args) throws Exception {
-    DumpTypedBytes dumptb = new DumpTypedBytes();
-    int res = ToolRunner.run(dumptb, args);
-    System.exit(res);
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/Environment.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/Environment.java
deleted file mode 100644
index 7871a4c969e..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/Environment.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.net.InetAddress;
-import java.nio.charset.StandardCharsets;
-import java.util.*;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.util.StringUtils;
-
-/**
- * This is a class used to get the current environment
- * on the host machines running the map/reduce. This class
- * assumes that setting the environment in streaming is 
- * allowed on windows/ix/linuz/freebsd/sunos/solaris/hp-ux
- */
-@InterfaceAudience.Private
-public class Environment extends Properties {
-
-  private static final long serialVersionUID = 1L;
-
-  public Environment() throws IOException {
-    // Extend this code to fit all operating
-    // environments that you expect to run in
-    // http://lopica.sourceforge.net/os.html
-    String command = null;
-    String OS = System.getProperty("os.name");
-    String lowerOs = StringUtils.toLowerCase(OS);
-    if (OS.indexOf("Windows") > -1) {
-      command = "cmd /C set";
-    } else if (lowerOs.indexOf("ix") > -1 || lowerOs.indexOf("linux") > -1
-               || lowerOs.indexOf("freebsd") > -1 || lowerOs.indexOf("sunos") > -1
-               || lowerOs.indexOf("solaris") > -1 || lowerOs.indexOf("hp-ux") > -1) {
-      command = "env";
-    } else if (lowerOs.startsWith("mac os x") || lowerOs.startsWith("darwin")) {
-      command = "env";
-    } else {
-      // Add others here
-    }
-
-    if (command == null) {
-      throw new RuntimeException("Operating system " + OS + " not supported by this class");
-    }
-
-    // Read the environment variables
-
-    Process pid = Runtime.getRuntime().exec(command);
-    BufferedReader in = new BufferedReader(
-        new InputStreamReader(pid.getInputStream(), StandardCharsets.UTF_8));
-    try {
-      while (true) {
-        String line = in.readLine();
-        if (line == null)
-          break;
-        int p = line.indexOf("=");
-        if (p != -1) {
-          String name = line.substring(0, p);
-          String value = line.substring(p + 1);
-          setProperty(name, value);
-        }
-      }
-      in.close();
-      in = null;
-    } finally {
-      IOUtils.closeStream(in);
-    }
-   
-    try {
-      pid.waitFor();
-    } catch (InterruptedException e) {
-      throw new IOException(e.getMessage());
-    }
-  }
-
-  // to be used with Runtime.exec(String[] cmdarray, String[] envp) 
-  String[] toArray() {
-    String[] arr = new String[super.size()];
-    Enumeration<Object> it = super.keys();
-    int i = -1;
-    while (it.hasMoreElements()) {
-      String key = (String) it.nextElement();
-      String val = (String) get(key);
-      i++;
-      arr[i] = key + "=" + val;
-    }
-    return arr;
-  }
-
-  public Map<String, String> toMap() {
-    Map<String, String> map = new HashMap<String, String>();
-    Enumeration<Object> it = super.keys();
-    while (it.hasMoreElements()) {
-      String key = (String) it.nextElement();
-      String val = (String) get(key);
-      map.put(key, val);
-    }
-    return map;
-  }
-  
-  public String getHost() {
-    String host = getProperty("HOST");
-    if (host == null) {
-      // HOST isn't always in the environment
-      try {
-        host = InetAddress.getLocalHost().getHostName();
-      } catch (IOException io) {
-        io.printStackTrace();
-      }
-    }
-    return host;
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/HadoopStreaming.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/HadoopStreaming.java
deleted file mode 100644
index 92f9d03866a..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/HadoopStreaming.java
+++ /dev/null
@@ -1,71 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.util.Arrays;
-
-import org.apache.hadoop.util.ToolRunner;
-
-/** The main entry point. Usually invoked with the script 
- *  bin/hadoop jar hadoop-streaming.jar args.
- */
-public class HadoopStreaming {
-
-  public static void main(String[] args) throws Exception {
-    if (args.length < 1) {
-      System.err.println("No Arguments Given!");
-      printUsage();
-      System.exit(1);
-    }
-    int returnStatus = 0;
-    String cmd = args[0];
-    String[] remainingArgs = Arrays.copyOfRange(args, 1, args.length);
-    if (cmd.equalsIgnoreCase("dumptb")) {
-      DumpTypedBytes dumptb = new DumpTypedBytes();
-      returnStatus = ToolRunner.run(dumptb, remainingArgs);
-    } else if (cmd.equalsIgnoreCase("loadtb")) {
-      LoadTypedBytes loadtb = new LoadTypedBytes();
-      returnStatus = ToolRunner.run(loadtb, remainingArgs);
-    } else if (cmd.equalsIgnoreCase("streamjob")) {
-      StreamJob job = new StreamJob();
-      returnStatus = ToolRunner.run(job, remainingArgs);
-    } else { // for backward compatibility
-      StreamJob job = new StreamJob();
-      returnStatus = ToolRunner.run(job, args);
-    }
-    if (returnStatus != 0) {
-      System.err.println("Streaming Command Failed!");
-      System.exit(returnStatus);
-    }
-  }
-  
-  private static void printUsage() {
-    System.out.println("Usage: mapred streaming [options]");
-    System.out.println("Options:");
-    System.out.println("  dumptb <glob-pattern> Dumps all files that match the" 
-        + " given pattern to ");
-    System.out.println("                        standard output as typed " +
-    		"bytes.");
-    System.out.println("  loadtb <path> Reads typed bytes from standard input" +
-        " and stores them in");
-    System.out.println("                a sequence file in the specified path");
-    System.out.println("  [streamjob] <args> Runs streaming job with given" +
-        " arguments");
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/JarBuilder.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/JarBuilder.java
deleted file mode 100644
index 1945127a9ba..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/JarBuilder.java
+++ /dev/null
@@ -1,208 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.util.*;
-import java.util.jar.*;
-import java.util.zip.ZipException;
-
-/**
- * This class is the main class for generating job.jar
- * for Hadoop Streaming jobs. It includes the files specified 
- * with the -file option and includes them in the jar. Also,
- * hadoop-streaming is a user level appplication, so all the classes
- * with hadoop-streaming that are needed in the job are also included
- * in the job.jar.
- */
-public class JarBuilder {
-
-  public JarBuilder() {
-  }
-
-  public void setVerbose(boolean v) {
-    this.verbose = v;
-  }
-
-  public void merge(List srcNames, List srcUnjar, String dstJar) throws IOException {
-    String source = null;
-    JarOutputStream jarOut = null;
-    JarFile jarSource = null;
-    jarOut = new JarOutputStream(new FileOutputStream(dstJar));
-    boolean throwing = false;
-    try {
-      if (srcNames != null) {
-        Iterator iter = srcNames.iterator();
-        while (iter.hasNext()) {
-          source = (String) iter.next();
-          File fsource = new File(source);
-          String base = getBasePathInJarOut(source);
-          if (!fsource.exists()) {
-            throwing = true;
-            throw new FileNotFoundException(fsource.getAbsolutePath());
-          }
-          if (fsource.isDirectory()) {
-            addDirectory(jarOut, base, fsource, 0);
-          } else {
-            addFileStream(jarOut, base, fsource);
-          }
-        }
-      }
-      if (srcUnjar != null) {
-        Iterator iter = srcUnjar.iterator();
-        while (iter.hasNext()) {
-          source = (String) iter.next();
-          jarSource = new JarFile(source);
-          addJarEntries(jarOut, jarSource);
-          jarSource.close();
-        }
-
-      }
-    } finally {
-      try {
-        jarOut.close();
-      } catch (ZipException z) {
-        if (!throwing) {
-          throw new IOException(z.toString());
-        }
-      }
-    }
-  }
-
-  protected String fileExtension(String file) {
-    int leafPos = file.lastIndexOf('/');
-    if (leafPos == file.length() - 1) return "";
-    String leafName = file.substring(leafPos + 1);
-    int dotPos = leafName.lastIndexOf('.');
-    if (dotPos == -1) return "";
-    String ext = leafName.substring(dotPos + 1);
-    return ext;
-  }
-
-  /** @return empty or a jar base path. Must not start with '/' */
-  protected String getBasePathInJarOut(String sourceFile) {
-    // TaskRunner will unjar and append to classpath: .:classes/:lib/*    	
-    String ext = fileExtension(sourceFile);
-    if (ext.equals("class")) {
-      return "classes/"; // or ""
-    } else if (ext.equals("jar") || ext.equals("zip")) {
-      return "lib/";
-    } else {
-      return "";
-    }
-  }
-
-  private void addJarEntries(JarOutputStream dst, JarFile src) throws IOException {
-    Enumeration entries = src.entries();
-    JarEntry entry = null;
-    while (entries.hasMoreElements()) {
-      entry = (JarEntry) entries.nextElement();
-      //if (entry.getName().startsWith("META-INF/")) continue; 
-      InputStream in = src.getInputStream(entry);
-      addNamedStream(dst, entry.getName(), in);
-    }
-  }
-
-  /** @param name path in jar for this jar element. Must not start with '/' */
-  void addNamedStream(JarOutputStream dst, String name, InputStream in) throws IOException {
-    if (verbose) {
-      System.err.println("JarBuilder.addNamedStream " + name);
-    }
-    try {
-      dst.putNextEntry(new JarEntry(name));
-      int bytesRead = 0;
-      while ((bytesRead = in.read(buffer, 0, BUFF_SIZE)) != -1) {
-        dst.write(buffer, 0, bytesRead);
-      }
-    } catch (ZipException ze) {
-      if (ze.getMessage().indexOf("duplicate entry") >= 0) {
-        if (verbose) {
-          System.err.println(ze + " Skip duplicate entry " + name);
-        }
-      } else {
-        throw ze;
-      }
-    } finally {
-      in.close();
-      dst.flush();
-      dst.closeEntry();
-    }
-  }
-
-  void addFileStream(JarOutputStream dst, String jarBaseName, File file) throws IOException {
-    FileInputStream in = new FileInputStream(file);
-    try {
-      String name = jarBaseName + file.getName();
-      addNamedStream(dst, name, in);
-    } finally {
-      in.close();
-    }
-  }
-
-  void addDirectory(JarOutputStream dst, String jarBaseName, File dir, int depth) throws IOException {
-    File[] contents = dir.listFiles();
-    if (contents != null) {
-      for (int i = 0; i < contents.length; i++) {
-        File f = contents[i];
-        String fBaseName = (depth == 0) ? "" : dir.getName();
-        if (jarBaseName.length() > 0) {
-          fBaseName = jarBaseName + "/" + fBaseName;
-        }
-        if (f.isDirectory()) {
-          addDirectory(dst, fBaseName, f, depth + 1);
-        } else {
-          addFileStream(dst, fBaseName + "/", f);
-        }
-      }
-    }
-  }
-
-  /** Test program */
-  public static void main(String args[]) {
-    // args = new String[] { "C:/Temp/merged.jar", "C:/jdk1.5.0/jre/lib/ext/dnsns.jar",  "/Temp/addtojar2.log", "C:/jdk1.5.0/jre/lib/ext/mtest.jar", "C:/Temp/base"};
-    if (args.length < 2) {
-      System.err.println("Usage: JarFiles merged.jar [src.jar | dir | file ]+");
-    } else {
-      JarBuilder jarFiles = new JarBuilder();
-      List names = new ArrayList();
-      List unjar = new ArrayList();
-      for (int i = 1; i < args.length; i++) {
-        String f = args[i];
-        String ext = jarFiles.fileExtension(f);
-        boolean expandAsJar = ext.equals("jar") || ext.equals("zip");
-        if (expandAsJar) {
-          unjar.add(f);
-        } else {
-          names.add(f);
-        }
-      }
-      try {
-        jarFiles.merge(names, unjar, args[0]);
-        Date lastMod = new Date(new File(args[0]).lastModified());
-        System.out.println("Merge done to " + args[0] + " " + lastMod);
-      } catch (Exception ge) {
-        ge.printStackTrace(System.err);
-      }
-    }
-  }
-
-  private static final int BUFF_SIZE = 32 * 1024;
-  private byte buffer[] = new byte[BUFF_SIZE];
-  protected boolean verbose = false;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/LoadTypedBytes.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/LoadTypedBytes.java
deleted file mode 100644
index 838cfa16279..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/LoadTypedBytes.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataInputStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.typedbytes.TypedBytesInput;
-import org.apache.hadoop.typedbytes.TypedBytesWritable;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-
-/**
- * Utility program that reads typed bytes from standard input and stores them in
- * a sequence file for which the path is given as an argument.
- */
-public class LoadTypedBytes implements Tool {
-
-  private Configuration conf;
-
-  public LoadTypedBytes(Configuration conf) {
-    this.conf = conf;
-  }
-  
-  public LoadTypedBytes() {
-    this(new Configuration());
-  }
-  
-  public Configuration getConf() {
-    return conf;
-  }
-
-  public void setConf(Configuration conf) {
-    this.conf = conf;
-  }
-  
-  /**
-   * The main driver for <code>LoadTypedBytes</code>.
-   */
-  public int run(String[] args) throws Exception {
-    if (args.length == 0) {
-      System.err.println("Too few arguments!");
-      printUsage();
-      return 1;
-    }
-    Path path = new Path(args[0]);
-    FileSystem fs = path.getFileSystem(getConf());
-    if (fs.exists(path)) {
-      System.err.println("given path exists already!");
-      return -1;
-    }
-    TypedBytesInput tbinput = new TypedBytesInput(new DataInputStream(System.in));
-    SequenceFile.Writer writer = SequenceFile.createWriter(fs, conf, path,
-      TypedBytesWritable.class, TypedBytesWritable.class);
-    try {
-      TypedBytesWritable key = new TypedBytesWritable();
-      TypedBytesWritable value = new TypedBytesWritable();
-      byte[] rawKey = tbinput.readRaw();
-      while (rawKey != null) {
-        byte[] rawValue = tbinput.readRaw();
-        key.set(rawKey, 0, rawKey.length);
-        value.set(rawValue, 0, rawValue.length);
-        writer.append(key, value);
-        rawKey = tbinput.readRaw();
-      }
-    } finally {
-      writer.close();
-    }
-    return 0;
-  }
-
-  private void printUsage() {
-    System.out.println("Usage: mapred streaming loadtb <path>");
-    System.out.println("  Reads typed bytes from standard input" +
-    " and stores them in a sequence file in");
-    System.out.println("  the specified path");
-  }
-
-  public static void main(String[] args) throws Exception {
-    LoadTypedBytes loadtb = new LoadTypedBytes();
-    int res = ToolRunner.run(loadtb, args);
-    System.exit(res);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PathFinder.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PathFinder.java
deleted file mode 100644
index d0010b6b975..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PathFinder.java
+++ /dev/null
@@ -1,102 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.fs.FileUtil;
-
-/**
- * Maps a relative pathname to an absolute pathname using the PATH environment.
- */
-@InterfaceAudience.Private
-public class PathFinder {
-  String pathenv; // a string of pathnames
-  String pathSep; // the path separator
-  String fileSep; // the file separator in a directory
-
-  /**
-   * Construct a PathFinder object using the path from java.class.path
-   */
-  public PathFinder() {
-    pathenv = System.getProperty("java.class.path");
-    pathSep = System.getProperty("path.separator");
-    fileSep = System.getProperty("file.separator");
-  }
-
-  /**
-   * Construct a PathFinder object using the path from the specified system
-   * environment variable.
-   */
-  public PathFinder(String envpath) {
-    pathenv = System.getenv(envpath);
-    pathSep = System.getProperty("path.separator");
-    fileSep = System.getProperty("file.separator");
-  }
-
-  /**
-   * Appends the specified component to the path list
-   */
-  public void prependPathComponent(String str) {
-    pathenv = str + pathSep + pathenv;
-  }
-
-  /**
-   * Returns the full path name of this file if it is listed in the path
-   */
-  public File getAbsolutePath(String filename) {
-    if (pathenv == null || pathSep == null || fileSep == null) {
-      return null;
-    }
-    int val = -1;
-    String classvalue = pathenv + pathSep;
-
-    while (((val = classvalue.indexOf(pathSep)) >= 0)
-        && classvalue.length() > 0) {
-      // Extract each entry from the pathenv
-      String entry = classvalue.substring(0, val).trim();
-      File f = new File(entry);
-
-      if (f.isDirectory()) {
-        // this entry in the pathenv is a directory.
-        // see if the required file is in this directory
-        f = new File(entry + fileSep + filename);
-      }
-      // see if the filename matches and we can read it
-      if (f.isFile() && FileUtil.canRead(f)) {
-        return f;
-      }
-      classvalue = classvalue.substring(val + 1).trim();
-    }
-    return null;
-  }
-
-  public static void main(String args[]) throws IOException {
-    if (args.length < 1) {
-      System.out.println("Usage: java PathFinder <filename>");
-      System.exit(1);
-    }
-    PathFinder finder = new PathFinder("PATH");
-    File file = finder.getAbsolutePath(args[0]);
-    if (file != null) {
-      System.out.println("Full path name = " + file.getCanonicalPath());
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeCombiner.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeCombiner.java
deleted file mode 100644
index 444042d74f4..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeCombiner.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.UnsupportedEncodingException;
-import java.net.URLDecoder;
-
-import org.apache.hadoop.mapred.JobConf;
-
-public class PipeCombiner extends PipeReducer {
-  String getPipeCommand(JobConf job) {
-    String str = job.get("stream.combine.streamprocessor");
-    try {
-      if (str != null) {
-        return URLDecoder.decode(str, "UTF-8");
-      }
-    } catch (UnsupportedEncodingException e) {
-      System.err.println("stream.combine.streamprocessor" + 
-                         " in jobconf not found");
-    }
-    return null;
-  }
-  boolean getDoPipe() {
-    return (getPipeCommand(job_) != null);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java
deleted file mode 100644
index ef62505c4c6..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java
+++ /dev/null
@@ -1,627 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.util.Map.Entry;
-import java.util.Arrays;
-import java.util.ArrayList;
-import java.util.Properties;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.streaming.io.InputWriter;
-import org.apache.hadoop.streaming.io.OutputReader;
-import org.apache.hadoop.streaming.io.TextInputWriter;
-import org.apache.hadoop.streaming.io.TextOutputReader;
-import org.apache.hadoop.util.LineReader;
-import org.apache.hadoop.util.ReflectionUtils;
-
-import org.apache.hadoop.io.Text;
-
-/** Shared functionality for PipeMapper, PipeReducer.
- */
-public abstract class PipeMapRed {
-
-  protected static final Logger LOG = LoggerFactory.getLogger(PipeMapRed.class.getName());
-
-  /**
-   * Returns the Configuration.
-   */
-  public Configuration getConfiguration() {
-    return job_;
-  }
-  
-  /**
-   * Returns the DataOutput to which the client input is written.
-   */
-  public DataOutput getClientOutput() {
-    return clientOut_;
-  }
-  
-  /**
-   * Returns the DataInput from which the client output is read.
-   */
-  public DataInput getClientInput() {
-    return clientIn_;
-  }
-  
-  /**
-   * Returns the input separator to be used.
-   */
-  public abstract byte[] getInputSeparator();
-  
-  /**
-   * Returns the field separator to be used.
-   */
-  public abstract byte[] getFieldSeparator();
-
-  /**
-   * Returns the number of key fields.
-   */
-  public abstract int getNumOfKeyFields();
-
-  
-  /** 
-   * Returns the command to be spawned as a subprocess.
-   * Mapper/Reducer operations will delegate to it
-   */
-  abstract String getPipeCommand(JobConf job);
-  
-  abstract boolean getDoPipe();
-
-  final static int OUTSIDE = 1;
-  final static int SINGLEQ = 2;
-  final static int DOUBLEQ = 3;
-  
-  private final static int BUFFER_SIZE = 128 * 1024;
-
-  static String[] splitArgs(String args) {
-    ArrayList argList = new ArrayList();
-    char[] ch = args.toCharArray();
-    int clen = ch.length;
-    int state = OUTSIDE;
-    int argstart = 0;
-    for (int c = 0; c <= clen; c++) {
-      boolean last = (c == clen);
-      int lastState = state;
-      boolean endToken = false;
-      if (!last) {
-        if (ch[c] == '\'') {
-          if (state == OUTSIDE) {
-            state = SINGLEQ;
-          } else if (state == SINGLEQ) {
-            state = OUTSIDE;
-          }
-          endToken = (state != lastState);
-        } else if (ch[c] == '"') {
-          if (state == OUTSIDE) {
-            state = DOUBLEQ;
-          } else if (state == DOUBLEQ) {
-            state = OUTSIDE;
-          }
-          endToken = (state != lastState);
-        } else if (ch[c] == ' ') {
-          if (state == OUTSIDE) {
-            endToken = true;
-          }
-        }
-      }
-      if (last || endToken) {
-        if (c == argstart) {
-          // unquoted space
-        } else {
-          String a;
-          a = args.substring(argstart, c);
-          argList.add(a);
-        }
-        argstart = c + 1;
-        lastState = state;
-      }
-    }
-    return (String[]) argList.toArray(new String[0]);
-  }
-
-  public void configure(JobConf job) {
-    try {
-      String argv = getPipeCommand(job);
-
-      joinDelay_ = job.getLong("stream.joindelay.milli", 0);
-
-      job_ = job;
-      
-      mapInputWriterClass_ = 
-        job_.getClass("stream.map.input.writer.class", 
-          TextInputWriter.class, InputWriter.class);
-      mapOutputReaderClass_ = 
-        job_.getClass("stream.map.output.reader.class",
-          TextOutputReader.class, OutputReader.class);
-      reduceInputWriterClass_ = 
-        job_.getClass("stream.reduce.input.writer.class",
-          TextInputWriter.class, InputWriter.class);
-      reduceOutputReaderClass_ = 
-        job_.getClass("stream.reduce.output.reader.class",
-          TextOutputReader.class, OutputReader.class);
-      nonZeroExitIsFailure_ = job_.getBoolean("stream.non.zero.exit.is.failure", true);
-      
-      doPipe_ = getDoPipe();
-      if (!doPipe_) return;
-
-      setStreamJobDetails(job);
-      
-      String[] argvSplit = splitArgs(argv);
-      String prog = argvSplit[0];
-      File currentDir = new File(".").getAbsoluteFile();
-      if (new File(prog).isAbsolute()) {
-        // we don't own it. Hope it is executable
-      } else {
-        FileUtil.chmod(new File(currentDir, prog).toString(), "a+x");
-      }
-
-      // 
-      // argvSplit[0]:
-      // An absolute path should be a preexisting valid path on all TaskTrackers
-      // A relative path is converted into an absolute pathname by looking
-      // up the PATH env variable. If it still fails, look it up in the
-      // tasktracker's local working directory
-      //
-      if (!new File(argvSplit[0]).isAbsolute()) {
-        PathFinder finder = new PathFinder("PATH");
-        finder.prependPathComponent(currentDir.toString());
-        File f = finder.getAbsolutePath(argvSplit[0]);
-        if (f != null) {
-          argvSplit[0] = f.getAbsolutePath();
-        }
-        f = null;
-      }
-      LOG.info("PipeMapRed exec " + Arrays.asList(argvSplit));
-      Environment childEnv = (Environment) StreamUtil.env().clone();
-      addJobConfToEnvironment(job_, childEnv);
-      addEnvironment(childEnv, job_.get("stream.addenvironment"));
-      // add TMPDIR environment variable with the value of java.io.tmpdir
-      envPut(childEnv, "TMPDIR", System.getProperty("java.io.tmpdir"));
-
-      // Start the process
-      ProcessBuilder builder = new ProcessBuilder(argvSplit);
-      builder.environment().putAll(childEnv.toMap());
-      sim = builder.start();
-
-      clientOut_ = new DataOutputStream(new BufferedOutputStream(
-                                              sim.getOutputStream(),
-                                              BUFFER_SIZE));
-      clientIn_ = new DataInputStream(new BufferedInputStream(
-                                              sim.getInputStream(),
-                                              BUFFER_SIZE));
-      clientErr_ = new DataInputStream(new BufferedInputStream(sim.getErrorStream()));
-      startTime_ = System.currentTimeMillis();
-
-    } catch (IOException e) {
-      LOG.error("configuration exception", e);
-      throw new RuntimeException("configuration exception", e);
-    } catch (InterruptedException e)  {
-      LOG.error("configuration exception", e);
-      throw new RuntimeException("configuration exception", e);
-    }
-  }
-  
-  void setStreamJobDetails(JobConf job) {
-    String s = job.get("stream.minRecWrittenToEnableSkip_");
-    if (s != null) {
-      minRecWrittenToEnableSkip_ = Long.parseLong(s);
-      LOG.info("JobConf set minRecWrittenToEnableSkip_ ="
-          + minRecWrittenToEnableSkip_);
-    }
-  }
-
-  void addJobConfToEnvironment(JobConf jobconf, Properties env) {
-    JobConf conf = new JobConf(jobconf);
-    conf.setDeprecatedProperties();
-    int lenLimit = conf.getInt("stream.jobconf.truncate.limit", -1);
-
-    for (Entry<String, String> confEntry: conf) {
-      String name = confEntry.getKey();
-      String value = conf.get(name); // does variable expansion
-      name = safeEnvVarName(name);
-      if (lenLimit > -1  && value.length() > lenLimit) {
-        LOG.warn("Environment variable " + name + " truncated to " + lenLimit
-            + " to  fit system limits.");
-        value = value.substring(0, lenLimit);
-      }
-      envPut(env, name, value);
-    }
-  }
-
-  String safeEnvVarName(String var) {
-    StringBuffer safe = new StringBuffer();
-    int len = var.length();
-    for (int i = 0; i < len; i++) {
-      char c = var.charAt(i);
-      char s;
-      if ((c >= '0' && c <= '9') || (c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z')) {
-        s = c;
-      } else {
-        s = '_';
-      }
-      safe.append(s);
-    }
-    return safe.toString();
-  }
-
-  void addEnvironment(Properties env, String nameVals) {
-    // encoding "a=b c=d" from StreamJob
-    if (nameVals == null) return;
-    String[] nv = nameVals.split(" ");
-    for (int i = 0; i < nv.length; i++) {
-      String[] pair = nv[i].split("=", 2);
-      if (pair.length != 2) {
-        LOG.info("Skip env entry:" + nv[i]);
-      } else {
-        envPut(env, pair[0], pair[1]);
-      }
-    }
-  }
-
-  void envPut(Properties env, String name, String value) {
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Add  env entry:" + name + "=" + value);
-    }
-    env.put(name, value);
-  }
-
-  void startOutputThreads(OutputCollector output, Reporter reporter) 
-    throws IOException {
-    inWriter_ = createInputWriter();
-    outReader_ = createOutputReader();
-    outThread_ = new MROutputThread(outReader_, output, reporter);
-    outThread_.start();
-    errThread_ = new MRErrorThread();
-    errThread_.setReporter(reporter);
-    errThread_.start();
-  }
-  
-  void waitOutputThreads() throws IOException {
-    try {
-      if (outThread_ == null) {
-        // This happens only when reducer has empty input(So reduce() is not
-        // called at all in this task). If reducer still generates output,
-        // which is very uncommon and we may not have to support this case.
-        // So we don't write this output to HDFS, but we consume/collect
-        // this output just to avoid reducer hanging forever.
-
-        OutputCollector collector = new OutputCollector() {
-          public void collect(Object key, Object value)
-            throws IOException {
-            //just consume it, no need to write the record anywhere
-          }
-        };
-        Reporter reporter = Reporter.NULL;//dummy reporter
-        startOutputThreads(collector, reporter);
-      }
-      int exitVal = sim.waitFor();
-      // how'd it go?
-      if (exitVal != 0) {
-        if (nonZeroExitIsFailure_) {
-          throw new RuntimeException("PipeMapRed.waitOutputThreads(): subprocess failed with code "
-                                     + exitVal);
-        } else {
-          LOG.info("PipeMapRed.waitOutputThreads(): subprocess exited with " +
-          		"code " + exitVal + " in " + PipeMapRed.class.getName());
-        }
-      }
-      if (outThread_ != null) {
-        outThread_.join(joinDelay_);
-      }
-      if (errThread_ != null) {
-        errThread_.join(joinDelay_);
-      }
-      if (outerrThreadsThrowable != null) {
-        throw new RuntimeException(outerrThreadsThrowable);
-      }
-    } catch (InterruptedException e) {
-      //ignore
-    }
-  }
-  
-  
-  abstract InputWriter createInputWriter() throws IOException;
-  
-  InputWriter createInputWriter(Class<? extends InputWriter> inputWriterClass) 
-    throws IOException {
-    InputWriter inputWriter =
-      ReflectionUtils.newInstance(inputWriterClass, job_);
-    inputWriter.initialize(this);
-    return inputWriter;
-  }
-
-  abstract OutputReader createOutputReader() throws IOException;
-
-  OutputReader createOutputReader(Class<? extends OutputReader> outputReaderClass) 
-    throws IOException {
-    OutputReader outputReader =
-      ReflectionUtils.newInstance(outputReaderClass, job_);
-    outputReader.initialize(this);
-    return outputReader;
-  }
-  
-  
-  class MROutputThread extends Thread {
-
-    MROutputThread(OutputReader outReader, OutputCollector outCollector,
-      Reporter reporter) {
-      setDaemon(true);
-      this.outReader = outReader;
-      this.outCollector = outCollector;
-      this.reporter = reporter;
-    }
-
-    public void run() {
-      try {
-        // 3/4 Tool to Hadoop
-        while (outReader.readKeyValue()) {
-          Object key = outReader.getCurrentKey();
-          Object value = outReader.getCurrentValue();
-          outCollector.collect(key, value);
-          numRecWritten_++;
-          long now = System.currentTimeMillis();
-          if (now-lastStdoutReport > reporterOutDelay_) {
-            lastStdoutReport = now;
-            String hline = "Records R/W=" + numRecRead_ + "/" + numRecWritten_;
-            if (!processProvidedStatus_) {
-              reporter.setStatus(hline);
-            } else {
-              reporter.progress();
-            }
-            LOG.info(hline);
-          }
-        }
-      } catch (Throwable th) {
-        outerrThreadsThrowable = th;
-        LOG.warn("{}", th);
-      } finally {
-        try {
-          if (clientIn_ != null) {
-            clientIn_.close();
-            clientIn_ = null;
-          }
-        } catch (IOException io) {
-          LOG.info("{}", io);
-        }
-      }
-    }
-
-    OutputReader outReader = null;
-    OutputCollector outCollector = null;
-    Reporter reporter = null;
-    long lastStdoutReport = 0;
-    
-  }
-
-  class MRErrorThread extends Thread {
-
-    public MRErrorThread() {
-      this.reporterPrefix = job_.get("stream.stderr.reporter.prefix", "reporter:");
-      this.counterPrefix = reporterPrefix + "counter:";
-      this.statusPrefix = reporterPrefix + "status:";
-      setDaemon(true);
-    }
-    
-    public void setReporter(Reporter reporter) {
-      this.reporter = reporter;
-    }
-      
-    public void run() {
-      Text line = new Text();
-      LineReader lineReader = null;
-      try {
-        lineReader = new LineReader((InputStream)clientErr_, job_);
-        while (lineReader.readLine(line) > 0) {
-          String lineStr = line.toString();
-          if (matchesReporter(lineStr)) {
-            if (matchesCounter(lineStr)) {
-              incrCounter(lineStr);
-            } else if (matchesStatus(lineStr)) {
-              processProvidedStatus_ = true;
-              setStatus(lineStr);
-            } else {
-              LOG.warn("Cannot parse reporter line: " + lineStr);
-            }
-          } else {
-            System.err.println(lineStr);
-          }
-          long now = System.currentTimeMillis(); 
-          if (reporter != null && now-lastStderrReport > reporterErrDelay_) {
-            lastStderrReport = now;
-            reporter.progress();
-          }
-          line.clear();
-        }
-        if (lineReader != null) {
-          lineReader.close();
-        }
-        if (clientErr_ != null) {
-          clientErr_.close();
-          clientErr_ = null;
-          LOG.info("MRErrorThread done");
-        }
-      } catch (Throwable th) {
-        outerrThreadsThrowable = th;
-        LOG.warn("{}", th);
-        try {
-          if (lineReader != null) {
-            lineReader.close();
-          }
-          if (clientErr_ != null) {
-            clientErr_.close();
-            clientErr_ = null;
-          }
-        } catch (IOException io) {
-          LOG.info("{}", io);
-        }
-      }
-    }
-    
-    private boolean matchesReporter(String line) {
-      return line.startsWith(reporterPrefix);
-    }
-
-    private boolean matchesCounter(String line) {
-      return line.startsWith(counterPrefix);
-    }
-
-    private boolean matchesStatus(String line) {
-      return line.startsWith(statusPrefix);
-    }
-
-    private void incrCounter(String line) {
-      String trimmedLine = line.substring(counterPrefix.length()).trim();
-      String[] columns = trimmedLine.split(",");
-      if (columns.length == 3) {
-        try {
-          reporter.incrCounter(columns[0], columns[1],
-              Long.parseLong(columns[2]));
-        } catch (NumberFormatException e) {
-          LOG.warn("Cannot parse counter increment '" + columns[2] +
-              "' from line: " + line);
-        }
-      } else {
-        LOG.warn("Cannot parse counter line: " + line);
-      }
-    }
-
-    private void setStatus(String line) {
-      reporter.setStatus(line.substring(statusPrefix.length()).trim());
-    }
-    
-    long lastStderrReport = 0;
-    volatile Reporter reporter;
-    private final String reporterPrefix;
-    private final String counterPrefix;
-    private final String statusPrefix;
-  }
-
-  public void mapRedFinished() {
-    try {
-      if (!doPipe_) {
-        LOG.info("mapRedFinished");
-        return;
-      }
-      if (clientOut_ != null) {
-        try {
-          clientOut_.flush();
-          clientOut_.close();
-        } catch (IOException io) {
-          LOG.warn("{}", io);
-        }
-      }
-      try {
-        waitOutputThreads();
-      } catch (IOException io) {
-        LOG.warn("{}", io);
-      }
-      if (sim != null) sim.destroy();
-      LOG.info("mapRedFinished");
-    } catch (RuntimeException e) {
-      LOG.info("PipeMapRed failed!", e);
-      throw e;
-    }
-  }
-
-  void maybeLogRecord() {
-    if (numRecRead_ >= nextRecReadLog_) {
-      String info = numRecInfo();
-      LOG.info(info);
-      if (nextRecReadLog_ < 100000) {
-	  nextRecReadLog_ *= 10;
-      } else {
-	  nextRecReadLog_ += 100000;
-      }
-    }
-  }
-
-  public String getContext() {
-
-    String s = numRecInfo() + "\n";
-    s += "minRecWrittenToEnableSkip_=" + minRecWrittenToEnableSkip_ + " ";
-    s += envline("HOST");
-    s += envline("USER");
-    s += envline("HADOOP_USER");
-    if (outThread_ != null) {
-      s += "last tool output: |" + outReader_.getLastOutput() + "|\n";
-    }
-    return s;
-  }
-
-  String envline(String var) {
-    return var + "=" + StreamUtil.env().get(var) + "\n";
-  }
-
-  String numRecInfo() {
-    long elapsed = (System.currentTimeMillis() - startTime_) / 1000;
-    return "R/W/S=" + numRecRead_ + "/" + numRecWritten_ + "/" + numRecSkipped_ + " in:"
-      + safeDiv(numRecRead_, elapsed) + " [rec/s]" + " out:" + safeDiv(numRecWritten_, elapsed)
-      + " [rec/s]";
-  }
-
-  String safeDiv(long n, long d) {
-    return (d == 0) ? "NA" : "" + n / d + "=" + n + "/" + d;
-  }
-
-  long startTime_;
-  long numRecRead_ = 0;
-  long numRecWritten_ = 0;
-  long numRecSkipped_ = 0;
-  long nextRecReadLog_ = 1;
-
-  long minRecWrittenToEnableSkip_ = Long.MAX_VALUE;
-
-  long reporterOutDelay_ = 10*1000L; 
-  long reporterErrDelay_ = 10*1000L; 
-  long joinDelay_;
-  JobConf job_;
-
-  boolean doPipe_;
-
-  Class<? extends InputWriter> mapInputWriterClass_;
-  Class<? extends OutputReader> mapOutputReaderClass_;
-  Class<? extends InputWriter> reduceInputWriterClass_;
-  Class<? extends OutputReader> reduceOutputReaderClass_;
-  boolean nonZeroExitIsFailure_;
-  
-  Process sim;
-  InputWriter inWriter_;
-  OutputReader outReader_;
-  MROutputThread outThread_;
-  MRErrorThread errThread_;
-  DataOutputStream clientOut_;
-  DataInputStream clientErr_;
-  DataInputStream clientIn_;
-
-  // set in PipeMapper/PipeReducer subclasses
-  int numExceptions_;
-
-  protected volatile Throwable outerrThreadsThrowable;
-
-  volatile boolean processProvidedStatus_ = false;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRunner.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRunner.java
deleted file mode 100644
index da2790b861e..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapRunner.java
+++ /dev/null
@@ -1,36 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import org.apache.hadoop.mapred.MapRunner;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.OutputCollector;
-
-import java.io.IOException;
-
-public class PipeMapRunner<K1, V1, K2, V2> extends MapRunner<K1, V1, K2, V2> {
-  public void run(RecordReader<K1, V1> input, OutputCollector<K2, V2> output,
-                  Reporter reporter)
-         throws IOException {
-    PipeMapper pipeMapper = (PipeMapper)getMapper();
-    pipeMapper.startOutputThreads(output, reporter);
-    super.run(input, output, reporter);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapper.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapper.java
deleted file mode 100644
index 438a00057ec..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapper.java
+++ /dev/null
@@ -1,157 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.net.URLDecoder;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Mapper;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.SkipBadRecords;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.streaming.io.InputWriter;
-import org.apache.hadoop.streaming.io.OutputReader;
-import org.apache.hadoop.streaming.io.TextInputWriter;
-
-/** A generic Mapper bridge.
- *  It delegates operations to an external program via stdin and stdout.
- */
-public class PipeMapper extends PipeMapRed implements Mapper {
-
-  private boolean ignoreKey = false;
-  private boolean skipping = false;
-
-  private byte[] mapOutputFieldSeparator;
-  private byte[] mapInputFieldSeparator;
-  private int numOfMapOutputKeyFields = 1;
-  
-  String getPipeCommand(JobConf job) {
-    String str = job.get("stream.map.streamprocessor");
-    if (str == null) {
-      return str;
-    }
-    try {
-      return URLDecoder.decode(str, "UTF-8");
-    }
-    catch (UnsupportedEncodingException e) {
-      System.err.println("stream.map.streamprocessor in jobconf not found");
-      return null;
-    }
-  }
-
-  boolean getDoPipe() {
-    return true;
-  }
-  
-  public void configure(JobConf job) {
-    super.configure(job);
-    //disable the auto increment of the counter. For streaming, no of 
-    //processed records could be different(equal or less) than the no of 
-    //records input.
-    SkipBadRecords.setAutoIncrMapperProcCount(job, false);
-    skipping = job.getBoolean(MRJobConfig.SKIP_RECORDS, false);
-    if (mapInputWriterClass_.getCanonicalName().equals(TextInputWriter.class.getCanonicalName())) {
-      String inputFormatClassName = job.getClass("mapred.input.format.class", TextInputFormat.class).getCanonicalName();
-      ignoreKey = job.getBoolean("stream.map.input.ignoreKey", 
-        inputFormatClassName.equals(TextInputFormat.class.getCanonicalName()));
-    }
-    
-    mapOutputFieldSeparator = job.get("stream.map.output.field.separator", "\t")
-            .getBytes(StandardCharsets.UTF_8);
-    mapInputFieldSeparator = job.get("stream.map.input.field.separator", "\t")
-            .getBytes(StandardCharsets.UTF_8);
-    numOfMapOutputKeyFields = job.getInt("stream.num.map.output.key.fields", 1);
-  }
-
-  // Do NOT declare default constructor
-  // (MapRed creates it reflectively)
-
-  public void map(Object key, Object value, OutputCollector output, Reporter reporter) throws IOException {
-    if (outerrThreadsThrowable != null) {
-      mapRedFinished();
-      throw new IOException("MROutput/MRErrThread failed:",
-          outerrThreadsThrowable);
-    }
-    try {
-      // 1/4 Hadoop in
-      numRecRead_++;
-      maybeLogRecord();
-
-      // 2/4 Hadoop to Tool
-      if (numExceptions_ == 0) {
-        if (!this.ignoreKey) {
-          inWriter_.writeKey(key);
-        }
-        inWriter_.writeValue(value);
-        if(skipping) {
-          //flush the streams on every record input if running in skip mode
-          //so that we don't buffer other records surrounding a bad record. 
-          clientOut_.flush();
-        }
-      } else {
-        numRecSkipped_++;
-      }
-    } catch (IOException io) {
-      numExceptions_++;
-      if (numExceptions_ > 1 || numRecWritten_ < minRecWrittenToEnableSkip_) {
-        // terminate with failure
-        LOG.info(getContext() , io);
-        mapRedFinished();
-        throw io;
-      } else {
-        // terminate with success:
-        // swallow input records although the stream processor failed/closed
-      }
-    }
-  }
-
-  public void close() {
-    mapRedFinished();
-  }
-
-  @Override
-  public byte[] getInputSeparator() {
-    return mapInputFieldSeparator;
-  }
-
-  @Override
-  public byte[] getFieldSeparator() {
-    return mapOutputFieldSeparator;
-  }
-
-  @Override
-  public int getNumOfKeyFields() {
-    return numOfMapOutputKeyFields;
-  }
-
-  @Override
-  InputWriter createInputWriter() throws IOException {
-    return super.createInputWriter(mapInputWriterClass_);
-  }
-
-  @Override
-  OutputReader createOutputReader() throws IOException {
-    return super.createOutputReader(mapOutputReaderClass_);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeReducer.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeReducer.java
deleted file mode 100644
index 1f5a247bb2a..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeReducer.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.IOException;
-import java.io.UnsupportedEncodingException;
-import java.nio.charset.StandardCharsets;
-import java.util.Iterator;
-import java.net.URLDecoder;
-
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.SkipBadRecords;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.streaming.io.InputWriter;
-import org.apache.hadoop.streaming.io.OutputReader;
-
-import org.apache.hadoop.io.Writable;
-
-/** A generic Reducer bridge.
- *  It delegates operations to an external program via stdin and stdout.
- */
-public class PipeReducer extends PipeMapRed implements Reducer {
-
-  private byte[] reduceOutFieldSeparator;
-  private byte[] reduceInputFieldSeparator;
-  private int numOfReduceOutputKeyFields = 1;
-  private boolean skipping = false;
-  
-  String getPipeCommand(JobConf job) {
-    String str = job.get("stream.reduce.streamprocessor");
-    if (str == null) {
-      return str;
-    }
-    try {
-      return URLDecoder.decode(str, "UTF-8");
-    } catch (UnsupportedEncodingException e) {
-      System.err.println("stream.reduce.streamprocessor in jobconf not found");
-      return null;
-    }
-  }
-
-  boolean getDoPipe() {
-    String argv = getPipeCommand(job_);
-    // Currently: null is identity reduce. REDUCE_NONE is no-map-outputs.
-    return (argv != null) && !StreamJob.REDUCE_NONE.equals(argv);
-  }
-
-  public void configure(JobConf job) {
-    super.configure(job);
-    //disable the auto increment of the counter. For streaming, no of 
-    //processed records could be different(equal or less) than the no of 
-    //records input.
-    SkipBadRecords.setAutoIncrReducerProcCount(job, false);
-    skipping = job.getBoolean(MRJobConfig.SKIP_RECORDS, false);
-
-    reduceOutFieldSeparator = job_.get("stream.reduce.output.field.separator", "\t")
-            .getBytes(StandardCharsets.UTF_8);
-    reduceInputFieldSeparator = job_.get("stream.reduce.input.field.separator", "\t")
-            .getBytes(StandardCharsets.UTF_8);
-    this.numOfReduceOutputKeyFields = job_.getInt("stream.num.reduce.output.key.fields", 1);
-  }
-
-  public void reduce(Object key, Iterator values, OutputCollector output,
-                     Reporter reporter) throws IOException {
-
-    // init
-    if (doPipe_ && outThread_ == null) {
-      startOutputThreads(output, reporter);
-    }
-    try {
-      while (values.hasNext()) {
-        Writable val = (Writable) values.next();
-        numRecRead_++;
-        maybeLogRecord();
-        if (doPipe_) {
-          if (outerrThreadsThrowable != null) {
-            mapRedFinished();
-            throw new IOException("MROutput/MRErrThread failed:",
-                outerrThreadsThrowable);
-          }
-          inWriter_.writeKey(key);
-          inWriter_.writeValue(val);
-        } else {
-          // "identity reduce"
-          output.collect(key, val);
-        }
-      }
-      if(doPipe_ && skipping) {
-        //flush the streams on every record input if running in skip mode
-        //so that we don't buffer other records surrounding a bad record. 
-        clientOut_.flush();
-      }
-    } catch (IOException io) {
-      // a common reason to get here is failure of the subprocess.
-      // Document that fact, if possible.
-      String extraInfo = "";
-      try {
-        int exitVal = sim.exitValue();
-	if (exitVal == 0) {
-	  extraInfo = "subprocess exited successfully\n";
-	} else {
-	  extraInfo = "subprocess exited with error code " + exitVal + "\n";
-	};
-      } catch (IllegalThreadStateException e) {
-        // hmm, but child is still running.  go figure.
-	extraInfo = "subprocess still running\n";
-      };
-      mapRedFinished();
-      throw new IOException(extraInfo + getContext() + io.getMessage());
-    }
-  }
-
-  public void close() {
-    mapRedFinished();
-  }
-
-  @Override
-  public byte[] getInputSeparator() {
-    return reduceInputFieldSeparator;
-  }
-
-  @Override
-  public byte[] getFieldSeparator() {
-    return reduceOutFieldSeparator;
-  }
-  
-  @Override
-  public int getNumOfKeyFields() {
-    return numOfReduceOutputKeyFields;
-  }
-  
-  @Override
-  InputWriter createInputWriter() throws IOException {
-    return super.createInputWriter(reduceInputWriterClass_);
-  }
-
-  @Override
-  OutputReader createOutputReader() throws IOException {
-    return super.createOutputReader(reduceOutputReaderClass_);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamBaseRecordReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamBaseRecordReader.java
deleted file mode 100644
index c757cf6d464..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamBaseRecordReader.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/** 
- * Shared functionality for hadoopStreaming formats.
- * A custom reader can be defined to be a RecordReader with the constructor below
- * and is selected with the option bin/hadoopStreaming -inputreader ...
- * @see StreamXmlRecordReader 
- */
-public abstract class StreamBaseRecordReader implements RecordReader<Text, Text> {
-
-  protected static final Logger LOG = LoggerFactory.getLogger(StreamBaseRecordReader.class.getName());
-
-  // custom JobConf properties for this class are prefixed with this namespace
-  final static String CONF_NS = "stream.recordreader.";
-
-  public StreamBaseRecordReader(FSDataInputStream in, FileSplit split, Reporter reporter,
-                                JobConf job, FileSystem fs) throws IOException {
-    in_ = in;
-    split_ = split;
-    start_ = split_.getStart();
-    length_ = split_.getLength();
-    end_ = start_ + length_;
-    splitName_ = split_.getPath().getName();
-    reporter_ = reporter;
-    job_ = job;
-    fs_ = fs;
-
-    statusMaxRecordChars_ = job_.getInt(CONF_NS + "statuschars", 200);
-  }
-
-  /// RecordReader API
-
-  /** Read a record. Implementation should call numRecStats at the end
-   */
-  public abstract boolean next(Text key, Text value) throws IOException;
-
-  /** Returns the current position in the input. */
-  public synchronized long getPos() throws IOException {
-    return in_.getPos();
-  }
-
-  /** Close this to future operations.*/
-  public synchronized void close() throws IOException {
-    in_.close();
-  }
-
-  public float getProgress() throws IOException {
-    if (end_ == start_) {
-      return 1.0f;
-    } else {
-      return ((float)(in_.getPos() - start_)) / ((float)(end_ - start_));
-    }
-  }
-  
-  public Text createKey() {
-    return new Text();
-  }
-
-  public Text createValue() {
-    return new Text();
-  }
-
-  /// StreamBaseRecordReader API
-
-  /** Implementation should seek forward in_ to the first byte of the next record.
-   *  The initial byte offset in the stream is arbitrary.
-   */
-  public abstract void seekNextRecordBoundary() throws IOException;
-
-  void numRecStats(byte[] record, int start, int len) throws IOException {
-    numRec_++;
-    if (numRec_ == nextStatusRec_) {
-      String recordStr = new String(record, start,
-              Math.min(len, statusMaxRecordChars_), StandardCharsets.UTF_8);
-      nextStatusRec_ += 100;//*= 10;
-      String status = getStatus(recordStr);
-      LOG.info(status);
-      reporter_.setStatus(status);
-    }
-  }
-
-  long lastMem = 0;
-
-  String getStatus(CharSequence record) {
-    long pos = -1;
-    try {
-      pos = getPos();
-    } catch (IOException io) {
-    }
-    String recStr;
-    if (record.length() > statusMaxRecordChars_) {
-      recStr = record.subSequence(0, statusMaxRecordChars_) + "...";
-    } else {
-      recStr = record.toString();
-    }
-    String unqualSplit = split_.getPath().getName() + ":" +
-                         split_.getStart() + "+" + split_.getLength();
-    String status = "HSTR " + StreamUtil.getHost() + " " + numRec_ + ". pos=" + pos + " " + unqualSplit
-      + " Processing record=" + recStr;
-    status += " " + splitName_;
-    return status;
-  }
-
-  FSDataInputStream in_;
-  FileSplit split_;
-  long start_;
-  long end_;
-  long length_;
-  String splitName_;
-  Reporter reporter_;
-  JobConf job_;
-  FileSystem fs_;
-  int numRec_ = 0;
-  int nextStatusRec_ = 1;
-  int statusMaxRecordChars_;
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamInputFormat.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamInputFormat.java
deleted file mode 100644
index 984c9651e5a..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamInputFormat.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.lang.reflect.*;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.io.Text;
-
-import org.apache.hadoop.mapred.*;
-
-/** An input format that selects a RecordReader based on a JobConf property.
- *  This should be used only for non-standard record reader such as 
- *  StreamXmlRecordReader. For all other standard 
- *  record readers, the appropriate input format classes should be used.
- */
-public class StreamInputFormat extends KeyValueTextInputFormat {
-
-  @SuppressWarnings("unchecked")
-  public RecordReader<Text, Text> getRecordReader(final InputSplit genericSplit,
-                                      JobConf job, Reporter reporter) throws IOException {
-    String c = job.get("stream.recordreader.class");
-    if (c == null || c.indexOf("LineRecordReader") >= 0) {
-      return super.getRecordReader(genericSplit, job, reporter);
-    }
-
-    // handling non-standard record reader (likely StreamXmlRecordReader) 
-    FileSplit split = (FileSplit) genericSplit;
-    LOG.info("getRecordReader start.....split=" + split);
-    reporter.setStatus(split.toString());
-
-    // Open the file and seek to the start of the split
-    FileSystem fs = split.getPath().getFileSystem(job);
-    FSDataInputStream in = fs.open(split.getPath());
-
-    // Factory dispatch based on available params..
-    Class readerClass;
-
-    {
-      readerClass = StreamUtil.goodClassOrNull(job, c, null);
-      if (readerClass == null) {
-        throw new RuntimeException("Class not found: " + c);
-      }
-    }
-
-    Constructor ctor;
-    try {
-      ctor = readerClass.getConstructor(new Class[] { FSDataInputStream.class,
-                                                      FileSplit.class, Reporter.class, JobConf.class, FileSystem.class });
-    } catch (NoSuchMethodException nsm) {
-      throw new RuntimeException(nsm);
-    }
-
-    RecordReader<Text, Text> reader;
-    try {
-      reader = (RecordReader<Text, Text>) ctor.newInstance(new Object[] { in, split,
-                                                              reporter, job, fs });
-    } catch (Exception nsm) {
-      throw new RuntimeException(nsm);
-    }
-    return reader;
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java
deleted file mode 100644
index 023371ce994..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamJob.java
+++ /dev/null
@@ -1,1105 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.net.URI;
-import java.net.URLEncoder;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.TreeMap;
-import java.util.TreeSet;
-
-import org.apache.commons.cli.BasicParser;
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.CommandLineParser;
-import org.apache.commons.cli.Option;
-import org.apache.commons.cli.Options;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.permission.FsAction;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRConfig;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.filecache.DistributedCache;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FileAlreadyExistsException;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.FileOutputFormat;
-import org.apache.hadoop.mapred.InvalidJobConfException;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.JobID;
-import org.apache.hadoop.mapred.KeyValueTextInputFormat;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.SequenceFileAsTextInputFormat;
-import org.apache.hadoop.mapred.SequenceFileInputFormat;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapred.TextOutputFormat;
-import org.apache.hadoop.mapred.lib.LazyOutputFormat;
-import org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorCombiner;
-import org.apache.hadoop.mapred.lib.aggregate.ValueAggregatorReducer;
-import org.apache.hadoop.security.AccessControlException;
-import org.apache.hadoop.streaming.io.IdentifierResolver;
-import org.apache.hadoop.streaming.io.InputWriter;
-import org.apache.hadoop.streaming.io.OutputReader;
-import org.apache.hadoop.util.GenericOptionsParser;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.util.RunJar;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.util.Tool;
-
-import static org.apache.hadoop.util.RunJar.MATCH_ANY;
-
-/** All the client-side work happens here.
- * (Jar packaging, MapRed job submission and monitoring)
- */
-public class StreamJob implements Tool {
-
-  protected static final Logger LOG = LoggerFactory.getLogger(StreamJob.class.getName());
-  final static String REDUCE_NONE = "NONE";
-
-  /** -----------Streaming CLI Implementation  **/
-  private CommandLineParser parser = new BasicParser();
-  private Options allOptions;
-  /**@deprecated use StreamJob() with ToolRunner or set the
-   * Configuration using {@link #setConf(Configuration)} and
-   * run with {@link #run(String[])}.
-   */
-  @Deprecated
-  public StreamJob(String[] argv, boolean mayExit) {
-    this();
-    argv_ = Arrays.copyOf(argv, argv.length);
-    this.config_ = new Configuration();
-  }
-
-  public StreamJob() {
-    setupOptions();
-    this.config_ = new Configuration();
-  }
-
-  @Override
-  public Configuration getConf() {
-    return config_;
-  }
-
-  @Override
-  public void setConf(Configuration conf) {
-    this.config_ = conf;
-  }
-
-  @Override
-  public int run(String[] args) throws Exception {
-    try {
-      this.argv_ = Arrays.copyOf(args, args.length);
-      init();
-
-      preProcessArgs();
-      parseArgv();
-      if (printUsage) {
-        printUsage(detailedUsage_);
-        return 0;
-      }
-      postProcessArgs();
-
-      setJobConf();
-    } catch (IllegalArgumentException ex) {
-      //ignore, since log will already be printed
-      // print the log in debug mode.
-      LOG.debug("Error in streaming job", ex);
-      return 1;
-    }
-    return submitAndMonitorJob();
-  }
-
-  /**
-   * This method creates a streaming job from the given argument list.
-   * The created object can be used and/or submitted to a jobtracker for
-   * execution by a job agent such as JobControl
-   * @param argv the list args for creating a streaming job
-   * @return the created JobConf object
-   * @throws IOException
-   */
-  static public JobConf createJob(String[] argv) throws IOException {
-    StreamJob job = new StreamJob();
-    job.argv_ = argv;
-    job.init();
-    job.preProcessArgs();
-    job.parseArgv();
-    job.postProcessArgs();
-    job.setJobConf();
-    return job.jobConf_;
-  }
-
-  /**
-   * This is the method that actually
-   * initializes the job conf and submits the job
-   * to the jobtracker
-   * @throws IOException
-   * @deprecated use {@link #run(String[])} instead.
-   */
-  @Deprecated
-  public int go() throws IOException {
-    try {
-      return run(argv_);
-    }
-    catch (Exception ex) {
-      throw new IOException(ex.getMessage());
-    }
-  }
-
-  protected void init() {
-    try {
-      env_ = new Environment();
-    } catch (IOException io) {
-      throw new RuntimeException(io);
-    }
-  }
-
-  void preProcessArgs() {
-    verbose_ = false;
-    // Unset HADOOP_ROOT_LOGGER in case streaming job
-    // invokes additional hadoop commands.
-    addTaskEnvironment_ = "HADOOP_ROOT_LOGGER=";
-  }
-
-  void postProcessArgs() throws IOException {
-
-    if (inputSpecs_.size() == 0) {
-      fail("Required argument: -input <name>");
-    }
-    if (output_ == null) {
-      fail("Required argument: -output ");
-    }
-    msg("addTaskEnvironment=" + addTaskEnvironment_);
-
-    for (final String packageFile : packageFiles_) {
-      File f = new File(packageFile);
-      if (f.isFile()) {
-        shippedCanonFiles_.add(f.getCanonicalPath());
-      }
-    }
-    msg("shippedCanonFiles_=" + shippedCanonFiles_);
-
-    // careful with class names..
-    mapCmd_ = unqualifyIfLocalPath(mapCmd_);
-    comCmd_ = unqualifyIfLocalPath(comCmd_);
-    redCmd_ = unqualifyIfLocalPath(redCmd_);
-  }
-
-  String unqualifyIfLocalPath(String cmd) throws IOException {
-    if (cmd == null) {
-      //
-    } else {
-      String prog = cmd;
-      String args = "";
-      int s = cmd.indexOf(" ");
-      if (s != -1) {
-        prog = cmd.substring(0, s);
-        args = cmd.substring(s + 1);
-      }
-      String progCanon;
-      try {
-        progCanon = new File(prog).getCanonicalPath();
-      } catch (IOException io) {
-        progCanon = prog;
-      }
-      boolean shipped = shippedCanonFiles_.contains(progCanon);
-      msg("shipped: " + shipped + " " + progCanon);
-      if (shipped) {
-        // Change path to simple filename.
-        // That way when PipeMapRed calls Runtime.exec(),
-        // it will look for the excutable in Task's working dir.
-        // And this is where TaskRunner unjars our job jar.
-        prog = new File(prog).getName();
-        if (args.length() > 0) {
-          cmd = prog + " " + args;
-        } else {
-          cmd = prog;
-        }
-      }
-    }
-    msg("cmd=" + cmd);
-    return cmd;
-  }
-
-  void parseArgv() {
-    CommandLine cmdLine = null;
-    try {
-      cmdLine = parser.parse(allOptions, argv_);
-    } catch(Exception oe) {
-      LOG.error(oe.getMessage());
-      exitUsage(argv_.length > 0 && "-info".equals(argv_[0]));
-    }
-
-    if (cmdLine != null) {
-      @SuppressWarnings("unchecked")
-      List<String> args = cmdLine.getArgList();
-      if(args != null && args.size() > 0) {
-        fail("Found " + args.size() + " unexpected arguments on the " +
-            "command line " + args);
-      }
-      
-      detailedUsage_ = cmdLine.hasOption("info");
-      if (cmdLine.hasOption("help") || detailedUsage_) {
-        printUsage = true;
-        return;
-      }
-      verbose_ =  cmdLine.hasOption("verbose");
-      background_ =  cmdLine.hasOption("background");
-      debug_ = cmdLine.hasOption("debug")? debug_ + 1 : debug_;
-
-      String[] values = cmdLine.getOptionValues("input");
-      if (values != null && values.length > 0) {
-        for (String input : values) {
-          inputSpecs_.add(input);
-        }
-      }
-      output_ =  cmdLine.getOptionValue("output");
-
-      mapCmd_ = cmdLine.getOptionValue("mapper");
-      comCmd_ = cmdLine.getOptionValue("combiner");
-      redCmd_ = cmdLine.getOptionValue("reducer");
-
-      lazyOutput_ = cmdLine.hasOption("lazyOutput");
-
-      values = cmdLine.getOptionValues("file");
-      if (values != null && values.length > 0) {
-        LOG.warn("-file option is deprecated, please use generic option" +
-        		" -files instead.");
-
-        StringBuffer fileList = new StringBuffer();
-        for (String file : values) {
-          packageFiles_.add(file);
-          try {
-            Path path = new Path(file);
-            FileSystem localFs = FileSystem.getLocal(config_);
-            Path qualifiedPath = path.makeQualified(
-                localFs.getUri(), localFs.getWorkingDirectory());
-            validate(qualifiedPath);
-            String finalPath = qualifiedPath.toString();
-            if(fileList.length() > 0) {
-              fileList.append(',');
-            }
-            fileList.append(finalPath);
-          } catch (Exception e) {
-            throw new IllegalArgumentException(e);
-          }
-        }
-        String tmpFiles = config_.get("tmpfiles", "");
-        if (tmpFiles.isEmpty()) {
-          tmpFiles = fileList.toString();
-        } else {
-          tmpFiles = tmpFiles + "," + fileList;
-        }
-        config_.set("tmpfiles", tmpFiles);
-      }
-
-      String fsName = cmdLine.getOptionValue("dfs");
-      if (null != fsName){
-        LOG.warn("-dfs option is deprecated, please use -fs instead.");
-        config_.set("fs.default.name", fsName);
-      }
-
-      additionalConfSpec_ = cmdLine.getOptionValue("additionalconfspec");
-      inputFormatSpec_ = cmdLine.getOptionValue("inputformat");
-      outputFormatSpec_ = cmdLine.getOptionValue("outputformat");
-      numReduceTasksSpec_ = cmdLine.getOptionValue("numReduceTasks");
-      partitionerSpec_ = cmdLine.getOptionValue("partitioner");
-      inReaderSpec_ = cmdLine.getOptionValue("inputreader");
-      mapDebugSpec_ = cmdLine.getOptionValue("mapdebug");
-      reduceDebugSpec_ = cmdLine.getOptionValue("reducedebug");
-      ioSpec_ = cmdLine.getOptionValue("io");
-
-      String[] car = cmdLine.getOptionValues("cacheArchive");
-      if (null != car && car.length > 0){
-        LOG.warn("-cacheArchive option is deprecated, please use -archives instead.");
-        for(String s : car){
-          cacheArchives = (cacheArchives == null)?s :cacheArchives + "," + s;
-        }
-      }
-
-      String[] caf = cmdLine.getOptionValues("cacheFile");
-      if (null != caf && caf.length > 0){
-        LOG.warn("-cacheFile option is deprecated, please use -files instead.");
-        for(String s : caf){
-          cacheFiles = (cacheFiles == null)?s :cacheFiles + "," + s;
-        }
-      }
-
-      String[] jobconf = cmdLine.getOptionValues("jobconf");
-      if (null != jobconf && jobconf.length > 0){
-        LOG.warn("-jobconf option is deprecated, please use -D instead.");
-        for(String s : jobconf){
-          String[] parts = s.split("=", 2);
-          config_.set(parts[0], parts[1]);
-        }
-      }
-
-      String[] cmd = cmdLine.getOptionValues("cmdenv");
-      if (null != cmd && cmd.length > 0){
-        for(String s : cmd) {
-          if (addTaskEnvironment_.length() > 0) {
-            addTaskEnvironment_ += " ";
-          }
-          addTaskEnvironment_ += s;
-        }
-      }
-    } else {
-      exitUsage(argv_.length > 0 && "-info".equals(argv_[0]));
-    }
-  }
-
-  protected void msg(String msg) {
-    if (verbose_) {
-      System.out.println("STREAM: " + msg);
-    }
-  }
-
-  private Option createOption(String name, String desc,
-                              String argName, int max, boolean required){
-    return Option.builder(name)
-           .argName(argName)
-           .hasArgs()
-           .numberOfArgs(max)
-           .desc(desc)
-           .required(required)
-           .build();
-  }
-
-  private Option createBoolOption(String name, String desc){
-    return Option.builder(name).desc(desc).build();
-  }
-
-  private void validate(final Path path) throws IOException {
-    try {
-      path.getFileSystem(config_).access(path, FsAction.READ);
-    } catch (FileNotFoundException e) {
-      fail("File: " + path + " does not exist.");
-    } catch (AccessControlException e) {
-      fail("File: " + path + " is not readable.");
-    }
-  }
-
-  private void setupOptions(){
-
-    // input and output are not required for -info and -help options,
-    // though they are required for streaming job to be run.
-    Option input   = createOption("input",
-                                  "DFS input file(s) for the Map step",
-                                  "path",
-                                  Integer.MAX_VALUE,
-                                  false);
-
-    Option output  = createOption("output",
-                                  "DFS output directory for the Reduce step",
-                                  "path", 1, false);
-    Option mapper  = createOption("mapper",
-                                  "The streaming command to run", "cmd", 1, false);
-    Option combiner = createOption("combiner",
-                                   "The streaming command to run", "cmd", 1, false);
-    // reducer could be NONE
-    Option reducer = createOption("reducer",
-                                  "The streaming command to run", "cmd", 1, false);
-    Option file = createOption("file",
-                               "File to be shipped in the Job jar file",
-                               "file", Integer.MAX_VALUE, false);
-    Option dfs = createOption("dfs",
-                              "Optional. Override DFS configuration", "<h:p>|local", 1, false);
-    Option additionalconfspec = createOption("additionalconfspec",
-                                             "Optional.", "spec", 1, false);
-    Option inputformat = createOption("inputformat",
-                                      "Optional.", "spec", 1, false);
-    Option outputformat = createOption("outputformat",
-                                       "Optional.", "spec", 1, false);
-    Option partitioner = createOption("partitioner",
-                                      "Optional.", "spec", 1, false);
-    Option numReduceTasks = createOption("numReduceTasks",
-        "Optional.", "spec",1, false );
-    Option inputreader = createOption("inputreader",
-                                      "Optional.", "spec", 1, false);
-    Option mapDebug = createOption("mapdebug",
-                                   "Optional.", "spec", 1, false);
-    Option reduceDebug = createOption("reducedebug",
-                                      "Optional", "spec",1, false);
-    Option jobconf =
-      createOption("jobconf",
-                   "(n=v) Optional. Add or override a JobConf property.",
-                   "spec", 1, false);
-
-    Option cmdenv =
-      createOption("cmdenv", "(n=v) Pass env.var to streaming commands.",
-                   "spec", 1, false);
-    Option cacheFile = createOption("cacheFile",
-                                    "File name URI", "fileNameURI", Integer.MAX_VALUE, false);
-    Option cacheArchive = createOption("cacheArchive",
-                                       "File name URI", "fileNameURI", Integer.MAX_VALUE, false);
-    Option io = createOption("io",
-                             "Optional.", "spec", 1, false);
-
-    // boolean properties
-
-    Option background = createBoolOption("background", "Submit the job and don't wait till it completes.");
-    Option verbose = createBoolOption("verbose", "print verbose output");
-    Option info = createBoolOption("info", "print verbose output");
-    Option help = createBoolOption("help", "print this help message");
-    Option debug = createBoolOption("debug", "print debug output");
-    Option lazyOutput = createBoolOption("lazyOutput", "create outputs lazily");
-
-    allOptions = new Options().
-      addOption(input).
-      addOption(output).
-      addOption(mapper).
-      addOption(combiner).
-      addOption(reducer).
-      addOption(file).
-      addOption(dfs).
-      addOption(additionalconfspec).
-      addOption(inputformat).
-      addOption(outputformat).
-      addOption(partitioner).
-      addOption(numReduceTasks).
-      addOption(inputreader).
-      addOption(mapDebug).
-      addOption(reduceDebug).
-      addOption(jobconf).
-      addOption(cmdenv).
-      addOption(cacheFile).
-      addOption(cacheArchive).
-      addOption(io).
-      addOption(background).
-      addOption(verbose).
-      addOption(info).
-      addOption(debug).
-      addOption(help).
-      addOption(lazyOutput);
-  }
-
-  public void exitUsage(boolean detailed) {
-    printUsage(detailed);
-    fail("");
-  }
-
-  private void printUsage(boolean detailed) {
-    System.out.println("Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar"
-        + " [options]");
-    System.out.println("Options:");
-    System.out.println("  -input          <path> DFS input file(s) for the Map"
-        + " step.");
-    System.out.println("  -output         <path> DFS output directory for the"
-        + " Reduce step.");
-    System.out.println("  -mapper         <cmd|JavaClassName> Optional. Command"
-        + " to be run as mapper.");
-    System.out.println("  -combiner       <cmd|JavaClassName> Optional. Command"
-        + " to be run as combiner.");
-    System.out.println("  -reducer        <cmd|JavaClassName> Optional. Command"
-        + " to be run as reducer.");
-    System.out.println("  -file           <file> Optional. File/dir to be "
-        + "shipped in the Job jar file.\n" +
-        "                  Deprecated. Use generic option \"-files\" instead.");
-    System.out.println("  -inputformat    <TextInputFormat(default)"
-        + "|SequenceFileAsTextInputFormat|JavaClassName>\n"
-        + "                  Optional. The input format class.");
-    System.out.println("  -outputformat   <TextOutputFormat(default)"
-        + "|JavaClassName>\n"
-        + "                  Optional. The output format class.");
-    System.out.println("  -partitioner    <JavaClassName>  Optional. The"
-        + " partitioner class.");
-    System.out.println("  -numReduceTasks <num> Optional. Number of reduce "
-        + "tasks.");
-    System.out.println("  -inputreader    <spec> Optional. Input recordreader"
-        + " spec.");
-    System.out.println("  -cmdenv         <n>=<v> Optional. Pass env.var to"
-        + " streaming commands.");
-    System.out.println("  -mapdebug       <cmd> Optional. "
-        + "To run this script when a map task fails.");
-    System.out.println("  -reducedebug    <cmd> Optional."
-        + " To run this script when a reduce task fails.");
-    System.out.println("  -io             <identifier> Optional. Format to use"
-        + " for input to and output");
-    System.out.println("                  from mapper/reducer commands");
-    System.out.println("  -lazyOutput     Optional. Lazily create Output.");
-    System.out.println("  -background     Optional. Submit the job and don't wait till it completes.");
-    System.out.println("  -verbose        Optional. Print verbose output.");
-    System.out.println("  -info           Optional. Print detailed usage.");
-    System.out.println("  -help           Optional. Print help message.");
-    System.out.println();
-    GenericOptionsParser.printGenericCommandUsage(System.out);
-
-    if (!detailed) {
-      System.out.println();
-      System.out.println("For more details about these options:");
-      System.out.println("Use " +
-          "$HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info");
-      return;
-    }
-    System.out.println();
-    System.out.println("Usage tips:");
-    System.out.println("In -input: globbing on <path> is supported and can "
-        + "have multiple -input");
-    System.out.println();
-    System.out.println("Default Map input format: a line is a record in UTF-8 "
-        + "the key part ends at first");
-    System.out.println("  TAB, the rest of the line is the value");
-    System.out.println();
-    System.out.println("To pass a Custom input format:");
-    System.out.println("  -inputformat package.MyInputFormat");
-    System.out.println();
-    System.out.println("Similarly, to pass a custom output format:");
-    System.out.println("  -outputformat package.MyOutputFormat");
-    System.out.println();
-    System.out.println("The files with extensions .class and .jar/.zip," +
-        " specified for the -file");
-    System.out.println("  argument[s], end up in \"classes\" and \"lib\" " +
-        "directories respectively inside");
-    System.out.println("  the working directory when the mapper and reducer are"
-        + " run. All other files");
-    System.out.println("  specified for the -file argument[s]" +
-        " end up in the working directory when the");
-    System.out.println("  mapper and reducer are run. The location of this " +
-        "working directory is");
-    System.out.println("  unspecified.");
-    System.out.println();
-    System.out.println("To set the number of reduce tasks (num. of output " +
-        "files) as, say 10:");
-    System.out.println("  Use -numReduceTasks 10");
-    System.out.println("To skip the sort/combine/shuffle/sort/reduce step:");
-    System.out.println("  Use -numReduceTasks 0");
-    System.out.println("  Map output then becomes a 'side-effect " +
-        "output' rather than a reduce input.");
-    System.out.println("  This speeds up processing. This also feels " +
-        "more like \"in-place\" processing");
-    System.out.println("  because the input filename and the map " +
-        "input order are preserved.");
-    System.out.println("  This is equivalent to -reducer NONE");
-    System.out.println();
-    System.out.println("To speed up the last maps:");
-    System.out.println("  -D " + MRJobConfig.MAP_SPECULATIVE + "=true");
-    System.out.println("To speed up the last reduces:");
-    System.out.println("  -D " + MRJobConfig.REDUCE_SPECULATIVE + "=true");
-    System.out.println("To name the job (appears in the JobTracker Web UI):");
-    System.out.println("  -D " + MRJobConfig.JOB_NAME + "='My Job'");
-    System.out.println("To change the local temp directory:");
-    System.out.println("  -D dfs.data.dir=/tmp/dfs");
-    System.out.println("  -D stream.tmpdir=/tmp/streaming");
-    System.out.println("Additional local temp directories with -jt local:");
-    System.out.println("  -D " + MRConfig.LOCAL_DIR + "=/tmp/local");
-    System.out.println("  -D " + JTConfig.JT_SYSTEM_DIR + "=/tmp/system");
-    System.out.println("  -D " + MRConfig.TEMP_DIR + "=/tmp/temp");
-    System.out.println("To treat tasks with non-zero exit status as SUCCEDED:");
-    System.out.println("  -D stream.non.zero.exit.is.failure=false");
-    System.out.println("Use a custom hadoop streaming build along with standard"
-        + " hadoop install:");
-    System.out.println("  $HADOOP_HOME/bin/hadoop jar " +
-        "/path/my-hadoop-streaming.jar [...]\\");
-    System.out.println("    [...] -D stream.shipped.hadoopstreaming=" +
-        "/path/my-hadoop-streaming.jar");
-    System.out.println("For more details about jobconf parameters see:");
-    System.out.println("  http://wiki.apache.org/hadoop/JobConfFile");
-    System.out.println("Truncate the values of the job configuration copied" +
-        "to the environment at the given length:");
-    System.out.println("   -D stream.jobconf.truncate.limit=-1");
-    System.out.println("To set an environment variable in a streaming " +
-        "command:");
-    System.out.println("   -cmdenv EXAMPLE_DIR=/home/example/dictionaries/");
-    System.out.println();
-    System.out.println("Shortcut:");
-    System.out.println("   setenv HSTREAMING \"$HADOOP_HOME/bin/hadoop jar " +
-        "hadoop-streaming.jar\"");
-    System.out.println();
-    System.out.println("Example: $HSTREAMING -mapper " +
-        "\"/usr/local/bin/perl5 filter.pl\"");
-    System.out.println("           -file /local/filter.pl -input " +
-        "\"/logs/0604*/*\" [...]");
-    System.out.println("  Ships a script, invokes the non-shipped perl " +
-        "interpreter. Shipped files go to");
-    System.out.println("  the working directory so filter.pl is found by perl. "
-        + "Input files are all the");
-    System.out.println("  daily logs for days in month 2006-04");
-  }
-
-  public void fail(String message) {
-    System.err.println(message);
-    System.err.println("Try -help for more information");
-    throw new IllegalArgumentException(message);
-  }
-
-  // --------------------------------------------
-
-  protected String getHadoopClientHome() {
-    String h = env_.getProperty("HADOOP_HOME"); // standard Hadoop
-    if (h == null) {
-      //fail("Missing required environment variable: HADOOP_HOME");
-      h = "UNDEF";
-    }
-    return h;
-  }
-
-  protected boolean isLocalHadoop() {
-    return StreamUtil.isLocalJobTracker(jobConf_);
-  }
-
-  @Deprecated
-  protected String getClusterNick() {
-    return "default";
-  }
-
-  /** @return path to the created Jar file or null if no files are necessary.
-   */
-  protected String packageJobJar() throws IOException {
-    ArrayList<String> unjarFiles = new ArrayList<String>();
-
-    // Runtime code: ship same version of code as self (job submitter code)
-    // usually found in: build/contrib or build/hadoop-<version>-dev-streaming.jar
-
-    // First try an explicit spec: it's too hard to find our own location in this case:
-    // $HADOOP_HOME/bin/hadoop jar /not/first/on/classpath/custom-hadoop-streaming.jar
-    // where findInClasspath() would find the version of hadoop-streaming.jar in $HADOOP_HOME
-    String runtimeClasses = config_.get("stream.shipped.hadoopstreaming"); // jar or class dir
-
-    if (runtimeClasses == null) {
-      runtimeClasses = StreamUtil.findInClasspath(StreamJob.class.getName());
-    }
-    if (runtimeClasses == null) {
-      throw new IOException("runtime classes not found: " + getClass().getPackage());
-    } else {
-      msg("Found runtime classes in: " + runtimeClasses);
-    }
-    if (isLocalHadoop()) {
-      // don't package class files (they might get unpackaged in "." and then
-      //  hide the intended CLASSPATH entry)
-      // we still package everything else (so that scripts and executable are found in
-      //  Task workdir like distributed Hadoop)
-    } else {
-      if (new File(runtimeClasses).isDirectory()) {
-        packageFiles_.add(runtimeClasses);
-      } else {
-        unjarFiles.add(runtimeClasses);
-      }
-    }
-    if (packageFiles_.size() + unjarFiles.size() == 0) {
-      return null;
-    }
-    String tmp = jobConf_.get("stream.tmpdir"); //, "/tmp/${mapreduce.job.user.name}/"
-    File tmpDir = (tmp == null) ? null : new File(tmp);
-    // tmpDir=null means OS default tmp dir
-    File jobJar = File.createTempFile("streamjob", ".jar", tmpDir);
-    System.out.println("packageJobJar: " + packageFiles_ + " " + unjarFiles + " " + jobJar
-                       + " tmpDir=" + tmpDir);
-    if (debug_ == 0) {
-      jobJar.deleteOnExit();
-    }
-    JarBuilder builder = new JarBuilder();
-    if (verbose_) {
-      builder.setVerbose(true);
-    }
-    String jobJarName = jobJar.getAbsolutePath();
-    builder.merge(packageFiles_, unjarFiles, jobJarName);
-    return jobJarName;
-  }
-
-  /**
-   * get the uris of all the files/caches
-   */
-  protected void getURIs(String lcacheArchives, String lcacheFiles) {
-    String archives[] = StringUtils.getStrings(lcacheArchives);
-    String files[] = StringUtils.getStrings(lcacheFiles);
-    fileURIs = StringUtils.stringToURI(files);
-    archiveURIs = StringUtils.stringToURI(archives);
-  }
-
-  protected void setJobConf() throws IOException {
-    if (additionalConfSpec_ != null) {
-      LOG.warn("-additionalconfspec option is deprecated, please use -conf instead.");
-      config_.addResource(new Path(additionalConfSpec_));
-    }
-
-    // general MapRed job properties
-    jobConf_ = new JobConf(config_, StreamJob.class);
-
-    // All streaming jobs get the task timeout value
-    // from the configuration settings.
-
-    // The correct FS must be set before this is called!
-    // (to resolve local vs. dfs drive letter differences)
-    // (mapreduce.job.working.dir will be lazily initialized ONCE and depends on FS)
-    for (int i = 0; i < inputSpecs_.size(); i++) {
-      FileInputFormat.addInputPaths(jobConf_,
-                        (String) inputSpecs_.get(i));
-    }
-
-    String defaultPackage = this.getClass().getPackage().getName();
-    Class c;
-    Class fmt = null;
-    if (inReaderSpec_ == null && inputFormatSpec_ == null) {
-      fmt = TextInputFormat.class;
-    } else if (inputFormatSpec_ != null) {
-      if (inputFormatSpec_.equals(TextInputFormat.class.getName())
-          || inputFormatSpec_.equals(TextInputFormat.class.getCanonicalName())
-          || inputFormatSpec_.equals(TextInputFormat.class.getSimpleName())) {
-        fmt = TextInputFormat.class;
-      } else if (inputFormatSpec_.equals(KeyValueTextInputFormat.class
-          .getName())
-          || inputFormatSpec_.equals(KeyValueTextInputFormat.class
-              .getCanonicalName())
-          || inputFormatSpec_.equals(KeyValueTextInputFormat.class.getSimpleName())) {
-        if (inReaderSpec_ == null) {
-          fmt = KeyValueTextInputFormat.class;
-        }
-      } else if (inputFormatSpec_.equals(SequenceFileInputFormat.class
-          .getName())
-          || inputFormatSpec_
-              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class
-                  .getCanonicalName())
-          || inputFormatSpec_
-              .equals(org.apache.hadoop.mapred.SequenceFileInputFormat.class.getSimpleName())) {
-        if (inReaderSpec_ == null) {
-          fmt = SequenceFileInputFormat.class;
-        }
-      } else if (inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class
-          .getName())
-          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class
-              .getCanonicalName())
-          || inputFormatSpec_.equals(SequenceFileAsTextInputFormat.class.getSimpleName())) {
-        fmt = SequenceFileAsTextInputFormat.class;
-      } else {
-        c = StreamUtil.goodClassOrNull(jobConf_, inputFormatSpec_, defaultPackage);
-        if (c != null) {
-          fmt = c;
-        } else {
-          fail("-inputformat : class not found : " + inputFormatSpec_);
-        }
-      }
-    }
-    if (fmt == null) {
-      fmt = StreamInputFormat.class;
-    }
-
-    jobConf_.setInputFormat(fmt);
-
-    if (ioSpec_ != null) {
-      jobConf_.set("stream.map.input", ioSpec_);
-      jobConf_.set("stream.map.output", ioSpec_);
-      jobConf_.set("stream.reduce.input", ioSpec_);
-      jobConf_.set("stream.reduce.output", ioSpec_);
-    }
-
-    Class<? extends IdentifierResolver> idResolverClass =
-      jobConf_.getClass("stream.io.identifier.resolver.class",
-        IdentifierResolver.class, IdentifierResolver.class);
-    IdentifierResolver idResolver = ReflectionUtils.newInstance(idResolverClass, jobConf_);
-
-    idResolver.resolve(jobConf_.get("stream.map.input", IdentifierResolver.TEXT_ID));
-    jobConf_.setClass("stream.map.input.writer.class",
-      idResolver.getInputWriterClass(), InputWriter.class);
-
-    idResolver.resolve(jobConf_.get("stream.reduce.input", IdentifierResolver.TEXT_ID));
-    jobConf_.setClass("stream.reduce.input.writer.class",
-      idResolver.getInputWriterClass(), InputWriter.class);
-
-    jobConf_.set("stream.addenvironment", addTaskEnvironment_);
-
-    boolean isMapperACommand = false;
-    if (mapCmd_ != null) {
-      c = StreamUtil.goodClassOrNull(jobConf_, mapCmd_, defaultPackage);
-      if (c != null) {
-        jobConf_.setMapperClass(c);
-      } else {
-        isMapperACommand = true;
-        jobConf_.setMapperClass(PipeMapper.class);
-        jobConf_.setMapRunnerClass(PipeMapRunner.class);
-        jobConf_.set("stream.map.streamprocessor",
-                     URLEncoder.encode(mapCmd_, "UTF-8"));
-      }
-    }
-
-    if (comCmd_ != null) {
-      c = StreamUtil.goodClassOrNull(jobConf_, comCmd_, defaultPackage);
-      if (c != null) {
-        jobConf_.setCombinerClass(c);
-      } else {
-        jobConf_.setCombinerClass(PipeCombiner.class);
-        jobConf_.set("stream.combine.streamprocessor", URLEncoder.encode(
-                comCmd_, "UTF-8"));
-      }
-    }
-
-    if (numReduceTasksSpec_!= null) {
-      int numReduceTasks = Integer.parseInt(numReduceTasksSpec_);
-      jobConf_.setNumReduceTasks(numReduceTasks);
-    }
-
-    boolean isReducerACommand = false;
-    if (redCmd_ != null) {
-      if (redCmd_.equals(REDUCE_NONE)) {
-        jobConf_.setNumReduceTasks(0);
-      }
-      if (jobConf_.getNumReduceTasks() != 0) {
-        if (redCmd_.compareToIgnoreCase("aggregate") == 0) {
-          jobConf_.setReducerClass(ValueAggregatorReducer.class);
-          jobConf_.setCombinerClass(ValueAggregatorCombiner.class);
-        } else {
-
-          c = StreamUtil.goodClassOrNull(jobConf_, redCmd_, defaultPackage);
-          if (c != null) {
-            jobConf_.setReducerClass(c);
-          } else {
-            isReducerACommand = true;
-            jobConf_.setReducerClass(PipeReducer.class);
-            jobConf_.set("stream.reduce.streamprocessor", URLEncoder.encode(
-                redCmd_, "UTF-8"));
-          }
-        }
-      }
-    }
-
-    idResolver.resolve(jobConf_.get("stream.map.output",
-        IdentifierResolver.TEXT_ID));
-    jobConf_.setClass("stream.map.output.reader.class",
-      idResolver.getOutputReaderClass(), OutputReader.class);
-    if (isMapperACommand || jobConf_.get("stream.map.output") != null) {
-      // if mapper is a command, then map output key/value classes come from the
-      // idResolver
-      jobConf_.setMapOutputKeyClass(idResolver.getOutputKeyClass());
-      jobConf_.setMapOutputValueClass(idResolver.getOutputValueClass());
-
-      if (jobConf_.getNumReduceTasks() == 0) {
-        jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());
-        jobConf_.setOutputValueClass(idResolver.getOutputValueClass());
-      }
-    }
-
-    idResolver.resolve(jobConf_.get("stream.reduce.output",
-        IdentifierResolver.TEXT_ID));
-    jobConf_.setClass("stream.reduce.output.reader.class",
-      idResolver.getOutputReaderClass(), OutputReader.class);
-    if (isReducerACommand || jobConf_.get("stream.reduce.output") != null) {
-      // if reducer is a command, then output key/value classes come from the
-      // idResolver
-      jobConf_.setOutputKeyClass(idResolver.getOutputKeyClass());
-      jobConf_.setOutputValueClass(idResolver.getOutputValueClass());
-    }
-
-    if (inReaderSpec_ != null) {
-      String[] args = inReaderSpec_.split(",");
-      String readerClass = args[0];
-      // this argument can only be a Java class
-      c = StreamUtil.goodClassOrNull(jobConf_, readerClass, defaultPackage);
-      if (c != null) {
-        jobConf_.set("stream.recordreader.class", c.getName());
-      } else {
-        fail("-inputreader: class not found: " + readerClass);
-      }
-      for (int i = 1; i < args.length; i++) {
-        String[] nv = args[i].split("=", 2);
-        String k = "stream.recordreader." + nv[0];
-        String v = (nv.length > 1) ? nv[1] : "";
-        jobConf_.set(k, v);
-      }
-    }
-
-    FileOutputFormat.setOutputPath(jobConf_, new Path(output_));
-    fmt = null;
-    if (outputFormatSpec_!= null) {
-      c = StreamUtil.goodClassOrNull(jobConf_, outputFormatSpec_, defaultPackage);
-      if (c != null) {
-        fmt = c;
-      } else {
-        fail("-outputformat : class not found : " + outputFormatSpec_);
-      }
-    }
-    if (fmt == null) {
-      fmt = TextOutputFormat.class;
-    }
-    if (lazyOutput_) {
-      LazyOutputFormat.setOutputFormatClass(jobConf_, fmt);
-    } else {
-      jobConf_.setOutputFormat(fmt);
-    }
-
-    if (partitionerSpec_!= null) {
-      c = StreamUtil.goodClassOrNull(jobConf_, partitionerSpec_, defaultPackage);
-      if (c != null) {
-        jobConf_.setPartitionerClass(c);
-      } else {
-        fail("-partitioner : class not found : " + partitionerSpec_);
-      }
-    }
-
-    if(mapDebugSpec_ != null){
-    	jobConf_.setMapDebugScript(mapDebugSpec_);
-    }
-    if(reduceDebugSpec_ != null){
-    	jobConf_.setReduceDebugScript(reduceDebugSpec_);
-    }
-    // last, allow user to override anything
-    // (although typically used with properties we didn't touch)
-
-    jar_ = packageJobJar();
-    if (jar_ != null) {
-      jobConf_.setJar(jar_);
-    }
-
-    if ((cacheArchives != null) || (cacheFiles != null)){
-      getURIs(cacheArchives, cacheFiles);
-      boolean b = DistributedCache.checkURIs(fileURIs, archiveURIs);
-      if (!b)
-        fail(LINK_URI);
-    }
-    // set the jobconf for the caching parameters
-    if (cacheArchives != null) {
-      Job.setCacheArchives(archiveURIs, jobConf_);
-    }
-    if (cacheFiles != null) {
-      Job.setCacheFiles(fileURIs, jobConf_);
-    }
-
-    if (verbose_) {
-      listJobConfProperties();
-    }
-
-    msg("submitting to jobconf: " + getJobTrackerHostPort());
-  }
-
-  /**
-   * Prints out the jobconf properties on stdout
-   * when verbose is specified.
-   */
-  protected void listJobConfProperties()
-  {
-    msg("==== JobConf properties:");
-    TreeMap<String,String> sorted = new TreeMap<String,String>();
-    for (final Map.Entry<String, String> en : jobConf_)  {
-      sorted.put(en.getKey(), en.getValue());
-    }
-    for (final Map.Entry<String,String> en: sorted.entrySet()) {
-      msg(en.getKey() + "=" + en.getValue());
-    }
-    msg("====");
-  }
-
-  protected String getJobTrackerHostPort() {
-    return jobConf_.get(JTConfig.JT_IPC_ADDRESS);
-  }
-
-  // Based on JobClient
-  public int submitAndMonitorJob() throws IOException {
-
-    if (jar_ != null && isLocalHadoop()) {
-      // getAbs became required when shell and subvm have different working dirs...
-      File wd = new File(".").getAbsoluteFile();
-      RunJar.unJar(new File(jar_), wd, MATCH_ANY);
-    }
-
-    // if jobConf_ changes must recreate a JobClient
-    jc_ = new JobClient(jobConf_);
-    running_ = null;
-    try {
-      running_ = jc_.submitJob(jobConf_);
-      jobId_ = running_.getID();
-      if (background_) {
-        LOG.info("Job is running in background.");
-      } else if (!jc_.monitorAndPrintJob(jobConf_, running_)) {
-        LOG.error("Job not successful!");
-        return 1;
-      }
-      LOG.info("Output directory: " + output_);
-    } catch(FileNotFoundException fe) {
-      LOG.error("Error launching job , bad input path : " + fe.getMessage());
-      return 2;
-    } catch(InvalidJobConfException je) {
-      LOG.error("Error launching job , Invalid job conf : " + je.getMessage());
-      return 3;
-    } catch(FileAlreadyExistsException fae) {
-      LOG.error("Error launching job , Output path already exists : "
-                + fae.getMessage());
-      return 4;
-    } catch(IOException ioe) {
-      LOG.error("Error Launching job : " + ioe.getMessage());
-      return 5;
-    } catch (InterruptedException ie) {
-      LOG.error("Error monitoring job : " + ie.getMessage());
-      return 6;
-    } finally {
-      jc_.close();
-    }
-    return 0;
-  }
-
-  protected String[] argv_;
-  protected boolean background_;
-  protected boolean verbose_;
-  protected boolean detailedUsage_;
-  protected boolean printUsage = false;
-  protected int debug_;
-
-  protected Environment env_;
-
-  protected String jar_;
-  protected boolean localHadoop_;
-  protected Configuration config_;
-  protected JobConf jobConf_;
-  protected JobClient jc_;
-
-  // command-line arguments
-  protected ArrayList<String> inputSpecs_ = new ArrayList<String>();
-  protected TreeSet<String> seenPrimary_ = new TreeSet<String>();
-  protected boolean hasSimpleInputSpecs_;
-  protected ArrayList<String> packageFiles_ = new ArrayList<String>();
-  protected ArrayList<String> shippedCanonFiles_ = new ArrayList<String>();
-  //protected TreeMap<String, String> userJobConfProps_ = new TreeMap<String, String>();
-  protected String output_;
-  protected String mapCmd_;
-  protected String comCmd_;
-  protected String redCmd_;
-  protected String cacheFiles;
-  protected String cacheArchives;
-  protected URI[] fileURIs;
-  protected URI[] archiveURIs;
-  protected String inReaderSpec_;
-  protected String inputFormatSpec_;
-  protected String outputFormatSpec_;
-  protected String partitionerSpec_;
-  protected String numReduceTasksSpec_;
-  protected String additionalConfSpec_;
-  protected String mapDebugSpec_;
-  protected String reduceDebugSpec_;
-  protected String ioSpec_;
-  protected boolean lazyOutput_;
-
-  // Use to communicate config to the external processes (ex env.var.HADOOP_USER)
-  // encoding "a=b c=d"
-  protected String addTaskEnvironment_;
-
-  protected boolean outputSingleNode_;
-  protected long minRecWrittenToEnableSkip_;
-
-  protected RunningJob running_;
-  protected JobID jobId_;
-  protected static final String LINK_URI = "You need to specify the uris as scheme://path#linkname," +
-    "Please specify a different link name for all of your caching URIs";
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamKeyValUtil.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamKeyValUtil.java
deleted file mode 100644
index fba45b11f24..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamKeyValUtil.java
+++ /dev/null
@@ -1,137 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.IOException;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.util.LineReader;
-
-public class StreamKeyValUtil {
-
-  /**
-   * Find the first occurred tab in a UTF-8 encoded string
-   * @param utf a byte array containing a UTF-8 encoded string
-   * @param start starting offset
-   * @param length no. of bytes
-   * @return position that first tab occurres otherwise -1
-   */
-  public static int findTab(byte [] utf, int start, int length) {
-    for(int i=start; i<(start+length); i++) {
-      if (utf[i]==(byte)'\t') {
-        return i;
-      }
-    }
-    return -1;      
-  }
-  /**
-   * Find the first occurred tab in a UTF-8 encoded string
-   * @param utf a byte array containing a UTF-8 encoded string
-   * @return position that first tab occurres otherwise -1
-   */
-  public static int findTab(byte [] utf) {
-    return org.apache.hadoop.util.UTF8ByteArrayUtils.findNthByte(utf, 0, 
-        utf.length, (byte)'\t', 1);
-  }
-
-  /**
-   * split a UTF-8 byte array into key and value 
-   * assuming that the delimilator is at splitpos. 
-   * @param utf utf-8 encoded string
-   * @param start starting offset
-   * @param length no. of bytes
-   * @param key contains key upon the method is returned
-   * @param val contains value upon the method is returned
-   * @param splitPos the split pos
-   * @param separatorLength the length of the separator between key and value
-   * @throws IOException
-   */
-  public static void splitKeyVal(byte[] utf, int start, int length, 
-                                 Text key, Text val, int splitPos,
-                                 int separatorLength) throws IOException {
-    if (splitPos<start || splitPos >= (start+length))
-      throw new IllegalArgumentException("splitPos must be in the range " +
-                                         "[" + start + ", " + (start+length) + "]: " + splitPos);
-    int keyLen = (splitPos-start);
-    int valLen = (start+length)-splitPos-separatorLength;
-    key.set(utf, start, keyLen);
-    val.set(utf, splitPos+separatorLength, valLen);
-  }
-
-  /**
-   * split a UTF-8 byte array into key and value 
-   * assuming that the delimilator is at splitpos. 
-   * @param utf utf-8 encoded string
-   * @param start starting offset
-   * @param length no. of bytes
-   * @param key contains key upon the method is returned
-   * @param val contains value upon the method is returned
-   * @param splitPos the split pos
-   * @throws IOException
-   */
-  public static void splitKeyVal(byte[] utf, int start, int length, 
-                                 Text key, Text val, int splitPos) throws IOException {
-    splitKeyVal(utf, start, length, key, val, splitPos, 1);
-  }
-  
-
-  /**
-   * split a UTF-8 byte array into key and value 
-   * assuming that the delimilator is at splitpos. 
-   * @param utf utf-8 encoded string
-   * @param key contains key upon the method is returned
-   * @param val contains value upon the method is returned
-   * @param splitPos the split pos
-   * @param separatorLength the length of the separator between key and value
-   * @throws IOException
-   */
-  public static void splitKeyVal(byte[] utf, Text key, Text val, int splitPos, 
-                                 int separatorLength) 
-    throws IOException {
-    splitKeyVal(utf, 0, utf.length, key, val, splitPos, separatorLength);
-  }
-
-  /**
-   * split a UTF-8 byte array into key and value 
-   * assuming that the delimilator is at splitpos. 
-   * @param utf utf-8 encoded string
-   * @param key contains key upon the method is returned
-   * @param val contains value upon the method is returned
-   * @param splitPos the split pos
-   * @throws IOException
-   */
-  public static void splitKeyVal(byte[] utf, Text key, Text val, int splitPos) 
-    throws IOException {
-    splitKeyVal(utf, 0, utf.length, key, val, splitPos, 1);
-  }
-  
-  /**
-   * Read a utf8 encoded line from a data input stream. 
-   * @param lineReader LineReader to read the line from.
-   * @param out Text to read into
-   * @return number of bytes read 
-   * @throws IOException
-   */
-  public static int readLine(LineReader lineReader, Text out) 
-  throws IOException {
-    out.clear();
-    return lineReader.readLine(out);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamUtil.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamUtil.java
deleted file mode 100644
index a6983e1c6c3..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamUtil.java
+++ /dev/null
@@ -1,205 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.net.InetAddress;
-import java.net.URL;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.MRConfig;
-
-/** 
- * Utilities used in streaming
- */
-@InterfaceAudience.Private
-public class StreamUtil {
-
-  /** It may seem strange to silently switch behaviour when a String
-   * is not a classname; the reason is simplified Usage:<pre>
-   * -mapper [classname | program ]
-   * instead of the explicit Usage:
-   * [-mapper program | -javamapper classname], -mapper and -javamapper are mutually exclusive.
-   * (repeat for -reducer, -combiner) </pre>
-   */
-  public static Class goodClassOrNull(Configuration conf, String className, String defaultPackage) {
-    Class clazz = null;
-    try {
-      clazz = conf.getClassByName(className);
-    } catch (ClassNotFoundException cnf) {
-    }
-    if (clazz == null) {
-      if (className.indexOf('.') == -1 && defaultPackage != null) {
-        className = defaultPackage + "." + className;
-        try {
-          clazz = conf.getClassByName(className);
-        } catch (ClassNotFoundException cnf) {
-        }
-      }
-    }
-    return clazz;
-  }
-
-  public static String findInClasspath(String className) {
-    return findInClasspath(className, StreamUtil.class.getClassLoader());
-  }
-
-  /** @return a jar file path or a base directory or null if not found.
-   */
-  public static String findInClasspath(String className, ClassLoader loader) {
-
-    String relPath = className;
-    relPath = relPath.replace('.', '/');
-    relPath += ".class";
-    java.net.URL classUrl = loader.getResource(relPath);
-
-    String codePath;
-    if (classUrl != null) {
-      boolean inJar = classUrl.getProtocol().equals("jar");
-      codePath = classUrl.toString();
-      if (codePath.startsWith("jar:")) {
-        codePath = codePath.substring("jar:".length());
-      }
-      if (codePath.startsWith("file:")) { // can have both
-        codePath = codePath.substring("file:".length());
-      }
-      if (inJar) {
-        // A jar spec: remove class suffix in /path/my.jar!/package/Class
-        int bang = codePath.lastIndexOf('!');
-        codePath = codePath.substring(0, bang);
-      } else {
-        // A class spec: remove the /my/package/Class.class portion
-        int pos = codePath.lastIndexOf(relPath);
-        if (pos == -1) {
-          throw new IllegalArgumentException("invalid codePath: className=" + className
-                                             + " codePath=" + codePath);
-        }
-        codePath = codePath.substring(0, pos);
-      }
-    } else {
-      codePath = null;
-    }
-    return codePath;
-  }
-
-  static String qualifyHost(String url) {
-    try {
-      return qualifyHost(new URL(url)).toString();
-    } catch (IOException io) {
-      return url;
-    }
-  }
-
-  static URL qualifyHost(URL url) {
-    try {
-      InetAddress a = InetAddress.getByName(url.getHost());
-      String qualHost = a.getCanonicalHostName();
-      URL q = new URL(url.getProtocol(), qualHost, url.getPort(), url.getFile());
-      return q;
-    } catch (IOException io) {
-      return url;
-    }
-  }
-
-  static final String regexpSpecials = "[]()?*+|.!^-\\~@";
-
-  public static String regexpEscape(String plain) {
-    StringBuffer buf = new StringBuffer();
-    char[] ch = plain.toCharArray();
-    int csup = ch.length;
-    for (int c = 0; c < csup; c++) {
-      if (regexpSpecials.indexOf(ch[c]) != -1) {
-        buf.append("\\");
-      }
-      buf.append(ch[c]);
-    }
-    return buf.toString();
-  }
-
-  static String slurp(File f) throws IOException {
-    int len = (int) f.length();
-    byte[] buf = new byte[len];
-    FileInputStream in = new FileInputStream(f);
-    String contents = null;
-    try {
-      in.read(buf, 0, len);
-      contents = new String(buf, StandardCharsets.UTF_8);
-    } finally {
-      in.close();
-    }
-    return contents;
-  }
-
-  static String slurpHadoop(Path p, FileSystem fs) throws IOException {
-    int len = (int) fs.getFileStatus(p).getLen();
-    byte[] buf = new byte[len];
-    FSDataInputStream in = fs.open(p);
-    String contents = null;
-    try {
-      in.readFully(in.getPos(), buf);
-      contents = new String(buf, StandardCharsets.UTF_8);
-    } finally {
-      in.close();
-    }
-    return contents;
-  }
-
-  static private Environment env;
-  private static String host;
-
-  public static String getHost(){
-    return host;
-  }
- 
-   
-  static {
-    try {
-      env = new Environment();
-      host = env.getHost();
-    } catch (IOException io) {
-      io.printStackTrace();
-    }
-  }
-
-  static Environment env() {
-    if (env != null) {
-      return env;
-    }
-    try {
-      env = new Environment();
-    } catch (IOException io) {
-      io.printStackTrace();
-    }
-    return env;
-  }
-
-  public static boolean isLocalJobTracker(JobConf job) {
-    String framework = 
-        job.get(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME); 
-    return framework.equals(MRConfig.LOCAL_FRAMEWORK_NAME);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamXmlRecordReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamXmlRecordReader.java
deleted file mode 100644
index 974cdc7c8d0..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/StreamXmlRecordReader.java
+++ /dev/null
@@ -1,302 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.nio.charset.StandardCharsets;
-import java.util.regex.*;
-
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.FileSplit;
-import org.apache.hadoop.mapred.JobConf;
-
-/** A way to interpret XML fragments as Mapper input records.
- *  Values are XML subtrees delimited by configurable tags.
- *  Keys could be the value of a certain attribute in the XML subtree, 
- *  but this is left to the stream processor application.
- *
- *  The name-value properties that StreamXmlRecordReader understands are:
- *    String begin (chars marking beginning of record)
- *    String end   (chars marking end of record)
- *    int maxrec   (maximum record size)
- *    int lookahead(maximum lookahead to sync CDATA)
- *    boolean slowmatch
- */
-public class StreamXmlRecordReader extends StreamBaseRecordReader {
-
-  public StreamXmlRecordReader(FSDataInputStream in, FileSplit split, Reporter reporter,
-                               JobConf job, FileSystem fs) throws IOException {
-    super(in, split, reporter, job, fs);
-
-    beginMark_ = checkJobGet(CONF_NS + "begin");
-    endMark_ = checkJobGet(CONF_NS + "end");
-
-    maxRecSize_ = job_.getInt(CONF_NS + "maxrec", 50 * 1000);
-    lookAhead_ = job_.getInt(CONF_NS + "lookahead", 2 * maxRecSize_);
-    synched_ = false;
-
-    slowMatch_ = job_.getBoolean(CONF_NS + "slowmatch", false);
-    if (slowMatch_) {
-      beginPat_ = makePatternCDataOrMark(beginMark_);
-      endPat_ = makePatternCDataOrMark(endMark_);
-    }
-    init();
-  }
-
-  public final void init() throws IOException {
-    LOG.info("StreamBaseRecordReader.init: " + " start_=" + start_ + " end_=" + end_ + " length_="
-             + length_ + " start_ > in_.getPos() =" + (start_ > in_.getPos()) + " " + start_ + " > "
-             + in_.getPos());
-    if (start_ > in_.getPos()) {
-      in_.seek(start_);
-    }
-    pos_ = start_;
-    bin_ = new BufferedInputStream(in_);
-    seekNextRecordBoundary();
-  }
-  
-  int numNext = 0;
-
-  public synchronized boolean next(Text key, Text value) throws IOException {
-    numNext++;
-    if (pos_ >= end_) {
-      return false;
-    }
-
-    DataOutputBuffer buf = new DataOutputBuffer();
-    if (!readUntilMatchBegin()) {
-      return false;
-    }
-    if (pos_ >= end_ || !readUntilMatchEnd(buf)) {
-      return false;
-    }
-
-    // There is only one elem..key/value splitting is not done here.
-    byte[] record = new byte[buf.getLength()];
-    System.arraycopy(buf.getData(), 0, record, 0, record.length);
-
-    numRecStats(record, 0, record.length);
-
-    key.set(record);
-    value.set("");
-
-    return true;
-  }
-
-  public void seekNextRecordBoundary() throws IOException {
-    readUntilMatchBegin();
-  }
-
-  boolean readUntilMatchBegin() throws IOException {
-    if (slowMatch_) {
-      return slowReadUntilMatch(beginPat_, false, null);
-    } else {
-      return fastReadUntilMatch(beginMark_, false, null);
-    }
-  }
-
-  private boolean readUntilMatchEnd(DataOutputBuffer buf) throws IOException {
-    if (slowMatch_) {
-      return slowReadUntilMatch(endPat_, true, buf);
-    } else {
-      return fastReadUntilMatch(endMark_, true, buf);
-    }
-  }
-
-  private boolean slowReadUntilMatch(Pattern markPattern, boolean includePat,
-                                     DataOutputBuffer outBufOrNull) throws IOException {
-    byte[] buf = new byte[Math.max(lookAhead_, maxRecSize_)];
-    int read = 0;
-    bin_.mark(Math.max(lookAhead_, maxRecSize_) + 2); //mark to invalidate if we read more
-    read = bin_.read(buf);
-    if (read == -1) return false;
-
-    String sbuf = new String(buf, 0, read, StandardCharsets.UTF_8);
-    Matcher match = markPattern.matcher(sbuf);
-
-    firstMatchStart_ = NA;
-    firstMatchEnd_ = NA;
-    int bufPos = 0;
-    int state = synched_ ? CDATA_OUT : CDATA_UNK;
-    int s = 0;
-
-    while (match.find(bufPos)) {
-      int input;
-      if (match.group(1) != null) {
-        input = CDATA_BEGIN;
-      } else if (match.group(2) != null) {
-        input = CDATA_END;
-        firstMatchStart_ = NA; // |<DOC CDATA[ </DOC> ]]> should keep it
-      } else {
-        input = RECORD_MAYBE;
-      }
-      if (input == RECORD_MAYBE) {
-        if (firstMatchStart_ == NA) {
-          firstMatchStart_ = match.start();
-          firstMatchEnd_ = match.end();
-        }
-      }
-      state = nextState(state, input, match.start());
-      if (state == RECORD_ACCEPT) {
-        break;
-      }
-      bufPos = match.end();
-      s++;
-    }
-    if (state != CDATA_UNK) {
-      synched_ = true;
-    }
-    boolean matched = (firstMatchStart_ != NA) && (state == RECORD_ACCEPT || state == CDATA_UNK);
-    if (matched) {
-      int endPos = includePat ? firstMatchEnd_ : firstMatchStart_;
-      bin_.reset();
-
-      for (long skiplen = endPos; skiplen > 0; ) {
-        skiplen -= bin_.skip(skiplen); // Skip succeeds as we have read this buffer
-      }
-
-      pos_ += endPos;
-      if (outBufOrNull != null) {
-        outBufOrNull.writeBytes(sbuf.substring(0,endPos));
-      }
-    }
-    return matched;
-  }
-
-  // states
-  private final static int CDATA_IN = 10;
-  private final static int CDATA_OUT = 11;
-  private final static int CDATA_UNK = 12;
-  private final static int RECORD_ACCEPT = 13;
-  // inputs
-  private final static int CDATA_BEGIN = 20;
-  private final static int CDATA_END = 21;
-  private final static int RECORD_MAYBE = 22;
-
-  /* also updates firstMatchStart_;*/
-  int nextState(int state, int input, int bufPos) {
-    switch (state) {
-    case CDATA_UNK:
-    case CDATA_OUT:
-      switch (input) {
-      case CDATA_BEGIN:
-        return CDATA_IN;
-      case CDATA_END:
-        if (state == CDATA_OUT) {
-          //System.out.println("buggy XML " + bufPos);
-        }
-        return CDATA_OUT;
-      case RECORD_MAYBE:
-        return (state == CDATA_UNK) ? CDATA_UNK : RECORD_ACCEPT;
-      }
-      break;
-    case CDATA_IN:
-      return (input == CDATA_END) ? CDATA_OUT : CDATA_IN;
-    }
-    throw new IllegalStateException(state + " " + input + " " + bufPos + " " + splitName_);
-  }
-
-  Pattern makePatternCDataOrMark(String escapedMark) {
-    StringBuffer pat = new StringBuffer();
-    addGroup(pat, StreamUtil.regexpEscape("CDATA[")); // CDATA_BEGIN
-    addGroup(pat, StreamUtil.regexpEscape("]]>")); // CDATA_END
-    addGroup(pat, escapedMark); // RECORD_MAYBE
-    return Pattern.compile(pat.toString());
-  }
-
-  void addGroup(StringBuffer pat, String escapedGroup) {
-    if (pat.length() > 0) {
-      pat.append("|");
-    }
-    pat.append("(");
-    pat.append(escapedGroup);
-    pat.append(")");
-  }
-
-  boolean fastReadUntilMatch(String textPat, boolean includePat, DataOutputBuffer outBufOrNull) throws IOException {
-    byte[] cpat = textPat.getBytes(StandardCharsets.UTF_8);
-    int m = 0;
-    boolean match = false;
-    int msup = cpat.length;
-    int LL = 120000 * 10;
-
-    bin_.mark(LL); // large number to invalidate mark
-    while (true) {
-      int b = bin_.read();
-      if (b == -1) break;
-
-      byte c = (byte) b; // this assumes eight-bit matching. OK with UTF-8
-      if (c == cpat[m]) {
-        m++;
-        if (m == msup) {
-          match = true;
-          break;
-        }
-      } else {
-        bin_.mark(LL); // rest mark so we could jump back if we found a match
-        if (outBufOrNull != null) {
-          outBufOrNull.write(cpat, 0, m);
-          outBufOrNull.write(c);
-        }
-        pos_ += m + 1; // skip m chars, +1 for 'c'
-        m = 0;
-      }
-    }
-    if (!includePat && match) {
-      bin_.reset();
-    } else if (outBufOrNull != null) {
-      outBufOrNull.write(cpat);
-      pos_ += msup;
-    }
-    return match;
-  }
-
-  String checkJobGet(String prop) throws IOException {
-    String val = job_.get(prop);
-    if (val == null) {
-      throw new IOException("JobConf: missing required property: " + prop);
-    }
-    return val;
-  }
-
-  String beginMark_;
-  String endMark_;
-
-  Pattern beginPat_;
-  Pattern endPat_;
-
-  boolean slowMatch_;
-  int lookAhead_; // bytes to read to try to synch CDATA/non-CDATA. Should be more than max record size
-  int maxRecSize_;
-
-  BufferedInputStream bin_; // Wrap FSDataInputStream for efficient backward seeks 
-  long pos_; // Keep track on position with respect encapsulated FSDataInputStream  
-
-  private final static int NA = -1;
-  int firstMatchStart_ = 0; // candidate record boundary. Might just be CDATA.
-  int firstMatchEnd_ = 0;
-
-  boolean synched_;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/IdentifierResolver.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/IdentifierResolver.java
deleted file mode 100644
index b0cd5b4fdb7..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/IdentifierResolver.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.typedbytes.TypedBytesWritable;
-
-/**
- * This class is used to resolve a string identifier into the required IO
- * classes. By extending this class and pointing the property
- * <tt>stream.io.identifier.resolver.class</tt> to this extension, additional
- * IO classes can be added by external code.
- */
-public class IdentifierResolver {
-
-  // note that the identifiers are case insensitive
-  public static final String TEXT_ID = "text";
-  public static final String RAW_BYTES_ID = "rawbytes";
-  public static final String TYPED_BYTES_ID = "typedbytes";
-  public static final String KEY_ONLY_TEXT_ID = "keyonlytext";
-  
-  private Class<? extends InputWriter> inputWriterClass = null;
-  private Class<? extends OutputReader> outputReaderClass = null;
-  private Class outputKeyClass = null;
-  private Class outputValueClass = null;
-  
-  /**
-   * Resolves a given identifier. This method has to be called before calling
-   * any of the getters.
-   */
-  public void resolve(String identifier) {
-    if (identifier.equalsIgnoreCase(RAW_BYTES_ID)) {
-      setInputWriterClass(RawBytesInputWriter.class);
-      setOutputReaderClass(RawBytesOutputReader.class);
-      setOutputKeyClass(BytesWritable.class);
-      setOutputValueClass(BytesWritable.class);
-    } else if (identifier.equalsIgnoreCase(TYPED_BYTES_ID)) {
-      setInputWriterClass(TypedBytesInputWriter.class);
-      setOutputReaderClass(TypedBytesOutputReader.class);
-      setOutputKeyClass(TypedBytesWritable.class);
-      setOutputValueClass(TypedBytesWritable.class);
-    } else if (identifier.equalsIgnoreCase(KEY_ONLY_TEXT_ID)) {
-      setInputWriterClass(KeyOnlyTextInputWriter.class);
-      setOutputReaderClass(KeyOnlyTextOutputReader.class);
-      setOutputKeyClass(Text.class);
-      setOutputValueClass(NullWritable.class);
-    } else { // assume TEXT_ID
-      setInputWriterClass(TextInputWriter.class);
-      setOutputReaderClass(TextOutputReader.class);
-      setOutputKeyClass(Text.class);
-      setOutputValueClass(Text.class);
-    }
-  }
-  
-  /**
-   * Returns the resolved {@link InputWriter} class.
-   */
-  public Class<? extends InputWriter> getInputWriterClass() {
-    return inputWriterClass;
-  }
-
-  /**
-   * Returns the resolved {@link OutputReader} class.
-   */
-  public Class<? extends OutputReader> getOutputReaderClass() {
-    return outputReaderClass;
-  }
-  
-  /**
-   * Returns the resolved output key class.
-   */
-  public Class getOutputKeyClass() {
-    return outputKeyClass;
-  }
-
-  /**
-   * Returns the resolved output value class.
-   */
-  public Class getOutputValueClass() {
-    return outputValueClass;
-  }
-  
-  
-  /**
-   * Sets the {@link InputWriter} class.
-   */
-  protected void setInputWriterClass(Class<? extends InputWriter>
-    inputWriterClass) {
-    this.inputWriterClass = inputWriterClass;
-  }
-  
-  /**
-   * Sets the {@link OutputReader} class.
-   */
-  protected void setOutputReaderClass(Class<? extends OutputReader>
-    outputReaderClass) {
-    this.outputReaderClass = outputReaderClass;
-  }
-  
-  /**
-   * Sets the output key class class.
-   */
-  protected void setOutputKeyClass(Class outputKeyClass) {
-    this.outputKeyClass = outputKeyClass;
-  }
-  
-  /**
-   * Sets the output value class.
-   */
-  protected void setOutputValueClass(Class outputValueClass) {
-    this.outputValueClass = outputValueClass;
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/InputWriter.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/InputWriter.java
deleted file mode 100644
index 35245513204..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/InputWriter.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.IOException;
-
-import org.apache.hadoop.streaming.PipeMapRed;
-
-/**
- * Abstract base for classes that write the client's input.
- */
-public abstract class InputWriter<K, V> {
-  
-  /**
-   * Initializes the InputWriter. This method has to be called before calling
-   * any of the other methods.
-   */
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    // nothing here yet, but that might change in the future
-  }
-  
-  /**
-   * Writes an input key.
-   */
-  public abstract void writeKey(K key) throws IOException;
-
-  /**
-   * Writes an input value.
-   */
-  public abstract void writeValue(V value) throws IOException;
-
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextInputWriter.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextInputWriter.java
deleted file mode 100644
index 366eff72f30..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextInputWriter.java
+++ /dev/null
@@ -1,35 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.IOException;
-
-
-public class KeyOnlyTextInputWriter extends TextInputWriter {
-
-  @Override
-  public void writeKey(Object key) throws IOException {
-    writeUTF8(key);
-    clientOut.write('\n');
-  }
-
-  @Override
-  public void writeValue(Object value) throws IOException {}
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextOutputReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextOutputReader.java
deleted file mode 100644
index 1c17659b778..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/KeyOnlyTextOutputReader.java
+++ /dev/null
@@ -1,86 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.DataInput;
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.streaming.PipeMapRed;
-import org.apache.hadoop.util.LineReader;
-
-/**
- * OutputReader that reads the client's output as text, interpreting each line
- * as a key and outputting NullWritables for values.
- */
-public class KeyOnlyTextOutputReader extends OutputReader<Text, NullWritable> {
-
-  private LineReader lineReader;
-  private byte[] bytes;
-  private DataInput clientIn;
-  private Configuration conf;
-  private Text key;
-  private Text line;
-  
-  @Override
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    super.initialize(pipeMapRed);
-    clientIn = pipeMapRed.getClientInput();
-    conf = pipeMapRed.getConfiguration();
-    lineReader = new LineReader((InputStream)clientIn, conf);
-    key = new Text();
-    line = new Text();
-  }
-  
-  @Override
-  public boolean readKeyValue() throws IOException {
-    if (lineReader.readLine(line) <= 0) {
-      return false;
-    }
-    bytes = line.getBytes();
-    key.set(bytes, 0, line.getLength());
-
-    line.clear();
-    return true;
-  }
-  
-  @Override
-  public Text getCurrentKey() throws IOException {
-    return key;
-  }
-  
-  @Override
-  public NullWritable getCurrentValue() throws IOException {
-    return NullWritable.get();
-  }
-
-  @Override
-  public String getLastOutput() {
-    if (bytes != null) {
-      return new String(bytes, StandardCharsets.UTF_8);
-    } else {
-      return null;
-    }
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/OutputReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/OutputReader.java
deleted file mode 100644
index 33f02991770..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/OutputReader.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.IOException;
-
-import org.apache.hadoop.streaming.PipeMapRed;
-
-/**
- * Abstract base for classes that read the client's output.
- */
-public abstract class OutputReader<K, V> {
-  
-  /**
-   * Initializes the OutputReader. This method has to be called before
-   * calling any of the other methods.
-   */
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    // nothing here yet, but that might change in the future
-  }
-  
-  /**
-   * Read the next key/value pair outputted by the client.
-   * @return true iff a key/value pair was read
-   */
-  public abstract boolean readKeyValue() throws IOException;
-  
-  /**
-   * Returns the current key.
-   */
-  public abstract K getCurrentKey() throws IOException;
-  
-  /**
-   * Returns the current value.
-   */
-  public abstract V getCurrentValue() throws IOException;
-  
-  /**
-   * Returns the last output from the client as a String.
-   */
-  public abstract String getLastOutput();
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesInputWriter.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesInputWriter.java
deleted file mode 100644
index effee05c513..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesInputWriter.java
+++ /dev/null
@@ -1,73 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.streaming.PipeMapRed;
-
-/**
- * InputWriter that writes the client's input as raw bytes.
- */
-public class RawBytesInputWriter extends InputWriter<Writable, Writable> {
-
-  private DataOutput clientOut;
-  private ByteArrayOutputStream bufferOut;
-  private DataOutputStream bufferDataOut;
-
-  @Override
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    super.initialize(pipeMapRed);
-    clientOut = pipeMapRed.getClientOutput();
-    bufferOut = new ByteArrayOutputStream();
-    bufferDataOut = new DataOutputStream(bufferOut);
-  }
-  
-  @Override
-  public void writeKey(Writable key) throws IOException {
-    writeRawBytes(key);
-  }
-
-  @Override
-  public void writeValue(Writable value) throws IOException {
-    writeRawBytes(value);
-  }
-
-  private void writeRawBytes(Writable writable) throws IOException {
-    if (writable instanceof BytesWritable) {
-      BytesWritable bw = (BytesWritable) writable;
-      byte[] bytes = bw.getBytes();
-      int length = bw.getLength();
-      clientOut.writeInt(length);
-      clientOut.write(bytes, 0, length);
-    } else {
-      bufferOut.reset();
-      writable.write(bufferDataOut);
-      byte[] bytes = bufferOut.toByteArray();
-      clientOut.writeInt(bytes.length);
-      clientOut.write(bytes);
-    }
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java
deleted file mode 100644
index 2ffb62c27a8..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/RawBytesOutputReader.java
+++ /dev/null
@@ -1,92 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.DataInput;
-import java.io.EOFException;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.streaming.PipeMapRed;
-
-/**
- * OutputReader that reads the client's output as raw bytes.
- */
-public class RawBytesOutputReader 
-  extends OutputReader<BytesWritable, BytesWritable> {
-
-  private DataInput clientIn;
-  private byte[] bytes;
-  private BytesWritable key;
-  private BytesWritable value;
-
-  @Override
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    super.initialize(pipeMapRed);
-    clientIn = pipeMapRed.getClientInput();
-    key = new BytesWritable();
-    value = new BytesWritable();
-  }
-  
-  @Override
-  public boolean readKeyValue() throws IOException {
-    int length = readLength();
-    if (length < 0) {
-      return false;
-    }
-    key.set(readBytes(length), 0, length);
-    length = readLength();
-    value.set(readBytes(length), 0, length);
-    return true;
-  }
-  
-  @Override
-  public BytesWritable getCurrentKey() throws IOException {
-    return key;
-  }
-  
-  @Override
-  public BytesWritable getCurrentValue() throws IOException {
-    return value;
-  }
-
-  @Override
-  public String getLastOutput() {
-    if (bytes != null) {
-      return new BytesWritable(bytes).toString();
-    } else {
-      return null;
-    }
-  }
-
-  private int readLength() throws IOException {
-    try {
-      return clientIn.readInt();
-    } catch (EOFException eof) {
-      return -1;
-    }
-  }
-  
-  private byte[] readBytes(int length) throws IOException {
-    bytes = new byte[length];
-    clientIn.readFully(bytes);
-    return bytes;
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextInputWriter.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextInputWriter.java
deleted file mode 100644
index 31513da71d9..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextInputWriter.java
+++ /dev/null
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.DataOutput;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.streaming.PipeMapRed;
-
-/**
- * InputWriter that writes the client's input as text.
- */
-public class TextInputWriter extends InputWriter<Object, Object> {
-  
-  protected DataOutput clientOut;
-  private byte[] inputSeparator;
-  
-  @Override
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    super.initialize(pipeMapRed);
-    clientOut = pipeMapRed.getClientOutput();
-    inputSeparator = pipeMapRed.getInputSeparator();
-  }
-  
-  @Override
-  public void writeKey(Object key) throws IOException {
-    writeUTF8(key);
-    clientOut.write(inputSeparator);
-  }
-
-  @Override
-  public void writeValue(Object value) throws IOException {
-    writeUTF8(value);
-    clientOut.write('\n');
-  }
-  
-  // Write an object to the output stream using UTF-8 encoding
-  protected void writeUTF8(Object object) throws IOException {
-    byte[] bval;
-    int valSize;
-    if (object instanceof BytesWritable) {
-      BytesWritable val = (BytesWritable) object;
-      bval = val.getBytes();
-      valSize = val.getLength();
-    } else if (object instanceof Text) {
-      Text val = (Text) object;
-      bval = val.getBytes();
-      valSize = val.getLength();
-    } else {
-      String sval = object.toString();
-      bval = sval.getBytes(StandardCharsets.UTF_8);
-      valSize = bval.length;
-    }
-    clientOut.write(bval, 0, valSize);
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextOutputReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextOutputReader.java
deleted file mode 100644
index 11c84a471f7..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TextOutputReader.java
+++ /dev/null
@@ -1,115 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.DataInput;
-import java.io.IOException;
-import java.io.InputStream;
-import java.nio.charset.CharacterCodingException;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.streaming.PipeMapRed;
-import org.apache.hadoop.streaming.StreamKeyValUtil;
-import org.apache.hadoop.util.LineReader;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.util.UTF8ByteArrayUtils;
-
-/**
- * OutputReader that reads the client's output as text.
- */
-public class TextOutputReader extends OutputReader<Text, Text> {
-
-  private LineReader lineReader;
-  private byte[] bytes;
-  private DataInput clientIn;
-  private Configuration conf;
-  private int numKeyFields;
-  private byte[] separator;
-  private Text key;
-  private Text value;
-  private Text line;
-  
-  @Override
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    super.initialize(pipeMapRed);
-    clientIn = pipeMapRed.getClientInput();
-    conf = pipeMapRed.getConfiguration();
-    numKeyFields = pipeMapRed.getNumOfKeyFields();
-    separator = pipeMapRed.getFieldSeparator();
-    lineReader = new LineReader((InputStream)clientIn, conf);
-    key = new Text();
-    value = new Text();
-    line = new Text();
-  }
-  
-  @Override
-  public boolean readKeyValue() throws IOException {
-    if (lineReader.readLine(line) <= 0) {
-      return false;
-    }
-    bytes = line.getBytes();
-    splitKeyVal(bytes, line.getLength(), key, value);
-    line.clear();
-    return true;
-  }
-  
-  @Override
-  public Text getCurrentKey() throws IOException {
-    return key;
-  }
-  
-  @Override
-  public Text getCurrentValue() throws IOException {
-    return value;
-  }
-
-  @Override
-  public String getLastOutput() {
-    if (bytes != null) {
-      return new String(bytes, StandardCharsets.UTF_8);
-    } else {
-      return null;
-    }
-  }
-
-  // split a UTF-8 line into key and value
-  private void splitKeyVal(byte[] line, int length, Text key, Text val)
-    throws IOException {
-    // Need to find numKeyFields separators
-    int pos = UTF8ByteArrayUtils.findBytes(line, 0, length, separator);
-    for(int k=1; k<numKeyFields && pos!=-1; k++) {
-      pos = UTF8ByteArrayUtils.findBytes(line, pos + separator.length, 
-        length, separator);
-    }
-    try {
-      if (pos == -1) {
-        key.set(line, 0, length);
-        val.set("");
-      } else {
-        StreamKeyValUtil.splitKeyVal(line, 0, length, key, val, pos,
-          separator.length);
-      }
-    } catch (CharacterCodingException e) {
-      throw new IOException(StringUtils.stringifyException(e));
-    }
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesInputWriter.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesInputWriter.java
deleted file mode 100644
index 9891f9856b0..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesInputWriter.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.streaming.PipeMapRed;
-import org.apache.hadoop.typedbytes.TypedBytesOutput;
-import org.apache.hadoop.typedbytes.TypedBytesWritableOutput;
-
-/**
- * InputWriter that writes the client's input as typed bytes.
- */
-public class TypedBytesInputWriter extends InputWriter<Object, Object> {
-
-  private TypedBytesOutput tbOut;
-  private TypedBytesWritableOutput tbwOut;
-
-  @Override
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    super.initialize(pipeMapRed);
-    DataOutput clientOut = pipeMapRed.getClientOutput();
-    tbOut = new TypedBytesOutput(clientOut);
-    tbwOut = new TypedBytesWritableOutput(clientOut);
-  }
-
-  @Override
-  public void writeKey(Object key) throws IOException {
-    writeTypedBytes(key);
-  }
-
-  @Override
-  public void writeValue(Object value) throws IOException {
-    writeTypedBytes(value);
-  }
-  
-  private void writeTypedBytes(Object value) throws IOException {
-    if (value instanceof Writable) {
-      tbwOut.write((Writable) value);
-    } else {
-      tbOut.write(value);
-    }
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java
deleted file mode 100644
index e5526ef6f71..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/io/TypedBytesOutputReader.java
+++ /dev/null
@@ -1,80 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.DataInput;
-import java.io.IOException;
-
-import org.apache.hadoop.streaming.PipeMapRed;
-import org.apache.hadoop.typedbytes.TypedBytesInput;
-import org.apache.hadoop.typedbytes.TypedBytesWritable;
-
-/**
- * OutputReader that reads the client's output as typed bytes.
- */
-public class TypedBytesOutputReader extends 
-  OutputReader<TypedBytesWritable, TypedBytesWritable> {
-
-  private byte[] bytes;
-  private DataInput clientIn;
-  private TypedBytesWritable key;
-  private TypedBytesWritable value;
-  private TypedBytesInput in;
-  
-  @Override
-  public void initialize(PipeMapRed pipeMapRed) throws IOException {
-    super.initialize(pipeMapRed);
-    clientIn = pipeMapRed.getClientInput();
-    key = new TypedBytesWritable();
-    value = new TypedBytesWritable();
-    in = new TypedBytesInput(clientIn);
-  }
-  
-  @Override
-  public boolean readKeyValue() throws IOException {
-    bytes = in.readRaw();
-    if (bytes == null) {
-      return false;
-    }
-    key.set(bytes, 0, bytes.length);
-    bytes = in.readRaw();
-    value.set(bytes, 0, bytes.length);
-    return true;
-  }
-  
-  @Override
-  public TypedBytesWritable getCurrentKey() throws IOException {
-    return key;
-  }
-  
-  @Override
-  public TypedBytesWritable getCurrentValue() throws IOException {
-    return value;
-  }
-
-  @Override
-  public String getLastOutput() {
-    if (bytes != null) {
-      return new TypedBytesWritable(bytes).toString();
-    } else {
-      return null;
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamBaseRecordReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamBaseRecordReader.java
deleted file mode 100644
index e3c14743cb3..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamBaseRecordReader.java
+++ /dev/null
@@ -1,154 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.mapreduce;
-
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.streaming.StreamUtil;
-
-/**
- * Shared functionality for hadoopStreaming formats. A custom reader can be
- * defined to be a RecordReader with the constructor below and is selected with
- * the option bin/hadoopStreaming -inputreader ...
- * 
- * @see StreamXmlRecordReader
- */
-public abstract class StreamBaseRecordReader extends RecordReader<Text, Text> {
-
-  protected static final Logger LOG = LoggerFactory
-      .getLogger(StreamBaseRecordReader.class.getName());
-
-  // custom JobConf properties for this class are prefixed with this namespace
-  final static String CONF_NS = "stream.recordreader.";
-
-  public StreamBaseRecordReader(FSDataInputStream in, FileSplit split,
-      TaskAttemptContext context, Configuration conf, FileSystem fs)
-      throws IOException {
-    in_ = in;
-    split_ = split;
-    start_ = split_.getStart();
-    length_ = split_.getLength();
-    end_ = start_ + length_;
-    splitName_ = split_.getPath().getName();
-    this.context_ = context;
-    conf_ = conf;
-    fs_ = fs;
-
-    statusMaxRecordChars_ = conf.getInt(CONF_NS + "statuschars", 200);
-  }
-
-  // / RecordReader API
-
-  /**
-   * Read a record. Implementation should call numRecStats at the end
-   */
-  public abstract boolean next(Text key, Text value) throws IOException;
-
-  /** Returns the current position in the input. */
-  public synchronized long getPos() throws IOException {
-    return in_.getPos();
-  }
-
-  /** Close this to future operations. */
-  public synchronized void close() throws IOException {
-    in_.close();
-  }
-
-  public float getProgress() throws IOException {
-    if (end_ == start_) {
-      return 1.0f;
-    } else {
-      return ((float) (in_.getPos() - start_)) / ((float) (end_ - start_));
-    }
-  }
-
-  public Text createKey() {
-    return new Text();
-  }
-
-  public Text createValue() {
-    return new Text();
-  }
-
-  // / StreamBaseRecordReader API
-
-  /**
-   * Implementation should seek forward in_ to the first byte of the next
-   * record. The initial byte offset in the stream is arbitrary.
-   */
-  public abstract void seekNextRecordBoundary() throws IOException;
-
-  void numRecStats(byte[] record, int start, int len) throws IOException {
-    numRec_++;
-    if (numRec_ == nextStatusRec_) {
-      String recordStr = new String(record, start, Math.min(len,
-          statusMaxRecordChars_), StandardCharsets.UTF_8);
-      nextStatusRec_ += 100;// *= 10;
-      String status = getStatus(recordStr);
-      LOG.info(status);
-      context_.setStatus(status);
-    }
-  }
-
-  long lastMem = 0;
-
-  String getStatus(CharSequence record) {
-    long pos = -1;
-    try {
-      pos = getPos();
-    } catch (IOException io) {
-    }
-    String recStr;
-    if (record.length() > statusMaxRecordChars_) {
-      recStr = record.subSequence(0, statusMaxRecordChars_) + "...";
-    } else {
-      recStr = record.toString();
-    }
-    String unqualSplit = split_.getPath().getName() + ":" + split_.getStart()
-        + "+" + split_.getLength();
-    String status = "HSTR " + StreamUtil.getHost() + " " + numRec_ + ". pos="
-        + pos + " " + unqualSplit + " Processing record=" + recStr;
-    status += " " + splitName_;
-    return status;
-  }
-
-  FSDataInputStream in_;
-  FileSplit split_;
-  long start_;
-  long end_;
-  long length_;
-  String splitName_;
-  TaskAttemptContext context_;
-  Configuration conf_;
-  FileSystem fs_;
-  int numRec_ = 0;
-  int nextStatusRec_ = 1;
-  int statusMaxRecordChars_;
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamInputFormat.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamInputFormat.java
deleted file mode 100644
index f44488c7c02..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamInputFormat.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.mapreduce;
-
-import java.io.IOException;
-import java.lang.reflect.Constructor;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FutureDataInputStreamBuilder;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;
-import org.apache.hadoop.streaming.StreamUtil;
-import org.apache.hadoop.util.functional.FutureIO;
-
-/**
- * An input format that selects a RecordReader based on a JobConf property. This
- * should be used only for non-standard record reader such as
- * StreamXmlRecordReader. For all other standard record readers, the appropriate
- * input format classes should be used.
- */
-public class StreamInputFormat extends KeyValueTextInputFormat {
-
-  @Override
-  public RecordReader<Text, Text> createRecordReader(InputSplit genericSplit,
-      TaskAttemptContext context) throws IOException {
-
-    Configuration conf = context.getConfiguration();
-
-    String c = conf.get("stream.recordreader.class");
-    if (c == null || c.indexOf("LineRecordReader") >= 0) {
-      return super.createRecordReader(genericSplit, context);
-    }
-
-    // handling non-standard record reader (likely StreamXmlRecordReader)
-    FileSplit split = (FileSplit) genericSplit;
-    // LOG.info("getRecordReader start.....split=" + split);
-    context.setStatus(split.toString());
-    context.progress();
-
-    // Open the file and seek to the start of the split
-    Path path = split.getPath();
-    FileSystem fs = path.getFileSystem(conf);
-    // open the file
-    final FutureDataInputStreamBuilder builder = fs.openFile(path);
-    FutureIO.propagateOptions(builder, conf,
-        MRJobConfig.INPUT_FILE_OPTION_PREFIX,
-        MRJobConfig.INPUT_FILE_MANDATORY_PREFIX);
-    FSDataInputStream in = FutureIO.awaitFuture(builder.build());
-
-    // Factory dispatch based on available params..
-    Class readerClass;
-
-    {
-      readerClass = StreamUtil.goodClassOrNull(conf, c, null);
-      if (readerClass == null) {
-        throw new RuntimeException("Class not found: " + c);
-      }
-    }
-    Constructor ctor;
-
-    try {
-      ctor = readerClass.getConstructor(new Class[] { FSDataInputStream.class,
-          FileSplit.class, TaskAttemptContext.class, Configuration.class,
-          FileSystem.class });
-    } catch (NoSuchMethodException nsm) {
-      throw new RuntimeException(nsm);
-    }
-
-    RecordReader<Text, Text> reader;
-    try {
-      reader = (RecordReader<Text, Text>) ctor.newInstance(new Object[] { in,
-          split, context, conf, fs });
-    } catch (Exception nsm) {
-      throw new RuntimeException(nsm);
-    }
-    return reader;
-
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamXmlRecordReader.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamXmlRecordReader.java
deleted file mode 100644
index aa8a4d8832c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/mapreduce/StreamXmlRecordReader.java
+++ /dev/null
@@ -1,341 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.mapreduce;
-
-import java.io.BufferedInputStream;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.streaming.StreamUtil;
-
-/**
- * A way to interpret XML fragments as Mapper input records. Values are XML
- * subtrees delimited by configurable tags. Keys could be the value of a certain
- * attribute in the XML subtree, but this is left to the stream processor
- * application.
- * 
- * The name-value properties that StreamXmlRecordReader understands are: String
- * begin (chars marking beginning of record) String end (chars marking end of
- * record) int maxrec (maximum record size) int lookahead(maximum lookahead to
- * sync CDATA) boolean slowmatch
- */
-public class StreamXmlRecordReader extends StreamBaseRecordReader {
-
-  private Text key;
-  private Text value;
-
-  public StreamXmlRecordReader(FSDataInputStream in, FileSplit split,
-      TaskAttemptContext context, Configuration conf, FileSystem fs)
-  throws IOException {
-    super(in, split, context, conf, fs);
-
-    beginMark_ = checkJobGet(CONF_NS + "begin");
-    endMark_ = checkJobGet(CONF_NS + "end");
-
-    maxRecSize_ = conf_.getInt(CONF_NS + "maxrec", 50 * 1000);
-    lookAhead_ = conf_.getInt(CONF_NS + "lookahead", 2 * maxRecSize_);
-    synched_ = false;
-
-    slowMatch_ = conf_.getBoolean(CONF_NS + "slowmatch", false);
-    if (slowMatch_) {
-      beginPat_ = makePatternCDataOrMark(beginMark_);
-      endPat_ = makePatternCDataOrMark(endMark_);
-    }
-    init();
-  }
-
-  public final void init() throws IOException {
-    LOG.info("StreamBaseRecordReader.init: " + " start_=" + start_ + " end_="
-        + end_ + " length_=" + length_ + " start_ > in_.getPos() ="
-        + (start_ > in_.getPos()) + " " + start_ + " > " + in_.getPos());
-    if (start_ > in_.getPos()) {
-      in_.seek(start_);
-    }
-    pos_ = start_;
-    bin_ = new BufferedInputStream(in_);
-    seekNextRecordBoundary();
-  }
-
-  int numNext = 0;
-
-  public synchronized boolean next(Text key, Text value) throws IOException {
-    numNext++;
-    if (pos_ >= end_) {
-      return false;
-    }
-
-    DataOutputBuffer buf = new DataOutputBuffer();
-    if (!readUntilMatchBegin()) {
-      return false;
-    }
-    if (pos_ >= end_ || !readUntilMatchEnd(buf)) {
-      return false;
-    }
-
-    // There is only one elem..key/value splitting is not done here.
-    byte[] record = new byte[buf.getLength()];
-    System.arraycopy(buf.getData(), 0, record, 0, record.length);
-
-    numRecStats(record, 0, record.length);
-
-    key.set(record);
-    value.set("");
-
-    return true;
-  }
-
-  public void seekNextRecordBoundary() throws IOException {
-    readUntilMatchBegin();
-  }
-
-  boolean readUntilMatchBegin() throws IOException {
-    if (slowMatch_) {
-      return slowReadUntilMatch(beginPat_, false, null);
-    } else {
-      return fastReadUntilMatch(beginMark_, false, null);
-    }
-  }
-
-  private boolean readUntilMatchEnd(DataOutputBuffer buf) throws IOException {
-    if (slowMatch_) {
-      return slowReadUntilMatch(endPat_, true, buf);
-    } else {
-      return fastReadUntilMatch(endMark_, true, buf);
-    }
-  }
-
-  private boolean slowReadUntilMatch(Pattern markPattern, boolean includePat,
-      DataOutputBuffer outBufOrNull) throws IOException {
-    byte[] buf = new byte[Math.max(lookAhead_, maxRecSize_)];
-    int read = 0;
-    bin_.mark(Math.max(lookAhead_, maxRecSize_) + 2); // mark to invalidate if
-    // we read more
-    read = bin_.read(buf);
-    if (read == -1)
-      return false;
-
-    String sbuf = new String(buf, 0, read, StandardCharsets.UTF_8);
-    Matcher match = markPattern.matcher(sbuf);
-
-    firstMatchStart_ = NA;
-    firstMatchEnd_ = NA;
-    int bufPos = 0;
-    int state = synched_ ? CDATA_OUT : CDATA_UNK;
-    int s = 0;
-
-    while (match.find(bufPos)) {
-      int input;
-      if (match.group(1) != null) {
-        input = CDATA_BEGIN;
-      } else if (match.group(2) != null) {
-        input = CDATA_END;
-        firstMatchStart_ = NA; // |<DOC CDATA[ </DOC> ]]> should keep it
-      } else {
-        input = RECORD_MAYBE;
-      }
-      if (input == RECORD_MAYBE) {
-        if (firstMatchStart_ == NA) {
-          firstMatchStart_ = match.start();
-          firstMatchEnd_ = match.end();
-        }
-      }
-      state = nextState(state, input, match.start());
-      if (state == RECORD_ACCEPT) {
-        break;
-      }
-      bufPos = match.end();
-      s++;
-    }
-    if (state != CDATA_UNK) {
-      synched_ = true;
-    }
-    boolean matched = (firstMatchStart_ != NA)
-    && (state == RECORD_ACCEPT || state == CDATA_UNK);
-    if (matched) {
-      int endPos = includePat ? firstMatchEnd_ : firstMatchStart_;
-      bin_.reset();
-
-      for (long skiplen = endPos; skiplen > 0;) {
-        skiplen -= bin_.skip(skiplen); // Skip succeeds as we have read this
-        // buffer
-      }
-
-      pos_ += endPos;
-      if (outBufOrNull != null) {
-        outBufOrNull.writeBytes(sbuf.substring(0, endPos));
-      }
-    }
-    return matched;
-  }
-
-  // states
-  private final static int CDATA_IN = 10;
-  private final static int CDATA_OUT = 11;
-  private final static int CDATA_UNK = 12;
-  private final static int RECORD_ACCEPT = 13;
-  // inputs
-  private final static int CDATA_BEGIN = 20;
-  private final static int CDATA_END = 21;
-  private final static int RECORD_MAYBE = 22;
-
-  /* also updates firstMatchStart_; */
-  int nextState(int state, int input, int bufPos) {
-    switch (state) {
-    case CDATA_UNK:
-    case CDATA_OUT:
-      switch (input) {
-      case CDATA_BEGIN:
-        return CDATA_IN;
-      case CDATA_END:
-        if (state == CDATA_OUT) {
-          // System.out.println("buggy XML " + bufPos);
-        }
-        return CDATA_OUT;
-      case RECORD_MAYBE:
-        return (state == CDATA_UNK) ? CDATA_UNK : RECORD_ACCEPT;
-      }
-      break;
-    case CDATA_IN:
-      return (input == CDATA_END) ? CDATA_OUT : CDATA_IN;
-    }
-    throw new IllegalStateException(state + " " + input + " " + bufPos + " "
-        + splitName_);
-  }
-
-  Pattern makePatternCDataOrMark(String escapedMark) {
-    StringBuffer pat = new StringBuffer();
-    addGroup(pat, StreamUtil.regexpEscape("CDATA[")); // CDATA_BEGIN
-    addGroup(pat, StreamUtil.regexpEscape("]]>")); // CDATA_END
-    addGroup(pat, escapedMark); // RECORD_MAYBE
-    return Pattern.compile(pat.toString());
-  }
-
-  void addGroup(StringBuffer pat, String escapedGroup) {
-    if (pat.length() > 0) {
-      pat.append("|");
-    }
-    pat.append("(");
-    pat.append(escapedGroup);
-    pat.append(")");
-  }
-
-  boolean fastReadUntilMatch(String textPat, boolean includePat,
-      DataOutputBuffer outBufOrNull) throws IOException {
-    byte[] cpat = textPat.getBytes(StandardCharsets.UTF_8);
-    int m = 0;
-    boolean match = false;
-    int msup = cpat.length;
-    int LL = 120000 * 10;
-
-    bin_.mark(LL); // large number to invalidate mark
-    while (true) {
-      int b = bin_.read();
-      if (b == -1)
-        break;
-
-      byte c = (byte) b; // this assumes eight-bit matching. OK with UTF-8
-      if (c == cpat[m]) {
-        m++;
-        if (m == msup) {
-          match = true;
-          break;
-        }
-      } else {
-        bin_.mark(LL); // rest mark so we could jump back if we found a match
-        if (outBufOrNull != null) {
-          outBufOrNull.write(cpat, 0, m);
-          outBufOrNull.write(c);
-        }
-        pos_ += m + 1; // skip m chars, +1 for 'c'
-        m = 0;
-      }
-    }
-    if (!includePat && match) {
-      bin_.reset();
-    } else if (outBufOrNull != null) {
-      outBufOrNull.write(cpat);
-      pos_ += msup;
-    }
-    return match;
-  }
-
-  String checkJobGet(String prop) throws IOException {
-    String val = conf_.get(prop);
-    if (val == null) {
-      throw new IOException("JobConf: missing required property: " + prop);
-    }
-    return val;
-  }
-
-  String beginMark_;
-  String endMark_;
-
-  Pattern beginPat_;
-  Pattern endPat_;
-
-  boolean slowMatch_;
-  int lookAhead_; // bytes to read to try to synch CDATA/non-CDATA. Should be
-  // more than max record size
-  int maxRecSize_;
-
-  BufferedInputStream bin_; // Wrap FSDataInputStream for efficient backward
-  // seeks
-  long pos_; // Keep track on position with respect encapsulated
-  // FSDataInputStream
-
-  private final static int NA = -1;
-  int firstMatchStart_ = 0; // candidate record boundary. Might just be CDATA.
-  int firstMatchEnd_ = 0;
-
-  boolean synched_;
-
-  @Override
-  public Text getCurrentKey() throws IOException, InterruptedException {
-    return key;
-  }
-
-  @Override
-  public Text getCurrentValue() throws IOException, InterruptedException {
-    return value;
-  }
-
-  @Override
-  public void initialize(InputSplit arg0, TaskAttemptContext arg1)
-  throws IOException, InterruptedException {
-
-  }
-
-  @Override
-  public boolean nextKeyValue() throws IOException, InterruptedException {
-    key = createKey();
-    value = createValue();
-    return next(key, value);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/package.html b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/package.html
deleted file mode 100644
index be64426757e..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/streaming/package.html
+++ /dev/null
@@ -1,27 +0,0 @@
-<html>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<body>
-
-<tt>Hadoop Streaming</tt> is a utility which allows users to create and run 
-Map-Reduce jobs with any executables (e.g. Unix shell utilities) as the mapper 
-and/or the reducer.
-
-</body>
-</html>
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/Type.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/Type.java
deleted file mode 100644
index 8922617ad5c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/Type.java
+++ /dev/null
@@ -1,50 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-/**
- * The possible type codes.
- */
-public enum Type {
-
-  // codes for supported types (< 50):
-  BYTES(0),
-  BYTE(1),
-  BOOL(2),
-  INT(3),
-  LONG(4),
-  FLOAT(5),
-  DOUBLE(6),
-  STRING(7),
-  VECTOR(8),
-  LIST(9),
-  MAP(10),
-  
-  // application-specific codes (50-200):
-  WRITABLE(50),
-  
-  // low-level codes (> 200):
-  MARKER(255);
-
-  final int code;
-
-  Type(int code) {
-    this.code = code;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesInput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesInput.java
deleted file mode 100644
index cff0be24b21..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesInput.java
+++ /dev/null
@@ -1,506 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-import java.io.DataInput;
-import java.io.EOFException;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.TreeMap;
-
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.record.Buffer;
-
-/**
- * Provides functionality for reading typed bytes.
- */
-public class TypedBytesInput {
-
-  private DataInput in;
-
-  private TypedBytesInput() {}
-
-  private void setDataInput(DataInput in) {
-    this.in = in;
-  }
-
-  private static final ThreadLocal<TypedBytesInput> TB_IN =
-      new ThreadLocal<TypedBytesInput>() {
-    @Override
-    protected TypedBytesInput initialValue() {
-      return new TypedBytesInput();
-    }
-  };
-
-  /**
-   * Get a thread-local typed bytes input for the supplied {@link DataInput}.
-   * @param in data input object
-   * @return typed bytes input corresponding to the supplied {@link DataInput}.
-   */
-  public static TypedBytesInput get(DataInput in) {
-    TypedBytesInput bin = TB_IN.get();
-    bin.setDataInput(in);
-    return bin;
-  }
-
-  /** Creates a new instance of TypedBytesInput. */
-  public TypedBytesInput(DataInput in) {
-    this.in = in;
-  }
-
-  /**
-   * Reads a typed bytes sequence and converts it to a Java object. The first 
-   * byte is interpreted as a type code, and then the right number of 
-   * subsequent bytes are read depending on the obtained type.
-   * @return the obtained object or null when the end of the file is reached
-   * @throws IOException
-   */
-  public Object read() throws IOException {
-    int code = 1;
-    try {
-      code = in.readUnsignedByte();
-    } catch (EOFException eof) {
-      return null;
-    }
-    if (code == Type.BYTES.code) {
-      return new Buffer(readBytes());
-    } else if (code == Type.BYTE.code) {
-      return readByte();
-    } else if (code == Type.BOOL.code) {
-      return readBool();
-    } else if (code == Type.INT.code) {
-      return readInt();
-    } else if (code == Type.LONG.code) {
-      return readLong();
-    } else if (code == Type.FLOAT.code) {
-      return readFloat();
-    } else if (code == Type.DOUBLE.code) {
-      return readDouble();
-    } else if (code == Type.STRING.code) {
-      return readString();
-    } else if (code == Type.VECTOR.code) {
-      return readVector();
-    } else if (code == Type.LIST.code) {
-      return readList();
-    } else if (code == Type.MAP.code) {
-      return readMap();
-    } else if (code == Type.MARKER.code) {
-      return null;
-    } else if (50 <= code && code <= 200) { // application-specific typecodes
-      return new Buffer(readBytes());
-    } else {
-      throw new RuntimeException("unknown type");
-    }
-  }
-
-  /**
-   * Reads a typed bytes sequence. The first byte is interpreted as a type code,
-   * and then the right number of subsequent bytes are read depending on the
-   * obtained type.
-   * 
-   * @return the obtained typed bytes sequence or null when the end of the file
-   *         is reached
-   * @throws IOException
-   */
-  public byte[] readRaw() throws IOException {
-    int code = -1;
-    try {
-      code = in.readUnsignedByte();
-    } catch (EOFException eof) {
-      return null;
-    }
-    if (code == Type.BYTES.code) {
-      return readRawBytes();
-    } else if (code == Type.BYTE.code) {
-      return readRawByte();
-    } else if (code == Type.BOOL.code) {
-      return readRawBool();
-    } else if (code == Type.INT.code) {
-      return readRawInt();
-    } else if (code == Type.LONG.code) {
-      return readRawLong();
-    } else if (code == Type.FLOAT.code) {
-      return readRawFloat();
-    } else if (code == Type.DOUBLE.code) {
-      return readRawDouble();
-    } else if (code == Type.STRING.code) {
-      return readRawString();
-    } else if (code == Type.VECTOR.code) {
-      return readRawVector();
-    } else if (code == Type.LIST.code) {
-      return readRawList();
-    } else if (code == Type.MAP.code) {
-      return readRawMap();
-    } else if (code == Type.MARKER.code) {
-      return null;
-    } else if (50 <= code && code <= 200) { // application-specific typecodes
-      return readRawBytes(code);
-    } else {
-      throw new RuntimeException("unknown type");
-    }
-  }
-
-  /**
-   * Reads a type byte and returns the corresponding {@link Type}.
-   * @return the obtained Type or null when the end of the file is reached
-   * @throws IOException
-   */
-  public Type readType() throws IOException {
-    int code = -1;
-    try {
-      code = in.readUnsignedByte();
-    } catch (EOFException eof) {
-      return null;
-    }
-    for (Type type : Type.values()) {
-      if (type.code == code) {
-        return type;
-      }
-    }
-    return null;
-  }
-
-  /**
-   * Skips a type byte.
-   * @return true iff the end of the file was not reached
-   * @throws IOException
-   */
-  public boolean skipType() throws IOException {
-    try {
-      in.readByte();
-      return true;
-    } catch (EOFException eof) {
-      return false;
-    }
-  }
-
-  /**
-   * Reads the bytes following a <code>Type.BYTES</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readBytes() throws IOException {
-    int length = in.readInt();
-    byte[] bytes = new byte[length];
-    in.readFully(bytes);
-    return bytes;
-  }
-
-  /**
-   * Reads the raw bytes following a custom code.
-   * @param code the custom type code
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawBytes(int code) throws IOException {
-    int length = in.readInt();
-    byte[] bytes = new byte[5 + length];
-    bytes[0] = (byte) code;
-    bytes[1] = (byte) (0xff & (length >> 24));
-    bytes[2] = (byte) (0xff & (length >> 16));
-    bytes[3] = (byte) (0xff & (length >> 8));
-    bytes[4] = (byte) (0xff & length);
-    in.readFully(bytes, 5, length);
-    return bytes;
-  }
-  
-  /**
-   * Reads the raw bytes following a <code>Type.BYTES</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawBytes() throws IOException {
-    return readRawBytes(Type.BYTES.code);
-  }
-
-  /**
-   * Reads the byte following a <code>Type.BYTE</code> code.
-   * @return the obtained byte
-   * @throws IOException
-   */
-  public byte readByte() throws IOException {
-    return in.readByte();
-  }
-
-  /**
-   * Reads the raw byte following a <code>Type.BYTE</code> code.
-   * @return the obtained byte
-   * @throws IOException
-   */
-  public byte[] readRawByte() throws IOException {
-    byte[] bytes = new byte[2];
-    bytes[0] = (byte) Type.BYTE.code;
-    in.readFully(bytes, 1, 1);
-    return bytes;
-  }
-
-  /**
-   * Reads the boolean following a <code>Type.BOOL</code> code.
-   * @return the obtained boolean
-   * @throws IOException
-   */
-  public boolean readBool() throws IOException {
-    return in.readBoolean();
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.BOOL</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawBool() throws IOException {
-    byte[] bytes = new byte[2];
-    bytes[0] = (byte) Type.BOOL.code;
-    in.readFully(bytes, 1, 1);
-    return bytes;
-  }
-
-  /**
-   * Reads the integer following a <code>Type.INT</code> code.
-   * @return the obtained integer
-   * @throws IOException
-   */
-  public int readInt() throws IOException {
-    return in.readInt();
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.INT</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawInt() throws IOException {
-    byte[] bytes = new byte[5];
-    bytes[0] = (byte) Type.INT.code;
-    in.readFully(bytes, 1, 4);
-    return bytes;
-  }
-
-  /**
-   * Reads the long following a <code>Type.LONG</code> code.
-   * @return the obtained long
-   * @throws IOException
-   */
-  public long readLong() throws IOException {
-    return in.readLong();
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.LONG</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawLong() throws IOException {
-    byte[] bytes = new byte[9];
-    bytes[0] = (byte) Type.LONG.code;
-    in.readFully(bytes, 1, 8);
-    return bytes;
-  }
-
-  /**
-   * Reads the float following a <code>Type.FLOAT</code> code.
-   * @return the obtained float
-   * @throws IOException
-   */
-  public float readFloat() throws IOException {
-    return in.readFloat();
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.FLOAT</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawFloat() throws IOException {
-    byte[] bytes = new byte[5];
-    bytes[0] = (byte) Type.FLOAT.code;
-    in.readFully(bytes, 1, 4);
-    return bytes;
-  }
-
-  /**
-   * Reads the double following a <code>Type.DOUBLE</code> code.
-   * @return the obtained double
-   * @throws IOException
-   */
-  public double readDouble() throws IOException {
-    return in.readDouble();
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.DOUBLE</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawDouble() throws IOException {
-    byte[] bytes = new byte[9];
-    bytes[0] = (byte) Type.DOUBLE.code;
-    in.readFully(bytes, 1, 8);
-    return bytes;
-  }
-
-  /**
-   * Reads the string following a <code>Type.STRING</code> code.
-   * @return the obtained string
-   * @throws IOException
-   */
-  public String readString() throws IOException {
-    return WritableUtils.readString(in);
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.STRING</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawString() throws IOException {
-    int length = in.readInt();
-    byte[] bytes = new byte[5 + length];
-    bytes[0] = (byte) Type.STRING.code;
-    bytes[1] = (byte) (0xff & (length >> 24));
-    bytes[2] = (byte) (0xff & (length >> 16));
-    bytes[3] = (byte) (0xff & (length >> 8));
-    bytes[4] = (byte) (0xff & length);
-    in.readFully(bytes, 5, length);
-    return bytes;
-  }
-
-  /**
-   * Reads the vector following a <code>Type.VECTOR</code> code.
-   * @return the obtained vector
-   * @throws IOException
-   */
-  @SuppressWarnings("unchecked")
-  public ArrayList readVector() throws IOException {
-    int length = readVectorHeader();
-    ArrayList result = new ArrayList(length);
-    for (int i = 0; i < length; i++) {
-      result.add(read());
-    }
-    return result;
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.VECTOR</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawVector() throws IOException {
-    Buffer buffer = new Buffer();
-    int length = readVectorHeader();
-    buffer.append(new byte[] {
-      (byte) Type.VECTOR.code,
-      (byte) (0xff & (length >> 24)), (byte) (0xff & (length >> 16)),
-      (byte) (0xff & (length >> 8)), (byte) (0xff & length)
-    });
-    for (int i = 0; i < length; i++) {
-      buffer.append(readRaw());
-    }
-    return buffer.get();
-  }
-
-  /**
-   * Reads the header following a <code>Type.VECTOR</code> code.
-   * @return the number of elements in the vector
-   * @throws IOException
-   */
-  public int readVectorHeader() throws IOException {
-    return in.readInt();
-  }
-
-  /**
-   * Reads the list following a <code>Type.LIST</code> code.
-   * @return the obtained list
-   * @throws IOException
-   */
-  @SuppressWarnings("unchecked")
-  public List readList() throws IOException {
-    List list = new ArrayList();
-    Object obj = read();
-    while (obj != null) {
-      list.add(obj);
-      obj = read();
-    }
-    return list;
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.LIST</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawList() throws IOException {
-    Buffer buffer = new Buffer(new byte[] { (byte) Type.LIST.code });
-    byte[] bytes = readRaw();
-    while (bytes != null) {
-      buffer.append(bytes);
-      bytes = readRaw();
-    }
-    buffer.append(new byte[] { (byte) Type.MARKER.code });
-    return buffer.get();
-  }
-
-  /**
-   * Reads the map following a <code>Type.MAP</code> code.
-   * @return the obtained map
-   * @throws IOException
-   */
-  @SuppressWarnings("unchecked")
-  public TreeMap readMap() throws IOException {
-    int length = readMapHeader();
-    TreeMap result = new TreeMap();
-    for (int i = 0; i < length; i++) {
-      Object key = read();
-      Object value = read();
-      result.put(key, value);
-    }
-    return result;
-  }
-
-  /**
-   * Reads the raw bytes following a <code>Type.MAP</code> code.
-   * @return the obtained bytes sequence
-   * @throws IOException
-   */
-  public byte[] readRawMap() throws IOException {
-    Buffer buffer = new Buffer();
-    int length = readMapHeader();
-    buffer.append(new byte[] {
-      (byte) Type.MAP.code,
-      (byte) (0xff & (length >> 24)), (byte) (0xff & (length >> 16)),
-      (byte) (0xff & (length >> 8)), (byte) (0xff & length)
-    });
-    for (int i = 0; i < length; i++) {
-      buffer.append(readRaw());
-      buffer.append(readRaw());
-    }
-    return buffer.get();
-  }
-
-  /**
-   * Reads the header following a <code>Type.MAP</code> code.
-   * @return the number of key-value pairs in the map
-   * @throws IOException
-   */
-  public int readMapHeader() throws IOException {
-    return in.readInt();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesOutput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesOutput.java
deleted file mode 100644
index ecb35656c6c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesOutput.java
+++ /dev/null
@@ -1,334 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-import java.io.DataOutput;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.Map.Entry;
-
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.record.Buffer;
-
-/**
- * Provides functionality for writing typed bytes.
- */
-public class TypedBytesOutput {
-
-  private DataOutput out;
-
-  private TypedBytesOutput() {}
-
-  private void setDataOutput(DataOutput out) {
-    this.out = out;
-  }
-
-  private static final ThreadLocal<TypedBytesOutput> TB_OUT =
-      new ThreadLocal<TypedBytesOutput>() {
-    @Override
-    protected TypedBytesOutput initialValue() {
-      return new TypedBytesOutput();
-    }
-  };
-
-  /**
-   * Get a thread-local typed bytes output for the supplied {@link DataOutput}.
-   * 
-   * @param out data output object
-   * @return typed bytes output corresponding to the supplied 
-   * {@link DataOutput}.
-   */
-  public static TypedBytesOutput get(DataOutput out) {
-    TypedBytesOutput bout = TB_OUT.get();
-    bout.setDataOutput(out);
-    return bout;
-  }
-
-  /** Creates a new instance of TypedBytesOutput. */
-  public TypedBytesOutput(DataOutput out) {
-    this.out = out;
-  }
-  
-  /**
-   * Writes a Java object as a typed bytes sequence.
-   * 
-   * @param obj the object to be written
-   * @throws IOException
-   */
-  public void write(Object obj) throws IOException {
-    if (obj instanceof Buffer) {
-      writeBytes((Buffer) obj);
-    } else if (obj instanceof Byte) {
-      writeByte((Byte) obj);
-    } else if (obj instanceof Boolean) {
-      writeBool((Boolean) obj);
-    } else if (obj instanceof Integer) {
-      writeInt((Integer) obj);
-    } else if (obj instanceof Long) {
-      writeLong((Long) obj);
-    } else if (obj instanceof Float) {
-      writeFloat((Float) obj);
-    } else if (obj instanceof Double) {
-      writeDouble((Double) obj);
-    } else if (obj instanceof String) {
-      writeString((String) obj);
-    } else if (obj instanceof ArrayList) {
-      writeVector((ArrayList) obj);
-    } else if (obj instanceof List) {
-      writeList((List) obj);
-    } else if (obj instanceof Map) {
-      writeMap((Map) obj);
-    } else {
-      throw new RuntimeException("cannot write objects of this type");
-    }
-  }
-
-  /**
-   * Writes a raw sequence of typed bytes.
-   * 
-   * @param bytes the bytes to be written
-   * @throws IOException
-   */
-  public void writeRaw(byte[] bytes) throws IOException {
-    out.write(bytes);
-  }
-
-  /**
-   * Writes a raw sequence of typed bytes.
-   * 
-   * @param bytes the bytes to be written
-   * @param offset an offset in the given array
-   * @param length number of bytes from the given array to write
-   * @throws IOException
-   */
-  public void writeRaw(byte[] bytes, int offset, int length)
-    throws IOException {
-    out.write(bytes, offset, length);
-  }
-
-  /**
-   * Writes a bytes array as a typed bytes sequence, using a given typecode 
-   * and length.
-   * 
-   * @param bytes the bytes array to be written
-   * @param code the typecode to use
-   * @param length the number of bytes to write, starting from position 0
-   * @throws IOException
-   */
-  public void writeBytes(byte[] bytes, int code, int length) throws IOException {
-    out.write(code);
-    out.writeInt(length);
-    out.write(bytes, 0, length);
-  }
-  
-  /**
-   * Writes a bytes array as a typed bytes sequence, using a given typecode.
-   * 
-   * @param bytes the bytes array to be written
-   * @param code the typecode to use
-   * @throws IOException
-   */
-  public void writeBytes(byte[] bytes, int code) throws IOException {
-    writeBytes(bytes, code, bytes.length);
-  }
-  
-  /**
-   * Writes a bytes array as a typed bytes sequence.
-   * 
-   * @param bytes the bytes array to be written
-   * @throws IOException
-   */
-  public void writeBytes(byte[] bytes) throws IOException {
-    writeBytes(bytes, Type.BYTES.code);
-  }
-  
-  /**
-   * Writes a bytes buffer as a typed bytes sequence.
-   * 
-   * @param buffer the bytes buffer to be written
-   * @throws IOException
-   */
-  public void writeBytes(Buffer buffer) throws IOException {
-    writeBytes(buffer.get(), Type.BYTES.code, buffer.getCount());
-  }
-
-  /**
-   * Writes a byte as a typed bytes sequence.
-   * 
-   * @param b the byte to be written
-   * @throws IOException
-   */
-  public void writeByte(byte b) throws IOException {
-    out.write(Type.BYTE.code);
-    out.write(b);
-  }
-
-  /**
-   * Writes a boolean as a typed bytes sequence.
-   * 
-   * @param b the boolean to be written
-   * @throws IOException
-   */
-  public void writeBool(boolean b) throws IOException {
-    out.write(Type.BOOL.code);
-    out.writeBoolean(b);
-  }
-
-  /**
-   * Writes an integer as a typed bytes sequence.
-   * 
-   * @param i the integer to be written
-   * @throws IOException
-   */
-  public void writeInt(int i) throws IOException {
-    out.write(Type.INT.code);
-    out.writeInt(i);
-  }
-
-  /**
-   * Writes a long as a typed bytes sequence.
-   * 
-   * @param l the long to be written
-   * @throws IOException
-   */
-  public void writeLong(long l) throws IOException {
-    out.write(Type.LONG.code);
-    out.writeLong(l);
-  }
-
-  /**
-   * Writes a float as a typed bytes sequence.
-   * 
-   * @param f the float to be written
-   * @throws IOException
-   */
-  public void writeFloat(float f) throws IOException {
-    out.write(Type.FLOAT.code);
-    out.writeFloat(f);
-  }
-
-  /**
-   * Writes a double as a typed bytes sequence.
-   * 
-   * @param d the double to be written
-   * @throws IOException
-   */
-  public void writeDouble(double d) throws IOException {
-    out.write(Type.DOUBLE.code);
-    out.writeDouble(d);
-  }
-
-  /**
-   * Writes a string as a typed bytes sequence.
-   * 
-   * @param s the string to be written
-   * @throws IOException
-   */
-  public void writeString(String s) throws IOException {
-    out.write(Type.STRING.code);
-    WritableUtils.writeString(out, s);
-  }
-
-  /**
-   * Writes a vector as a typed bytes sequence.
-   * 
-   * @param vector the vector to be written
-   * @throws IOException
-   */
-  public void writeVector(ArrayList vector) throws IOException {
-    writeVectorHeader(vector.size());
-    for (Object obj : vector) {
-      write(obj);
-    }
-  }
-
-  /**
-   * Writes a vector header.
-   * 
-   * @param length the number of elements in the vector
-   * @throws IOException
-   */
-  public void writeVectorHeader(int length) throws IOException {
-    out.write(Type.VECTOR.code);
-    out.writeInt(length);
-  }
-
-  /**
-   * Writes a list as a typed bytes sequence.
-   * 
-   * @param list the list to be written
-   * @throws IOException
-   */
-  public void writeList(List list) throws IOException {
-    writeListHeader();
-    for (Object obj : list) {
-      write(obj);
-    }
-    writeListFooter();
-  }
-
-  /**
-   * Writes a list header.
-   * 
-   * @throws IOException
-   */
-  public void writeListHeader() throws IOException {
-    out.write(Type.LIST.code);
-  }
-
-  /**
-   * Writes a list footer.
-   * 
-   * @throws IOException
-   */
-  public void writeListFooter() throws IOException {
-    out.write(Type.MARKER.code);
-  }
-
-  /**
-   * Writes a map as a typed bytes sequence.
-   * 
-   * @param map the map to be written
-   * @throws IOException
-   */
-  @SuppressWarnings("unchecked")
-  public void writeMap(Map map) throws IOException {
-    writeMapHeader(map.size());
-    Set<Entry> entries = map.entrySet();
-    for (Entry entry : entries) {
-      write(entry.getKey());
-      write(entry.getValue());
-    }
-  }
-
-  /**
-   * Writes a map header.
-   * 
-   * @param length the number of key-value pairs in the map
-   * @throws IOException
-   */
-  public void writeMapHeader(int length) throws IOException {
-    out.write(Type.MAP.code);
-    out.writeInt(length);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordInput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordInput.java
deleted file mode 100644
index e5e918c7d7c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordInput.java
+++ /dev/null
@@ -1,161 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-import java.io.DataInput;
-import java.io.IOException;
-
-import org.apache.hadoop.record.Buffer;
-import org.apache.hadoop.record.Index;
-import org.apache.hadoop.record.RecordInput;
-
-/**
- * Serializer for records that writes typed bytes.
- */
-public class TypedBytesRecordInput implements RecordInput {
-
-  private TypedBytesInput in;
-
-  private TypedBytesRecordInput() {}
-
-  private void setTypedBytesInput(TypedBytesInput in) {
-    this.in = in;
-  }
-
-  private static final ThreadLocal<TypedBytesRecordInput> TB_IN =
-      new ThreadLocal<TypedBytesRecordInput>() {
-    @Override
-    protected TypedBytesRecordInput initialValue() {
-      return new TypedBytesRecordInput();
-    }
-  };
-
-  /**
-   * Get a thread-local typed bytes record input for the supplied
-   * {@link TypedBytesInput}.
-   * 
-   * @param in typed bytes input object
-   * @return typed bytes record input corresponding to the supplied
-   *         {@link TypedBytesInput}.
-   */
-  public static TypedBytesRecordInput get(TypedBytesInput in) {
-    TypedBytesRecordInput bin = TB_IN.get();
-    bin.setTypedBytesInput(in);
-    return bin;
-  }
-
-  /**
-   * Get a thread-local typed bytes record input for the supplied
-   * {@link DataInput}.
-   * 
-   * @param in data input object
-   * @return typed bytes record input corresponding to the supplied
-   *         {@link DataInput}.
-   */
-  public static TypedBytesRecordInput get(DataInput in) {
-    return get(TypedBytesInput.get(in));
-  }
-
-  /** Creates a new instance of TypedBytesRecordInput. */
-  public TypedBytesRecordInput(TypedBytesInput in) {
-    this.in = in;
-  }
-
-  /** Creates a new instance of TypedBytesRecordInput. */
-  public TypedBytesRecordInput(DataInput in) {
-    this(new TypedBytesInput(in));
-  }
-
-  public boolean readBool(String tag) throws IOException {
-    in.skipType();
-    return in.readBool();
-  }
-
-  public Buffer readBuffer(String tag) throws IOException {
-    in.skipType();
-    return new Buffer(in.readBytes());
-  }
-
-  public byte readByte(String tag) throws IOException {
-    in.skipType();
-    return in.readByte();
-  }
-
-  public double readDouble(String tag) throws IOException {
-    in.skipType();
-    return in.readDouble();
-  }
-
-  public float readFloat(String tag) throws IOException {
-    in.skipType();
-    return in.readFloat();
-  }
-
-  public int readInt(String tag) throws IOException {
-    in.skipType();
-    return in.readInt();
-  }
-
-  public long readLong(String tag) throws IOException {
-    in.skipType();
-    return in.readLong();
-  }
-
-  public String readString(String tag) throws IOException {
-    in.skipType();
-    return in.readString();
-  }
-
-  public void startRecord(String tag) throws IOException {
-    in.skipType();
-  }
-
-  public Index startVector(String tag) throws IOException {
-    in.skipType();
-    return new TypedBytesIndex(in.readVectorHeader());
-  }
-
-  public Index startMap(String tag) throws IOException {
-    in.skipType();
-    return new TypedBytesIndex(in.readMapHeader());
-  }
-
-  public void endRecord(String tag) throws IOException {}
-
-  public void endVector(String tag) throws IOException {}
-
-  public void endMap(String tag) throws IOException {}
-
-  private static  final class TypedBytesIndex implements Index {
-    private int nelems;
-
-    private TypedBytesIndex(int nelems) {
-      this.nelems = nelems;
-    }
-
-    public boolean done() {
-      return (nelems <= 0);
-    }
-
-    public void incr() {
-      nelems--;
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordOutput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordOutput.java
deleted file mode 100644
index 8f55206f7d3..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesRecordOutput.java
+++ /dev/null
@@ -1,139 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-import java.io.DataOutput;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.TreeMap;
-
-import org.apache.hadoop.record.Buffer;
-import org.apache.hadoop.record.Record;
-import org.apache.hadoop.record.RecordOutput;
-
-/**
- * Deserialized for records that reads typed bytes.
- */
-public class TypedBytesRecordOutput implements RecordOutput {
-
-  private TypedBytesOutput out;
-
-  private TypedBytesRecordOutput() {}
-
-  private void setTypedBytesOutput(TypedBytesOutput out) {
-    this.out = out;
-  }
-
-  private static final ThreadLocal<TypedBytesRecordOutput> TB_OUT =
-      new ThreadLocal<TypedBytesRecordOutput>() {
-    @Override
-    protected TypedBytesRecordOutput initialValue() {
-      return new TypedBytesRecordOutput();
-    }
-  };
-
-  /**
-   * Get a thread-local typed bytes record input for the supplied
-   * {@link TypedBytesOutput}.
-   * 
-   * @param out typed bytes output object
-   * @return typed bytes record output corresponding to the supplied
-   *         {@link TypedBytesOutput}.
-   */
-  public static TypedBytesRecordOutput get(TypedBytesOutput out) {
-    TypedBytesRecordOutput bout = TB_OUT.get();
-    bout.setTypedBytesOutput(out);
-    return bout;
-  }
-
-  /**
-   * Get a thread-local typed bytes record output for the supplied
-   * {@link DataOutput}.
-   * 
-   * @param out data output object
-   * @return typed bytes record output corresponding to the supplied
-   *         {@link DataOutput}.
-   */
-  public static TypedBytesRecordOutput get(DataOutput out) {
-    return get(TypedBytesOutput.get(out));
-  }
-
-  /** Creates a new instance of TypedBytesRecordOutput. */
-  public TypedBytesRecordOutput(TypedBytesOutput out) {
-    this.out = out;
-  }
-
-  /** Creates a new instance of TypedBytesRecordOutput. */
-  public TypedBytesRecordOutput(DataOutput out) {
-    this(new TypedBytesOutput(out));
-  }
-
-  public void writeBool(boolean b, String tag) throws IOException {
-    out.writeBool(b);
-  }
-
-  public void writeBuffer(Buffer buf, String tag) throws IOException {
-    out.writeBytes(buf.get());
-  }
-
-  public void writeByte(byte b, String tag) throws IOException {
-    out.writeByte(b);
-  }
-
-  public void writeDouble(double d, String tag) throws IOException {
-    out.writeDouble(d);
-  }
-
-  public void writeFloat(float f, String tag) throws IOException {
-    out.writeFloat(f);
-  }
-
-  public void writeInt(int i, String tag) throws IOException {
-    out.writeInt(i);
-  }
-
-  public void writeLong(long l, String tag) throws IOException {
-    out.writeLong(l);
-  }
-
-  public void writeString(String s, String tag) throws IOException {
-    out.writeString(s);
-  }
-
-  public void startRecord(Record r, String tag) throws IOException {
-    out.writeListHeader();
-  }
-
-  public void startVector(ArrayList v, String tag) throws IOException {
-    out.writeVectorHeader(v.size());
-  }
-
-  public void startMap(TreeMap m, String tag) throws IOException {
-    out.writeMapHeader(m.size());
-  }
-
-  public void endRecord(Record r, String tag) throws IOException {
-    out.writeListFooter();
-  }
-
-  public void endVector(ArrayList v, String tag) throws IOException {}
-
-  public void endMap(TreeMap m, String tag) throws IOException {}
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritable.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritable.java
deleted file mode 100644
index d0cca560059..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritable.java
+++ /dev/null
@@ -1,88 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.io.BytesWritable;
-
-/**
- * Writable for typed bytes.
- */
-public class TypedBytesWritable extends BytesWritable {
-
-  /** Create a TypedBytesWritable. */
-  public TypedBytesWritable() {
-    super();
-  }
-
-  /** Create a TypedBytesWritable with a given byte array as initial value. */
-  public TypedBytesWritable(byte[] bytes) {
-    super(bytes);
-  }
-
-  /** Set the typed bytes from a given Java object. */
-  public void setValue(Object obj) {
-    try {
-      ByteArrayOutputStream baos = new ByteArrayOutputStream();
-      TypedBytesOutput tbo = TypedBytesOutput.get(new DataOutputStream(baos));
-      tbo.write(obj);
-      byte[] bytes = baos.toByteArray();
-      set(bytes, 0, bytes.length);
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  /** Get the typed bytes as a Java object. */
-  public Object getValue() {
-    try {
-      ByteArrayInputStream bais = new ByteArrayInputStream(getBytes());
-      TypedBytesInput tbi = TypedBytesInput.get(new DataInputStream(bais));
-      Object obj = tbi.read();
-      return obj;
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  /** Get the type code embedded in the first byte. */
-  public Type getType() {
-    byte[] bytes = getBytes();
-    if (bytes == null || bytes.length == 0) {
-      return null;
-    }
-    for (Type type : Type.values()) {
-      if (type.code == (int) bytes[0]) {
-        return type;
-      }
-    }
-    return null;
-  }
-
-  /** Generate a suitable string representation. */
-  public String toString() {
-    return getValue().toString();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableInput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableInput.java
deleted file mode 100644
index 70b2100361c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableInput.java
+++ /dev/null
@@ -1,385 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-import java.io.ByteArrayInputStream;
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configurable;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.BooleanWritable;
-import org.apache.hadoop.io.ByteWritable;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.MapWritable;
-import org.apache.hadoop.io.SortedMapWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.VIntWritable;
-import org.apache.hadoop.io.VLongWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.util.ReflectionUtils;
-
-/**
- * Provides functionality for reading typed bytes as Writable objects.
- * 
- * @see TypedBytesInput
- */
-public class TypedBytesWritableInput implements Configurable {
-
-  private TypedBytesInput in;
-  private Configuration conf;
-
-  private TypedBytesWritableInput() {
-    conf = new Configuration();
-  }
-
-  private void setTypedBytesInput(TypedBytesInput in) {
-    this.in = in;
-  }
-
-  private static final ThreadLocal<TypedBytesWritableInput> TB_IN =
-      new ThreadLocal<TypedBytesWritableInput>() {
-    @Override
-    protected TypedBytesWritableInput initialValue() {
-      return new TypedBytesWritableInput();
-    }
-  };
-
-  /**
-   * Get a thread-local typed bytes writable input for the supplied
-   * {@link TypedBytesInput}.
-   * 
-   * @param in typed bytes input object
-   * @return typed bytes writable input corresponding to the supplied
-   *         {@link TypedBytesInput}.
-   */
-  public static TypedBytesWritableInput get(TypedBytesInput in) {
-    TypedBytesWritableInput bin = TB_IN.get();
-    bin.setTypedBytesInput(in);
-    return bin;
-  }
-
-  /**
-   * Get a thread-local typed bytes writable input for the supplied
-   * {@link DataInput}.
-   * 
-   * @param in data input object
-   * @return typed bytes writable input corresponding to the supplied
-   *         {@link DataInput}.
-   */
-  public static TypedBytesWritableInput get(DataInput in) {
-    return get(TypedBytesInput.get(in));
-  }
-
-  /** Creates a new instance of TypedBytesWritableInput. */
-  public TypedBytesWritableInput(TypedBytesInput in) {
-    this();
-    this.in = in;
-  }
-
-  /** Creates a new instance of TypedBytesWritableInput. */
-  public TypedBytesWritableInput(DataInput din) {
-    this(new TypedBytesInput(din));
-  }
-
-  public Writable read() throws IOException {
-    Type type = in.readType();
-    if (type == null) {
-      return null;
-    }
-    switch (type) {
-    case BYTES:
-      return readBytes();
-    case BYTE:
-      return readByte();
-    case BOOL:
-      return readBoolean();
-    case INT:
-      return readVInt();
-    case LONG:
-      return readVLong();
-    case FLOAT:
-      return readFloat();
-    case DOUBLE:
-      return readDouble();
-    case STRING:
-      return readText();
-    case VECTOR:
-      return readArray();
-    case MAP:
-      return readMap();
-    case WRITABLE:
-      return readWritable();
-    default:
-      throw new RuntimeException("unknown type");
-    }
-  }
-
-  public Class<? extends Writable> readType() throws IOException {
-    Type type = in.readType();
-    if (type == null) {
-      return null;
-    }
-    switch (type) {
-    case BYTES:
-      return BytesWritable.class;
-    case BYTE:
-      return ByteWritable.class;
-    case BOOL:
-      return BooleanWritable.class;
-    case INT:
-      return VIntWritable.class;
-    case LONG:
-      return VLongWritable.class;
-    case FLOAT:
-      return FloatWritable.class;
-    case DOUBLE:
-      return DoubleWritable.class;
-    case STRING:
-      return Text.class;
-    case VECTOR:
-      return ArrayWritable.class;
-    case MAP:
-      return MapWritable.class;
-    case WRITABLE:
-      return Writable.class;
-    default:
-      throw new RuntimeException("unknown type");
-    }
-  }
-
-  public BytesWritable readBytes(BytesWritable bw) throws IOException {
-    byte[] bytes = in.readBytes();
-    if (bw == null) {
-      bw = new BytesWritable(bytes);
-    } else {
-      bw.set(bytes, 0, bytes.length);
-    }
-    return bw;
-  }
-
-  public BytesWritable readBytes() throws IOException {
-    return readBytes(null);
-  }
-
-  public ByteWritable readByte(ByteWritable bw) throws IOException {
-    if (bw == null) {
-      bw = new ByteWritable();
-    }
-    bw.set(in.readByte());
-    return bw;
-  }
-
-  public ByteWritable readByte() throws IOException {
-    return readByte(null);
-  }
-
-  public BooleanWritable readBoolean(BooleanWritable bw) throws IOException {
-    if (bw == null) {
-      bw = new BooleanWritable();
-    }
-    bw.set(in.readBool());
-    return bw;
-  }
-
-  public BooleanWritable readBoolean() throws IOException {
-    return readBoolean(null);
-  }
-
-  public IntWritable readInt(IntWritable iw) throws IOException {
-    if (iw == null) {
-      iw = new IntWritable();
-    }
-    iw.set(in.readInt());
-    return iw;
-  }
-
-  public IntWritable readInt() throws IOException {
-    return readInt(null);
-  }
-
-  public VIntWritable readVInt(VIntWritable iw) throws IOException {
-    if (iw == null) {
-      iw = new VIntWritable();
-    }
-    iw.set(in.readInt());
-    return iw;
-  }
-
-  public VIntWritable readVInt() throws IOException {
-    return readVInt(null);
-  }
-
-  public LongWritable readLong(LongWritable lw) throws IOException {
-    if (lw == null) {
-      lw = new LongWritable();
-    }
-    lw.set(in.readLong());
-    return lw;
-  }
-
-  public LongWritable readLong() throws IOException {
-    return readLong(null);
-  }
-
-  public VLongWritable readVLong(VLongWritable lw) throws IOException {
-    if (lw == null) {
-      lw = new VLongWritable();
-    }
-    lw.set(in.readLong());
-    return lw;
-  }
-
-  public VLongWritable readVLong() throws IOException {
-    return readVLong(null);
-  }
-
-  public FloatWritable readFloat(FloatWritable fw) throws IOException {
-    if (fw == null) {
-      fw = new FloatWritable();
-    }
-    fw.set(in.readFloat());
-    return fw;
-  }
-
-  public FloatWritable readFloat() throws IOException {
-    return readFloat(null);
-  }
-
-  public DoubleWritable readDouble(DoubleWritable dw) throws IOException {
-    if (dw == null) {
-      dw = new DoubleWritable();
-    }
-    dw.set(in.readDouble());
-    return dw;
-  }
-
-  public DoubleWritable readDouble() throws IOException {
-    return readDouble(null);
-  }
-
-  public Text readText(Text t) throws IOException {
-    if (t == null) {
-      t = new Text();
-    }
-    t.set(in.readString());
-    return t;
-  }
-
-  public Text readText() throws IOException {
-    return readText(null);
-  }
-
-  public ArrayWritable readArray(ArrayWritable aw) throws IOException {
-    if (aw == null) {
-      aw = new ArrayWritable(TypedBytesWritable.class);
-    } else if (!aw.getValueClass().equals(TypedBytesWritable.class)) {
-      throw new RuntimeException("value class has to be TypedBytesWritable");
-    }
-    int length = in.readVectorHeader();
-    Writable[] writables = new Writable[length];
-    for (int i = 0; i < length; i++) {
-      writables[i] = new TypedBytesWritable(in.readRaw());
-    }
-    aw.set(writables);
-    return aw;
-  }
-
-  public ArrayWritable readArray() throws IOException {
-    return readArray(null);
-  }
-
-  public MapWritable readMap(MapWritable mw) throws IOException {
-    if (mw == null) {
-      mw = new MapWritable();
-    }
-    int length = in.readMapHeader();
-    for (int i = 0; i < length; i++) {
-      Writable key = read();
-      Writable value = read();
-      mw.put(key, value);
-    }
-    return mw;
-  }
-
-  public MapWritable readMap() throws IOException {
-    return readMap(null);
-  }
-
-  public <K extends WritableComparable<? super K>>
-    SortedMapWritable<K> readSortedMap(SortedMapWritable<K> mw)
-    throws IOException {
-    if (mw == null) {
-      mw = new SortedMapWritable<K>();
-    }
-    int length = in.readMapHeader();
-    for (int i = 0; i < length; i++) {
-      @SuppressWarnings("unchecked")
-      K key = (K) read();
-      Writable value = read();
-      mw.put(key, value);
-    }
-    return mw;
-  }
-
-  public <K extends WritableComparable<? super K>> SortedMapWritable<K>
-    readSortedMap() throws IOException {
-    return readSortedMap((SortedMapWritable<K>)null);
-  }
-  
-  public Writable readWritable(Writable writable) throws IOException {
-    ByteArrayInputStream bais = new ByteArrayInputStream(in.readBytes());
-    DataInputStream dis = new DataInputStream(bais);
-    String className = WritableUtils.readString(dis);
-    if (writable == null) {
-      try {
-        Class<? extends Writable> cls = 
-          conf.getClassByName(className).asSubclass(Writable.class);
-        writable = (Writable) ReflectionUtils.newInstance(cls, conf);
-      } catch (ClassNotFoundException e) {
-        throw new IOException(e);
-      }
-    } else if (!writable.getClass().getName().equals(className)) {
-      throw new IOException("wrong Writable class given");
-    }
-    writable.readFields(dis);
-    return writable;
-  }
-
-  public Writable readWritable() throws IOException {
-    return readWritable(null);
-  }
-
-  public Configuration getConf() {
-    return conf;
-  }
-
-  public void setConf(Configuration conf) {
-    this.conf = conf;
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableOutput.java b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableOutput.java
deleted file mode 100644
index e7afe93548c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/TypedBytesWritableOutput.java
+++ /dev/null
@@ -1,224 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Map;
-
-import org.apache.hadoop.io.ArrayWritable;
-import org.apache.hadoop.io.BooleanWritable;
-import org.apache.hadoop.io.ByteWritable;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.DoubleWritable;
-import org.apache.hadoop.io.FloatWritable;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.MapWritable;
-import org.apache.hadoop.io.SortedMapWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.VIntWritable;
-import org.apache.hadoop.io.VLongWritable;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.record.Record;
-
-/**
- * Provides functionality for writing Writable objects as typed bytes.
- * 
- * @see TypedBytesOutput
- */
-public class TypedBytesWritableOutput {
-
-  private TypedBytesOutput out;
-
-  private TypedBytesWritableOutput() {}
-
-  private void setTypedBytesOutput(TypedBytesOutput out) {
-    this.out = out;
-  }
-
-  private static final ThreadLocal<TypedBytesWritableOutput> TB_OUT =
-      new ThreadLocal<TypedBytesWritableOutput>() {
-    @Override
-    protected TypedBytesWritableOutput initialValue() {
-      return new TypedBytesWritableOutput();
-    }
-  };
-
-  /**
-   * Get a thread-local typed bytes writable input for the supplied
-   * {@link TypedBytesOutput}.
-   * 
-   * @param out typed bytes output object
-   * @return typed bytes writable output corresponding to the supplied
-   *         {@link TypedBytesOutput}.
-   */
-  public static TypedBytesWritableOutput get(TypedBytesOutput out) {
-    TypedBytesWritableOutput bout = TB_OUT.get();
-    bout.setTypedBytesOutput(out);
-    return bout;
-  }
-
-  /**
-   * Get a thread-local typed bytes writable output for the supplied
-   * {@link DataOutput}.
-   * 
-   * @param out data output object
-   * @return typed bytes writable output corresponding to the supplied
-   *         {@link DataOutput}.
-   */
-  public static TypedBytesWritableOutput get(DataOutput out) {
-    return get(TypedBytesOutput.get(out));
-  }
-
-  /** Creates a new instance of TypedBytesWritableOutput. */
-  public TypedBytesWritableOutput(TypedBytesOutput out) {
-    this();
-    this.out = out;
-  }
-
-  /** Creates a new instance of TypedBytesWritableOutput. */
-  public TypedBytesWritableOutput(DataOutput dout) {
-    this(new TypedBytesOutput(dout));
-  }
-
-  public void write(Writable w) throws IOException {
-    if (w instanceof TypedBytesWritable) {
-      writeTypedBytes((TypedBytesWritable) w);
-    } else if (w instanceof BytesWritable) {
-      writeBytes((BytesWritable) w);
-    } else if (w instanceof ByteWritable) {
-      writeByte((ByteWritable) w);
-    } else if (w instanceof BooleanWritable) {
-      writeBoolean((BooleanWritable) w);
-    } else if (w instanceof IntWritable) {
-      writeInt((IntWritable) w);
-    } else if (w instanceof VIntWritable) {
-      writeVInt((VIntWritable) w);
-    } else if (w instanceof LongWritable) {
-      writeLong((LongWritable) w);
-    } else if (w instanceof VLongWritable) {
-      writeVLong((VLongWritable) w);
-    } else if (w instanceof FloatWritable) {
-      writeFloat((FloatWritable) w);
-    } else if (w instanceof DoubleWritable) {
-      writeDouble((DoubleWritable) w);
-    } else if (w instanceof Text) {
-      writeText((Text) w);
-    } else if (w instanceof ArrayWritable) {
-      writeArray((ArrayWritable) w);
-    } else if (w instanceof MapWritable) {
-      writeMap((MapWritable) w);
-    } else if (w instanceof SortedMapWritable) {
-      writeSortedMap((SortedMapWritable<?>) w);
-    } else if (w instanceof Record) {
-      writeRecord((Record) w);
-    } else {
-      writeWritable(w); // last resort
-    }
-  }
-
-  public void writeTypedBytes(TypedBytesWritable tbw) throws IOException {
-    out.writeRaw(tbw.getBytes(), 0, tbw.getLength());
-  }
-
-  public void writeBytes(BytesWritable bw) throws IOException {
-    byte[] bytes = Arrays.copyOfRange(bw.getBytes(), 0, bw.getLength());
-    out.writeBytes(bytes);
-  }
-
-  public void writeByte(ByteWritable bw) throws IOException {
-    out.writeByte(bw.get());
-  }
-
-  public void writeBoolean(BooleanWritable bw) throws IOException {
-    out.writeBool(bw.get());
-  }
-
-  public void writeInt(IntWritable iw) throws IOException {
-    out.writeInt(iw.get());
-  }
-
-  public void writeVInt(VIntWritable viw) throws IOException {
-    out.writeInt(viw.get());
-  }
-
-  public void writeLong(LongWritable lw) throws IOException {
-    out.writeLong(lw.get());
-  }
-
-  public void writeVLong(VLongWritable vlw) throws IOException {
-    out.writeLong(vlw.get());
-  }
-
-  public void writeFloat(FloatWritable fw) throws IOException {
-    out.writeFloat(fw.get());
-  }
-
-  public void writeDouble(DoubleWritable dw) throws IOException {
-    out.writeDouble(dw.get());
-  }
-
-  public void writeText(Text t) throws IOException {
-    out.writeString(t.toString());
-  }
-
-  public void writeArray(ArrayWritable aw) throws IOException {
-    Writable[] writables = aw.get();
-    out.writeVectorHeader(writables.length);
-    for (Writable writable : writables) {
-      write(writable);
-    }
-  }
-
-  public void writeMap(MapWritable mw) throws IOException {
-    out.writeMapHeader(mw.size());
-    for (Map.Entry<Writable, Writable> entry : mw.entrySet()) {
-      write(entry.getKey());
-      write(entry.getValue());
-    }
-  }
-
-  public void writeSortedMap(SortedMapWritable<?> smw) throws IOException {
-    out.writeMapHeader(smw.size());
-    for (Map.Entry<? extends WritableComparable<?>, Writable> entry : smw.entrySet()) {
-      write(entry.getKey());
-      write(entry.getValue());
-    }
-  }
-
-  public void writeRecord(Record r) throws IOException {
-    r.serialize(TypedBytesRecordOutput.get(out));
-  }
-
-  public void writeWritable(Writable w) throws IOException {
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    DataOutputStream dos = new DataOutputStream(baos);
-    WritableUtils.writeString(dos, w.getClass().getName());
-    w.write(dos);
-    dos.close();
-    out.writeBytes(baos.toByteArray(), Type.WRITABLE.code);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/package.html b/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/package.html
deleted file mode 100644
index 3494fbd8586..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/java/org/apache/hadoop/typedbytes/package.html
+++ /dev/null
@@ -1,67 +0,0 @@
-<html>
-
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<body>
-
-Typed bytes are sequences of bytes in which the first byte is a type code. They are especially useful as a 
-(simple and very straightforward) binary format for transferring data to and from Hadoop Streaming programs.
-
-<h3>Type Codes</h3>
-
-Each typed bytes sequence starts with an unsigned byte that contains the type code. Possible values are:
-
-<table border="1" cellpadding="2" summary="Type Codes">
-<tr><th>Code</th><th>Type</th></tr>
-<tr><td><i>0</i></td><td>A sequence of bytes.</td></tr>
-<tr><td><i>1</i></td><td>A byte.</td></tr>
-<tr><td><i>2</i></td><td>A boolean.</td></tr>
-<tr><td><i>3</i></td><td>An integer.</td></tr>
-<tr><td><i>4</i></td><td>A long.</td></tr>
-<tr><td><i>5</i></td><td>A float.</td></tr>
-<tr><td><i>6</i></td><td>A double.</td></tr>
-<tr><td><i>7</i></td><td>A string.</td></tr>
-<tr><td><i>8</i></td><td>A vector.</td></tr>
-<tr><td><i>9</i></td><td>A list.</td></tr>
-<tr><td><i>10</i></td><td>A map.</td></tr>
-</table>
-
-The type codes <i>50</i> to <i>200</i> are treated as aliases for <i>0</i>, and can thus be used for
-application-specific serialization.
-
-<h3>Subsequent Bytes</h3>
-
-These are the subsequent bytes for the different type codes (everything is big-endian and unpadded):
-
-<table border="1" cellpadding="2" summary="Subsequent Bytes">
-<tr><th>Code</th><th>Subsequent Bytes</th></tr>
-<tr><td><i>0</i></td><td>&lt;32-bit signed integer&gt; &lt;as many bytes as indicated by the integer&gt;</td></tr>
-<tr><td><i>1</i></td><td>&lt;signed byte&gt;</td></tr>
-<tr><td><i>2</i></td><td>&lt;signed byte (<i>0</i> = <i>false</i> and <i>1</i> = <i>true</i>)&gt;</td></tr>
-<tr><td><i>3</i></td><td>&lt;32-bit signed integer&gt;</td></tr>
-<tr><td><i>4</i></td><td>&lt;64-bit signed integer&gt;</td></tr>
-<tr><td><i>5</i></td><td>&lt;32-bit IEEE floating point number&gt;</td></tr>
-<tr><td><i>6</i></td><td>&lt;64-bit IEEE floating point number&gt;</td></tr>
-<tr><td><i>7</i></td><td>&lt;32-bit signed integer&gt; &lt;as many UTF-8 bytes as indicated by the integer&gt;</td></tr>
-<tr><td><i>8</i></td><td>&lt;32-bit signed integer&gt; &lt;as many typed bytes sequences as indicated by the integer&gt;</td></tr>
-<tr><td><i>9</i></td><td>&lt;variable number of typed bytes sequences&gt; &lt;<i>255</i> written as an unsigned byte&gt;</td></tr>
-<tr><td><i>10</i></td><td>&lt;32-bit signed integer&gt; &lt;as many (key-value) pairs of typed bytes sequences as indicated by the integer&gt;</td></tr>
-</table>
-
-</body>
-</html>
diff --git a/hadoop-tools/hadoop-streaming/src/main/shellprofile.d/hadoop-streaming.sh b/hadoop-tools/hadoop-streaming/src/main/shellprofile.d/hadoop-streaming.sh
deleted file mode 100755
index be76b060595..00000000000
--- a/hadoop-tools/hadoop-streaming/src/main/shellprofile.d/hadoop-streaming.sh
+++ /dev/null
@@ -1,51 +0,0 @@
-#!/usr/bin/env bash
-
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-if ! declare -f mapred_subcommand_streaming >/dev/null 2>/dev/null; then
-
-  if [[ "${HADOOP_SHELL_EXECNAME}" = mapred ]]; then
-    hadoop_add_subcommand "streaming" client "launch a mapreduce streaming job"
-  fi
-
-## @description  streaming command for mapred
-## @audience     public
-## @stability    stable
-## @replaceable  yes
-function mapred_subcommand_streaming
-{
-  declare jarname
-  declare oldifs
-
-  # shellcheck disable=SC2034
-  HADOOP_CLASSNAME=org.apache.hadoop.util.RunJar
-  hadoop_add_to_classpath_tools hadoop-streaming
-
-  # locate the streaming jar so we have something to
-  # give to RunJar
-  oldifs=${IFS}
-  IFS=:
-  for jarname in ${CLASSPATH}; do
-    if [[ "${jarname}" =~ hadoop-streaming-[0-9] ]]; then
-      HADOOP_SUBCMD_ARGS=("${jarname}" "${HADOOP_SUBCMD_ARGS[@]}")
-      break
-    fi
-  done
-
-  IFS=${oldifs}
-}
-
-fi
diff --git a/hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm b/hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm
deleted file mode 100644
index f125b631b43..00000000000
--- a/hadoop-tools/hadoop-streaming/src/site/markdown/HadoopStreaming.md.vm
+++ /dev/null
@@ -1,576 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-   http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-#set ( $H3 = '###' )
-#set ( $H4 = '####' )
-#set ( $H5 = '#####' )
-
-Hadoop Streaming
-================
-
-* [Hadoop Streaming](#Hadoop_Streaming)
-    * [Hadoop Streaming](#Hadoop_Streaming)
-    * [How Streaming Works](#How_Streaming_Works)
-    * [Streaming Command Options](#Streaming_Command_Options)
-        * [Specifying a Java Class as the Mapper/Reducer](#Specifying_a_Java_Class_as_the_MapperReducer)
-        * [Packaging Files With Job Submissions](#Packaging_Files_With_Job_Submissions)
-        * [Specifying Other Plugins for Jobs](#Specifying_Other_Plugins_for_Jobs)
-        * [Setting Environment Variables](#Setting_Environment_Variables)
-    * [Generic Command Options](#Generic_Command_Options)
-        * [Specifying Configuration Variables with the -D Option](#Specifying_Configuration_Variables_with_the_-D_Option)
-            * [Specifying Directories](#Specifying_Directories)
-            * [Specifying Map-Only Jobs](#Specifying_Map-Only_Jobs)
-            * [Specifying the Number of Reducers](#Specifying_the_Number_of_Reducers)
-            * [Customizing How Lines are Split into Key/Value Pairs](#Customizing_How_Lines_are_Split_into_KeyValue_Pairs)
-        * [Working with Large Files and Archives](#Working_with_Large_Files_and_Archives)
-            * [Making Files Available to Tasks](#Making_Files_Available_to_Tasks)
-            * [Making Archives Available to Tasks](#Making_Archives_Available_to_Tasks)
-    * [More Usage Examples](#More_Usage_Examples)
-        * [Hadoop Partitioner Class](#Hadoop_Partitioner_Class)
-        * [Hadoop Comparator Class](#Hadoop_Comparator_Class)
-        * [Hadoop Aggregate Package](#Hadoop_Aggregate_Package)
-        * [Hadoop Field Selection Class](#Hadoop_Field_Selection_Class)
-    * [Frequently Asked Questions](#Frequently_Asked_Questions)
-        * [How do I use Hadoop Streaming to run an arbitrary set of (semi) independent tasks?](#How_do_I_use_Hadoop_Streaming_to_run_an_arbitrary_set_of_semi_independent_tasks)
-        * [How do I process files, one per map?](#How_do_I_process_files_one_per_map)
-        * [How many reducers should I use?](#How_many_reducers_should_I_use)
-        * [If I set up an alias in my shell script, will that work after -mapper?](#If_I_set_up_an_alias_in_my_shell_script_will_that_work_after_-mapper)
-        * [Can I use UNIX pipes?](#Can_I_use_UNIX_pipes)
-        * [What do I do if I get the "No space left on device" error?](#What_do_I_do_if_I_get_the_No_space_left_on_device_error)
-        * [How do I specify multiple input directories?](#How_do_I_specify_multiple_input_directories)
-        * [How do I generate output files with gzip format?](#How_do_I_generate_output_files_with_gzip_format)
-        * [How do I provide my own input/output format with streaming?](#How_do_I_provide_my_own_inputoutput_format_with_streaming)
-        * [How do I parse XML documents using streaming?](#How_do_I_parse_XML_documents_using_streaming)
-        * [How do I update counters in streaming applications?](#How_do_I_update_counters_in_streaming_applications)
-        * [How do I update status in streaming applications?](#How_do_I_update_status_in_streaming_applications)
-        * [How do I get the Job variables in a streaming job's mapper/reducer?](#How_do_I_get_the_Job_variables_in_a_streaming_jobs_mapperreducer)
-        * [What do I do if I get a "error=7, Argument list too long"](#What_do_I_do_if_I_get_a_error_Argument_list_too_long)
-
-Hadoop Streaming
-----------------
-
-Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer. For example:
-
-    mapred streaming \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper /bin/cat \
-      -reducer /usr/bin/wc
-
-How Streaming Works
--------------------
-
-In the above example, both the mapper and the reducer are executables that read the input from stdin (line by line) and emit the output to stdout. The utility will create a Map/Reduce job, submit the job to an appropriate cluster, and monitor the progress of the job until it completes.
-
-When an executable is specified for mappers, each mapper task will launch the executable as a separate process when the mapper is initialized. As the mapper task runs, it converts its inputs into lines and feed the lines to the stdin of the process. In the meantime, the mapper collects the line oriented outputs from the stdout of the process and converts each line into a key/value pair, which is collected as the output of the mapper. By default, the *prefix of a line up to the first tab character* is the `key` and the rest of the line (excluding the tab character) will be the `value`. If there is no tab character in the line, then entire line is considered as key and the value is null. However, this can be customized by setting `-inputformat` command option, as discussed later.
-
-When an executable is specified for reducers, each reducer task will launch the executable as a separate process then the reducer is initialized. As the reducer task runs, it converts its input key/values pairs into lines and feeds the lines to the stdin of the process. In the meantime, the reducer collects the line oriented outputs from the stdout of the process, converts each line into a key/value pair, which is collected as the output of the reducer. By default, the prefix of a line up to the first tab character is the key and the rest of the line (excluding the tab character) is the value. However, this can be customized by setting `-outputformat` command option, as discussed later.
-
-This is the basis for the communication protocol between the Map/Reduce framework and the streaming mapper/reducer.
-
-User can specify `stream.non.zero.exit.is.failure` as `true` or `false` to make a streaming task that exits with a non-zero status to be `Failure` or `Success` respectively. By default, streaming tasks exiting with non-zero status are considered to be failed tasks.
-
-Streaming Command Options
--------------------------
-
-Streaming supports streaming command options as well as [generic command options](#Generic_Command_Options). The general command line syntax is shown below.
-
-**Note:** Be sure to place the generic options before the streaming options, otherwise the command will fail. For an example, see [Making Archives Available to Tasks](#Making_Archives_Available_to_Tasks).
-
-    mapred streaming [genericOptions] [streamingOptions]
-
-The Hadoop streaming command options are listed here:
-
-| Parameter | Optional/Required | Description |
-|:---- |:---- |:---- |
-| -input directoryname or filename | Required | Input location for mapper |
-| -output directoryname | Required | Output location for reducer |
-| -mapper executable or JavaClassName | Optional | Mapper executable. If not specified, IdentityMapper is used as the default |
-| -reducer executable or JavaClassName | Optional | Reducer executable. If not specified, IdentityReducer is used as the default |
-| -file filename | Optional | Make the mapper, reducer, or combiner executable available locally on the compute nodes |
-| -inputformat JavaClassName | Optional | Class you supply should return key/value pairs of Text class. If not specified, TextInputFormat is used as the default |
-| -outputformat JavaClassName | Optional | Class you supply should take key/value pairs of Text class. If not specified, TextOutputformat is used as the default |
-| -partitioner JavaClassName | Optional | Class that determines which reduce a key is sent to |
-| -combiner streamingCommand or JavaClassName | Optional | Combiner executable for map output |
-| -cmdenv name=value | Optional | Pass environment variable to streaming commands |
-| -inputreader | Optional | For backwards-compatibility: specifies a record reader class (instead of an input format class) |
-| -verbose | Optional | Verbose output |
-| -lazyOutput | Optional | Create output lazily. For example, if the output format is based on FileOutputFormat, the output file is created only on the first call to Context.write |
-| -numReduceTasks | Optional | Specify the number of reducers |
-| -mapdebug | Optional | Script to call when map task fails |
-| -reducedebug | Optional | Script to call when reduce task fails |
-
-$H3 Specifying a Java Class as the Mapper/Reducer
-
-You can supply a Java class as the mapper and/or the reducer.
-
-    mapred streaming \
-      -input myInputDirs \
-      -output myOutputDir \
-      -inputformat org.apache.hadoop.mapred.KeyValueTextInputFormat \
-      -mapper org.apache.hadoop.mapred.lib.IdentityMapper \
-      -reducer /usr/bin/wc
-
-You can specify `stream.non.zero.exit.is.failure` as `true` or `false` to make a streaming task that exits with a non-zero status to be `Failure` or `Success` respectively. By default, streaming tasks exiting with non-zero status are considered to be failed tasks.
-
-$H3 Packaging Files With Job Submissions
-
-You can specify any executable as the mapper and/or the reducer. The executables do not need to pre-exist on the machines in the cluster; however, if they don't, you will need to use "-file" option to tell the framework to pack your executable files as a part of job submission. For example:
-
-    mapred streaming \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper myPythonScript.py \
-      -reducer /usr/bin/wc \
-      -file myPythonScript.py
-
-The above example specifies a user defined Python executable as the mapper. The option "-file myPythonScript.py" causes the python executable shipped to the cluster machines as a part of job submission.
-
-In addition to executable files, you can also package other auxiliary files (such as dictionaries, configuration files, etc) that may be used by the mapper and/or the reducer. For example:
-
-    mapred streaming \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper myPythonScript.py \
-      -reducer /usr/bin/wc \
-      -file myPythonScript.py \
-      -file myDictionary.txt
-
-$H3 Specifying Other Plugins for Jobs
-
-Just as with a normal Map/Reduce job, you can specify other plugins for a streaming job:
-
-     -inputformat JavaClassName
-     -outputformat JavaClassName
-     -partitioner JavaClassName
-     -combiner streamingCommand or JavaClassName
-
-The class you supply for the input format should return key/value pairs of Text class. If you do not specify an input format class, the TextInputFormat is used as the default. Since the TextInputFormat returns keys of LongWritable class, which are actually not part of the input data, the keys will be discarded; only the values will be piped to the streaming mapper.
-
-The class you supply for the output format is expected to take key/value pairs of Text class. If you do not specify an output format class, the TextOutputFormat is used as the default.
-
-$H3 Setting Environment Variables
-
-To set an environment variable in a streaming command use:
-
-     -cmdenv EXAMPLE_DIR=/home/example/dictionaries/
-
-Generic Command Options
------------------------
-
-Streaming supports [streaming command options](#Streaming_Command_Options) as well as generic command options. The general command line syntax is shown below.
-
-**Note:** Be sure to place the generic options before the streaming options, otherwise the command will fail. For an example, see [Making Archives Available to Tasks](#Making_Archives_Available_to_Tasks).
-
-    hadoop command [genericOptions] [streamingOptions]
-
-The Hadoop generic command options you can use with streaming are listed here:
-
-| Parameter | Optional/Required | Description |
-|:---- |:---- |:---- |
-| -conf configuration\_file | Optional | Specify an application configuration file |
-| -D property=value | Optional | Use value for given property |
-| -fs host:port or local | Optional | Specify a namenode |
-| -files | Optional | Specify comma-separated files to be copied to the Map/Reduce cluster |
-| -libjars | Optional | Specify comma-separated jar files to include in the classpath |
-| -archives | Optional | Specify comma-separated archives to be unarchived on the compute machines |
-
-$H3 Specifying Configuration Variables with the -D Option
-
-You can specify additional configuration variables by using "-D \<property\>=\<value\>".
-
-$H4 Specifying Directories
-
-To change the local temp directory use:
-
-     -D dfs.data.dir=/tmp
-
-To specify additional local temp directories use:
-
-     -D mapred.local.dir=/tmp/local
-     -D mapred.system.dir=/tmp/system
-     -D mapred.temp.dir=/tmp/temp
-
-**Note:** For more details on job configuration parameters see: [mapred-default.xml](../hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)
-
-$H4 Specifying Map-Only Jobs
-
-Often, you may want to process input data using a map function only. To do this, simply set `mapreduce.job.reduces` to zero. The Map/Reduce framework will not create any reducer tasks. Rather, the outputs of the mapper tasks will be the final output of the job.
-
-     -D mapreduce.job.reduces=0
-
-To be backward compatible, Hadoop Streaming also supports the "-reducer NONE" option, which is equivalent to "-D mapreduce.job.reduces=0".
-
-$H4 Specifying the Number of Reducers
-
-To specify the number of reducers, for example two, use:
-
-    mapred streaming \
-      -D mapreduce.job.reduces=2 \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper /bin/cat \
-      -reducer /usr/bin/wc
-
-$H4 Customizing How Lines are Split into Key/Value Pairs
-
-As noted earlier, when the Map/Reduce framework reads a line from the stdout of the mapper, it splits the line into a key/value pair. By default, the prefix of the line up to the first tab character is the key and the rest of the line (excluding the tab character) is the value.
-
-However, you can customize this default. You can specify a field separator other than the tab character (the default), and you can specify the nth (n \>= 1) character rather than the first character in a line (the default) as the separator between the key and value. For example:
-
-    mapred streaming \
-      -D stream.map.output.field.separator=. \
-      -D stream.num.map.output.key.fields=4 \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper /bin/cat \
-      -reducer /bin/cat
-
-In the above example, "-D stream.map.output.field.separator=." specifies "." as the field separator for the map outputs, and the prefix up to the fourth "." in a line will be the key and the rest of the line (excluding the fourth ".") will be the value. If a line has less than four "."s, then the whole line will be the key and the value will be an empty Text object (like the one created by new Text("")).
-
-Similarly, you can use "-D stream.reduce.output.field.separator=SEP" and "-D stream.num.reduce.output.fields=NUM" to specify the nth field separator in a line of the reduce outputs as the separator between the key and the value.
-
-Similarly, you can specify "stream.map.input.field.separator" and "stream.reduce.input.field.separator" as the input separator for Map/Reduce inputs. By default the separator is the tab character.
-
-$H3 Working with Large Files and Archives
-
-The -files and -archives options allow you to make files and archives available to the tasks. The argument is a URI to the file or archive that you have already uploaded to HDFS. These files and archives are cached across jobs. You can retrieve the host and fs\_port values from the fs.default.name config variable.
-
-**Note:** The -files and -archives options are generic options. Be sure to place the generic options before the command options, otherwise the command will fail.
-
-$H4 Making Files Available to Tasks
-
-The -files option creates a symlink in the current working directory of the tasks that points to the local copy of the file.
-
-In this example, Hadoop automatically creates a symlink named testfile.txt in the current working directory of the tasks. This symlink points to the local copy of testfile.txt.
-
-    -files hdfs://host:fs_port/user/testfile.txt
-
-User can specify a different symlink name for -files using \#.
-
-    -files hdfs://host:fs_port/user/testfile.txt#testfile
-
-Multiple entries can be specified like this:
-
-    -files hdfs://host:fs_port/user/testfile1.txt,hdfs://host:fs_port/user/testfile2.txt
-
-$H4 Making Archives Available to Tasks
-
-The -archives option allows you to copy jars locally to the current working directory of tasks and automatically unjar the files.
-
-In this example, Hadoop automatically creates a symlink named testfile.jar in the current working directory of tasks. This symlink points to the directory that stores the unjarred contents of the uploaded jar file.
-
-    -archives hdfs://host:fs_port/user/testfile.jar
-
-User can specify a different symlink name for -archives using \#.
-
-    -archives hdfs://host:fs_port/user/testfile.tgz#tgzdir
-
-In this example, the input.txt file has two lines specifying the names of the two files: cachedir.jar/cache.txt and cachedir.jar/cache2.txt. "cachedir.jar" is a symlink to the archived directory, which has the files "cache.txt" and "cache2.txt".
-
-    mapred streaming \
-                    -archives 'hdfs://hadoop-nn1.example.com/user/me/samples/cachefile/cachedir.jar' \
-                    -D mapreduce.job.maps=1 \
-                    -D mapreduce.job.reduces=1 \
-                    -D mapreduce.job.name="Experiment" \
-                    -input "/user/me/samples/cachefile/input.txt" \
-                    -output "/user/me/samples/cachefile/out" \
-                    -mapper "xargs cat" \
-                    -reducer "cat"
-    
-    $ ls test_jar/
-    cache.txt  cache2.txt
-    
-    $ jar cvf cachedir.jar -C test_jar/ .
-    added manifest
-    adding: cache.txt(in = 30) (out= 29)(deflated 3%)
-    adding: cache2.txt(in = 37) (out= 35)(deflated 5%)
-    
-    $ hdfs dfs -put cachedir.jar samples/cachefile
-    
-    $ hdfs dfs -cat /user/me/samples/cachefile/input.txt
-    cachedir.jar/cache.txt
-    cachedir.jar/cache2.txt
-    
-    $ cat test_jar/cache.txt
-    This is just the cache string
-    
-    $ cat test_jar/cache2.txt
-    This is just the second cache string
-    
-    $ hdfs dfs -ls /user/me/samples/cachefile/out
-    Found 2 items
-    -rw-r--r-* 1 me supergroup        0 2013-11-14 17:00 /user/me/samples/cachefile/out/_SUCCESS
-    -rw-r--r-* 1 me supergroup       69 2013-11-14 17:00 /user/me/samples/cachefile/out/part-00000
-    
-    $ hdfs dfs -cat /user/me/samples/cachefile/out/part-00000
-    This is just the cache string
-    This is just the second cache string
-
-More Usage Examples
--------------------
-
-$H3 Hadoop Partitioner Class
-
-Hadoop has a library class, [KeyFieldBasedPartitioner](../api/org/apache/hadoop/mapred/lib/KeyFieldBasedPartitioner.html), that is useful for many applications. This class allows the Map/Reduce framework to partition the map outputs based on certain key fields, not the whole keys. For example:
-
-    mapred streaming \
-      -D stream.map.output.field.separator=. \
-      -D stream.num.map.output.key.fields=4 \
-      -D map.output.key.field.separator=. \
-      -D mapreduce.partition.keypartitioner.options=-k1,2 \
-      -D mapreduce.job.reduces=12 \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper /bin/cat \
-      -reducer /bin/cat \
-      -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
-
-Here, *-D stream.map.output.field.separator=.* and *-D stream.num.map.output.key.fields=4* are as explained in previous example. The two variables are used by streaming to identify the key/value pair of mapper.
-
-The map output keys of the above Map/Reduce job normally have four fields separated by ".". However, the Map/Reduce framework will partition the map outputs by the first two fields of the keys using the *-D mapred.text.key.partitioner.options=-k1,2* option. Here, *-D map.output.key.field.separator=.* specifies the separator for the partition. This guarantees that all the key/value pairs with the same first two fields in the keys will be partitioned into the same reducer.
-
-*This is effectively equivalent to specifying the first two fields as the primary key and the next two fields as the secondary. The primary key is used for partitioning, and the combination of the primary and secondary keys is used for sorting.* A simple illustration is shown here:
-
-Output of map (the keys)
-
-    11.12.1.2
-    11.14.2.3
-    11.11.4.1
-    11.12.1.1
-    11.14.2.2
-
-Partition into 3 reducers (the first 2 fields are used as keys for partition)
-
-    11.11.4.1
-    -----------
-    11.12.1.2
-    11.12.1.1
-    -----------
-    11.14.2.3
-    11.14.2.2
-
-Sorting within each partition for the reducer(all 4 fields used for sorting)
-
-    11.11.4.1
-    -----------
-    11.12.1.1
-    11.12.1.2
-    -----------
-    11.14.2.2
-    11.14.2.3
-
-$H3 Hadoop Comparator Class
-
-Hadoop has a library class, [KeyFieldBasedComparator](../api/org/apache/hadoop/mapreduce/lib/partition/KeyFieldBasedComparator.html), that is useful for many applications. This class provides a subset of features provided by the Unix/GNU Sort. For example:
-
-    mapred streaming \
-      -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
-      -D stream.map.output.field.separator=. \
-      -D stream.num.map.output.key.fields=4 \
-      -D mapreduce.map.output.key.field.separator=. \
-      -D mapreduce.partition.keycomparator.options=-k2,2nr \
-      -D mapreduce.job.reduces=1 \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper /bin/cat \
-      -reducer /bin/cat
-
-The map output keys of the above Map/Reduce job normally have four fields separated by ".". However, the Map/Reduce framework will sort the outputs by the second field of the keys using the *-D mapreduce.partition.keycomparator.options=-k2,2nr* option. Here, *-n* specifies that the sorting is numerical sorting and *-r* specifies that the result should be reversed. A simple illustration is shown below:
-
-Output of map (the keys)
-
-    11.12.1.2
-    11.14.2.3
-    11.11.4.1
-    11.12.1.1
-    11.14.2.2
-
-Sorting output for the reducer (where second field used for sorting)
-
-    11.14.2.3
-    11.14.2.2
-    11.12.1.2
-    11.12.1.1
-    11.11.4.1
-
-$H3 Hadoop Aggregate Package
-
-Hadoop has a library package called [Aggregate](../api/org/apache/hadoop/mapred/lib/aggregate/package-summary.html). Aggregate provides a special reducer class and a special combiner class, and a list of simple aggregators that perform aggregations such as "sum", "max", "min" and so on over a sequence of values. Aggregate allows you to define a mapper plugin class that is expected to generate "aggregatable items" for each input key/value pair of the mappers. The combiner/reducer will aggregate those aggregatable items by invoking the appropriate aggregators.
-
-To use Aggregate, simply specify "-reducer aggregate":
-
-    mapred streaming \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper myAggregatorForKeyCount.py \
-      -reducer aggregate \
-      -file myAggregatorForKeyCount.py
-
-The python program myAggregatorForKeyCount.py looks like:
-
-    #!/usr/bin/python3
-
-    import sys
-
-    def generateLongCountToken(id):
-        return "LongValueSum:" + id + "\t" + "1"
-
-    def main(argv):
-        line = sys.stdin.readline()
-        try:
-            while line:
-                line = line[:-1]
-                fields = line.split("\t")
-                print(generateLongCountToken(fields[0]))
-                line = sys.stdin.readline()
-        except "end of file":
-            return None
-
-    if __name__ == "__main__":
-         main(sys.argv)
-
-$H3 Hadoop Field Selection Class
-
-Hadoop has a library class, [FieldSelectionMapReduce](../api/org/apache/hadoop/mapred/lib/FieldSelectionMapReduce.html), that effectively allows you to process text data like the unix "cut" utility. The map function defined in the class treats each input key/value pair as a list of fields. You can specify the field separator (the default is the tab character). You can select an arbitrary list of fields as the map output key, and an arbitrary list of fields as the map output value. Similarly, the reduce function defined in the class treats each input key/value pair as a list of fields. You can select an arbitrary list of fields as the reduce output key, and an arbitrary list of fields as the reduce output value. For example:
-
-    mapred streaming \
-      -D mapreduce.map.output.key.field.separator=. \
-      -D mapreduce.partition.keypartitioner.options=-k1,2 \
-      -D mapreduce.fieldsel.data.field.separator=. \
-      -D mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0- \
-      -D mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5- \
-      -D mapreduce.map.output.key.class=org.apache.hadoop.io.Text \
-      -D mapreduce.job.reduces=12 \
-      -input myInputDirs \
-      -output myOutputDir \
-      -mapper org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \
-      -reducer org.apache.hadoop.mapred.lib.FieldSelectionMapReduce \
-      -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner
-
-The option "-D mapreduce.fieldsel.map.output.key.value.fields.spec=6,5,1-3:0-" specifies key/value selection for the map outputs. Key selection spec and value selection spec are separated by ":". In this case, the map output key will consist of fields 6, 5, 1, 2, and 3. The map output value will consist of all fields (0- means field 0 and all the subsequent fields).
-
-The option "-D mapreduce.fieldsel.reduce.output.key.value.fields.spec=0-2:5-" specifies key/value selection for the reduce outputs. In this case, the reduce output key will consist of fields 0, 1, 2 (corresponding to the original fields 6, 5, 1). The reduce output value will consist of all fields starting from field 5 (corresponding to all the original fields).
-
-Frequently Asked Questions
---------------------------
-
-$H3 How do I use Hadoop Streaming to run an arbitrary set of (semi) independent tasks?
-
-Often you do not need the full power of Map Reduce, but only need to run multiple instances of the same program - either on different parts of the data, or on the same data, but with different parameters. You can use Hadoop Streaming to do this.
-
-$H3 How do I process files, one per map?
-
-As an example, consider the problem of zipping (compressing) a set of files across the hadoop cluster. You can achieve this by using Hadoop Streaming and custom mapper script:
-
-*   Generate a file containing the full HDFS path of the input files. Each map
-    task would get one file name as input.
-
-*   Create a mapper script which, given a filename, will get the file to local
-    disk, gzip the file and put it back in the desired output directory.
-
-$H3 How many reducers should I use?
-
-See MapReduce Tutorial for details: [Reducer](../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Reducer)
-
-$H3 If I set up an alias in my shell script, will that work after -mapper?
-
-For example, say I do: alias c1='cut -f1'. Will -mapper "c1" work?
-
-Using an alias will not work, but variable substitution is allowed as shown in this example:
-
-    $ hdfs dfs -cat /user/me/samples/student_marks
-    alice   50
-    bruce   70
-    charlie 80
-    dan     75
-
-    $ c2='cut -f2'; mapred streaming \
-      -D mapreduce.job.name='Experiment' \
-      -input /user/me/samples/student_marks \
-      -output /user/me/samples/student_out \
-      -mapper "$c2" -reducer 'cat'
-
-    $ hdfs dfs -cat /user/me/samples/student_out/part-00000
-    50
-    70
-    75
-    80
-
-$H3 Can I use UNIX pipes?
-
-For example, will -mapper "cut -f1 | sed s/foo/bar/g" work?
-
-Currently this does not work and gives an "java.io.IOException: Broken pipe" error. This is probably a bug that needs to be investigated.
-
-$H3 What do I do if I get the "No space left on device" error?
-
-For example, when I run a streaming job by distributing large executables (for example, 3.6G) through the -file option, I get a "No space left on device" error.
-
-The jar packaging happens in a directory pointed to by the configuration variable stream.tmpdir. The default value of stream.tmpdir is /tmp. Set the value to a directory with more space:
-
-  -D stream.tmpdir=/export/bigspace/...
-
-$H3 How do I specify multiple input directories?
-
-You can specify multiple input directories with multiple '-input' options:
-
-    mapred streaming \
-      -input '/user/foo/dir1' -input '/user/foo/dir2' \
-        (rest of the command)
-
-$H3 How do I generate output files with gzip format?
-
-Instead of plain text files, you can generate gzip files as your generated output. Pass '-D mapreduce.output.fileoutputformat.compress=true -D mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec' as option to your streaming job.
-
-$H3 How do I provide my own input/output format with streaming?
-
-You can specify your own custom class by packing them and putting the custom jar to `$HADOOP_CLASSPATH`.
-
-$H3 How do I parse XML documents using streaming?
-
-You can use the record reader StreamXmlRecordReader to process XML documents.
-
-    mapred streaming \
-      -inputreader "StreamXmlRecord,begin=BEGIN_STRING,end=END_STRING" \
-        (rest of the command)
-
-Anything found between BEGIN\_STRING and END\_STRING would be treated as one record for map tasks.
-
-The name-value properties that StreamXmlRecordReader understands are:
-
-*   (strings) 'begin' - Characters marking beginning of record, and 'end' - Characters marking end of record.
-*   (boolean) 'slowmatch' - Toggle to look for begin and end characters, but within CDATA instead of regular tags. Defaults to false.
-*   (integer) 'lookahead' - Maximum lookahead bytes to sync CDATA when using 'slowmatch', should be larger than 'maxrec'. Defaults to 2*'maxrec'.
-*   (integer) 'maxrec' - Maximum record size to read between each match during 'slowmatch'. Defaults to 50000 bytes.
-
-$H3 How do I update counters in streaming applications?
-
-A streaming process can use the stderr to emit counter information. `reporter:counter:<group>,<counter>,<amount>` should be sent to stderr to update the counter.
-
-$H3 How do I update status in streaming applications?
-
-A streaming process can use the stderr to emit status information. To set a status, `reporter:status:<message>` should be sent to stderr.
-
-$H3 How do I get the Job variables in a streaming job's mapper/reducer?
-
-See [Configured Parameters](../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html#Configured_Parameters). During the execution of a streaming job, the names of the "mapred" parameters are transformed. The dots ( . ) become underscores ( \_ ). For example, mapreduce.job.id becomes mapreduce\_job\_id and mapreduce.job.jar becomes mapreduce\_job\_jar. In your code, use the parameter names with the underscores.
-
-$H3 What do I do if I get a "error=7, Argument list too long"
-
-The job copies the whole configuration to the environment. If the job is processing a large number of input files adding the job configuration to the environment could cause an overrun of the environment. The job configuration copy in the environment is not essential for running the job and can be truncated by setting:
-
-  -D stream.jobconf.truncate.limit=20000
-
-By default the values are not truncated (-1). Zero (0) will only copy the names and not values. For almost all cases 20000 is a safe value that will prevent the overrun of the environment.
diff --git a/hadoop-tools/hadoop-streaming/src/site/resources/css/site.css b/hadoop-tools/hadoop-streaming/src/site/resources/css/site.css
deleted file mode 100644
index f830baafa8c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/site/resources/css/site.css
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one or more
-* contributor license agreements.  See the NOTICE file distributed with
-* this work for additional information regarding copyright ownership.
-* The ASF licenses this file to You under the Apache License, Version 2.0
-* (the "License"); you may not use this file except in compliance with
-* the License.  You may obtain a copy of the License at
-*
-*     http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing, software
-* distributed under the License is distributed on an "AS IS" BASIS,
-* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-* See the License for the specific language governing permissions and
-* limitations under the License.
-*/
-#banner {
-  height: 93px;
-  background: none;
-}
-
-#bannerLeft img {
-  margin-left: 30px;
-  margin-top: 10px;
-}
-
-#bannerRight img {
-  margin: 17px;
-}
-
diff --git a/hadoop-tools/hadoop-streaming/src/test/bin/cat.cmd b/hadoop-tools/hadoop-streaming/src/test/bin/cat.cmd
deleted file mode 100644
index 4b38e3e3b4b..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/bin/cat.cmd
+++ /dev/null
@@ -1,18 +0,0 @@
-@rem Licensed to the Apache Software Foundation (ASF) under one
-@rem or more contributor license agreements.  See the NOTICE file
-@rem distributed with this work for additional information
-@rem regarding copyright ownership.  The ASF licenses this file
-@rem to you under the Apache License, Version 2.0 (the
-@rem "License"); you may not use this file except in compliance
-@rem with the License.  You may obtain a copy of the License at
-@rem
-@rem     http://www.apache.org/licenses/LICENSE-2.0
-@rem
-@rem Unless required by applicable law or agreed to in writing, software
-@rem distributed under the License is distributed on an "AS IS" BASIS,
-@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-@rem See the License for the specific language governing permissions and
-@rem limitations under the License.
-
-@for /F "usebackq tokens=* delims=" %%A in (`findstr .`) do @echo %%A
-@rem lines have been copied from stdin to stdout
diff --git a/hadoop-tools/hadoop-streaming/src/test/bin/xargs_cat.cmd b/hadoop-tools/hadoop-streaming/src/test/bin/xargs_cat.cmd
deleted file mode 100644
index f398a8d65c3..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/bin/xargs_cat.cmd
+++ /dev/null
@@ -1,18 +0,0 @@
-@rem Licensed to the Apache Software Foundation (ASF) under one
-@rem or more contributor license agreements.  See the NOTICE file
-@rem distributed with this work for additional information
-@rem regarding copyright ownership.  The ASF licenses this file
-@rem to you under the Apache License, Version 2.0 (the
-@rem "License"); you may not use this file except in compliance
-@rem with the License.  You may obtain a copy of the License at
-@rem
-@rem     http://www.apache.org/licenses/LICENSE-2.0
-@rem
-@rem Unless required by applicable law or agreed to in writing, software
-@rem distributed under the License is distributed on an "AS IS" BASIS,
-@rem WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-@rem See the License for the specific language governing permissions and
-@rem limitations under the License.
-
-@for /F "usebackq tokens=* delims=" %%A in (`findstr .`) do @type %%A
-@rem files named on stdin have been copied to stdout
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/ClassWithNoPackage.java b/hadoop-tools/hadoop-streaming/src/test/java/ClassWithNoPackage.java
deleted file mode 100644
index 5bb597ffa56..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/ClassWithNoPackage.java
+++ /dev/null
@@ -1,20 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-public class ClassWithNoPackage {
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/DelayEchoApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/DelayEchoApp.java
deleted file mode 100644
index a912dc9c23b..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/DelayEchoApp.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-
-/**
- * A simple Java app that will consume all input from stdin, wait a few seconds
- * and echoing it to stdout.
- */
-public class DelayEchoApp {
-
-  public DelayEchoApp() {
-  }
-
-  public void go(int seconds) throws IOException, InterruptedException {
-    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-    String line;
-
-    // Consume all input (to make sure streaming will still count this
-    // task as failed even if all input was consumed).
-    while ((line = in.readLine()) != null) {
-      Thread.sleep(seconds * 1000L);
-      System.out.println(line);
-    }
-  }
-
-  public static void main(String[] args) throws IOException, InterruptedException {
-    int seconds = 5;
-    if (args.length >= 1) {
-      try {
-        seconds = Integer.parseInt(args[0]);
-      } catch (NumberFormatException e) {
-        // just use default 5.
-      }
-    }
-
-    DelayEchoApp app = new DelayEchoApp();
-    app.go(seconds);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/FailApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/FailApp.java
deleted file mode 100644
index 47479564ef2..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/FailApp.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-
-/**
- * A simple Java app that will consume all input from stdin, echoing
- * it to stdout, and then optionally throw an exception (which should
- * cause a non-zero exit status for the process).
- */
-public class FailApp
-{
-
-  public FailApp() {
-  }
-
-  public void go(boolean fail) throws IOException {
-    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-    String line;
-
-    // Consume all input (to make sure streaming will still count this
-    // task as failed even if all input was consumed).
-    while ((line = in.readLine()) != null) {
-      System.out.println(line);
-    }
-
-    if (fail) {
-      throw new RuntimeException("Intentionally failing task");
-    }
-  }
-
-  public static void main(String[] args) throws IOException {
-    boolean fail = true;
-    if (args.length >= 1 && "false".equals(args[0])) {
-      fail = false;
-    }
-    
-    FailApp app = new FailApp();
-    app.go(fail);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/OutputOnlyApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/OutputOnlyApp.java
deleted file mode 100644
index 0a659ce187a..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/OutputOnlyApp.java
+++ /dev/null
@@ -1,38 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.IOException;
-
-/**
- * An application that outputs a specified number of lines
- * without consuming any input.
- */
-public class OutputOnlyApp {
-  public static void main(String[] args) throws IOException {
-    if (args.length < 1) {
-      System.err.println("Usage: OutputOnlyApp NUMRECORDS");
-      return;
-    }
-    int numRecords = Integer.parseInt(args[0]);
-    while (numRecords-- > 0) {
-      System.out.println("key\tvalue");
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/RawBytesMapApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/RawBytesMapApp.java
deleted file mode 100644
index e9c2740ee88..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/RawBytesMapApp.java
+++ /dev/null
@@ -1,66 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.BufferedReader;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.io.IntWritable;
-
-public class RawBytesMapApp {
-  private String find;
-  private DataOutputStream dos;
-
-  public RawBytesMapApp(String find) {
-    this.find = find;
-    dos = new DataOutputStream(System.out);
-  }
-
-  public void go() throws IOException {
-    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-    String line;
-    while ((line = in.readLine()) != null) {
-      for (String part : line.split(find)) {
-        writeString(part);  // write key
-        writeInt(1);        // write value
-      }
-    }
-    System.out.flush();
-  }
-  
-  public static void main(String[] args) throws IOException {
-    RawBytesMapApp app = new RawBytesMapApp(args[0].replace(".","\\."));
-    app.go();
-  }
-  
-  private void writeString(String str) throws IOException {
-    byte[] bytes = str.getBytes(StandardCharsets.UTF_8);
-    dos.writeInt(bytes.length);
-    dos.write(bytes);
-  }
-  
-  private void writeInt(int i) throws IOException {
-    dos.writeInt(4);
-    IntWritable iw = new IntWritable(i);
-    iw.write(dos);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/RawBytesReduceApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/RawBytesReduceApp.java
deleted file mode 100644
index 4a21f11f58c..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/RawBytesReduceApp.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataInputStream;
-import java.io.EOFException;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.io.IntWritable;
-
-public class RawBytesReduceApp {
-  private DataInputStream dis;
-
-  public RawBytesReduceApp() {
-    dis = new DataInputStream(System.in);
-  }
-  
-  public void go() throws IOException {
-    String prevKey = null;
-    int sum = 0;
-    String key = readString();
-    while (key != null) {
-      if (prevKey != null && !key.equals(prevKey)) {
-        System.out.println(prevKey + "\t" + sum);
-        sum = 0;
-      }
-      sum += readInt();
-      prevKey = key;
-      key = readString();
-    }
-    System.out.println(prevKey + "\t" + sum);
-    System.out.flush();
-  }
-
-  public static void main(String[] args) throws IOException {
-    RawBytesReduceApp app = new RawBytesReduceApp();
-    app.go();
-  }
-  
-  private String readString() throws IOException {
-    int length;
-    try {
-      length = dis.readInt();
-    } catch (EOFException eof) {
-      return null;
-    }
-    byte[] bytes = new byte[length];
-    dis.readFully(bytes);
-    return new String(bytes, StandardCharsets.UTF_8);
-  }
-  
-  private int readInt() throws IOException {
-    dis.readInt(); // ignore (we know it's 4)
-    IntWritable iw = new IntWritable();
-    iw.readFields(dis);
-    return iw.get();
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StderrApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StderrApp.java
deleted file mode 100644
index 17c90537576..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StderrApp.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-
-/**
- * Output an arbitrary number of stderr lines before or after
- * consuming the keys/values from stdin.
- */
-public class StderrApp
-{
-  /**
-   * Print preWriteLines to stderr, pausing sleep ms between each
-   * output, then consume stdin and echo it to stdout, then write
-   * postWriteLines to stderr.
-   */
-  public static void go(int preWriteLines, int sleep, int postWriteLines) throws IOException {
-    go(preWriteLines, sleep, postWriteLines, false);
-  }
-  
-  public static void go(int preWriteLines, int sleep, int postWriteLines, boolean status) throws IOException {
-    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-    String line;
-    
-    if (status) {
-      System.err.println("reporter:status:starting echo");
-    }      
-       
-    while (preWriteLines > 0) {
-      --preWriteLines;
-      System.err.println("some stderr output before reading input, "
-                         + preWriteLines + " lines remaining, sleeping " + sleep);
-      try {
-        Thread.sleep(sleep);
-      } catch (InterruptedException e) {}
-    }
-    
-    while ((line = in.readLine()) != null) {
-      System.out.println(line);
-    }
-    
-    while (postWriteLines > 0) {
-      --postWriteLines;
-      System.err.println("some stderr output after reading input, lines remaining "
-                         + postWriteLines);
-    }
-  }
-
-  public static void main(String[] args) throws IOException {
-    if (args.length < 3) {
-      System.err.println("Usage: StderrApp PREWRITE SLEEP POSTWRITE [STATUS]");
-      return;
-    }
-    int preWriteLines = Integer.parseInt(args[0]);
-    int sleep = Integer.parseInt(args[1]);
-    int postWriteLines = Integer.parseInt(args[2]);
-    boolean status = args.length > 3 ? Boolean.parseBoolean(args[3]) : false;
-    
-    go(preWriteLines, sleep, postWriteLines, status);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StreamAggregate.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StreamAggregate.java
deleted file mode 100644
index 2a3236ac70f..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/StreamAggregate.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-
-import org.apache.hadoop.streaming.Environment;
-
-/** 
-    Used to test the usage of external applications without adding
-    platform-specific dependencies.
- */
-public class StreamAggregate extends TrApp
-{
-
-  public StreamAggregate()
-  {
-    super('.', ' ');
-  }
-
-  public void go() throws IOException
-  {
-    testParentJobConfToEnvVars();
-    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-    String line;
-
-    while ((line = in.readLine()) != null) {
-      String [] words = line.split(" ");
-      for (int i = 0; i< words.length; i++) {
-        String out = "LongValueSum:" + words[i].trim() + "\t" + "1";
-        System.out.println(out);
-      }
-    }
-  }
-
-  public static void main(String[] args) throws IOException
-  {
-    TrApp app = new StreamAggregate();
-    app.go();
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestAutoInputFormat.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestAutoInputFormat.java
deleted file mode 100644
index 4ff3f54ecfb..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestAutoInputFormat.java
+++ /dev/null
@@ -1,113 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.IOException;
-import java.io.OutputStreamWriter;
-import java.io.Writer;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IntWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.InputSplit;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.FileInputFormat;
-import org.apache.hadoop.mapred.RecordReader;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.streaming.AutoInputFormat;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestAutoInputFormat {
-
-  private static Configuration conf = new Configuration();
-
-  private static final int LINES_COUNT = 3;
-
-  private static final int RECORDS_COUNT = 3;
-
-  private static final int SPLITS_COUNT = 2;
-
-  @SuppressWarnings( { "unchecked", "deprecation" })
-  @Test
-  public void testFormat() throws IOException {
-    JobConf job = new JobConf(conf);
-    FileSystem fs = FileSystem.getLocal(conf);
-    Path dir = new Path(System.getProperty("test.build.data", ".") + "/mapred");
-    Path txtFile = new Path(dir, "auto.txt");
-    Path seqFile = new Path(dir, "auto.seq");
-
-    fs.delete(dir, true);
-
-    FileInputFormat.setInputPaths(job, dir);
-
-    Writer txtWriter = new OutputStreamWriter(fs.create(txtFile));
-    try {
-      for (int i = 0; i < LINES_COUNT; i++) {
-        txtWriter.write("" + (10 * i));
-        txtWriter.write("\n");
-      }
-    } finally {
-      txtWriter.close();
-    }
-
-    SequenceFile.Writer seqWriter = SequenceFile.createWriter(fs, conf,
-      seqFile, IntWritable.class, LongWritable.class);
-    try {
-      for (int i = 0; i < RECORDS_COUNT; i++) {
-        IntWritable key = new IntWritable(11 * i);
-        LongWritable value = new LongWritable(12 * i);
-        seqWriter.append(key, value);
-      }
-    } finally {
-      seqWriter.close();
-    }
-
-    AutoInputFormat format = new AutoInputFormat();
-    InputSplit[] splits = format.getSplits(job, SPLITS_COUNT);
-    for (InputSplit split : splits) {
-      RecordReader reader = format.getRecordReader(split, job, Reporter.NULL);
-      Object key = reader.createKey();
-      Object value = reader.createValue();
-      try {
-        while (reader.next(key, value)) {
-          if (key instanceof LongWritable) {
-            assertEquals("Wrong value class.", Text.class, value.getClass());
-            assertTrue("Invalid value", Integer.parseInt(((Text) value)
-              .toString()) % 10 == 0);
-          } else {
-            assertEquals("Wrong key class.", IntWritable.class, key.getClass());
-            assertEquals("Wrong value class.", LongWritable.class, value
-              .getClass());
-            assertTrue("Invalid key.", ((IntWritable) key).get() % 11 == 0);
-            assertTrue("Invalid value.", ((LongWritable) value).get() % 12 == 0);
-          }
-        }
-      } finally {
-        reader.close();
-      }
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestClassWithNoPackage.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestClassWithNoPackage.java
deleted file mode 100644
index 4d5cf61af62..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestClassWithNoPackage.java
+++ /dev/null
@@ -1,56 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.net.URL;
-import java.net.URLClassLoader;
-import java.net.MalformedURLException;
-
-import org.apache.hadoop.util.JarFinder;
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * Test Hadoop StreamUtil successfully returns a class loaded by the job conf
- * but has no package name.
- */
-public class TestClassWithNoPackage
-{
-  @Test
-  public void testGoodClassOrNull() throws Exception {
-    String NAME = "ClassWithNoPackage";
-    ClassLoader cl = TestClassWithNoPackage.class.getClassLoader();
-    String JAR = JarFinder.getJar(cl.loadClass(NAME));
-
-    // Add testjob jar file to classpath.
-    Configuration conf = new Configuration();
-    conf.setClassLoader(new URLClassLoader(new URL[]{new URL("file", null, JAR)}, 
-                                           null));
-    // Get class with no package name.
-    String defaultPackage = this.getClass().getPackage().getName();
-    Class c = StreamUtil.goodClassOrNull(conf, NAME, defaultPackage);
-    assertNotNull("Class " + NAME + " not found!", c);
-  }
-  public static void main(String[]args) throws Exception
-  {
-    new TestClassWithNoPackage().testGoodClassOrNull();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestDumpTypedBytes.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestDumpTypedBytes.java
deleted file mode 100644
index bbfc02068e7..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestDumpTypedBytes.java
+++ /dev/null
@@ -1,94 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.OutputStreamWriter;
-import java.io.PrintStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.streaming.DumpTypedBytes;
-import org.apache.hadoop.typedbytes.TypedBytesInput;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestDumpTypedBytes {
-
-  @Test
-  public void testDumping() throws Exception {
-    Configuration conf = new Configuration();
-    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2)
-        .build();
-    FileSystem fs = cluster.getFileSystem();
-    PrintStream psBackup = System.out;
-    ByteArrayOutputStream out = new ByteArrayOutputStream();
-    PrintStream psOut = new PrintStream(out);
-    System.setOut(psOut);
-    DumpTypedBytes dumptb = new DumpTypedBytes(conf);
-
-    try {
-      Path root = new Path("/typedbytestest");
-      assertTrue(fs.mkdirs(root));
-      assertTrue(fs.exists(root));
-      OutputStreamWriter writer = new OutputStreamWriter(fs.create(new Path(
-        root, "test.txt")));
-      try {
-        for (int i = 0; i < 100; i++) {
-          writer.write("" + (10 * i) + "\n");
-        }
-      } finally {
-        writer.close();
-      }
-
-      String[] args = new String[1];
-      args[0] = "/typedbytestest";
-      int ret = dumptb.run(args);
-      assertEquals("Return value != 0.", 0, ret);
-
-      ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());
-      TypedBytesInput tbinput = new TypedBytesInput(new DataInputStream(in));
-      int counter = 0;
-      Object key = tbinput.read();
-      while (key != null) {
-        assertEquals(Long.class, key.getClass()); // offset
-        Object value = tbinput.read();
-        assertEquals(String.class, value.getClass());
-        assertTrue("Invalid output.",
-          Integer.parseInt(value.toString()) % 10 == 0);
-        counter++;
-        key = tbinput.read();
-      }
-      assertEquals("Wrong number of outputs.", 100, counter);
-    } finally {
-      try {
-        fs.close();
-      } catch (Exception e) {
-      }
-      System.setOut(psBackup);
-      cluster.shutdown();
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestFileArgs.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestFileArgs.java
deleted file mode 100644
index 901abba885b..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestFileArgs.java
+++ /dev/null
@@ -1,118 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.Map;
-
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.util.Shell;
-import org.junit.After;
-import org.junit.Before;
-
-/**
- * This class tests that the '-file' argument to streaming results
- * in files being unpacked in the job working directory.
- */
-public class TestFileArgs extends TestStreaming
-{
-  private MiniDFSCluster dfs = null;
-  private MiniMRCluster mr = null;
-  private FileSystem fileSys = null;
-  private String namenode = null;
-  private Configuration conf = null;
-
-  private static final String EXPECTED_OUTPUT =
-    "job.jar\t\nsidefile\t\n";
-
-  private static final String LS_PATH = Shell.WINDOWS ? "cmd /c dir /B" :
-    "/bin/ls";
-
-  public TestFileArgs() throws IOException
-  {
-    // Set up mini cluster
-    conf = new Configuration();
-    dfs = new MiniDFSCluster.Builder(conf).build();
-    fileSys = dfs.getFileSystem();
-    namenode = fileSys.getUri().getAuthority();
-    mr  = new MiniMRCluster(1, namenode, 1);
-
-    map = LS_PATH;
-    FileSystem.setDefaultUri(conf, "hdfs://" + namenode);
-    setTestDir(new File("/tmp/TestFileArgs"));
-  }
-
-  @Before
-  @Override
-  public void setUp() throws IOException {
-    // Set up side file
-    FileSystem localFs = FileSystem.getLocal(conf);
-    DataOutputStream dos = localFs.create(new Path("target/sidefile"));
-    dos.write("hello world\n".getBytes(StandardCharsets.UTF_8));
-    dos.close();
-
-    // Since ls doesn't read stdin, we don't want to write anything
-    // to it, or else we risk Broken Pipe exceptions.
-    input = "";
-  }
-
-  @After
-  @Override
-  public void tearDown() {
-    if (mr != null) {
-      mr.shutdown();
-    }
-    if (dfs != null) {
-      dfs.shutdown();
-    }
-  }
-
-  @Override
-  protected String getExpectedOutput() {
-    return EXPECTED_OUTPUT;
-  }
-
-  @Override
-  protected Configuration getConf() {
-    return conf;
-  }
-
-  @Override
-  protected String[] genArgs() {
-    for (Map.Entry<String, String> entry : mr.createJobConf()) {
-      args.add("-jobconf");
-      args.add(entry.getKey() + "=" + entry.getValue());
-    }
-    args.add("-file");
-    args.add(new java.io.File("target/sidefile").getAbsolutePath());
-    args.add("-numReduceTasks");
-    args.add("0");
-    args.add("-jobconf");
-    args.add("mapred.jar=" + STREAMING_JAR);
-    args.add("-verbose");
-    return super.genArgs();
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestGzipInput.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestGzipInput.java
deleted file mode 100644
index dc12e4eff97..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestGzipInput.java
+++ /dev/null
@@ -1,44 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.zip.GZIPOutputStream;
-
-/**
- * This class tests gzip input streaming in MapReduce local mode.
- */
-public class TestGzipInput extends TestStreaming
-{
-
-  public TestGzipInput() throws IOException {
-    INPUT_FILE = new File(TEST_DIR, "input.txt.gz");
-  }
-  
-  protected void createInput() throws IOException
-  {
-    GZIPOutputStream out = new GZIPOutputStream(
-      new FileOutputStream(INPUT_FILE.getAbsoluteFile()));
-    out.write(input.getBytes(StandardCharsets.UTF_8));
-    out.close();
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestLoadTypedBytes.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestLoadTypedBytes.java
deleted file mode 100644
index 0c004e7135f..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestLoadTypedBytes.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataOutputStream;
-import java.io.InputStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.typedbytes.TypedBytesOutput;
-import org.apache.hadoop.typedbytes.TypedBytesWritable;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestLoadTypedBytes {
-
-  @Test
-  public void testLoading() throws Exception {
-    Configuration conf = new Configuration();
-    MiniDFSCluster cluster = new MiniDFSCluster.Builder(conf).numDataNodes(2)
-        .build();
-    FileSystem fs = cluster.getFileSystem();
-    
-    ByteArrayOutputStream out = new ByteArrayOutputStream();
-    TypedBytesOutput tboutput = new TypedBytesOutput(new DataOutputStream(out));
-    for (int i = 0; i < 100; i++) {
-      tboutput.write(new Long(i)); // key
-      tboutput.write("" + (10 * i)); // value
-    }
-    InputStream isBackup = System.in;
-    ByteArrayInputStream in = new ByteArrayInputStream(out.toByteArray());
-    System.setIn(in);
-    LoadTypedBytes loadtb = new LoadTypedBytes(conf);
-
-    try {
-      Path root = new Path("/typedbytestest");
-      assertTrue(fs.mkdirs(root));
-      assertTrue(fs.exists(root));
-      
-      String[] args = new String[1];
-      args[0] = "/typedbytestest/test.seq";
-      int ret = loadtb.run(args);
-      assertEquals("Return value != 0.", 0, ret);
-
-      Path file = new Path(root, "test.seq");
-      assertTrue(fs.exists(file));
-      SequenceFile.Reader reader = new SequenceFile.Reader(fs, file, conf);
-      int counter = 0;
-      TypedBytesWritable key = new TypedBytesWritable();
-      TypedBytesWritable value = new TypedBytesWritable();
-      while (reader.next(key, value)) {
-        assertEquals(Long.class, key.getValue().getClass());
-        assertEquals(String.class, value.getValue().getClass());
-        assertTrue("Invalid record.",
-          Integer.parseInt(value.toString()) % 10 == 0);
-        counter++;
-      }
-      assertEquals("Wrong number of records.", 100, counter);
-    } finally {
-      try {
-        fs.close();
-      } catch (Exception e) {
-      }
-      System.setIn(isBackup);
-      cluster.shutdown();
-    }
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMRFramework.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMRFramework.java
deleted file mode 100644
index 17dc7ad633d..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMRFramework.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.streaming;
-
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.MRConfig;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.junit.Test;
-
-public class TestMRFramework {
-
-  @Test
-  public void testFramework() {
-    JobConf jobConf = new JobConf();
-    jobConf.set(JTConfig.JT_IPC_ADDRESS, MRConfig.LOCAL_FRAMEWORK_NAME);
-    jobConf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);
-    assertFalse("Expected 'isLocal' to be false", 
-        StreamUtil.isLocalJobTracker(jobConf));
-    
-    jobConf.set(JTConfig.JT_IPC_ADDRESS, MRConfig.LOCAL_FRAMEWORK_NAME);
-    jobConf.set(MRConfig.FRAMEWORK_NAME, MRConfig.CLASSIC_FRAMEWORK_NAME);
-    assertFalse("Expected 'isLocal' to be false", 
-        StreamUtil.isLocalJobTracker(jobConf));
-    
-    jobConf.set(JTConfig.JT_IPC_ADDRESS, "jthost:9090");
-    jobConf.set(MRConfig.FRAMEWORK_NAME, MRConfig.LOCAL_FRAMEWORK_NAME);
-    assertTrue("Expected 'isLocal' to be true", 
-        StreamUtil.isLocalJobTracker(jobConf));
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleArchiveFiles.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleArchiveFiles.java
deleted file mode 100644
index 041d527ab17..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleArchiveFiles.java
+++ /dev/null
@@ -1,140 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.DataOutputStream;
-import java.nio.charset.StandardCharsets;
-import java.util.Map;
-import java.util.zip.ZipEntry;
-import java.util.zip.ZipOutputStream;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.*;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-
-/**
- * This class tests cacheArchive option of streaming
- * The test case creates 2 archive files, ships it with hadoop
- * streaming and compares the output with expected output
- */
-public class TestMultipleArchiveFiles extends TestStreaming
-{
-  private static final Logger LOG = LoggerFactory.getLogger(TestMultipleArchiveFiles.class);
-
-  private StreamJob job;
-  private String INPUT_DIR = "multiple-archive-files/";
-  private String INPUT_FILE = INPUT_DIR + "input.txt";
-  private String CACHE_ARCHIVE_1 = INPUT_DIR + "cacheArchive1.zip";
-  private File CACHE_FILE_1 = null;
-  private String CACHE_ARCHIVE_2 = INPUT_DIR + "cacheArchive2.zip";
-  private File CACHE_FILE_2 = null;
-  private String expectedOutput = null;
-  private String OUTPUT_DIR = "out";
-  private Configuration conf = null;
-  private MiniDFSCluster dfs = null;
-  private MiniMRCluster mr = null;
-  private FileSystem fileSys = null;
-  private String namenode = null;
-
-  public TestMultipleArchiveFiles() throws Exception {
-    CACHE_FILE_1 = new File("cacheArchive1");
-    CACHE_FILE_2 = new File("cacheArchive2");
-    input = "HADOOP";
-    expectedOutput = "HADOOP\t\nHADOOP\t\n";
-    conf = new Configuration();
-    dfs = new MiniDFSCluster.Builder(conf).build();
-    fileSys = dfs.getFileSystem();
-    namenode = fileSys.getUri().getAuthority();
-    mr  = new MiniMRCluster(1, namenode, 1);
-
-    map = XARGS_CAT;
-    reduce = CAT;
-  }
-
-  @Override
-  protected void setInputOutput() {
-    inputFile = INPUT_FILE;
-    outDir = OUTPUT_DIR;
-  }
-
-  protected void createInput() throws IOException
-  {
-    fileSys.delete(new Path(INPUT_DIR), true);
-    DataOutputStream dos = fileSys.create(new Path(INPUT_FILE));
-    String inputFileString = "symlink1" + File.separator
-      + "cacheArchive1\nsymlink2" + File.separator + "cacheArchive2";
-    dos.write(inputFileString.getBytes(StandardCharsets.UTF_8));
-    dos.close();
-
-    DataOutputStream out = fileSys.create(new Path(CACHE_ARCHIVE_1.toString()));
-    ZipOutputStream zos = new ZipOutputStream(out);
-    ZipEntry ze = new ZipEntry(CACHE_FILE_1.toString());
-    zos.putNextEntry(ze);
-    zos.write(input.getBytes(StandardCharsets.UTF_8));
-    zos.closeEntry();
-    zos.close();
-
-    out = fileSys.create(new Path(CACHE_ARCHIVE_2.toString()));
-    zos = new ZipOutputStream(out);
-    ze = new ZipEntry(CACHE_FILE_2.toString());
-    zos.putNextEntry(ze);
-    zos.write(input.getBytes(StandardCharsets.UTF_8));
-    zos.closeEntry();
-    zos.close();
-  }
-
-  protected String[] genArgs() {
-    String workDir = fileSys.getWorkingDirectory().toString() + "/";
-    String cache1 = workDir + CACHE_ARCHIVE_1 + "#symlink1";
-    String cache2 = workDir + CACHE_ARCHIVE_2 + "#symlink2";
-
-    for (Map.Entry<String, String> entry : mr.createJobConf()) {
-      args.add("-jobconf");
-      args.add(entry.getKey() + "=" + entry.getValue());
-    }
-    args.add("-jobconf");
-    args.add("mapreduce.job.reduces=1");
-    args.add("-cacheArchive");
-    args.add(cache1);
-    args.add("-cacheArchive");
-    args.add(cache2);
-    args.add("-jobconf");
-    args.add("mapred.jar=" + STREAMING_JAR);
-    return super.genArgs();
-  }
-
-  protected void checkOutput() throws IOException {
-    StringBuffer output = new StringBuffer(256);
-    Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus(
-                                            new Path(OUTPUT_DIR)));
-    for (int i = 0; i < fileList.length; i++){
-      LOG.info("Adding output from file: " + fileList[i]);
-      output.append(StreamUtil.slurpHadoop(fileList[i], fileSys));
-    }
-    assertOutput(expectedOutput, output.toString());
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleCachefiles.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleCachefiles.java
deleted file mode 100644
index 36565e3fc1b..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestMultipleCachefiles.java
+++ /dev/null
@@ -1,157 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.BufferedReader;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.mapred.Utils;
-
-/**
- * This test case tests the symlink creation
- * utility provided by distributed caching 
- */
-public class TestMultipleCachefiles
-{
-  String INPUT_FILE = "/testing-streaming/input.txt";
-  String OUTPUT_DIR = "/testing-streaming/out";
-  String CACHE_FILE = "/testing-streaming/cache.txt";
-  String CACHE_FILE_2 = "/testing-streaming/cache2.txt";
-  String input = "check to see if we can read this none reduce";
-  String map = TestStreaming.XARGS_CAT;
-  String reduce = TestStreaming.CAT;
-  String mapString = "testlink";
-  String mapString2 = "testlink2";
-  String cacheString = "This is just the cache string";
-  String cacheString2 = "This is just the second cache string";
-  StreamJob job;
-
-  public TestMultipleCachefiles() throws IOException
-  {
-  }
-
-  @Test
-  public void testMultipleCachefiles() throws Exception
-  {
-    boolean mayExit = false;
-    MiniMRCluster mr = null;
-    MiniDFSCluster dfs = null; 
-    try{
-      Configuration conf = new Configuration();
-      dfs = new MiniDFSCluster.Builder(conf).build();
-      FileSystem fileSys = dfs.getFileSystem();
-      String namenode = fileSys.getUri().toString();
-
-      mr  = new MiniMRCluster(1, namenode, 3);
-
-      List<String> args = new ArrayList<String>();
-      for (Map.Entry<String, String> entry : mr.createJobConf()) {
-        args.add("-jobconf");
-        args.add(entry.getKey() + "=" + entry.getValue());
-      }
-
-      String argv[] = new String[] {
-        "-input", INPUT_FILE,
-        "-output", OUTPUT_DIR,
-        "-mapper", map,
-        "-reducer", reduce,
-        "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-        "-jobconf", 
-          JobConf.MAPRED_MAP_TASK_JAVA_OPTS + "=" +
-            "-Dcontrib.name=" + System.getProperty("contrib.name") + " " +
-            "-Dbuild.test=" + System.getProperty("build.test") + " " +
-            conf.get(JobConf.MAPRED_MAP_TASK_JAVA_OPTS, 
-                     conf.get(JobConf.MAPRED_TASK_JAVA_OPTS, "")),
-        "-jobconf", 
-          JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS + "=" +
-            "-Dcontrib.name=" + System.getProperty("contrib.name") + " " +
-            "-Dbuild.test=" + System.getProperty("build.test") + " " +
-            conf.get(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, 
-                     conf.get(JobConf.MAPRED_TASK_JAVA_OPTS, "")),
-        "-cacheFile", fileSys.getUri() + CACHE_FILE + "#" + mapString,
-        "-cacheFile", fileSys.getUri() + CACHE_FILE_2 + "#" + mapString2,
-        "-jobconf", "mapred.jar=" + TestStreaming.STREAMING_JAR,
-      };
-
-      for (String arg : argv) {
-        args.add(arg);
-      }
-      argv = args.toArray(new String[args.size()]);
-      
-      fileSys.delete(new Path(OUTPUT_DIR), true);
-      
-      DataOutputStream file = fileSys.create(new Path(INPUT_FILE));
-      file.writeBytes(mapString + "\n");
-      file.writeBytes(mapString2 + "\n");
-      file.close();
-      file = fileSys.create(new Path(CACHE_FILE));
-      file.writeBytes(cacheString + "\n");
-      file.close();
-      file = fileSys.create(new Path(CACHE_FILE_2));
-      file.writeBytes(cacheString2 + "\n");
-      file.close();
-        
-      job = new StreamJob(argv, mayExit);     
-      job.go();
-
-      fileSys = dfs.getFileSystem();
-      String line = null;
-      String line2 = null;
-      Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus(
-                                   new Path(OUTPUT_DIR),
-                                   new Utils.OutputFileUtils
-                                            .OutputFilesFilter()));
-      for (int i = 0; i < fileList.length; i++){
-        System.out.println(fileList[i].toString());
-        BufferedReader bread =
-          new BufferedReader(new InputStreamReader(fileSys.open(fileList[i])));
-        line = bread.readLine();
-        System.out.println(line);
-        line2 = bread.readLine();
-        System.out.println(line2);
-      }
-      assertEquals(cacheString + "\t", line);
-      assertEquals(cacheString2 + "\t", line2);
-    } finally{
-      if (dfs != null) { dfs.shutdown(); }
-      if (mr != null) { mr.shutdown();}
-    }
-  }
-
-  public static void main(String[]args) throws Exception
-  {
-    new TestMultipleCachefiles().testMultipleCachefiles();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestRawBytesStreaming.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestRawBytesStreaming.java
deleted file mode 100644
index 09adb3d5fd4..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestRawBytesStreaming.java
+++ /dev/null
@@ -1,97 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileUtil;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestRawBytesStreaming {
-
-  protected File INPUT_FILE = new File("target/input.txt");
-  protected File OUTPUT_DIR = new File("target/out");
-  protected String input = "roses.are.red\nviolets.are.blue\nbunnies.are.pink\n";
-  protected String map = UtilTest.makeJavaCommand(RawBytesMapApp.class, new String[]{"."});
-  protected String reduce = UtilTest.makeJavaCommand(RawBytesReduceApp.class, new String[0]);
-  protected String outputExpect = "are\t3\nblue\t1\nbunnies\t1\npink\t1\nred\t1\nroses\t1\nviolets\t1\n";
-  
-  public TestRawBytesStreaming() throws IOException {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected void createInput() throws IOException {
-    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));
-    out.write(input.getBytes(StandardCharsets.UTF_8));
-    out.close();
-  }
-
-  protected String[] genArgs() {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", map,
-      "-reducer", reduce,
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-      "-jobconf", "stream.map.output=rawbytes",
-      "-jobconf", "stream.reduce.input=rawbytes",
-      "-verbose"
-    };
-  }
-
-  @Test
-  public void testCommandLine() throws Exception {
-    try {
-      try {
-        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-      } catch (Exception e) {
-      }
-
-      createInput();
-      OUTPUT_DIR.delete();
-
-      // During tests, the default Configuration will use a local mapred
-      // So don't specify -config or -cluster
-      StreamJob job = new StreamJob();
-      job.setConf(new Configuration());
-      job.run(genArgs());
-      File outFile = new File(OUTPUT_DIR, "part-00000").getAbsoluteFile();
-      String output = StreamUtil.slurp(outFile);
-      outFile.delete();
-      System.out.println("   map=" + map);
-      System.out.println("reduce=" + reduce);
-      System.err.println("outEx1=" + outputExpect);
-      System.err.println("  out1=" + output);
-      assertEquals(outputExpect, output);
-    } finally {
-      INPUT_FILE.delete();
-      FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamAggregate.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamAggregate.java
deleted file mode 100644
index b303c8c9772..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamAggregate.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-import java.io.*;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-
-/**
- * This class tests hadoopStreaming in MapReduce local mode.
- * It uses Hadoop Aggregate to count the numbers of word occurrences 
- * in the input.
- */
-public class TestStreamAggregate
-{
-  protected File INPUT_FILE = new File("stream_aggregate_input.txt");
-  protected File OUTPUT_DIR = new File("stream_aggregate_out");
-  protected String input = "roses are red\nviolets are blue\nbunnies are pink\n";
-  // map parses input lines and generates count entries for each word.
-  protected String map = UtilTest.makeJavaCommand(StreamAggregate.class, new String[]{".", "\\n"});
-  // Use the aggregate combine, reducei to aggregate the counts
-  protected String outputExpect = "are\t3\nblue\t1\nbunnies\t1\npink\t1\nred\t1\nroses\t1\nviolets\t1\n";
-
-  private StreamJob job;
-
-  public TestStreamAggregate() throws IOException
-  {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected void createInput() throws IOException
-  {
-    DataOutputStream out = new DataOutputStream(
-                                                new FileOutputStream(INPUT_FILE.getAbsoluteFile()));
-    out.write(input.getBytes(StandardCharsets.UTF_8));
-    out.close();
-  }
-
-  protected String[] genArgs() {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", map,
-      "-reducer", "aggregate",
-      "-jobconf", MRJobConfig.PRESERVE_FAILED_TASK_FILES + "=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp")
-    };
-  }
-  
-  @Test
-  public void testCommandLine() throws Exception {
-    try {
-      try {
-        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-      } catch (Exception e) {
-      }
-
-      createInput();
-      boolean mayExit = false;
-
-      // During tests, the default Configuration will use a local mapred
-      // So don't specify -config or -cluster
-      job = new StreamJob(genArgs(), mayExit);      
-      job.go();
-      File outFile = new File(OUTPUT_DIR, "part-00000").getAbsoluteFile();
-      String output = StreamUtil.slurp(outFile);
-      outFile.delete();
-      System.err.println("outEx1=" + outputExpect);
-      System.err.println("  out1=" + output);
-      assertEquals(outputExpect, output);
-    } finally {
-      INPUT_FILE.delete();
-      FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    }
-  }
-
-  public static void main(String[]args) throws Exception
-  {
-    new TestStreaming().testCommandLine();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamDataProtocol.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamDataProtocol.java
deleted file mode 100644
index 14f0f9607e5..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamDataProtocol.java
+++ /dev/null
@@ -1,117 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.util.*;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-/**
- * This class tests hadoopStreaming in MapReduce local mode.
- */
-public class TestStreamDataProtocol
-{
-
-  // "map" command: grep -E (red|green|blue)
-  // reduce command: uniq
-  protected File INPUT_FILE = new File("input_for_data_protocol_test.txt");
-  protected File OUTPUT_DIR = new File("out_for_data_protocol_test");
-  protected String input = "roses.smell.good\nroses.look.good\nroses.need.care\nroses.attract.bees\nroses.are.red\nroses.are.not.blue\nbunnies.are.pink\nbunnies.run.fast\nbunnies.have.short.tail\nbunnies.have.long.ears\n";
-  // map behaves like "/usr/bin/cat"; 
-  protected String map = UtilTest.makeJavaCommand(TrApp.class, new String[]{".", "."});
-  // reduce counts the number of values for each key
-  protected String reduce = "org.apache.hadoop.streaming.ValueCountReduce";
-  protected String outputExpect = "bunnies.are\t1\nbunnies.have\t2\nbunnies.run\t1\nroses.are\t2\nroses.attract\t1\nroses.look\t1\nroses.need\t1\nroses.smell\t1\n";
-
-  private StreamJob job;
-
-  public TestStreamDataProtocol() throws IOException
-  {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected void createInput() throws IOException
-  {
-    DataOutputStream out = new DataOutputStream(
-                                                new FileOutputStream(INPUT_FILE.getAbsoluteFile()));
-    out.write(input.getBytes("UTF-8"));
-    out.close();
-  }
-
-  protected String[] genArgs() {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", map,
-      "-reducer", reduce,
-      "-partitioner", KeyFieldBasedPartitioner.class.getCanonicalName(),
-      "-jobconf", "stream.map.output.field.separator=.",
-      "-jobconf", "stream.num.map.output.key.fields=2",
-      "-jobconf", "mapreduce.map.output.key.field.separator=.",
-      "-jobconf", "num.key.fields.for.partition=1",
-      "-jobconf", "mapreduce.job.reduces=2",
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp")
-    };
-  }
-
-  @Test
-  public void testCommandLine() throws Exception
-  {
-    try {
-      FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    } catch (Exception e) {
-    }
-
-    try {
-      createInput();
-      boolean mayExit = false;
-
-      // During tests, the default Configuration will use a local mapred
-      // So don't specify -config or -cluster
-      job = new StreamJob(genArgs(), mayExit);      
-      job.go();
-      File outFile = new File(OUTPUT_DIR, "part-00000").getAbsoluteFile();
-      String output = StreamUtil.slurp(outFile);
-      outFile.delete();
-      System.err.println("outEx1=" + outputExpect);
-      System.err.println("  out1=" + output);
-      System.err.println("  equals=" + outputExpect.compareTo(output));
-      assertEquals(outputExpect, output);
-    } finally {
-      INPUT_FILE.delete();
-      FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    }
-  }
-
-  public static void main(String[]args) throws Exception
-  {
-    new TestStreamDataProtocol().testCommandLine();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamJob.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamJob.java
deleted file mode 100644
index 3e9036fca92..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamJob.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.KeyValueTextInputFormat;
-import org.apache.hadoop.mapred.SequenceFileInputFormat;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-/**
- * This class tests hadoop Streaming's StreamJob class.
- */
-public class TestStreamJob {
-  
-  @Test(expected = IllegalArgumentException.class)
-  public void testCreateJobWithExtraArgs() throws IOException {
-    ArrayList<String> dummyArgs = new ArrayList<String>();
-    dummyArgs.add("-input"); dummyArgs.add("dummy");
-    dummyArgs.add("-output"); dummyArgs.add("dummy");
-    dummyArgs.add("-mapper"); dummyArgs.add("dummy");
-    dummyArgs.add("dummy");
-    dummyArgs.add("-reducer"); dummyArgs.add("dummy");
-    StreamJob.createJob(dummyArgs.toArray(new String[] {}));
-  }
-  
-  @Test
-  public void testCreateJob() throws IOException {
-    JobConf job;
-    ArrayList<String> dummyArgs = new ArrayList<String>();
-    dummyArgs.add("-input"); dummyArgs.add("dummy");
-    dummyArgs.add("-output"); dummyArgs.add("dummy");
-    dummyArgs.add("-mapper"); dummyArgs.add("dummy");
-    dummyArgs.add("-reducer"); dummyArgs.add("dummy");
-    ArrayList<String> args;
-    
-    args = new ArrayList<String>(dummyArgs);
-    args.add("-inputformat");
-    args.add("org.apache.hadoop.mapred.KeyValueTextInputFormat");
-    job = StreamJob.createJob(args.toArray(new String[] {}));
-    assertEquals(KeyValueTextInputFormat.class, job.getInputFormat().getClass());
-    
-    args = new ArrayList<String>(dummyArgs);
-    args.add("-inputformat");
-    args.add("org.apache.hadoop.mapred.SequenceFileInputFormat");
-    job = StreamJob.createJob(args.toArray(new String[] {}));
-    assertEquals(SequenceFileInputFormat.class, job.getInputFormat().getClass());
-    
-    args = new ArrayList<String>(dummyArgs);
-    args.add("-inputformat");
-    args.add("org.apache.hadoop.mapred.KeyValueTextInputFormat");
-    args.add("-inputreader");
-    args.add("StreamXmlRecordReader,begin=<doc>,end=</doc>");
-    job = StreamJob.createJob(args.toArray(new String[] {}));
-    assertEquals(StreamInputFormat.class, job.getInputFormat().getClass());
-  }
-  
-  @Test
-  public void testOptions() throws Exception {
-    StreamJob streamJob = new StreamJob();
-    assertEquals(1, streamJob.run(new String[0]));
-    assertEquals(0, streamJob.run(new String[] {"-help"}));
-    assertEquals(0, streamJob.run(new String[] {"-info"}));
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamReduceNone.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamReduceNone.java
deleted file mode 100644
index 766402184ce..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamReduceNone.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.util.*;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-
-import static org.junit.Assert.*;
-import org.junit.Test;
-
-/**
- * This class tests hadoopStreaming in MapReduce local mode.
- * It tests the case where number of reducers is set to 0.
-   In this case, the mappers are expected to write out outputs directly.
-   No reducer/combiner will be activated.
- */
-public class TestStreamReduceNone
-{
-  protected File INPUT_FILE = new File("stream_reduce_none_input.txt");
-  protected File OUTPUT_DIR = new File("stream_reduce_none_out");
-  protected String input = "roses.are.red\nviolets.are.blue\nbunnies.are.pink\n";
-  // map parses input lines and generates count entries for each word.
-  protected String map = UtilTest.makeJavaCommand(TrApp.class, new String[]{".", "\\n"});
-  protected String outputExpect = "roses\t\nare\t\nred\t\nviolets\t\nare\t\nblue\t\nbunnies\t\nare\t\npink\t\n";
-
-  private StreamJob job;
-
-  public TestStreamReduceNone() throws IOException
-  {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected void createInput() throws IOException
-  {
-    DataOutputStream out = new DataOutputStream(
-                                                new FileOutputStream(INPUT_FILE.getAbsoluteFile()));
-    out.write(input.getBytes("UTF-8"));
-    out.close();
-  }
-
-  protected String[] genArgs() {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", map,
-      "-reducer", "org.apache.hadoop.mapred.lib.IdentityReducer",
-      "-numReduceTasks", "0",
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "mapreduce.job.maps=1",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp")
-    };
-  }
-
-  @Test
-  public void testCommandLine() throws Exception
-  {
-    String outFileName = "part-00000";
-    File outFile = null;
-    try {
-      try {
-        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-      } catch (Exception e) {
-      }
-
-      createInput();
-      boolean mayExit = false;
-
-      // During tests, the default Configuration will use a local mapred
-      // So don't specify -config or -cluster
-      job = new StreamJob(genArgs(), mayExit);      
-      job.go();
-      outFile = new File(OUTPUT_DIR, outFileName).getAbsoluteFile();
-      String output = StreamUtil.slurp(outFile);
-      System.err.println("outEx1=" + outputExpect);
-      System.err.println("  out1=" + output);
-      assertEquals(outputExpect, output);
-    } finally {
-      INPUT_FILE.delete();
-      FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    }
-  }
-
-  public static void main(String[]args) throws Exception
-  {
-    new TestStreamReduceNone().testCommandLine();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlMultipleRecords.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlMultipleRecords.java
deleted file mode 100644
index fc8c20dd695..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlMultipleRecords.java
+++ /dev/null
@@ -1,184 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.junit.Before;
-import org.junit.Test;
-
-/**
- * Tests if StreamXmlRecordReader will read the next record, _after_ the
- * end of a split if the split falls before the end of end-tag of a record.
- * Also tests if StreamXmlRecordReader will read a record twice if end of a
- * split is after few characters after the end-tag of a record but before the
- * begin-tag of next record.
- */
-public class TestStreamXmlMultipleRecords extends TestStreaming
-{
-  private static final Logger LOG = LoggerFactory.getLogger(
-      TestStreamXmlMultipleRecords.class);
-
-  private boolean hasPerl = false;
-  private long blockSize;
-  private String isSlowMatch;
-
-  // Our own configuration used for creating FileSystem object where
-  // fs.local.block.size is set to 60 OR 80.
-  // See 60th char in input. It is before the end of end-tag of a record.
-  // See 80th char in input. It is in between the end-tag of a record and
-  // the begin-tag of next record.
-  private Configuration conf = null;
-
-  private String myPerlMapper =
-      "perl -n -a -e 'print join(qq(\\n), map { qq($_\\t1) } @F), qq(\\n);'";
-  private String myPerlReducer =
-      "perl -n -a -e '$freq{$F[0]}++; END { print qq(is\\t$freq{is}\\n); }'";
-
-  public TestStreamXmlMultipleRecords() throws IOException {
-    super();
-
-    input = "<line>This is a single line,\nand it is containing multiple" +
-        " words.</line>                     <line>Only is appears more than" +
-        " once.</line>\n";
-    outputExpect = "is\t3\n";
-
-    map = myPerlMapper;
-    reduce = myPerlReducer;
-
-    hasPerl = UtilTest.hasPerlSupport();
-  }
-
-  @Override
-  @Before
-  public void setUp() throws IOException {
-    super.setUp();
-    // Without this closeAll() call, setting of FileSystem block size is
-    // not effective and will be old block size set in earlier test.
-    FileSystem.closeAll();
-  }
-
-  // Set file system block size such that split falls
-  // (a) before the end of end-tag of a record (testStreamXmlMultiInner...) OR
-  // (b) between records(testStreamXmlMultiOuter...)
-  @Override
-  protected Configuration getConf() {
-    conf = new Configuration();
-    conf.setLong("fs.local.block.size", blockSize);
-    return conf;
-  }
-
-  @Override
-  protected String[] genArgs() {
-    args.add("-inputreader");
-    args.add("StreamXmlRecordReader,begin=<line>,end=</line>,slowmatch=" +
-        isSlowMatch);
-    return super.genArgs();
-  }
-
-  /**
-   * Tests if StreamXmlRecordReader will read the next record, _after_ the
-   * end of a split if the split falls before the end of end-tag of a record.
-   * Tests with slowmatch=false.
-   * @throws Exception
-   */
-  @Test
-  public void testStreamXmlMultiInnerFast() throws Exception {
-    if (hasPerl) {
-      blockSize = 60;
-
-      isSlowMatch = "false";
-      super.testCommandLine();
-    }
-    else {
-      LOG.warn("No perl; skipping test.");
-    }
-  }
-
-  /**
-   * Tests if StreamXmlRecordReader will read a record twice if end of a
-   * split is after few characters after the end-tag of a record but before the
-   * begin-tag of next record.
-   * Tests with slowmatch=false.
-   * @throws Exception
-   */
-  @Test
-  public void testStreamXmlMultiOuterFast() throws Exception {
-    if (hasPerl) {
-      blockSize = 80;
-
-      isSlowMatch = "false";
-      super.testCommandLine();
-    }
-    else {
-      LOG.warn("No perl; skipping test.");
-    }
-  }
-
-  /**
-   * Tests if StreamXmlRecordReader will read the next record, _after_ the
-   * end of a split if the split falls before the end of end-tag of a record.
-   * Tests with slowmatch=true.
-   * @throws Exception
-   */
-  @Test
-  public void testStreamXmlMultiInnerSlow() throws Exception {
-    if (hasPerl) {
-      blockSize = 60;
-
-      isSlowMatch = "true";
-      super.testCommandLine();
-    }
-    else {
-      LOG.warn("No perl; skipping test.");
-    }
-  }
-
-  /**
-   * Tests if StreamXmlRecordReader will read a record twice if end of a
-   * split is after few characters after the end-tag of a record but before the
-   * begin-tag of next record.
-   * Tests with slowmatch=true.
-   * @throws Exception
-   */
-  @Test
-  public void testStreamXmlMultiOuterSlow() throws Exception {
-    if (hasPerl) {
-      blockSize = 80;
-
-      isSlowMatch = "true";
-      super.testCommandLine();
-    }
-    else {
-      LOG.warn("No perl; skipping test.");
-    }
-  }
-
-  @Override
-  @Test
-  public void testCommandLine() {
-    // Do nothing
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlRecordReader.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlRecordReader.java
deleted file mode 100644
index 53009dbbabc..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamXmlRecordReader.java
+++ /dev/null
@@ -1,61 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-
-/**
- * This class tests StreamXmlRecordReader
- * The test creates an XML file, uses StreamXmlRecordReader and compares
- * the expected output against the generated output
- */
-public class TestStreamXmlRecordReader extends TestStreaming {
-
-  public TestStreamXmlRecordReader() throws IOException {
-    INPUT_FILE = new File("target/input.xml");
-    input = "<xmltag>\t\nroses.are.red\t\nviolets.are.blue\t\n" +
-        "bunnies.are.pink\t\n</xmltag>\t\n";
-    map = CAT;
-    reduce = "NONE";
-    outputExpect = input;
-  }
-
-  @Override
-  protected void createInput() throws IOException
-  {
-    FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());
-    String dummyXmlStartTag = "<PATTERN>\n";
-    String dummyXmlEndTag = "</PATTERN>\n";
-    out.write(dummyXmlStartTag.getBytes("UTF-8"));
-    out.write(input.getBytes("UTF-8"));
-    out.write(dummyXmlEndTag.getBytes("UTF-8"));
-    out.close();
-  }
-
-  @Override
-  protected String[] genArgs() {
-    args.add("-inputreader");
-    args.add("StreamXmlRecordReader,begin=<xmltag>,end=</xmltag>");
-    args.add("-jobconf");
-    args.add("mapreduce.job.maps=1");
-    return super.genArgs();
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreaming.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreaming.java
deleted file mode 100644
index 4f39120a162..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreaming.java
+++ /dev/null
@@ -1,203 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.hadoop.util.JarFinder;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.util.Shell;
-
-/**
- * This class tests hadoopStreaming in MapReduce local mode.
- */
-public class TestStreaming
-{
-
-  public static final String STREAMING_JAR = JarFinder.getJar(StreamJob.class);
-
-  /**
-   * cat command used for copying stdin to stdout as mapper or reducer function.
-   * On Windows, use a cmd script that approximates the functionality of cat.
-   */
-  static final String CAT = Shell.WINDOWS ?
-    "cmd /c " + new File("target/bin/cat.cmd").getAbsolutePath() : "cat";
-
-  /**
-   * Command used for iterating through file names on stdin and copying each
-   * file's contents to stdout, used as mapper or reducer function.  On Windows,
-   * use a cmd script that approximates the functionality of xargs cat.
-   */
-  static final String XARGS_CAT = Shell.WINDOWS ?
-    "cmd /c " + new File("target/bin/xargs_cat.cmd").getAbsolutePath() :
-    "xargs cat";
-
-  // "map" command: grep -E (red|green|blue)
-  // reduce command: uniq
-  protected File TEST_DIR;
-  protected File INPUT_FILE;
-  protected File OUTPUT_DIR;
-  protected String inputFile;
-  protected String outDir;
-  protected String input = "roses.are.red\nviolets.are.blue\nbunnies.are.pink\n";
-  // map behaves like "/usr/bin/tr . \\n"; (split words into lines)
-  protected String map = UtilTest.makeJavaCommand(TrApp.class, new String[]{".", "\\n"});
-  // reduce behave like /usr/bin/uniq. But also prepend lines with R.
-  // command-line combiner does not have any effect any more.
-  protected String reduce = UtilTest.makeJavaCommand(UniqApp.class, new String[]{"R"});
-  protected String outputExpect = "Rare\t\nRblue\t\nRbunnies\t\nRpink\t\nRred\t\nRroses\t\nRviolets\t\n";
-
-  protected ArrayList<String> args = new ArrayList<String>();
-  protected StreamJob job;
-
-  public TestStreaming() throws IOException
-  {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-    setTestDir(new File("target/TestStreaming").getAbsoluteFile());
-  }
-
-  /**
-   * Sets root of test working directory and resets any other paths that must be
-   * children of the test working directory.  Typical usage is for subclasses
-   * that use HDFS to override the test directory to the form "/tmp/<test name>"
-   * so that on Windows, tests won't attempt to use paths containing a ':' from
-   * the drive specifier.  The ':' character is considered invalid by HDFS.
-   * 
-   * @param testDir File to set
-   */
-  protected void setTestDir(File testDir) {
-    TEST_DIR = testDir;
-    OUTPUT_DIR = new File(testDir, "out");
-    INPUT_FILE = new File(testDir, "input.txt");
-  }
-
-  @Before
-  public void setUp() throws IOException {
-    UtilTest.recursiveDelete(TEST_DIR);
-    assertTrue("Creating " + TEST_DIR, TEST_DIR.mkdirs());
-    args.clear();
-  }
-
-  @After
-  public void tearDown() {
-    UtilTest.recursiveDelete(TEST_DIR);
-  }
-
-  protected String getInputData() {
-    return input;
-  }
-
-  protected void createInput() throws IOException
-  {
-    DataOutputStream out = getFileSystem().create(new Path(
-      INPUT_FILE.getPath()));
-    out.write(getInputData().getBytes("UTF-8"));
-    out.close();
-  }
-
-  protected void setInputOutput() {
-    inputFile = INPUT_FILE.getPath();
-    outDir = OUTPUT_DIR.getPath();
-  }
-
-  protected String[] genArgs() {
-    args.add("-input");args.add(inputFile);
-    args.add("-output");args.add(outDir);
-    args.add("-mapper");args.add(map);
-    args.add("-reducer");args.add(reduce);
-    args.add("-jobconf");
-    args.add("mapreduce.task.files.preserve.failedtasks=true");
-    args.add("-jobconf");
-    args.add("stream.tmpdir="+System.getProperty("test.build.data","/tmp"));
-
-    String str[] = new String [args.size()];
-    args.toArray(str);
-    return str;
-  }
-
-  protected Configuration getConf() {
-    return new Configuration();
-  }
-
-  protected FileSystem getFileSystem() throws IOException {
-    return FileSystem.get(getConf());
-  }
-
-  protected String getExpectedOutput() {
-    return outputExpect;
-  }
-
-  protected void checkOutput() throws IOException {
-    Path outPath = new Path(OUTPUT_DIR.getPath(), "part-00000");
-    FileSystem fs = getFileSystem();
-    String output = StreamUtil.slurpHadoop(outPath, fs);
-    fs.delete(outPath, true);
-    System.err.println("outEx1=" + getExpectedOutput());
-    System.err.println("  out1=" + output);
-    assertOutput(getExpectedOutput(), output);
-  }
-
-  protected void assertOutput(String expectedOutput, String output) throws IOException {
-    String[] words = expectedOutput.split("\t\n");
-    Set<String> expectedWords = new HashSet<String>(Arrays.asList(words));
-    words = output.split("\t\n");
-    Set<String> returnedWords = new HashSet<String>(Arrays.asList(words));
-//    PrintWriter writer = new PrintWriter(new OutputStreamWriter(new FileOutputStream(new File("/tmp/tucu.txt"), true)), true);
-//    writer.println("** Expected: " + expectedOutput);
-//    writer.println("** Output  : " + output);
-    assertTrue(returnedWords.containsAll(expectedWords));
-  }
-
-  /**
-   * Runs a streaming job with the given arguments
-   * @return the streaming job return status
-   * @throws IOException
-   */
-  protected int runStreamJob() throws IOException {
-    setInputOutput();
-    createInput();
-    boolean mayExit = false;
-
-    // During tests, the default Configuration will use a local mapred
-    // So don't specify -config or -cluster
-    job = new StreamJob(genArgs(), mayExit);
-    return job.go();
-  }
-
-  @Test
-  public void testCommandLine() throws Exception
-  {
-    int ret = runStreamJob();
-    assertEquals(0, ret);
-    checkOutput();
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBackground.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBackground.java
deleted file mode 100644
index c18c283dd75..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBackground.java
+++ /dev/null
@@ -1,87 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-
-import org.junit.Before;
-import org.junit.Test;
-
-/**
- * This class tests if hadoopStreaming background works fine. A DelayEchoApp
- * with 10 seconds delay is submited. 
- */
-public class TestStreamingBackground {
-  protected File TEST_DIR = new File("target/TestStreamingBackground")
-      .getAbsoluteFile();
-  protected File INPUT_FILE = new File(TEST_DIR, "input.txt");
-  protected File OUTPUT_DIR = new File(TEST_DIR, "out");
-
-  protected String tenSecondsTask = UtilTest.makeJavaCommand(
-      DelayEchoApp.class, new String[] { "10" });
-
-  public TestStreamingBackground() throws IOException {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected String[] args = new String[] { 
-      "-background",
-      "-input", INPUT_FILE.getAbsolutePath(), 
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", tenSecondsTask, 
-      "-reducer", tenSecondsTask, 
-      "-jobconf", "stream.tmpdir=" + System.getProperty("test.build.data", "/tmp"),
-      "-jobconf", "mapreduce.task.io.sort.mb=10" 
-  };
-
-  @Before
-  public void setUp() throws IOException {
-    UtilTest.recursiveDelete(TEST_DIR);
-    assertTrue(TEST_DIR.mkdirs());
-
-    FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());
-    out.write("hello\n".getBytes());
-    out.close();
-  }
-
-  public void runStreamJob() throws Exception {
-    boolean mayExit = false;
-    int returnStatus = 0;
-
-    StreamJob job = new StreamJob(args, mayExit);
-    returnStatus = job.go();
-
-    assertEquals("Streaming Job expected to succeed", 0, returnStatus);
-    job.running_.killJob();
-    job.running_.waitForCompletion();
-  }
-
-  @Test
-  public void testBackgroundSubmitOk() throws Exception {
-    runStreamJob();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBadRecords.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBadRecords.java
deleted file mode 100644
index 5a4e3a960d2..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingBadRecords.java
+++ /dev/null
@@ -1,329 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.BufferedReader;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.io.Writer;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Properties;
-import java.util.StringTokenizer;
-
-import org.junit.BeforeClass;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.ClusterMapReduceTestCase;
-import org.apache.hadoop.mapred.Counters;
-import org.apache.hadoop.mapred.RunningJob;
-import org.apache.hadoop.mapred.SkipBadRecords;
-import org.apache.hadoop.mapred.Utils;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.junit.Before;
-import org.junit.Test;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-
-public class TestStreamingBadRecords extends ClusterMapReduceTestCase
-{
-
-  private static final Logger LOG =
-    LoggerFactory.getLogger(TestStreamingBadRecords.class);
-  
-  private static final List<String> MAPPER_BAD_RECORDS = 
-    Arrays.asList("hey022","hey023","hey099");
-  
-  private static final List<String> REDUCER_BAD_RECORDS = 
-    Arrays.asList("hey001","hey018");
-  
-  private static final String badMapper = 
-    UtilTest.makeJavaCommand(BadApp.class, new String[]{});
-  private static final String badReducer = 
-    UtilTest.makeJavaCommand(BadApp.class, new String[]{"true"});
-  private static final int INPUTSIZE=100;
-
-  @BeforeClass
-  public static void setupClass() throws Exception {
-    setupClassBase(TestStreamingBadRecords.class);
-  }
-
-  public TestStreamingBadRecords() throws IOException
-  {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  @Before
-  public void setUp() throws Exception {
-    Properties props = new Properties();
-    props.setProperty(JTConfig.JT_RETIREJOBS, "false");
-    props.setProperty(JTConfig.JT_PERSIST_JOBSTATUS, "false");
-    startCluster(true, props);
-  }
-
-  private void createInput() throws Exception {
-    OutputStream os = getFileSystem().create(new Path(getInputDir(), 
-        "text.txt"));
-    Writer wr = new OutputStreamWriter(os);
-    //increasing the record size so that we have stream flushing
-    String prefix = new String(new byte[20*1024]);
-    for(int i=1;i<=INPUTSIZE;i++) {
-      String str = ""+i;
-      int zerosToPrepend = 3 - str.length();
-      for(int j=0;j<zerosToPrepend;j++){
-        str = "0"+str;
-      }
-      wr.write(prefix + "hey"+str+"\n");
-    }wr.close();
-  }
-  
-  private void validateOutput(RunningJob runningJob, boolean validateCount) 
-    throws Exception {
-    LOG.info(runningJob.getCounters().toString());
-    assertTrue(runningJob.isSuccessful());
-    
-    if(validateCount) {
-     //validate counters
-      String counterGrp = "org.apache.hadoop.mapred.Task$Counter";
-      Counters counters = runningJob.getCounters();
-      assertEquals(counters.findCounter(counterGrp, "MAP_SKIPPED_RECORDS").
-          getCounter(),MAPPER_BAD_RECORDS.size());
-      
-      int mapRecs = INPUTSIZE - MAPPER_BAD_RECORDS.size();
-      assertEquals(counters.findCounter(counterGrp, "MAP_INPUT_RECORDS").
-          getCounter(),mapRecs);
-      assertEquals(counters.findCounter(counterGrp, "MAP_OUTPUT_RECORDS").
-          getCounter(),mapRecs);
-      
-      int redRecs = mapRecs - REDUCER_BAD_RECORDS.size();
-      assertEquals(counters.findCounter(counterGrp, "REDUCE_SKIPPED_RECORDS").
-          getCounter(),REDUCER_BAD_RECORDS.size());
-      assertEquals(counters.findCounter(counterGrp, "REDUCE_SKIPPED_GROUPS").
-          getCounter(),REDUCER_BAD_RECORDS.size());
-      assertEquals(counters.findCounter(counterGrp, "REDUCE_INPUT_GROUPS").
-          getCounter(),redRecs);
-      assertEquals(counters.findCounter(counterGrp, "REDUCE_INPUT_RECORDS").
-          getCounter(),redRecs);
-      assertEquals(counters.findCounter(counterGrp, "REDUCE_OUTPUT_RECORDS").
-          getCounter(),redRecs);
-    }
-    
-    List<String> badRecs = new ArrayList<String>();
-    badRecs.addAll(MAPPER_BAD_RECORDS);
-    badRecs.addAll(REDUCER_BAD_RECORDS);
-    Path[] outputFiles = FileUtil.stat2Paths(
-        getFileSystem().listStatus(getOutputDir(),
-        new Utils.OutputFileUtils.OutputFilesFilter()));
-    
-    if (outputFiles.length > 0) {
-      InputStream is = getFileSystem().open(outputFiles[0]);
-      BufferedReader reader = new BufferedReader(new InputStreamReader(is));
-      String line = reader.readLine();
-      int counter = 0;
-      while (line != null) {
-        counter++;
-        StringTokenizer tokeniz = new StringTokenizer(line, "\t");
-        String value = tokeniz.nextToken();
-        int index = value.indexOf("hey");
-        assertTrue(index>-1);
-        if(index>-1) {
-          String heyStr = value.substring(index);
-          assertTrue(!badRecs.contains(heyStr));
-        }
-        
-        line = reader.readLine();
-      }
-      reader.close();
-      if(validateCount) {
-        assertEquals(INPUTSIZE-badRecs.size(), counter);
-      }
-    }
-  }
-
-  /*
-   * Disable test as skipping bad records not supported in 0.23
-   */
-  /*
-  public void testSkip() throws Exception {
-    JobConf clusterConf = createJobConf();
-    createInput();
-    int attSkip =0;
-    SkipBadRecords.setAttemptsToStartSkipping(clusterConf,attSkip);
-    //the no of attempts to successfully complete the task depends 
-    //on the no of bad records.
-    int mapperAttempts = attSkip+1+MAPPER_BAD_RECORDS.size();
-    int reducerAttempts = attSkip+1+REDUCER_BAD_RECORDS.size();
-    
-    String[] args =  new String[] {
-      "-input", (new Path(getInputDir(), "text.txt")).toString(),
-      "-output", getOutputDir().toString(),
-      "-mapper", badMapper,
-      "-reducer", badReducer,
-      "-verbose",
-      "-inputformat", "org.apache.hadoop.mapred.KeyValueTextInputFormat",
-      "-jobconf", "mapreduce.task.skip.start.attempts="+attSkip,
-      "-jobconf", "mapreduce.job.skip.outdir=none",
-      "-jobconf", "mapreduce.map.maxattempts="+mapperAttempts,
-      "-jobconf", "mapreduce.reduce.maxattempts="+reducerAttempts,
-      "-jobconf", "mapreduce.map.skip.maxrecords="+Long.MAX_VALUE,
-      "-jobconf", "mapreduce.reduce.skip.maxgroups="+Long.MAX_VALUE,
-      "-jobconf", "mapreduce.job.maps=1",
-      "-jobconf", "mapreduce.job.reduces=1",
-      "-jobconf", "fs.default.name="+clusterConf.get("fs.default.name"),
-      "-jobconf", "mapreduce.jobtracker.address=" + 
-                   clusterConf.get(JTConfig.JT_IPC_ADDRESS),
-      "-jobconf", "mapreduce.jobtracker.http.address="
-                    +clusterConf.get(JTConfig.JT_HTTP_ADDRESS),
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-      "-jobconf", "mapred.jar=" + TestStreaming.STREAMING_JAR,
-      "-jobconf", "mapreduce.framework.name=yarn"
-    };
-    StreamJob job = new StreamJob(args, false);      
-    job.go();
-    validateOutput(job.running_, false);
-    //validate that there is no skip directory as it has been set to "none"
-    assertTrue(SkipBadRecords.getSkipOutputPath(job.jobConf_)==null);
-  }
-  */
-  
-  /*
-   * Disable test as skipping bad records not supported in 0.23
-   */
-  /*
-  public void testNarrowDown() throws Exception {
-    createInput();
-    JobConf clusterConf = createJobConf();
-    String[] args =  new String[] {
-      "-input", (new Path(getInputDir(), "text.txt")).toString(),
-      "-output", getOutputDir().toString(),
-      "-mapper", badMapper,
-      "-reducer", badReducer,
-      "-verbose",
-      "-inputformat", "org.apache.hadoop.mapred.KeyValueTextInputFormat",
-      "-jobconf", "mapreduce.task.skip.start.attempts=1",
-      //actually fewer attempts are required than specified
-      //but to cater to the case of slow processed counter update, need to 
-      //have more attempts
-      "-jobconf", "mapreduce.map.maxattempts=20",
-      "-jobconf", "mapreduce.reduce.maxattempts=15",
-      "-jobconf", "mapreduce.map.skip.maxrecords=1",
-      "-jobconf", "mapreduce.reduce.skip.maxgroups=1",
-      "-jobconf", "mapreduce.job.maps=1",
-      "-jobconf", "mapreduce.job.reduces=1",
-      "-jobconf", "fs.default.name="+clusterConf.get("fs.default.name"),
-      "-jobconf", "mapreduce.jobtracker.address="+clusterConf.get(JTConfig.JT_IPC_ADDRESS),
-      "-jobconf", "mapreduce.jobtracker.http.address="
-                    +clusterConf.get(JTConfig.JT_HTTP_ADDRESS),
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-      "-jobconf", "mapred.jar=" + TestStreaming.STREAMING_JAR,
-      "-jobconf", "mapreduce.framework.name=yarn"
-    };
-    StreamJob job = new StreamJob(args, false);      
-    job.go();
-    
-    validateOutput(job.running_, true);
-    assertTrue(SkipBadRecords.getSkipOutputPath(job.jobConf_)!=null);
-  }
-  */
-
-  @Test
-  public void testNoOp() {
-    // Added to avoid warnings when running this disabled test
-  }
-  
-  static class App{
-    boolean isReducer;
-    
-    public App(String[] args) throws Exception{
-      if(args.length>0) {
-        isReducer = Boolean.parseBoolean(args[0]);
-      }
-      String counter = SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS;
-      if(isReducer) {
-        counter = SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS;
-      }
-      BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-      String line;
-      int count = 0;
-      while ((line = in.readLine()) != null) {
-        processLine(line);
-        count++;
-        if(count>=10) {
-          System.err.println("reporter:counter:"+SkipBadRecords.COUNTER_GROUP+
-              ","+counter+","+count);
-          count = 0;
-        }
-      }
-    }
-    
-    protected void processLine(String line) throws Exception{
-      System.out.println(line);
-    }
-    
-    
-    public static void main(String[] args) throws Exception{
-      new App(args);
-    }
-  }
-  
-  static class BadApp extends App{
-    
-    public BadApp(String[] args) throws Exception {
-      super(args);
-    }
-
-    protected void processLine(String line) throws Exception {
-      List<String> badRecords = MAPPER_BAD_RECORDS;
-      if(isReducer) {
-        badRecords = REDUCER_BAD_RECORDS;
-      }
-      if(badRecords.size()>0 && line.contains(badRecords.get(0))) {
-        LOG.warn("Encountered BAD record");
-        System.exit(-1);
-      }
-      else if(badRecords.size()>1 && line.contains(badRecords.get(1))) {
-        LOG.warn("Encountered BAD record");
-        throw new Exception("Got bad record..crashing");
-      }
-      else if(badRecords.size()>2 && line.contains(badRecords.get(2))) {
-        LOG.warn("Encountered BAD record");
-        System.exit(-1);
-      }
-      super.processLine(line);
-    }
-    
-    public static void main(String[] args) throws Exception{
-      new BadApp(args);
-    }
-  }
-  
-  
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCombiner.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCombiner.java
deleted file mode 100644
index 3076d8e6ae7..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCombiner.java
+++ /dev/null
@@ -1,54 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.IOException;
-
-import org.apache.hadoop.mapred.Counters;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestStreamingCombiner extends TestStreaming {
-
-  protected String combine = UtilTest.makeJavaCommand(
-      UniqApp.class, new String[]{""});
-  
-  public TestStreamingCombiner() throws IOException {
-    super();
-  }
-  
-  protected String[] genArgs() {
-    args.add("-combiner");
-    args.add(combine);
-    return super.genArgs();
-  }
-
-  @Test
-  public void testCommandLine() throws Exception {
-    super.testCommandLine();
-    // validate combiner counters
-    String counterGrp = "org.apache.hadoop.mapred.Task$Counter";
-    Counters counters = job.running_.getCounters();
-    assertTrue(counters.findCounter(
-               counterGrp, "COMBINE_INPUT_RECORDS").getValue() != 0);
-    assertTrue(counters.findCounter(
-               counterGrp, "COMBINE_OUTPUT_RECORDS").getValue() != 0);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCounters.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCounters.java
deleted file mode 100644
index 3748f4fb93a..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingCounters.java
+++ /dev/null
@@ -1,53 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import java.io.IOException;
-
-import org.apache.hadoop.mapred.Counters;
-import org.apache.hadoop.mapred.Counters.Counter;
-import org.apache.hadoop.mapred.Counters.Group;
-
-/**
- * This class tests streaming counters in MapReduce local mode.
- */
-public class TestStreamingCounters extends TestStreaming {
-  public TestStreamingCounters() throws IOException {
-    super();
-  }
-
-  @Test
-  public void testCommandLine() throws Exception {
-    super.testCommandLine();
-    validateCounters();
-  }
-  
-  private void validateCounters() throws IOException {
-    Counters counters = job.running_.getCounters();
-    assertNotNull("Counters", counters);
-    Group group = counters.getGroup("UserCounters");
-    assertNotNull("Group", group);
-    Counter counter = group.getCounterForName("InputLines");
-    assertNotNull("Counter", counter);
-    assertEquals(3, counter.getCounter());
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingExitStatus.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingExitStatus.java
deleted file mode 100644
index 411f740fcda..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingExitStatus.java
+++ /dev/null
@@ -1,109 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import org.junit.Test;
-import org.junit.Before;
-import static org.junit.Assert.*;
-
-import java.io.*;
-import java.util.*;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-/**
- * This class tests if hadoopStreaming fails a job when the mapper or
- * reducers have non-zero exit status and the
- * stream.non.zero.exit.status.is.failure jobconf is set.
- */
-public class TestStreamingExitStatus
-{
-  protected File TEST_DIR =
-    new File("target/TestStreamingExitStatus").getAbsoluteFile();
-  protected File INPUT_FILE = new File(TEST_DIR, "input.txt");
-  protected File OUTPUT_DIR = new File(TEST_DIR, "out");
-
-  protected String failingTask = UtilTest.makeJavaCommand(FailApp.class, new String[]{"true"});
-  protected String echoTask = UtilTest.makeJavaCommand(FailApp.class, new String[]{"false"});
-
-  public TestStreamingExitStatus() throws IOException {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected String[] genArgs(boolean exitStatusIsFailure, boolean failMap) {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", (failMap ? failingTask : echoTask),
-      "-reducer", (failMap ? echoTask : failingTask),
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "stream.non.zero.exit.is.failure=" + exitStatusIsFailure,
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-      "-jobconf", "mapreduce.task.io.sort.mb=10"
-    };
-  }
-
-  @Before
-  public void setUp() throws IOException {
-    UtilTest.recursiveDelete(TEST_DIR);
-    assertTrue(TEST_DIR.mkdirs());
-
-    FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());
-    out.write("hello\n".getBytes());
-    out.close();
-  }
-
-  public void runStreamJob(boolean exitStatusIsFailure, boolean failMap) throws Exception {
-    boolean mayExit = false;
-    int returnStatus = 0;
-
-    StreamJob job = new StreamJob(genArgs(exitStatusIsFailure, failMap), mayExit);
-    returnStatus = job.go();
-    
-    if (exitStatusIsFailure) {
-      assertEquals("Streaming Job failure code expected", /*job not successful:*/1, returnStatus);
-    } else {
-      assertEquals("Streaming Job expected to succeed", 0, returnStatus);
-    }
-  }
-
-  @Test
-  public void testMapFailOk() throws Exception {
-    runStreamJob(false, true);
-  }
-
-  @Test
-  public void testMapFailNotOk() throws Exception {
-    runStreamJob(true, true);
-  }
-
-  @Test
-  public void testReduceFailOk() throws Exception {
-    runStreamJob(false, false);
-  }
-  
-  @Test
-  public void testReduceFailNotOk() throws Exception {
-    runStreamJob(true, false);
-  }  
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingFailure.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingFailure.java
deleted file mode 100644
index 29901cc77d8..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingFailure.java
+++ /dev/null
@@ -1,55 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import org.junit.Test;
-import static org.junit.Assert.assertEquals;
-
-import java.io.File;
-import java.io.IOException;
-
-/**
- * This class tests if hadoopStreaming returns Exception 
- * on failure when submitted an invalid/failed job
- * The test case provides an invalid input file for map/reduce job as
- * a unit test case
- */
-public class TestStreamingFailure extends TestStreaming
-{
-
-  protected File INVALID_INPUT_FILE;
-
-  public TestStreamingFailure() throws IOException
-  {
-    INVALID_INPUT_FILE = new File("invalid_input.txt");
-  }
-
-  @Override
-  protected void setInputOutput() {
-    inputFile = INVALID_INPUT_FILE.getAbsolutePath();
-    outDir = OUTPUT_DIR.getAbsolutePath();
-  }
-
-  @Override
-  @Test
-  public void testCommandLine() throws IOException {
-    int returnStatus = runStreamJob();
-    assertEquals("Streaming Job Failure code expected", 5, returnStatus);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingKeyValue.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingKeyValue.java
deleted file mode 100644
index c21cb159f4f..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingKeyValue.java
+++ /dev/null
@@ -1,139 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import java.io.*;
-
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-
-/**
- * This class tests hadoopStreaming in MapReduce local mode.
- * This testcase looks at different cases of tab position in input. 
- */
-public class TestStreamingKeyValue
-{
-  protected File INPUT_FILE = new File("target/input.txt");
-  protected File OUTPUT_DIR = new File("target/stream_out");
-  // First line of input has 'key' 'tab' 'value'
-  // Second line of input starts with a tab character. 
-  // So, it has empty key and the whole line as value.
-  // Third line of input does not have any tab character.
-  // So, the whole line is the key and value is empty.
-  protected String input = 
-    "roses are \tred\t\n\tviolets are blue\nbunnies are pink\n" +
-    "this is for testing a big\tinput line\n" +
-    "small input\n";
-  protected String outputWithoutKey = 
-    "\tviolets are blue\nbunnies are pink\t\n" + 
-    "roses are \tred\t\n" +
-    "small input\t\n" +
-    "this is for testing a big\tinput line\n";
-  protected String outputWithKey = 
-    "0\troses are \tred\t\n" +  
-    "16\t\tviolets are blue\n" +
-    "34\tbunnies are pink\n" +
-    "51\tthis is for testing a big\tinput line\n" +
-    "88\tsmall input\n";
-
-  private StreamJob job;
-
-  public TestStreamingKeyValue() throws IOException
-  {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected void createInput() throws IOException
-  {
-    DataOutputStream out = new DataOutputStream(
-       new FileOutputStream(INPUT_FILE.getAbsoluteFile()));
-    out.write(input.getBytes("UTF-8"));
-    out.close();
-  }
-
-  protected String[] genArgs(boolean ignoreKey) {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", TestStreaming.CAT,
-      "-jobconf", MRJobConfig.PRESERVE_FAILED_TASK_FILES + "=true", 
-      "-jobconf", "stream.non.zero.exit.is.failure=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-      "-jobconf", "stream.map.input.ignoreKey="+ignoreKey,      
-    };
-  }
-  
-  public void runStreamJob(final String outputExpect, boolean ignoreKey) 
-      throws Exception
-  {
-    String outFileName = "part-00000";
-    File outFile = null;
-    try {
-      try {
-        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-      } catch (Exception e) {
-      }
-
-      createInput();
-      boolean mayExit = false;
-
-      // During tests, the default Configuration will use a local mapred
-      // So don't specify -config or -cluster
-      job = new StreamJob(genArgs(ignoreKey), mayExit);      
-      job.go();
-      outFile = new File(OUTPUT_DIR, outFileName).getAbsoluteFile();
-      String output = StreamUtil.slurp(outFile);
-      System.err.println("outEx1=" + outputExpect);
-      System.err.println("  out1=" + output);
-      assertEquals(outputExpect, output);
-    } finally {
-      INPUT_FILE.delete();
-      FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    }
-  }
-
-  /**
-   * Run the job with the indicating the input format key should be emitted. 
-   */
-  @Test
-  public void testCommandLineWithKey() throws Exception
-  {
-    runStreamJob(outputWithKey, false);
-  }
-
-  /**
-   * Run the job the default way (the input format key is not emitted).
-   */
-  @Test
-  public void testCommandLineWithoutKey() throws Exception
-  {
-      runStreamJob(outputWithoutKey, true);
-  }
-  
-  public static void main(String[]args) throws Exception
-  {
-    new TestStreamingKeyValue().testCommandLineWithKey();    
-    new TestStreamingKeyValue().testCommandLineWithoutKey();
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputKeyValueTypes.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputKeyValueTypes.java
deleted file mode 100644
index 35eb752b23a..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputKeyValueTypes.java
+++ /dev/null
@@ -1,198 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapred.MapReduceBase;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-import org.apache.hadoop.mapred.TextInputFormat;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-/**
- * Tests stream job with java tasks, commands in MapReduce local mode.
- * Validates if user-set config properties
- * {@link MRJobConfig#MAP_OUTPUT_KEY_CLASS} and
- * {@link MRJobConfig#OUTPUT_KEY_CLASS} are honored by streaming jobs.
- */
-public class TestStreamingOutputKeyValueTypes extends TestStreaming {
-
-  public TestStreamingOutputKeyValueTypes() throws IOException {
-    super();
-    input = "one line dummy input\n";
-  }
-
-  @Before
-  @Override
-  public void setUp() throws IOException {
-    args.clear();
-    super.setUp();
-  }
-
-  @Override
-  protected String[] genArgs() {
-    // set the testcase-specific config properties first and the remaining
-    // arguments are set in TestStreaming.genArgs().
-    args.add("-jobconf");
-    args.add(MRJobConfig.MAP_OUTPUT_KEY_CLASS +
-        "=org.apache.hadoop.io.LongWritable");
-    args.add("-jobconf");
-    args.add(MRJobConfig.OUTPUT_KEY_CLASS +
-        "=org.apache.hadoop.io.LongWritable");
-
-    // Using SequenceFileOutputFormat here because with TextOutputFormat, the
-    // mapred.output.key.class set in JobConf (which we want to test here) is
-    // not read/used at all.
-    args.add("-outputformat");
-    args.add("org.apache.hadoop.mapred.SequenceFileOutputFormat");
-
-    return super.genArgs();
-  }
-
-  @Override
-  protected void checkOutput() throws IOException {
-    // No need to validate output for the test cases in this class
-  }
-
-  public static class MyReducer<K, V>
-  extends MapReduceBase implements Reducer<K, V, LongWritable, Text> {
-
-    public void reduce(K key, Iterator<V> values,
-        OutputCollector<LongWritable, Text> output, Reporter reporter)
-        throws IOException {
-      LongWritable l = new LongWritable();
-      while (values.hasNext()) {
-        output.collect(l, new Text(values.next().toString()));
-      }
-    }
-  }
-
-  // Check with Java Mapper, Java Reducer
-  @Test
-  public void testJavaMapperAndJavaReducer() throws Exception {
-    map = "org.apache.hadoop.mapred.lib.IdentityMapper";
-    reduce = "org.apache.hadoop.mapred.lib.IdentityReducer";
-    super.testCommandLine();
-  }
-
-  // Check with Java Mapper, Java Reducer and -numReduceTasks 0
-  @Test
-  public void testJavaMapperAndJavaReducerAndZeroReduces() throws Exception {
-    map = "org.apache.hadoop.mapred.lib.IdentityMapper";
-    reduce = "org.apache.hadoop.mapred.lib.IdentityReducer";
-    args.add("-numReduceTasks");
-    args.add("0");
-    super.testCommandLine();
-  }
-
-  // Check with Java Mapper, Reducer = "NONE"
-  @Test
-  public void testJavaMapperWithReduceNone() throws Exception {
-    map = "org.apache.hadoop.mapred.lib.IdentityMapper";
-    reduce = "NONE";
-    super.testCommandLine();
-  }
-
-  // Check with Java Mapper, command Reducer
-  @Test
-  public void testJavaMapperAndCommandReducer() throws Exception {
-    map = "org.apache.hadoop.mapred.lib.IdentityMapper";
-    reduce = CAT;
-    super.testCommandLine();
-  }
-
-  // Check with Java Mapper, command Reducer and -numReduceTasks 0
-  @Test
-  public void testJavaMapperAndCommandReducerAndZeroReduces() throws Exception {
-    map = "org.apache.hadoop.mapred.lib.IdentityMapper";
-    reduce = CAT;
-    args.add("-numReduceTasks");
-    args.add("0");
-    super.testCommandLine();
-  }
-
-  // Check with Command Mapper, Java Reducer
-  @Test
-  public void testCommandMapperAndJavaReducer() throws Exception {
-    map = CAT;
-    reduce = MyReducer.class.getName();
-    super.testCommandLine();
-  }
-
-  // Check with Command Mapper, Java Reducer and -numReduceTasks 0
-  @Test
-  public void testCommandMapperAndJavaReducerAndZeroReduces() throws Exception {
-    map = CAT;
-    reduce = MyReducer.class.getName();
-    args.add("-numReduceTasks");
-    args.add("0");
-    super.testCommandLine();
-  }
-
-  // Check with Command Mapper, Reducer = "NONE"
-  @Test
-  public void testCommandMapperWithReduceNone() throws Exception {
-    map = CAT;
-    reduce = "NONE";
-    super.testCommandLine();
-  }
-
-  // Check with Command Mapper, Command Reducer
-  @Test
-  public void testCommandMapperAndCommandReducer() throws Exception {
-    map = CAT;
-    reduce = CAT;
-    super.testCommandLine();
-  }
-
-  // Check with Command Mapper, Command Reducer and -numReduceTasks 0
-  @Test
-  public void testCommandMapperAndCommandReducerAndZeroReduces()
-      throws Exception {
-    map = CAT;
-    reduce = CAT;
-    args.add("-numReduceTasks");
-    args.add("0");
-    super.testCommandLine();
-  }
-  
-  @Test
-  public void testDefaultToIdentityReducer() throws Exception {
-    args.add("-mapper");args.add(map);
-    args.add("-jobconf");
-    args.add("mapreduce.task.files.preserve.failedtasks=true");
-    args.add("-jobconf");
-    args.add("stream.tmpdir="+System.getProperty("test.build.data","/tmp"));
-    args.add("-inputformat");args.add(TextInputFormat.class.getName());
-    super.testCommandLine();
-  }
-
-  @Override
-  @Test
-  public void testCommandLine() {
-    // Do nothing
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputOnlyKeys.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputOnlyKeys.java
deleted file mode 100644
index ee2647ea3e4..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingOutputOnlyKeys.java
+++ /dev/null
@@ -1,51 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.IOException;
-
-import org.junit.Test;
-
-public class TestStreamingOutputOnlyKeys extends TestStreaming {
-
-  public TestStreamingOutputOnlyKeys() throws IOException {
-    super();
-  }
-  
-  @Test
-  public void testOutputOnlyKeys() throws Exception {
-    args.add("-jobconf"); args.add("stream.reduce.input" +
-        "=keyonlytext");
-    args.add("-jobconf"); args.add("stream.reduce.output" +
-        "=keyonlytext");
-    super.testCommandLine();
-  }
-  
-  @Override
-  public String getExpectedOutput() {
-    return outputExpect.replaceAll("\t", "");
-  }
-  
-  @Override
-  @Test
-  public void testCommandLine() {
-    // Do nothing
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingSeparator.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingSeparator.java
deleted file mode 100644
index f8167bbdd7a..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingSeparator.java
+++ /dev/null
@@ -1,122 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import java.io.*;
-import java.util.*;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-
-/**
- * This class tests hadoopStreaming with customized separator in MapReduce local mode.
- */
-public class TestStreamingSeparator
-{
-
-  // "map" command: grep -E (red|green|blue)
-  // reduce command: uniq
-  protected File INPUT_FILE = new File("TestStreamingSeparator.input.txt");
-  protected File OUTPUT_DIR = new File("TestStreamingSeparator.out");
-  protected String input = "roses1are.red\nviolets1are.blue\nbunnies1are.pink\n";
-  // mapreduce.input.keyvaluelinerecordreader.key.value.separator reads 1 as separator
-  // stream.map.input.field.separator uses 2 as separator
-  // map behaves like "/usr/bin/tr 2 3"; (translate 2 to 3)
-  protected String map = UtilTest.makeJavaCommand(TrApp.class, new String[]{"2", "3"});
-  // stream.map.output.field.separator recognize 3 as separator
-  // stream.reduce.input.field.separator recognize 3 as separator
-  // reduce behaves like "/usr/bin/tr 3 4"; (translate 3 to 4)
-  protected String reduce = UtilTest.makeJavaCommand(TrAppReduce.class, new String[]{"3", "4"});
-  // stream.reduce.output.field.separator recognize 4 as separator
-  // mapreduce.output.textoutputformat.separator outputs 5 as separator
-  protected String outputExpect = "bunnies5are.pink\nroses5are.red\nviolets5are.blue\n";
-
-  private StreamJob job;
-
-  public TestStreamingSeparator() throws IOException
-  {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected void createInput() throws IOException
-  {
-    DataOutputStream out = new DataOutputStream(
-                                                new FileOutputStream(INPUT_FILE.getAbsoluteFile()));
-    out.write(input.getBytes("UTF-8"));
-    out.close();
-  }
-
-  protected String[] genArgs() {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", map,
-      "-reducer", reduce,
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-      "-inputformat", "KeyValueTextInputFormat",
-      "-jobconf", "mapreduce.input.keyvaluelinerecordreader.key.value.separator=1",
-      "-jobconf", "stream.map.input.field.separator=2",
-      "-jobconf", "stream.map.output.field.separator=3",
-      "-jobconf", "stream.reduce.input.field.separator=3",
-      "-jobconf", "stream.reduce.output.field.separator=4",
-      "-jobconf", "mapreduce.output.textoutputformat.separator=5",
-    };
-  }
-  
-  @Test
-  public void testCommandLine() throws Exception
-  {
-    try {
-      try {
-        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-      } catch (Exception e) {
-      }
-
-      createInput();
-      boolean mayExit = false;
-
-      // During tests, the default Configuration will use a local mapred
-      // So don't specify -config or -cluster
-      job = new StreamJob(genArgs(), mayExit);      
-      job.go();
-      File outFile = new File(OUTPUT_DIR, "part-00000").getAbsoluteFile();
-      String output = StreamUtil.slurp(outFile);
-      outFile.delete();
-      System.err.println("outEx1=" + outputExpect);
-      System.err.println("  out1=" + output);
-      assertEquals(outputExpect, output);
-    } finally {
-      INPUT_FILE.delete();
-      FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    }
-  }
-
-  public static void main(String[]args) throws Exception
-  {
-    new TestStreamingSeparator().testCommandLine();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStatus.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStatus.java
deleted file mode 100644
index 5cdb0d4b85f..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStatus.java
+++ /dev/null
@@ -1,336 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.File;
-
-import org.apache.hadoop.mapred.MiniMRClientCluster;
-import org.apache.hadoop.mapred.MiniMRClientClusterFactory;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.Counters;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.TaskAttemptID;
-import org.apache.hadoop.mapred.TaskID;
-import org.apache.hadoop.mapred.TaskLog;
-import org.apache.hadoop.mapred.TaskReport;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.MapReduceTestUtil;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-
-
-/**
- * Tests if mapper/reducer with empty/nonempty input works properly if
- * reporting is done using lines like "reporter:status:" and
- * "reporter:counter:" before map()/reduce() method is called.
- * Validates the task's log of STDERR if messages are written to stderr before
- * map()/reduce() is called.
- * Also validates job output.
- * Uses MiniMR since the local jobtracker doesn't track task status. 
- */
-public class TestStreamingStatus {
-  protected static String TEST_ROOT_DIR =
-    new File(System.getProperty("test.build.data","/tmp"),
-    TestStreamingStatus.class.getSimpleName())
-    .toURI().toString().replace(' ', '+');
-  protected String INPUT_FILE = TEST_ROOT_DIR + "/input.txt";
-  protected String OUTPUT_DIR = TEST_ROOT_DIR + "/out";
-  protected String input = "roses.are.red\nviolets.are.blue\nbunnies.are.pink\n";
-  protected String map = null;
-  protected String reduce = null;
-
-  protected String scriptFile = TEST_ROOT_DIR + "/perlScript.pl";
-  protected String scriptFileName = new Path(scriptFile).toUri().getPath();
-
-
-  String expectedStderr = "my error msg before consuming input\n" +
-      "my error msg after consuming input\n";
-  String expectedOutput = null;// inited in setUp()
-  String expectedStatus = "before consuming input";
-
-  // This script does the following
-  // (a) setting task status before reading input
-  // (b) writing to stderr before reading input and after reading input
-  // (c) writing to stdout before reading input
-  // (d) incrementing user counter before reading input and after reading input
-  // Write lines to stdout before reading input{(c) above} is to validate
-  // the hanging task issue when input to task is empty(because of not starting
-  // output thread).
-  protected String script =
-    "#!/usr/bin/perl\n" +
-    "print STDERR \"reporter:status:" + expectedStatus + "\\n\";\n" +
-    "print STDERR \"reporter:counter:myOwnCounterGroup,myOwnCounter,1\\n\";\n" +
-    "print STDERR \"my error msg before consuming input\\n\";\n" +
-    "for($count = 1500; $count >= 1; $count--) {print STDOUT \"$count \";}" +
-    "while(<STDIN>) {chomp;}\n" +
-    "print STDERR \"my error msg after consuming input\\n\";\n" +
-    "print STDERR \"reporter:counter:myOwnCounterGroup,myOwnCounter,1\\n\";\n";
-
-  private MiniMRClientCluster mr;
-  FileSystem fs = null;
-  JobConf conf = null;
-
-  /**
-   * Start the cluster and create input file before running the actual test.
-   *
-   * @throws IOException
-   */
-  @Before
-  public void setUp() throws IOException {
-    conf = new JobConf();
-    conf.setBoolean(JTConfig.JT_RETIREJOBS, false);
-    conf.setBoolean(JTConfig.JT_PERSIST_JOBSTATUS, false);
-
-    mr = MiniMRClientClusterFactory.create(this.getClass(), 3, conf);
-
-    Path inFile = new Path(INPUT_FILE);
-    fs = inFile.getFileSystem(mr.getConfig());
-    clean(fs);
-
-    buildExpectedJobOutput();
-  }
-
-  /**
-   * Kill the cluster after the test is done.
-   */
-  @After
-  public void tearDown() throws IOException {
-    if (fs != null) {
-      clean(fs);
-    }
-    if (mr != null) {
-      mr.stop();
-    }
-  }
-
-  // Updates expectedOutput to have the expected job output as a string
-  void buildExpectedJobOutput() {
-    if (expectedOutput == null) {
-      expectedOutput = "";
-      for(int i = 1500; i >= 1; i--) {
-        expectedOutput = expectedOutput.concat(Integer.toString(i) + " ");
-      }
-      expectedOutput = expectedOutput.trim();
-    }
-  }
-
-  // Create empty/nonempty input file.
-  // Create script file with the specified content.
-  protected void createInputAndScript(boolean isEmptyInput,
-      String script) throws IOException {
-    makeInput(fs, isEmptyInput ? "" : input);
-
-    // create script file
-    DataOutputStream file = fs.create(new Path(scriptFileName));
-    file.writeBytes(script);
-    file.close();
-  }
-
-  protected String[] genArgs(String jobtracker, String rmAddress,
-                             String mapper, String reducer)
-  {
-    return new String[] {
-        "-input", INPUT_FILE,
-        "-output", OUTPUT_DIR,
-        "-mapper", mapper,
-        "-reducer", reducer,
-        "-jobconf", MRJobConfig.NUM_MAPS + "=1",
-        "-jobconf", MRJobConfig.NUM_REDUCES + "=1",
-        "-jobconf", MRJobConfig.PRESERVE_FAILED_TASK_FILES + "=true",
-        "-jobconf", YarnConfiguration.RM_ADDRESS + "=" + rmAddress,
-        "-jobconf", "stream.tmpdir=" +
-        new Path(TEST_ROOT_DIR).toUri().getPath(),
-        "-jobconf", JTConfig.JT_IPC_ADDRESS + "="+jobtracker,
-        "-jobconf", "fs.default.name=file:///",
-        "-jobconf", "mapred.jar=" + TestStreaming.STREAMING_JAR,
-        "-jobconf", "mapreduce.framework.name=yarn"
-    };
-  }
-
-  // create input file with the given content
-  public void makeInput(FileSystem fs, String input) throws IOException {
-    Path inFile = new Path(INPUT_FILE);
-    DataOutputStream file = fs.create(inFile);
-    file.writeBytes(input);
-    file.close();
-  }
-
-  // Delete output directory
-  protected void deleteOutDir(FileSystem fs) {
-    try {
-      Path outDir = new Path(OUTPUT_DIR);
-      fs.delete(outDir, true);
-    } catch (Exception e) {}
-  }
-
-  // Delete input file, script file and output directory
-  public void clean(FileSystem fs) {
-    deleteOutDir(fs);
-    try {
-      Path file = new Path(INPUT_FILE);
-      if (fs.exists(file)) {
-        fs.delete(file, false);
-      }
-      file = new Path(scriptFile);
-      if (fs.exists(file)) {
-        fs.delete(file, false);
-      }
-    } catch (Exception e) {
-      e.printStackTrace();
-    }
-  }
-
-  /**
-   * Check if mapper/reducer with empty/nonempty input works properly if
-   * reporting is done using lines like "reporter:status:" and
-   * "reporter:counter:" before map()/reduce() method is called.
-   * Validate the task's log of STDERR if messages are written
-   * to stderr before map()/reduce() is called.
-   * Also validate job output.
-   *
-   * @throws IOException
-   */
-  @Test
-  public void testReporting() throws Exception {
-    testStreamJob(false);// nonempty input
-    testStreamJob(true);// empty input
-  }
-
-  /**
-   * Run a streaming job with the given script as mapper and validate.
-   * Run another streaming job with the given script as reducer and validate.
-   *
-   * @param isEmptyInput Should the input to the script be empty ?
-   */
-  private void testStreamJob(boolean isEmptyInput)
-      throws Exception {
-
-      createInputAndScript(isEmptyInput, script);
-
-      // Check if streaming mapper works as expected
-      map = scriptFileName;
-      reduce = "/bin/cat";
-      runStreamJob(TaskType.MAP, isEmptyInput);
-      deleteOutDir(fs);
-
-      // Check if streaming reducer works as expected.
-      map = "/bin/cat";
-      reduce = scriptFileName;
-      runStreamJob(TaskType.REDUCE, isEmptyInput);
-      clean(fs);
-  }
-
-  // Run streaming job for the specified input file, mapper and reducer and
-  // (1) Validate if the job succeeds.
-  // (2) Validate if user counter is incremented properly for the cases of
-  //   (a) nonempty input to map
-  //   (b) empty input to map and
-  //   (c) nonempty input to reduce
-  // (3) Validate task status for the cases of (2)(a),(2)(b),(2)(c).
-  //     Because empty input to reduce task => reporter is dummy and ignores
-  //     all "reporter:status" and "reporter:counter" lines. 
-  // (4) Validate stderr of task of given task type.
-  // (5) Validate job output
-  private void runStreamJob(TaskType type, boolean isEmptyInput)
-      throws Exception {
-    StreamJob job = new StreamJob();
-    int returnValue = job.run(genArgs(
-        mr.getConfig().get(JTConfig.JT_IPC_ADDRESS),
-        mr.getConfig().get(YarnConfiguration.RM_ADDRESS), map, reduce));
-    assertEquals(0, returnValue);
-
-    // If input to reducer is empty, dummy reporter(which ignores all
-    // reporting lines) is set for MRErrorThread in waitOutputThreads(). So
-    // expectedCounterValue is 0 for empty-input-to-reducer case.
-    // Output of reducer is also empty for empty-input-to-reducer case.
-    int expectedCounterValue = 0;
-    if (type == TaskType.MAP || !isEmptyInput) {
-      validateTaskStatus(job, type);
-      // output is from "print STDOUT" statements in perl script
-      validateJobOutput(job.getConf());
-      expectedCounterValue = 2;
-    }
-    validateUserCounter(job, expectedCounterValue);
-    validateTaskStderr(job, type);
-
-    deleteOutDir(fs);
-  }
-
-  // validate task status of task of given type(validates 1st task of that type)
-  void validateTaskStatus(StreamJob job, TaskType type) throws IOException {
-    // Map Task has 2 phases: map, sort
-    // Reduce Task has 3 phases: copy, sort, reduce
-    String finalPhaseInTask;
-    TaskReport[] reports;
-    if (type == TaskType.MAP) {
-      reports = job.jc_.getMapTaskReports(job.jobId_);
-      finalPhaseInTask = "sort";
-    } else {// reduce task
-      reports = job.jc_.getReduceTaskReports(job.jobId_);
-      finalPhaseInTask = "reduce";
-    }
-    assertEquals(1, reports.length);
-    assertEquals(expectedStatus + " > " + finalPhaseInTask,
-        reports[0].getState());
-  }
-
-  // Validate the job output
-  void validateJobOutput(Configuration conf)
-      throws IOException {
-
-    String output = MapReduceTestUtil.readOutput(
-        new Path(OUTPUT_DIR), conf).trim();
-
-    assertTrue(output.equals(expectedOutput));
-  }
-
-  // Validate stderr task log of given task type(validates 1st
-  // task of that type).
-  void validateTaskStderr(StreamJob job, TaskType type)
-      throws IOException {
-    TaskAttemptID attemptId =
-        new TaskAttemptID(new TaskID(job.jobId_, type, 0), 0);
-
-    String log = MapReduceTestUtil.readTaskLog(TaskLog.LogName.STDERR,
-        attemptId, false);
-
-    // trim() is called on expectedStderr here because the method
-    // MapReduceTestUtil.readTaskLog() returns trimmed String.
-    assertTrue(log.equals(expectedStderr.trim()));
-  }
-
-  // Validate if user counter is incremented properly
-  void validateUserCounter(StreamJob job, int expectedCounterValue)
-      throws IOException {
-    Counters counters = job.running_.getCounters();
-    assertEquals(expectedCounterValue, counters.findCounter(
-        "myOwnCounterGroup", "myOwnCounter").getValue());
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStderr.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStderr.java
deleted file mode 100644
index b80777968ef..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestStreamingStderr.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.util.*;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-/**
- * Test that streaming consumes stderr from the streaming process
- * (before, during, and after the main processing of mapred input),
- * and that stderr messages count as task progress.
- */
-public class TestStreamingStderr
-{
-  public TestStreamingStderr() throws IOException {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected String[] genArgs(File input, File output, int preLines, int duringLines, int postLines) {
-    return new String[] {
-      "-input", input.getAbsolutePath(),
-      "-output", output.getAbsolutePath(),
-      "-mapper", UtilTest.makeJavaCommand(StderrApp.class,
-                                            new String[]{Integer.toString(preLines),
-                                                         Integer.toString(duringLines),
-                                                         Integer.toString(postLines)}),
-      "-reducer", StreamJob.REDUCE_NONE,
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "mapreduce.task.timeout=5000",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp")
-    };
-  }
-
-  protected File setupInput(String base, boolean hasInput) throws IOException {
-    File input = new File(base + "-input.txt");
-    UtilTest.recursiveDelete(input);
-    FileOutputStream in = new FileOutputStream(input.getAbsoluteFile());
-    if (hasInput) {
-      in.write("hello\n".getBytes());      
-    }
-    in.close();
-    return input;
-  }
-  
-  protected File setupOutput(String base) throws IOException {
-    File output = new File(base + "-out");
-    UtilTest.recursiveDelete(output);
-    return output;
-  }
-
-  public void runStreamJob(String baseName, boolean hasInput,
-                           int preLines, int duringLines, int postLines)
-    throws Exception {
-    File input = setupInput(baseName, hasInput);
-    File output = setupOutput(baseName);
-    boolean mayExit = false;
-    int returnStatus = 0;
-
-    StreamJob job = new StreamJob(genArgs(input, output, preLines, duringLines, postLines), mayExit);
-    returnStatus = job.go();
-    assertEquals("StreamJob success", 0, returnStatus);
-  }
-
-  // This test will fail by blocking forever if the stderr isn't
-  // consumed by Hadoop for tasks that don't have any input.
-  @Test
-  public void testStderrNoInput() throws Exception {
-    runStreamJob("target/stderr-pre", false, 10000, 0, 0);
-  }
-
-  // Streaming should continue to read stderr even after all input has
-  // been consumed.
-  @Test
-  public void testStderrAfterOutput() throws Exception {
-    runStreamJob("target/stderr-post", false, 0, 0, 10000);
-  }
-
-  // This test should produce a task timeout if stderr lines aren't
-  // counted as progress. This won't actually work until
-  // LocalJobRunner supports timeouts.
-  @Test
-  public void testStderrCountsAsProgress() throws Exception {
-    runStreamJob("target/stderr-progress", true, 10, 1000, 0);
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestSymLink.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestSymLink.java
deleted file mode 100644
index 7994952578f..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestSymLink.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.BufferedReader;
-import java.io.DataOutputStream;
-import java.io.InputStreamReader;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Map;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.MiniMRCluster;
-import org.apache.hadoop.mapred.Utils;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-/**
- * This test case tests the symlink creation
- * utility provided by distributed caching 
- */
-public class TestSymLink
-{
-  String INPUT_FILE = "/testing-streaming/input.txt";
-  String OUTPUT_DIR = "/testing-streaming/out";
-  String CACHE_FILE = "/testing-streaming/cache.txt";
-  String input = "check to see if we can read this none reduce";
-  String map = TestStreaming.XARGS_CAT;
-  String reduce = TestStreaming.CAT;
-  String mapString = "testlink\n";
-  String cacheString = "This is just the cache string";
-  StreamJob job;
-
-  @Test (timeout = 120000)
-  public void testSymLink() throws Exception
-  {
-    boolean mayExit = false;
-    MiniMRCluster mr = null;
-    MiniDFSCluster dfs = null; 
-    try {
-      Configuration conf = new Configuration();
-      dfs = new MiniDFSCluster.Builder(conf).build();
-      FileSystem fileSys = dfs.getFileSystem();
-      String namenode = fileSys.getUri().toString();
-      mr  = new MiniMRCluster(1, namenode, 3);
-
-      List<String> args = new ArrayList<String>();
-      for (Map.Entry<String, String> entry : mr.createJobConf()) {
-        args.add("-jobconf");
-        args.add(entry.getKey() + "=" + entry.getValue());
-      }
-
-      // During tests, the default Configuration will use a local mapred
-      // So don't specify -config or -cluster
-      String argv[] = new String[] {
-        "-input", INPUT_FILE,
-        "-output", OUTPUT_DIR,
-        "-mapper", map,
-        "-reducer", reduce,
-        "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-        "-jobconf", 
-          JobConf.MAPRED_MAP_TASK_JAVA_OPTS+ "=" +
-            "-Dcontrib.name=" + System.getProperty("contrib.name") + " " +
-            "-Dbuild.test=" + System.getProperty("build.test") + " " +
-            conf.get(JobConf.MAPRED_MAP_TASK_JAVA_OPTS, 
-                     conf.get(JobConf.MAPRED_TASK_JAVA_OPTS, "")),
-        "-jobconf", 
-          JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS+ "=" +
-            "-Dcontrib.name=" + System.getProperty("contrib.name") + " " +
-            "-Dbuild.test=" + System.getProperty("build.test") + " " +
-            conf.get(JobConf.MAPRED_REDUCE_TASK_JAVA_OPTS, 
-                     conf.get(JobConf.MAPRED_TASK_JAVA_OPTS, "")),
-        "-cacheFile", fileSys.getUri() + CACHE_FILE + "#testlink",
-        "-jobconf", "mapred.jar=" + TestStreaming.STREAMING_JAR,
-      };
-
-      for (String arg : argv) {
-        args.add(arg);
-      }
-      argv = args.toArray(new String[args.size()]);
-  
-      fileSys.delete(new Path(OUTPUT_DIR), true);
-      
-      DataOutputStream file = fileSys.create(new Path(INPUT_FILE));
-      file.writeBytes(mapString);
-      file.close();
-      file = fileSys.create(new Path(CACHE_FILE));
-      file.writeBytes(cacheString);
-      file.close();
-        
-      job = new StreamJob(argv, mayExit);      
-      job.go();
-
-      fileSys = dfs.getFileSystem();
-      String line = null;
-      Path[] fileList = FileUtil.stat2Paths(fileSys.listStatus(
-                                              new Path(OUTPUT_DIR),
-                                              new Utils.OutputFileUtils
-                                                       .OutputFilesFilter()));
-      for (int i = 0; i < fileList.length; i++){
-        System.out.println(fileList[i].toString());
-        BufferedReader bread =
-          new BufferedReader(new InputStreamReader(fileSys.open(fileList[i])));
-        line = bread.readLine();
-        System.out.println(line);
-      }
-      assertEquals(cacheString + "\t", line);
-    } finally{
-      if (dfs != null) { dfs.shutdown(); }
-      if (mr != null) { mr.shutdown();}
-    }
-    
-  }
-
-  public static void main(String[]args) throws Exception
-  {
-    new TestStreaming().testCommandLine();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestTypedBytesStreaming.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestTypedBytesStreaming.java
deleted file mode 100644
index 05a050cac83..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestTypedBytesStreaming.java
+++ /dev/null
@@ -1,91 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileUtil;
-
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestTypedBytesStreaming {
-
-  protected File INPUT_FILE = new File("target/input.txt");
-  protected File OUTPUT_DIR = new File("target/out");
-  protected String input = "roses.are.red\nviolets.are.blue\nbunnies.are.pink\n";
-  protected String map = UtilTest.makeJavaCommand(TypedBytesMapApp.class, new String[]{"."});
-  protected String reduce = UtilTest.makeJavaCommand(TypedBytesReduceApp.class, new String[0]);
-  protected String outputExpect = "are\t3\nred\t1\nblue\t1\npink\t1\nroses\t1\nbunnies\t1\nviolets\t1\n";
-  
-  public TestTypedBytesStreaming() throws IOException {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected void createInput() throws IOException {
-    DataOutputStream out = new DataOutputStream(new FileOutputStream(INPUT_FILE.getAbsoluteFile()));
-    out.write(input.getBytes("UTF-8"));
-    out.close();
-  }
-
-  protected String[] genArgs() {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", map,
-      "-reducer", reduce,
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp"),
-      "-io", "typedbytes"
-    };
-  }
-
-  @Before
-  @After
-  public void cleanupOutput() throws Exception {
-    FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    INPUT_FILE.delete();
-    createInput();
-  }
-
-  @Test
-  public void testCommandLine() throws Exception {
-    // During tests, the default Configuration will use a local mapred
-    // So don't specify -config or -cluster
-    StreamJob job = new StreamJob();
-    job.setConf(new Configuration());
-    job.run(genArgs());
-    File outFile = new File(OUTPUT_DIR, "part-00000").getAbsoluteFile();
-    String output = StreamUtil.slurp(outFile);
-    outFile.delete();
-    System.out.println("   map=" + map);
-    System.out.println("reduce=" + reduce);
-    System.err.println("outEx1=" + outputExpect);
-    System.err.println("  out1=" + output);
-    assertEquals(outputExpect, output);
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestUnconsumedInput.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestUnconsumedInput.java
deleted file mode 100644
index b2bc84b4f90..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TestUnconsumedInput.java
+++ /dev/null
@@ -1,104 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import static org.junit.Assert.*;
-
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileUtil;
-import org.junit.Test;
-
-public class TestUnconsumedInput {
-  protected final int EXPECTED_OUTPUT_SIZE = 10000;
-  protected File INPUT_FILE = new File("stream_uncinput_input.txt");
-  protected File OUTPUT_DIR = new File("stream_uncinput_out");
-  // map parses input lines and generates count entries for each word.
-  protected String input = "roses.are.red\nviolets.are.blue\nbunnies.are.pink\n";
-  protected String map = UtilTest.makeJavaCommand(OutputOnlyApp.class,
-      new String[]{Integer.toString(EXPECTED_OUTPUT_SIZE)});
-
-  private StreamJob job;
-
-  public TestUnconsumedInput() throws IOException
-  {
-    UtilTest utilTest = new UtilTest(getClass().getName());
-    utilTest.checkUserDir();
-    utilTest.redirectIfAntJunit();
-  }
-
-  protected void createInput() throws IOException
-  {
-    try (DataOutputStream out = new DataOutputStream(
-            new FileOutputStream(INPUT_FILE.getAbsoluteFile()))) {
-      for (int i=0; i<10000; ++i) {
-        out.write(input.getBytes(StandardCharsets.UTF_8));
-      }
-    }
-  }
-
-  protected String[] genArgs() {
-    return new String[] {
-      "-input", INPUT_FILE.getAbsolutePath(),
-      "-output", OUTPUT_DIR.getAbsolutePath(),
-      "-mapper", map,
-      "-reducer", "org.apache.hadoop.mapred.lib.IdentityReducer",
-      "-numReduceTasks", "0",
-      "-jobconf", "mapreduce.task.files.preserve.failedtasks=true",
-      "-jobconf", "stream.tmpdir="+System.getProperty("test.build.data","/tmp")
-    };
-  }
-
-  @Test
-  public void testUnconsumedInput() throws Exception
-  {
-    String outFileName = "part-00000";
-    File outFile = null;
-    try {
-      try {
-        FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-      } catch (Exception e) {
-      }
-
-      createInput();
-
-      // setup config to ignore unconsumed input
-      Configuration conf = new Configuration();
-      conf.set("stream.minRecWrittenToEnableSkip_", "0");
-
-      job = new StreamJob();
-      job.setConf(conf);
-      int exitCode = job.run(genArgs());
-      assertEquals("Job failed", 0, exitCode);
-      outFile = new File(OUTPUT_DIR, outFileName).getAbsoluteFile();
-      String output = StreamUtil.slurp(outFile);
-      assertEquals("Output was truncated", EXPECTED_OUTPUT_SIZE,
-          StringUtils.countMatches(output, "\t"));
-    } finally {
-      INPUT_FILE.delete();
-      FileUtil.fullyDelete(OUTPUT_DIR.getAbsoluteFile());
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrApp.java
deleted file mode 100644
index d0583320e70..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrApp.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-
-import org.apache.hadoop.streaming.Environment;
-
-/** A minimal Java implementation of /usr/bin/tr.
- *  Used to test the usage of external applications without adding
- *  platform-specific dependencies.
- *  Use TrApp as mapper only. For reducer, use TrAppReduce.
- */
-public class TrApp
-{
-
-  public TrApp(char find, char replace)
-  {
-    this.find = find;
-    this.replace = replace;
-  }
-
-  void testParentJobConfToEnvVars() throws IOException
-  {
-    env = new Environment();
-    // test that some JobConf properties are exposed as expected     
-    // Note the dots translated to underscore: 
-    // property names have been escaped in PipeMapRed.safeEnvVarName()
-    expectDefined("mapreduce_cluster_local_dir");
-    expect("mapreduce_map_output_key_class", "org.apache.hadoop.io.Text");
-    expect("mapreduce_map_output_value_class", "org.apache.hadoop.io.Text");
-
-    expect("mapreduce_task_ismap", "true");
-    expectDefined("mapreduce_task_attempt_id");
-
-    expectDefined("mapreduce_map_input_file");
-    expectDefined("mapreduce_map_input_length");
-
-    expectDefined("mapreduce_task_io_sort_factor");
-
-    // the FileSplit context properties are not available in local hadoop..
-    // so can't check them in this test.
-
-    // verify some deprecated properties appear for older stream jobs
-    expect("map_input_file", env.getProperty("mapreduce_map_input_file"));
-    expect("map_input_length", env.getProperty("mapreduce_map_input_length"));
-  }
-
-  // this runs in a subprocess; won't use JUnit's assertTrue()    
-  void expect(String evName, String evVal) throws IOException
-  {
-    String got = env.getProperty(evName);
-    if (!evVal.equals(got)) {
-      String msg = "FAIL evName=" + evName + " got=" + got + " expect=" + evVal;
-      throw new IOException(msg);
-    }
-  }
-
-  void expectDefined(String evName) throws IOException
-  {
-    String got = env.getProperty(evName);
-    if (got == null) {
-      String msg = "FAIL evName=" + evName + " is undefined. Expect defined.";
-      throw new IOException(msg);
-    }
-  }
-
-  public void go() throws IOException
-  {
-    testParentJobConfToEnvVars();
-    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-    String line;
-
-    while ((line = in.readLine()) != null) {
-      String out = line.replace(find, replace);
-      System.out.println(out);
-      System.err.println("reporter:counter:UserCounters,InputLines,1");
-    }
-  }
-
-  public static void main(String[] args) throws IOException
-  {
-    args[0] = CUnescape(args[0]);
-    args[1] = CUnescape(args[1]);
-    TrApp app = new TrApp(args[0].charAt(0), args[1].charAt(0));
-    app.go();
-  }
-
-  public static String CUnescape(String s)
-  {
-    if (s.equals("\\n")) {
-      return "\n";
-    } else {
-      return s;
-    }
-  }
-  char find;
-  char replace;
-  Environment env;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrAppReduce.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrAppReduce.java
deleted file mode 100644
index e373f07d0cd..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TrAppReduce.java
+++ /dev/null
@@ -1,112 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-
-import org.apache.hadoop.streaming.Environment;
-
-/** A minimal Java implementation of /usr/bin/tr.
- *  Used to test the usage of external applications without adding
- *  platform-specific dependencies.
- *  Use TrAppReduce as reducer only. For mapper, use TrApp.
- */
-public class TrAppReduce
-{
-
-  public TrAppReduce(char find, char replace)
-  {
-    this.find = find;
-    this.replace = replace;
-  }
-
-  void testParentJobConfToEnvVars() throws IOException
-  {
-    env = new Environment();
-    // test that some JobConf properties are exposed as expected     
-    // Note the dots translated to underscore: 
-    // property names have been escaped in PipeMapRed.safeEnvVarName()
-    expect("mapreduce_jobtracker_address", "local");
-    //expect("mapred_local_dir", "build/test/mapred/local");
-    expectDefined("mapreduce_cluster_local_dir");
-    expect("mapred_output_format_class", "org.apache.hadoop.mapred.TextOutputFormat");
-    expect("mapreduce_job_output_key_class", "org.apache.hadoop.io.Text");
-    expect("mapreduce_job_output_value_class", "org.apache.hadoop.io.Text");
-
-    expect("mapreduce_task_ismap", "false");
-    expectDefined("mapreduce_task_attempt_id");
-
-    expectDefined("mapreduce_task_io_sort_factor");
-
-    // the FileSplit context properties are not available in local hadoop..
-    // so can't check them in this test.
-
-  }
-
-  // this runs in a subprocess; won't use JUnit's assertTrue()    
-  void expect(String evName, String evVal) throws IOException
-  {
-    String got = env.getProperty(evName);
-    if (!evVal.equals(got)) {
-      String msg = "FAIL evName=" + evName + " got=" + got + " expect=" + evVal;
-      throw new IOException(msg);
-    }
-  }
-
-  void expectDefined(String evName) throws IOException
-  {
-    String got = env.getProperty(evName);
-    if (got == null) {
-      String msg = "FAIL evName=" + evName + " is undefined. Expect defined.";
-      throw new IOException(msg);
-    }
-  }
-
-  public void go() throws IOException
-  {
-    testParentJobConfToEnvVars();
-    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-    String line;
-
-    while ((line = in.readLine()) != null) {
-      String out = line.replace(find, replace);
-      System.out.println(out);
-    }
-  }
-
-  public static void main(String[] args) throws IOException
-  {
-    args[0] = CUnescape(args[0]);
-    args[1] = CUnescape(args[1]);
-    TrAppReduce app = new TrAppReduce(args[0].charAt(0), args[1].charAt(0));
-    app.go();
-  }
-
-  public static String CUnescape(String s)
-  {
-    if (s.equals("\\n")) {
-      return "\n";
-    } else {
-      return s;
-    }
-  }
-  char find;
-  char replace;
-  Environment env;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TypedBytesMapApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TypedBytesMapApp.java
deleted file mode 100644
index 6dd268c2c58..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TypedBytesMapApp.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.typedbytes.TypedBytesInput;
-import org.apache.hadoop.typedbytes.TypedBytesOutput;
-
-public class TypedBytesMapApp {
-
-  private String find;
-
-  public TypedBytesMapApp(String find) {
-    this.find = find;
-  }
-
-  public void go() throws IOException {
-    TypedBytesInput tbinput = new TypedBytesInput(new DataInputStream(System.in));
-    TypedBytesOutput tboutput = new TypedBytesOutput(new DataOutputStream(System.out));
-
-    Object key = tbinput.readRaw();
-    while (key != null) {
-      Object value = tbinput.read();
-      for (String part : value.toString().split(find)) {
-        tboutput.write(part);  // write key
-        tboutput.write(1);     // write value
-      }
-      System.err.println("reporter:counter:UserCounters,InputLines,1");
-      key = tbinput.readRaw();
-    }
-    
-    System.out.flush();
-  }
-  
-  public static void main(String[] args) throws IOException {
-    TypedBytesMapApp app = new TypedBytesMapApp(args[0].replace(".","\\."));
-    app.go();
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TypedBytesReduceApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TypedBytesReduceApp.java
deleted file mode 100644
index 4ea0caa5d13..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/TypedBytesReduceApp.java
+++ /dev/null
@@ -1,58 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.hadoop.typedbytes.TypedBytesInput;
-import org.apache.hadoop.typedbytes.TypedBytesOutput;
-
-public class TypedBytesReduceApp {
-
-  public void go() throws IOException {
-    TypedBytesInput tbinput = new TypedBytesInput(new DataInputStream(System.in));
-    TypedBytesOutput tboutput = new TypedBytesOutput(new DataOutputStream(System.out));
-    
-    Object prevKey = null;
-    int sum = 0;
-    Object key = tbinput.read();
-    while (key != null) {
-      if (prevKey != null && !key.equals(prevKey)) {
-        tboutput.write(prevKey);  // write key
-        tboutput.write(sum);      // write value
-        sum = 0;
-      }
-      sum += (Integer) tbinput.read();
-      prevKey = key;
-      key = tbinput.read();
-    }
-    tboutput.write(prevKey);
-    tboutput.write(sum);
-    
-    System.out.flush();
-  }
-
-  public static void main(String[] args) throws IOException {
-    TypedBytesReduceApp app = new TypedBytesReduceApp();
-    app.go();
-  }
-  
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UniqApp.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UniqApp.java
deleted file mode 100644
index 30188932c59..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UniqApp.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.util.Date;
-
-/** A minimal Java implementation of /usr/bin/uniq
-    Used to test the usage of external applications without adding
-    platform-specific dependencies.
-    Uniques lines and prepends a header on the line.
- */
-public class UniqApp
-{
-
-  public UniqApp(String header)
-  {
-    this.header = header;
-  }
-  public void go() throws IOException
-  {
-    BufferedReader in = new BufferedReader(new InputStreamReader(System.in));
-    String line;
-    String prevLine = null;
-    while ((line = in.readLine()) != null) {
-      if (!line.equals(prevLine)) {
-        System.out.println(header + line);
-      }
-      prevLine = line;
-    }
-  }
-
-  public static void main(String[] args) throws IOException
-  {
-    String h = (args.length < 1) ? "" : args[0];
-    UniqApp app = new UniqApp(h);
-    app.go();
-  }
-
-  String header;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UtilTest.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UtilTest.java
deleted file mode 100644
index 2378c7b4149..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/UtilTest.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.ArrayList;
-import java.util.Iterator;
-import java.util.List;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.util.Shell.ShellCommandExecutor;
-
-class UtilTest {
-
-  private static final Logger LOG = LoggerFactory.getLogger(UtilTest.class);
-
-  /**
-   * Utility routine to recurisvely delete a directory.
-   * On normal return, the file does not exist.
-   *
-   * @param file File or directory to delete.
-   *
-   * @throws RuntimeException if the file, or some file within
-   * it, could not be deleted.
-   */
-  static void recursiveDelete(File file) {
-    file = file.getAbsoluteFile();
-
-    if (!file.exists()) return;
-    
-    if (file.isDirectory()) {
-      for (File child : file.listFiles()) {
-	recursiveDelete(child);
-      }
-    }
-    if (!file.delete()) {
-      throw new RuntimeException("Failed to delete " + file);
-    }
-  }
-  
-  public UtilTest(String testName) {
-    testName_ = testName;
-    userDir_ = System.getProperty("user.dir");
-    antTestDir_ = System.getProperty("test.build.data", userDir_);
-    System.out.println("test.build.data-or-user.dir=" + antTestDir_);
-  }
-
-  void checkUserDir() {
-//    // trunk/src/contrib/streaming --> trunk/build/contrib/streaming/test/data
-//    if (!userDir_.equals(antTestDir_)) {
-//      // because changes to user.dir are ignored by File static methods.
-//      throw new IllegalStateException("user.dir != test.build.data. The junit Ant task must be forked.");
-//    }
-  }
-
-  void redirectIfAntJunit() throws IOException
-  {
-    boolean fromAntJunit = System.getProperty("test.build.data") != null;
-    if (fromAntJunit) {
-      new File(antTestDir_).mkdirs();
-      File outFile = new File(antTestDir_, testName_+".log");
-      PrintStream out = new PrintStream(new FileOutputStream(outFile));
-      System.setOut(out);
-      System.setErr(out);
-    }
-  }
-
-  public static String collate(List<String> args, String sep) {
-    StringBuffer buf = new StringBuffer();
-    Iterator<String> it = args.iterator();
-    while (it.hasNext()) {
-      if (buf.length() > 0) {
-        buf.append(" ");
-      }
-      buf.append(it.next());
-    }
-    return buf.toString();
-  }
-
-  public static String makeJavaCommand(Class<?> main, String[] argv) {
-    ArrayList<String> vargs = new ArrayList<String>();
-    File javaHomeBin = new File(System.getProperty("java.home"), "bin");
-    File jvm = new File(javaHomeBin, "java");
-    vargs.add(jvm.toString());
-    // copy parent classpath
-    vargs.add("-classpath");
-    vargs.add("\"" + System.getProperty("java.class.path") + "\"");
-  
-    // add heap-size limit
-    vargs.add("-Xmx" + Runtime.getRuntime().maxMemory());
-  
-    // Add main class and its arguments
-    vargs.add(main.getName());
-    for (int i = 0; i < argv.length; i++) {
-      vargs.add(argv[i]);
-    }
-    return collate(vargs, " ");
-  }
-
-  /**
-   * Is perl supported on this machine ?
-   * @return true if perl is available and is working as expected
-   */
-  public static boolean hasPerlSupport() {
-    boolean hasPerl = false;
-    ShellCommandExecutor shexec = new ShellCommandExecutor(
-      new String[] { "perl", "-e", "print 42" });
-    try {
-      shexec.execute();
-      if (shexec.getOutput().equals("42")) {
-        hasPerl = true;
-      }
-      else {
-        LOG.warn("Perl is installed, but isn't behaving as expected.");
-      }
-    } catch (Exception e) {
-      LOG.warn("Could not run perl: " + e);
-    }
-    return hasPerl;
-  }
-
-  private String userDir_;
-  private String antTestDir_;
-  private String testName_;
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/ValueCountReduce.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/ValueCountReduce.java
deleted file mode 100644
index 025b6d999a6..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/ValueCountReduce.java
+++ /dev/null
@@ -1,64 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming;
-
-import java.io.*;
-import java.util.Date;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.OutputCollector;
-import org.apache.hadoop.mapred.Reducer;
-import org.apache.hadoop.mapred.Reporter;
-
-public class ValueCountReduce implements Reducer {
-
-  /**
-   * @param args
-   */
-  public static void main(String[] args) {
-    // TODO Auto-generated method stub
-
-  }
-
-  public void reduce(Object arg0, Iterator arg1, OutputCollector arg2, Reporter arg3) throws IOException {
-    int count = 0;
-    while (arg1.hasNext()) {
-      count += 1;
-      arg1.next();
-    }
-    arg2.collect(arg0, new Text("" + count));
-  }
-
-  public void configure(JobConf arg0) {
-    // TODO Auto-generated method stub
-    
-  }
-
-  public void close() throws IOException {
-    // TODO Auto-generated method stub
-    
-  }
-
-}
-
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/io/TestKeyOnlyTextOutputReader.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/io/TestKeyOnlyTextOutputReader.java
deleted file mode 100644
index d606fe106c2..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/io/TestKeyOnlyTextOutputReader.java
+++ /dev/null
@@ -1,68 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.io;
-
-import java.io.ByteArrayInputStream;
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.IOException;
-
-import org.junit.Assert;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.streaming.PipeMapRed;
-import org.apache.hadoop.streaming.PipeMapper;
-import org.junit.Test;
-
-public class TestKeyOnlyTextOutputReader {
-  @Test
-  public void testKeyOnlyTextOutputReader() throws IOException {
-    String text = "key,value\nkey2,value2\nnocomma\n";
-    PipeMapRed pipeMapRed = new MyPipeMapRed(text);
-    KeyOnlyTextOutputReader outputReader = new KeyOnlyTextOutputReader();
-    outputReader.initialize(pipeMapRed);
-    outputReader.readKeyValue();
-    Assert.assertEquals(new Text("key,value"), outputReader.getCurrentKey());
-    outputReader.readKeyValue();
-    Assert.assertEquals(new Text("key2,value2"), outputReader.getCurrentKey());
-    outputReader.readKeyValue();
-    Assert.assertEquals(new Text("nocomma"), outputReader.getCurrentKey());
-    Assert.assertEquals(false, outputReader.readKeyValue());
-  }
-  
-  private class MyPipeMapRed extends PipeMapper {
-    private DataInput clientIn;
-    private Configuration conf = new Configuration();
-    
-    public MyPipeMapRed(String text) {
-      clientIn = new DataInputStream(new ByteArrayInputStream(text.getBytes()));
-    }
-    
-    @Override
-    public DataInput getClientInput() {
-      return clientIn;
-    }
-    
-    @Override
-    public Configuration getConfiguration() {
-      return conf;
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/mapreduce/TestStreamXmlRecordReader.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/mapreduce/TestStreamXmlRecordReader.java
deleted file mode 100644
index 5bf2fe52d44..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/streaming/mapreduce/TestStreamXmlRecordReader.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.streaming.mapreduce;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.Set;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.junit.After;
-import org.junit.Before;
-import org.junit.Test;
-
-/**
- * This class tests StreamXmlRecordReader The test creates an XML file, uses
- * StreamXmlRecordReader and compares the expected output against the generated
- * output
- */
-public class TestStreamXmlRecordReader {
-
-  private File INPUT_FILE;
-  private String input;
-  private String outputExpect;
-  Path OUTPUT_DIR;
-  FileSystem fs;
-
-  public TestStreamXmlRecordReader() throws IOException {
-    INPUT_FILE = new File("target/input.xml");
-    input = "<xmltag>\t\nroses.are.red\t\nviolets.are.blue\t\n"
-        + "bunnies.are.pink\t\n</xmltag>\t\n";
-    outputExpect = input;
-  }
-
-  protected void assertOutput(String expectedOutput, String output)
-      throws IOException {
-    String[] words = expectedOutput.split("\t\n");
-    Set<String> expectedWords = new HashSet<String>(Arrays.asList(words));
-    words = output.split("\t\n");
-    Set<String> returnedWords = new HashSet<String>(Arrays.asList(words));
-    assertTrue(returnedWords.containsAll(expectedWords));
-  }
-
-  protected void checkOutput() throws IOException {
-    File outFile = new File(OUTPUT_DIR.toString());
-    Path outPath = new Path(outFile.getAbsolutePath(), "part-r-00000");
-    String output = slurpHadoop(outPath, fs);
-    fs.delete(outPath, true);
-    outputExpect = "<PATTERN>\n" + outputExpect + "</PATTERN>";
-    System.err.println("outEx1=" + outputExpect);
-    System.err.println("  out1=" + output);
-    assertOutput(outputExpect, output);
-  }
-
-  private String slurpHadoop(Path p, FileSystem fs) throws IOException {
-    int len = (int) fs.getFileStatus(p).getLen();
-    byte[] buf = new byte[len];
-    FSDataInputStream in = fs.open(p);
-    String contents = null;
-    try {
-      in.readFully(in.getPos(), buf);
-      contents = new String(buf, StandardCharsets.UTF_8);
-    } finally {
-      in.close();
-    }
-    return contents;
-  }
-
-  @Before
-  public void createInput() throws IOException {
-    FileOutputStream out = new FileOutputStream(INPUT_FILE.getAbsoluteFile());
-    String dummyXmlStartTag = "<PATTERN>\n";
-    String dummyXmlEndTag = "</PATTERN>\n";
-    out.write(dummyXmlStartTag.getBytes("UTF-8"));
-    out.write(input.getBytes("UTF-8"));
-    out.write(dummyXmlEndTag.getBytes("UTF-8"));
-    out.close();
-  }
-
-  @Test
-  public void testStreamXmlRecordReader() throws Exception {
-
-    Job job = Job.getInstance();
-    Configuration conf = job.getConfiguration();
-    job.setJarByClass(TestStreamXmlRecordReader.class);
-    job.setMapperClass(Mapper.class);
-    conf.set("stream.recordreader.class",
-        "org.apache.hadoop.streaming.mapreduce.StreamXmlRecordReader");
-    conf.set("stream.recordreader.begin", "<PATTERN>");
-    conf.set("stream.recordreader.end", "</PATTERN>");
-    job.setInputFormatClass(StreamInputFormat.class);
-    job.setMapOutputKeyClass(Text.class);
-    job.setMapOutputValueClass(Text.class);
-    job.setOutputKeyClass(Text.class);
-    job.setOutputValueClass(Text.class);
-    FileInputFormat.addInputPath(job, new Path("target/input.xml"));
-    OUTPUT_DIR = new Path("target/output");
-    fs = FileSystem.get(conf);
-    if (fs.exists(OUTPUT_DIR)) {
-      fs.delete(OUTPUT_DIR, true);
-    }
-    FileOutputFormat.setOutputPath(job, OUTPUT_DIR);
-    boolean ret = job.waitForCompletion(true);
-
-    assertEquals(true, ret);
-    checkOutput();
-
-  }
-
-  @After
-  public void tearDown() throws IOException {
-    fs.delete(OUTPUT_DIR, true);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/typedbytes/TestTypedBytesWritable.java b/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/typedbytes/TestTypedBytesWritable.java
deleted file mode 100644
index 1ca543610f6..00000000000
--- a/hadoop-tools/hadoop-streaming/src/test/java/org/apache/hadoop/typedbytes/TestTypedBytesWritable.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.typedbytes;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-public class TestTypedBytesWritable {
-
-  @Test
-  public void testToString() {
-    TypedBytesWritable tbw = new TypedBytesWritable();
-    tbw.setValue(true);
-    assertEquals("true", tbw.toString());
-    tbw.setValue(12345);
-    assertEquals("12345", tbw.toString());
-    tbw.setValue(123456789L);
-    assertEquals("123456789", tbw.toString());
-    tbw.setValue((float) 1.23);
-    assertEquals("1.23", tbw.toString());
-    tbw.setValue(1.23456789);
-    assertEquals("1.23456789", tbw.toString());
-    tbw.setValue("random text");
-    assertEquals("random text", tbw.toString());
-  }
-
-  @Test
-  public void testIO() throws IOException {
-    TypedBytesWritable tbw = new TypedBytesWritable();
-    tbw.setValue(12345);
-    ByteArrayOutputStream baos = new ByteArrayOutputStream();
-    DataOutput dout = new DataOutputStream(baos);
-    tbw.write(dout);
-    ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());
-    DataInput din = new DataInputStream(bais);
-    TypedBytesWritable readTbw = new TypedBytesWritable();
-    readTbw.readFields(din);
-    assertEquals(tbw, readTbw);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-tools-dist/pom.xml b/hadoop-tools/hadoop-tools-dist/pom.xml
index cf641d633fc..e4b67499db0 100644
--- a/hadoop-tools/hadoop-tools-dist/pom.xml
+++ b/hadoop-tools/hadoop-tools-dist/pom.xml
@@ -34,17 +34,6 @@
   </properties>
 
   <dependencies>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-streaming</artifactId>
-      <scope>compile</scope>
-      <exclusions>
-        <exclusion>
-          <groupId>org.projectlombok</groupId>
-          <artifactId>lombok</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-distcp</artifactId>
diff --git a/hadoop-tools/pom.xml b/hadoop-tools/pom.xml
index 2b2842fe9fc..329157f3fd2 100644
--- a/hadoop-tools/pom.xml
+++ b/hadoop-tools/pom.xml
@@ -30,7 +30,7 @@
   <packaging>pom</packaging>
 
   <modules>
-    <module>hadoop-streaming</module>
+
     <module>hadoop-distcp</module>
     <module>hadoop-federation-balance</module>
     <module>hadoop-tools-dist</module>
