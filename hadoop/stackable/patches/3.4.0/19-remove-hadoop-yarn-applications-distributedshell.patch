Entirely remove hadoop-yarn-applications-distributedshell

From: Lars Francke <git@lars-francke.de>


---
 .../main/resources/assemblies/hadoop-yarn-dist.xml |    7 
 hadoop-project/pom.xml                             |    6 
 .../pom.xml                                        |  207 --
 .../distributedshell/ApplicationMaster.java        | 1954 --------------------
 .../yarn/applications/distributedshell/Client.java | 1429 ---------------
 .../applications/distributedshell/DSConstants.java |   52 -
 .../DistributedShellTimelinePlugin.java            |   78 -
 .../distributedshell/Log4jPropertyHelper.java      |   48 
 .../distributedshell/PlacementSpec.java            |  116 -
 .../distributedshell/package-info.java             |   19 
 .../ContainerLaunchFailAppMaster.java              |   84 -
 .../distributedshell/DistributedShellBaseTest.java |  607 ------
 .../applications/distributedshell/TestClient.java  |   42 
 .../distributedshell/TestDSAppMaster.java          |  252 ---
 .../distributedshell/TestDSFailedAppMaster.java    |   82 -
 .../distributedshell/TestDSSleepingAppMaster.java  |   63 -
 .../distributedshell/TestDSTimelineV10.java        |  843 ---------
 .../distributedshell/TestDSTimelineV15.java        |  100 -
 .../distributedshell/TestDSTimelineV20.java        |  484 -----
 .../TestDSWithMultipleNodeManager.java             |  503 -----
 .../src/test/resources/a.txt                       |   15 
 .../src/test/resources/b.txt                       |   15 
 .../src/test/resources/log4j.properties            |   19 
 .../src/test/resources/yarn-site.xml               |   21 
 .../hadoop-yarn/hadoop-yarn-applications/pom.xml   |    1 
 25 files changed, 7047 deletions(-)
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellTimelinePlugin.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Log4jPropertyHelper.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/PlacementSpec.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/package-info.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/ContainerLaunchFailAppMaster.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellBaseTest.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestClient.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSAppMaster.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSFailedAppMaster.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSSleepingAppMaster.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV10.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV15.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV20.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSWithMultipleNodeManager.java
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/a.txt
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/b.txt
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/log4j.properties
 delete mode 100644 hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/yarn-site.xml

diff --git a/hadoop-assemblies/src/main/resources/assemblies/hadoop-yarn-dist.xml b/hadoop-assemblies/src/main/resources/assemblies/hadoop-yarn-dist.xml
index 9230f0c7a75..7b0bf7219cc 100644
--- a/hadoop-assemblies/src/main/resources/assemblies/hadoop-yarn-dist.xml
+++ b/hadoop-assemblies/src/main/resources/assemblies/hadoop-yarn-dist.xml
@@ -79,13 +79,6 @@
         <include>*-sources.jar</include>
       </includes>
     </fileSet>
-    <fileSet>
-      <directory>hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/target</directory>
-      <outputDirectory>/share/hadoop/${hadoop.component}/sources</outputDirectory>
-      <includes>
-        <include>*-sources.jar</include>
-      </includes>
-    </fileSet>
     <fileSet>
       <directory>hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/target</directory>
       <outputDirectory>/share/hadoop/${hadoop.component}/sources</outputDirectory>
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index 26c428044d6..5792423d725 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -555,12 +555,6 @@
         <version>${hadoop.version}</version>
       </dependency>
 
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-yarn-applications-distributedshell</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
-
       <dependency>
         <groupId>org.apache.hadoop</groupId>
         <artifactId>hadoop-yarn-server-timeline-pluginstorage</artifactId>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
deleted file mode 100644
index fdd0fefb442..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/pom.xml
+++ /dev/null
@@ -1,207 +0,0 @@
-<?xml version="1.0"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <parent>
-    <artifactId>hadoop-yarn-applications</artifactId>
-    <groupId>org.apache.hadoop</groupId>
-    <version>3.4.0</version>
-  </parent>
-  <modelVersion>4.0.0</modelVersion>
-  <artifactId>hadoop-yarn-applications-distributedshell</artifactId>
-  <version>3.4.0</version>
-  <name>Apache Hadoop YARN DistributedShell</name>
-
-  <properties>
-    <!-- Needed for generating FindBugs warnings using parent pom -->
-    <yarn.basedir>${project.parent.parent.basedir}</yarn.basedir>
-  </properties>
-
-  <dependencies>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <scope>provided</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>junit</groupId>
-      <artifactId>junit</artifactId>
-      <scope>test</scope>
-    </dependency>
-
-    <dependency>
-      <groupId>ch.qos.reload4j</groupId>
-      <artifactId>reload4j</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop.thirdparty</groupId>
-      <artifactId>hadoop-shaded-guava</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>commons-cli</groupId>
-      <artifactId>commons-cli</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>commons-io</groupId>
-      <artifactId>commons-io</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-applicationhistoryservice</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-timelineservice</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-annotations</artifactId>
-    </dependency>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-api</artifactId>
-    </dependency>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-common</artifactId>
-    </dependency>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-client</artifactId>
-    </dependency>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-nodemanager</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-resourcemanager</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <!-- 'mvn dependency:analyze' fails to detect use of this dependency -->
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-tests</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.mockito</groupId>
-      <artifactId>mockito-core</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-timeline-pluginstorage</artifactId>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-timeline-pluginstorage</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-common</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs</artifactId>
-      <scope>test</scope>
-      <exclusions>
-        <exclusion>
-          <groupId>org.ow2.asm</groupId>
-          <artifactId>asm-commons</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs-client</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs</artifactId>
-      <scope>test</scope>
-      <type>test-jar</type>
-      <exclusions>
-        <exclusion>
-          <groupId>org.ow2.asm</groupId>
-          <artifactId>asm-commons</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
-  </dependencies>
-
-  <build>
-    <plugins>
-      <plugin>
-        <artifactId>maven-jar-plugin</artifactId>
-        <executions>
-          <execution>
-            <goals>
-              <goal>jar</goal>
-            </goals>
-            <!-- strictly speaking, the unit test is really a regression test. It
-                 needs the main jar to be available to be able to run. -->
-            <phase>test-compile</phase>
-          </execution>
-        </executions>
-        <configuration>
-           <archive>
-             <manifest>
-               <mainClass>org.apache.hadoop.yarn.applications.distributedshell.Client</mainClass>
-             </manifest>
-           </archive>
-        </configuration>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-surefire-plugin</artifactId>
-        <configuration>
-          <forkedProcessTimeoutInSeconds>1800</forkedProcessTimeoutInSeconds>
-          <environmentVariables>
-            <JAVA_HOME>${java.home}</JAVA_HOME>
-          </environmentVariables>
-       </configuration>
-      </plugin>
-    </plugins>
-  </build>
-
-
-</project>
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
deleted file mode 100644
index a15c78e4267..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java
+++ /dev/null
@@ -1,1954 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.io.BufferedReader;
-import java.io.DataInputStream;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.StringReader;
-import java.io.UncheckedIOException;
-import java.lang.reflect.UndeclaredThrowableException;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.nio.ByteBuffer;
-import java.nio.charset.StandardCharsets;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.Vector;
-import java.util.Base64;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.Arrays;
-import java.util.concurrent.atomic.AtomicLong;
-
-import org.apache.hadoop.thirdparty.com.google.common.base.Strings;
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.Options;
-import org.apache.commons.cli.ParseException;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceAudience.Private;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.net.NetUtils;
-import org.apache.hadoop.security.Credentials;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.security.token.Token;
-import org.apache.hadoop.util.ExitUtil;
-import org.apache.hadoop.util.Shell;
-import org.apache.hadoop.yarn.api.ApplicationConstants;
-import org.apache.hadoop.yarn.api.ApplicationConstants.Environment;
-import org.apache.hadoop.yarn.api.ApplicationMasterProtocol;
-import org.apache.hadoop.yarn.api.ContainerManagementProtocol;
-import org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest;
-import org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse;
-import org.apache.hadoop.yarn.api.protocolrecords.FinishApplicationMasterRequest;
-import org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse;
-import org.apache.hadoop.yarn.api.protocolrecords.StartContainerRequest;
-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.Container;
-import org.apache.hadoop.yarn.api.records.ContainerExitStatus;
-import org.apache.hadoop.yarn.api.records.ContainerId;
-import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;
-import org.apache.hadoop.yarn.api.records.ContainerRetryContext;
-import org.apache.hadoop.yarn.api.records.ContainerRetryPolicy;
-import org.apache.hadoop.yarn.api.records.ContainerState;
-import org.apache.hadoop.yarn.api.records.ContainerStatus;
-import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;
-import org.apache.hadoop.yarn.api.records.LocalResource;
-import org.apache.hadoop.yarn.api.records.LocalResourceType;
-import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;
-import org.apache.hadoop.yarn.api.records.NodeReport;
-import org.apache.hadoop.yarn.api.records.Priority;
-import org.apache.hadoop.yarn.api.records.RejectedSchedulingRequest;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.api.records.ResourceRequest;
-import org.apache.hadoop.yarn.api.records.ResourceSizing;
-import org.apache.hadoop.yarn.api.records.SchedulingRequest;
-import org.apache.hadoop.yarn.api.records.URL;
-import org.apache.hadoop.yarn.api.records.UpdatedContainer;
-import org.apache.hadoop.yarn.api.records.ExecutionType;
-import org.apache.hadoop.yarn.api.records.ExecutionTypeRequest;
-import org.apache.hadoop.yarn.api.records.UpdateContainerRequest;
-import org.apache.hadoop.yarn.api.records.ContainerUpdateType;
-import org.apache.hadoop.yarn.api.records.timeline.TimelineEntity;
-import org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId;
-import org.apache.hadoop.yarn.api.records.timeline.TimelineEvent;
-import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse;
-import org.apache.hadoop.yarn.api.resource.PlacementConstraint;
-import org.apache.hadoop.yarn.client.api.AMRMClient;
-import org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest;
-import org.apache.hadoop.yarn.client.api.TimelineClient;
-import org.apache.hadoop.yarn.client.api.TimelineV2Client;
-import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;
-import org.apache.hadoop.yarn.client.api.async.NMClientAsync;
-import org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;
-import org.apache.hadoop.yarn.security.AMRMTokenIdentifier;
-import org.apache.hadoop.yarn.util.BoundedAppender;
-import org.apache.hadoop.yarn.util.SystemClock;
-import org.apache.hadoop.yarn.util.TimelineServiceHelper;
-import org.apache.hadoop.yarn.util.resource.ResourceUtils;
-import org.apache.hadoop.yarn.util.timeline.TimelineUtils;
-import org.apache.log4j.LogManager;
-
-import org.apache.hadoop.classification.VisibleForTesting;
-import com.sun.jersey.api.client.ClientHandlerException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * An ApplicationMaster for executing shell commands on a set of launched
- * containers using the YARN framework.
- * 
- * <p>
- * This class is meant to act as an example on how to write yarn-based
- * application masters.
- * </p>
- * 
- * <p>
- * The ApplicationMaster is started on a container by the
- * <code>ResourceManager</code>'s launcher. The first thing that the
- * <code>ApplicationMaster</code> needs to do is to connect and register itself
- * with the <code>ResourceManager</code>. The registration sets up information
- * within the <code>ResourceManager</code> regarding what host:port the
- * ApplicationMaster is listening on to provide any form of functionality to a
- * client as well as a tracking url that a client can use to keep track of
- * status/job history if needed. However, in the distributedshell, trackingurl
- * and appMasterHost:appMasterRpcPort are not supported.
- * </p>
- * 
- * <p>
- * The <code>ApplicationMaster</code> needs to send a heartbeat to the
- * <code>ResourceManager</code> at regular intervals to inform the
- * <code>ResourceManager</code> that it is up and alive. The
- * {@link ApplicationMasterProtocol#allocate} to the <code>ResourceManager</code> from the
- * <code>ApplicationMaster</code> acts as a heartbeat.
- * 
- * <p>
- * For the actual handling of the job, the <code>ApplicationMaster</code> has to
- * request the <code>ResourceManager</code> via {@link AllocateRequest} for the
- * required no. of containers using {@link ResourceRequest} with the necessary
- * resource specifications such as node location, computational
- * (memory/disk/cpu) resource requirements. The <code>ResourceManager</code>
- * responds with an {@link AllocateResponse} that informs the
- * <code>ApplicationMaster</code> of the set of newly allocated containers,
- * completed containers as well as current state of available resources.
- * </p>
- * 
- * <p>
- * For each allocated container, the <code>ApplicationMaster</code> can then set
- * up the necessary launch context via {@link ContainerLaunchContext} to specify
- * the allocated container id, local resources required by the executable, the
- * environment to be setup for the executable, commands to execute, etc. and
- * submit a {@link StartContainerRequest} to the {@link ContainerManagementProtocol} to
- * launch and execute the defined commands on the given allocated container.
- * </p>
- * 
- * <p>
- * The <code>ApplicationMaster</code> can monitor the launched container by
- * either querying the <code>ResourceManager</code> using
- * {@link ApplicationMasterProtocol#allocate} to get updates on completed containers or via
- * the {@link ContainerManagementProtocol} by querying for the status of the allocated
- * container's {@link ContainerId}.
- *
- * <p>
- * After the job has been completed, the <code>ApplicationMaster</code> has to
- * send a {@link FinishApplicationMasterRequest} to the
- * <code>ResourceManager</code> to inform it that the
- * <code>ApplicationMaster</code> has been completed.
- */
-@InterfaceAudience.Public
-@InterfaceStability.Unstable
-public class ApplicationMaster {
-
-  private static final Logger LOG = LoggerFactory
-      .getLogger(ApplicationMaster.class);
-
-  @VisibleForTesting
-  @Private
-  public enum DSEvent {
-    DS_APP_ATTEMPT_START, DS_APP_ATTEMPT_END, DS_CONTAINER_START, DS_CONTAINER_END
-  }
-  
-  @VisibleForTesting
-  @Private
-  public enum DSEntity {
-    DS_APP_ATTEMPT, DS_CONTAINER
-  }
-
-  private static final String YARN_SHELL_ID = "YARN_SHELL_ID";
-
-  // Configuration
-  private Configuration conf;
-
-  // Handle to communicate with the Resource Manager
-  @SuppressWarnings("rawtypes")
-  private AMRMClientAsync amRMClient;
-
-  // In both secure and non-secure modes, this points to the job-submitter.
-  @VisibleForTesting
-  UserGroupInformation appSubmitterUgi;
-
-  private Path homeDirectory;
-
-  // Handle to communicate with the Node Manager
-  private NMClientAsync nmClientAsync;
-  // Listen to process the response from the Node Manager
-  private NMCallbackHandler containerListener;
-
-  // Application Attempt Id ( combination of attemptId and fail count )
-  @VisibleForTesting
-  protected ApplicationAttemptId appAttemptID;
-
-  private ApplicationId appId;
-  private String appName;
-
-  // TODO
-  // For status update for clients - yet to be implemented
-  // Hostname of the container
-  private String appMasterHostname = "";
-  // Port on which the app master listens for status updates from clients
-  private int appMasterRpcPort = -1;
-  // Tracking url to which app master publishes info for clients to monitor
-  private String appMasterTrackingUrl = "";
-
-  private boolean timelineServiceV2Enabled = false;
-
-  private boolean timelineServiceV1Enabled = false;
-
-  // App Master configuration
-  // No. of containers to run shell command on
-  @VisibleForTesting
-  protected int numTotalContainers = 1;
-  // Memory to request for the container on which the shell command will run
-  private static final long DEFAULT_CONTAINER_MEMORY = 10;
-  private long containerMemory = DEFAULT_CONTAINER_MEMORY;
-  // VirtualCores to request for the container on which the shell command will run
-  private static final int DEFAULT_CONTAINER_VCORES = 1;
-  private int containerVirtualCores = DEFAULT_CONTAINER_VCORES;
-  // All other resources to request for the container
-  // on which the shell command will run
-  private Map<String, Long> containerResources = new HashMap<>();
-  // Priority of the request
-  private int requestPriority;
-  // Execution type of the containers.
-  // Default GUARANTEED.
-  private ExecutionType containerType = ExecutionType.GUARANTEED;
-  // Whether to automatically promote opportunistic containers.
-  private boolean autoPromoteContainers = false;
-  // Whether to enforce execution type of the containers.
-  private boolean enforceExecType = false;
-
-  // Resource profile for the container
-  private String containerResourceProfile = "";
-  Map<String, Resource> resourceProfiles;
-
-  private boolean keepContainersAcrossAttempts = false;
-
-  // Counter for completed containers ( complete denotes successful or failed )
-  private AtomicInteger numCompletedContainers = new AtomicInteger();
-  // Allocated container count so that we know how many containers has the RM
-  // allocated to us
-  @VisibleForTesting
-  protected AtomicInteger numAllocatedContainers = new AtomicInteger();
-  // Count of failed containers
-  private AtomicInteger numFailedContainers = new AtomicInteger();
-  // Count of containers already requested from the RM
-  // Needed as once requested, we should not request for containers again.
-  // Only request for more if the original requirement changes.
-  @VisibleForTesting
-  protected AtomicInteger numRequestedContainers = new AtomicInteger();
-
-  protected AtomicInteger numIgnore = new AtomicInteger();
-
-  protected AtomicInteger totalRetries = new AtomicInteger(10);
-
-  // Shell command to be executed
-  private String shellCommand = "";
-  // Args to be passed to the shell command
-  private String shellArgs = "";
-  // Env variables to be setup for the shell command
-  private Map<String, String> shellEnv = new HashMap<String, String>();
-
-  // Location of shell script ( obtained from info set in env )
-  // Shell script path in fs
-  private String scriptPath = "";
-  // Timestamp needed for creating a local resource
-  private long shellScriptPathTimestamp = 0;
-  // File length needed for local resource
-  private long shellScriptPathLen = 0;
-
-  // Placement Specifications
-  private Map<String, PlacementSpec> placementSpecs = null;
-
-  // Container retry options
-  private ContainerRetryPolicy containerRetryPolicy =
-      ContainerRetryPolicy.NEVER_RETRY;
-  private Set<Integer> containerRetryErrorCodes = null;
-  private int containerMaxRetries = 0;
-  private int containrRetryInterval = 0;
-  private long containerFailuresValidityInterval = -1;
-
-  private List<String> localizableFiles = new ArrayList<>();
-
-  // Timeline domain ID
-  private String domainId = null;
-
-  // Hardcoded path to shell script in launch container's local env
-  private static final String EXEC_SHELL_STRING_PATH = Client.SCRIPT_PATH
-      + ".sh";
-  private static final String EXEC_BAT_SCRIPT_STRING_PATH = Client.SCRIPT_PATH
-      + ".bat";
-
-  // Hardcoded path to custom log_properties
-  private static final String log4jPath = "log4j.properties";
-
-  private static final String shellCommandPath = "shellCommands";
-  private static final String shellArgsPath = "shellArgs";
-
-  private volatile boolean done;
-
-  private ByteBuffer allTokens;
-
-  // Launch threads
-  private List<Thread> launchThreads = new ArrayList<Thread>();
-
-  // Timeline Client
-  @VisibleForTesting
-  TimelineClient timelineClient;
-
-  // Timeline v2 Client
-  @VisibleForTesting
-  TimelineV2Client timelineV2Client;
-
-  static final String CONTAINER_ENTITY_GROUP_ID = "CONTAINERS";
-  static final String APPID_TIMELINE_FILTER_NAME = "appId";
-  static final String USER_TIMELINE_FILTER_NAME = "user";
-  static final String DIAGNOSTICS = "Diagnostics";
-
-  private final String linux_bash_command = "bash";
-  private final String windows_command = "cmd /c";
-
-  private int yarnShellIdCounter = 1;
-  private final AtomicLong allocIdCounter = new AtomicLong(1);
-
-  @VisibleForTesting
-  protected final Set<ContainerId> launchedContainers =
-      Collections.newSetFromMap(new ConcurrentHashMap<ContainerId, Boolean>());
-
-  private BoundedAppender diagnostics = new BoundedAppender(64 * 1024);
-
-  /**
-   * Container start times used to set id prefix while publishing entity
-   * to ATSv2.
-   */
-  private final ConcurrentMap<ContainerId, Long> containerStartTimes =
-      new ConcurrentHashMap<ContainerId, Long>();
-
-  private ConcurrentMap<ContainerId, Long> getContainerStartTimes() {
-    return containerStartTimes;
-  }
-
-  /**
-   * @param args Command line args
-   */
-  public static void main(String[] args) {
-    boolean result = false;
-    ApplicationMaster appMaster = null;
-    try {
-      appMaster = new ApplicationMaster();
-      LOG.info("Initializing ApplicationMaster");
-      boolean doRun = appMaster.init(args);
-      if (!doRun) {
-        System.exit(0);
-      }
-      appMaster.run();
-      result = appMaster.finish();
-    } catch (Throwable t) {
-      LOG.error("Error running ApplicationMaster", t);
-      LogManager.shutdown();
-      ExitUtil.terminate(1, t);
-    } finally {
-      if (appMaster != null) {
-        appMaster.cleanup();
-      }
-    }
-    if (result) {
-      LOG.info("Application Master completed successfully. exiting");
-      System.exit(0);
-    } else {
-      LOG.error("Application Master failed. exiting");
-      System.exit(2);
-    }
-  }
-
-  /**
-   * Dump out contents of $CWD and the environment to stdout for debugging
-   */
-  private void dumpOutDebugInfo() {
-
-    LOG.info("Dump debug output");
-    Map<String, String> envs = System.getenv();
-    for (Map.Entry<String, String> env : envs.entrySet()) {
-      LOG.info("System env: key=" + env.getKey() + ", val=" + env.getValue());
-      System.out.println("System env: key=" + env.getKey() + ", val="
-          + env.getValue());
-    }
-
-    BufferedReader buf = null;
-    try {
-      String lines = Shell.WINDOWS ? Shell.execCommand("cmd", "/c", "dir") :
-        Shell.execCommand("ls", "-al");
-      buf = new BufferedReader(new StringReader(lines));
-      String line = "";
-      while ((line = buf.readLine()) != null) {
-        LOG.info("System CWD content: " + line);
-        System.out.println("System CWD content: " + line);
-      }
-    } catch (IOException e) {
-      e.printStackTrace();
-    } finally {
-      IOUtils.cleanupWithLogger(LOG, buf);
-    }
-  }
-
-  public ApplicationMaster() {
-    // Set up the configuration
-    conf = new YarnConfiguration();
-  }
-
-  /**
-   * Parse command line options
-   *
-   * @param args Command line args
-   * @return Whether init successful and run should be invoked
-   * @throws ParseException
-   * @throws IOException
-   */
-  public boolean init(String[] args) throws ParseException, IOException {
-    Options opts = new Options();
-    opts.addOption("appname", true,
-        "Application Name. Default value - DistributedShell");
-    opts.addOption("app_attempt_id", true,
-        "App Attempt ID. Not to be used unless for testing purposes");
-    opts.addOption("shell_env", true,
-        "Environment for shell script. Specified as env_key=env_val pairs");
-    opts.addOption("container_type", true,
-        "Container execution type, GUARANTEED or OPPORTUNISTIC");
-    opts.addOption("promote_opportunistic_after_start", false,
-        "Flag to indicate whether to automatically promote opportunistic"
-            + " containers to guaranteed.");
-    opts.addOption("enforce_execution_type", false,
-        "Flag to indicate whether to enforce execution type of containers");
-    opts.addOption("container_memory", true,
-        "Amount of memory in MB to be requested to run the shell command");
-    opts.addOption("container_vcores", true,
-        "Amount of virtual cores to be requested to run the shell command");
-    opts.addOption("container_resources", true,
-        "Amount of resources to be requested to run the shell command. " +
-        "Specified as resource type=value pairs separated by commas. " +
-        "E.g. -container_resources memory-mb=512,vcores=1");
-    opts.addOption("container_resource_profile", true,
-        "Resource profile to be requested to run the shell command");
-    opts.addOption("num_containers", true,
-        "No. of containers on which the shell command needs to be executed");
-    opts.addOption("priority", true, "Application Priority. Default 0");
-    opts.addOption("container_retry_policy", true,
-        "Retry policy when container fails to run, "
-            + "0: NEVER_RETRY, 1: RETRY_ON_ALL_ERRORS, "
-            + "2: RETRY_ON_SPECIFIC_ERROR_CODES");
-    opts.addOption("container_retry_error_codes", true,
-        "When retry policy is set to RETRY_ON_SPECIFIC_ERROR_CODES, error "
-            + "codes is specified with this option, "
-            + "e.g. --container_retry_error_codes 1,2,3");
-    opts.addOption("container_max_retries", true,
-        "If container could retry, it specifies max retires");
-    opts.addOption("container_retry_interval", true,
-        "Interval between each retry, unit is milliseconds");
-    opts.addOption("container_failures_validity_interval", true,
-        "Failures which are out of the time window will not be added to"
-            + " the number of container retry attempts");
-    opts.addOption("placement_spec", true, "Placement specification");
-    opts.addOption("debug", false, "Dump out debug information");
-    opts.addOption("keep_containers_across_application_attempts", false,
-        "Flag to indicate whether to keep containers across application "
-            + "attempts."
-            + " If the flag is true, running containers will not be killed when"
-            + " application attempt fails and these containers will be "
-            + "retrieved by"
-            + " the new application attempt ");
-    opts.addOption("localized_files", true, "List of localized files");
-    opts.addOption("homedir", true, "Home Directory of Job Owner");
-
-    opts.addOption("help", false, "Print usage");
-    CommandLine cliParser = new GnuParser().parse(opts, args);
-
-    if (args.length == 0) {
-      printUsage(opts);
-      throw new IllegalArgumentException(
-          "No args specified for application master to initialize");
-    }
-
-    //Check whether customer log4j.properties file exists
-    if (fileExist(log4jPath)) {
-      try {
-        Log4jPropertyHelper.updateLog4jConfiguration(ApplicationMaster.class,
-            log4jPath);
-      } catch (Exception e) {
-        LOG.warn("Can not set up custom log4j properties. " + e);
-      }
-    }
-
-    appName = cliParser.getOptionValue("appname", "DistributedShell");
-
-    if (cliParser.hasOption("help")) {
-      printUsage(opts);
-      return false;
-    }
-
-    if (cliParser.hasOption("debug")) {
-      dumpOutDebugInfo();
-    }
-
-    homeDirectory = cliParser.hasOption("homedir") ?
-        new Path(cliParser.getOptionValue("homedir")) :
-        new Path("/user/" + System.getenv(ApplicationConstants.
-        Environment.USER.name()));
-
-    if (cliParser.hasOption("placement_spec")) {
-      String placementSpec = cliParser.getOptionValue("placement_spec");
-      String decodedSpec = getDecodedPlacementSpec(placementSpec);
-      LOG.info("Placement Spec received [{}]", decodedSpec);
-
-      this.numTotalContainers = 0;
-      int globalNumOfContainers = Integer
-          .parseInt(cliParser.getOptionValue("num_containers", "0"));
-      parsePlacementSpecs(decodedSpec, globalNumOfContainers);
-      LOG.info("Total num containers requested [{}]", numTotalContainers);
-
-      if (numTotalContainers == 0) {
-        throw new IllegalArgumentException(
-            "Cannot run distributed shell with no containers");
-      }
-    }
-
-    Map<String, String> envs = System.getenv();
-
-    if (!envs.containsKey(Environment.CONTAINER_ID.name())) {
-      if (cliParser.hasOption("app_attempt_id")) {
-        String appIdStr = cliParser.getOptionValue("app_attempt_id", "");
-        appAttemptID = ApplicationAttemptId.fromString(appIdStr);
-      } else {
-        throw new IllegalArgumentException(
-            "Application Attempt Id not set in the environment");
-      }
-    } else {
-      ContainerId containerId = ContainerId.fromString(envs
-          .get(Environment.CONTAINER_ID.name()));
-      appAttemptID = containerId.getApplicationAttemptId();
-      appId = appAttemptID.getApplicationId();
-    }
-
-    if (!envs.containsKey(ApplicationConstants.APP_SUBMIT_TIME_ENV)) {
-      throw new RuntimeException(ApplicationConstants.APP_SUBMIT_TIME_ENV
-          + " not set in the environment");
-    }
-    if (!envs.containsKey(Environment.NM_HOST.name())) {
-      throw new RuntimeException(Environment.NM_HOST.name()
-          + " not set in the environment");
-    }
-    if (!envs.containsKey(Environment.NM_HTTP_PORT.name())) {
-      throw new RuntimeException(Environment.NM_HTTP_PORT
-          + " not set in the environment");
-    }
-    if (!envs.containsKey(Environment.NM_PORT.name())) {
-      throw new RuntimeException(Environment.NM_PORT.name()
-          + " not set in the environment");
-    }
-
-    LOG.info("Application master for app" + ", appId="
-        + appAttemptID.getApplicationId().getId() + ", clustertimestamp="
-        + appAttemptID.getApplicationId().getClusterTimestamp()
-        + ", attemptId=" + appAttemptID.getAttemptId());
-
-    if (!fileExist(shellCommandPath)
-        && envs.get(DSConstants.DISTRIBUTEDSHELLSCRIPTLOCATION).isEmpty()) {
-      throw new IllegalArgumentException(
-          "No shell command or shell script specified to be executed by application master");
-    }
-
-    if (fileExist(shellCommandPath)) {
-      shellCommand = readContent(shellCommandPath);
-    }
-
-    if (fileExist(shellArgsPath)) {
-      shellArgs = readContent(shellArgsPath);
-    }
-
-    if (cliParser.hasOption("shell_env")) {
-      String shellEnvs[] = cliParser.getOptionValues("shell_env");
-      for (String env : shellEnvs) {
-        env = env.trim();
-        int index = env.indexOf('=');
-        if (index == -1) {
-          shellEnv.put(env, "");
-          continue;
-        }
-        String key = env.substring(0, index);
-        String val = "";
-        if (index < (env.length() - 1)) {
-          val = env.substring(index + 1);
-        }
-        shellEnv.put(key, val);
-      }
-    }
-
-    if (envs.containsKey(DSConstants.DISTRIBUTEDSHELLSCRIPTLOCATION)) {
-      scriptPath = envs.get(DSConstants.DISTRIBUTEDSHELLSCRIPTLOCATION);
-
-      if (envs.containsKey(DSConstants.DISTRIBUTEDSHELLSCRIPTTIMESTAMP)) {
-        shellScriptPathTimestamp = Long.parseLong(envs
-            .get(DSConstants.DISTRIBUTEDSHELLSCRIPTTIMESTAMP));
-      }
-      if (envs.containsKey(DSConstants.DISTRIBUTEDSHELLSCRIPTLEN)) {
-        shellScriptPathLen = Long.parseLong(envs
-            .get(DSConstants.DISTRIBUTEDSHELLSCRIPTLEN));
-      }
-      if (!scriptPath.isEmpty()
-          && (shellScriptPathTimestamp <= 0 || shellScriptPathLen <= 0)) {
-        LOG.error("Illegal values in env for shell script path" + ", path="
-            + scriptPath + ", len=" + shellScriptPathLen + ", timestamp="
-            + shellScriptPathTimestamp);
-        throw new IllegalArgumentException(
-            "Illegal values in env for shell script path");
-      }
-    }
-
-    if (envs.containsKey(DSConstants.DISTRIBUTEDSHELLTIMELINEDOMAIN)) {
-      domainId = envs.get(DSConstants.DISTRIBUTEDSHELLTIMELINEDOMAIN);
-    }
-
-    if (cliParser.hasOption("container_type")) {
-      String containerTypeStr = cliParser.getOptionValue("container_type");
-      if (Arrays.stream(ExecutionType.values()).noneMatch(
-          executionType -> executionType.toString()
-              .equals(containerTypeStr))) {
-        throw new IllegalArgumentException("Invalid container_type: "
-            + containerTypeStr);
-      }
-      containerType = ExecutionType.valueOf(containerTypeStr);
-    }
-    if (cliParser.hasOption("promote_opportunistic_after_start")) {
-      autoPromoteContainers = true;
-    }
-    if (cliParser.hasOption("enforce_execution_type")) {
-      enforceExecType = true;
-    }
-    containerMemory = Integer.parseInt(cliParser.getOptionValue(
-        "container_memory", "-1"));
-    containerVirtualCores = Integer.parseInt(cliParser.getOptionValue(
-        "container_vcores", "-1"));
-    containerResources = new HashMap<>();
-    if (cliParser.hasOption("container_resources")) {
-      Map<String, Long> resources = Client.parseResourcesString(
-          cliParser.getOptionValue("container_resources"));
-      for (Map.Entry<String, Long> entry : resources.entrySet()) {
-        containerResources.put(entry.getKey(), entry.getValue());
-      }
-    }
-    containerResourceProfile =
-        cliParser.getOptionValue("container_resource_profile", "");
-
-    keepContainersAcrossAttempts = cliParser.hasOption(
-        "keep_containers_across_application_attempts");
-
-    if (this.placementSpecs == null) {
-      numTotalContainers = Integer.parseInt(cliParser.getOptionValue(
-          "num_containers", "1"));
-    }
-    if (numTotalContainers == 0) {
-      throw new IllegalArgumentException(
-          "Cannot run distributed shell with no containers");
-    }
-    requestPriority = Integer.parseInt(cliParser
-        .getOptionValue("priority", "0"));
-
-    containerRetryPolicy = ContainerRetryPolicy.values()[
-        Integer.parseInt(cliParser.getOptionValue(
-            "container_retry_policy", "0"))];
-    if (cliParser.hasOption("container_retry_error_codes")) {
-      containerRetryErrorCodes = new HashSet<>();
-      for (String errorCode :
-          cliParser.getOptionValue("container_retry_error_codes").split(",")) {
-        containerRetryErrorCodes.add(Integer.parseInt(errorCode));
-      }
-    }
-    containerMaxRetries = Integer.parseInt(
-        cliParser.getOptionValue("container_max_retries", "0"));
-    containrRetryInterval = Integer.parseInt(cliParser.getOptionValue(
-        "container_retry_interval", "0"));
-    containerFailuresValidityInterval = Long.parseLong(
-        cliParser.getOptionValue("container_failures_validity_interval", "-1"));
-    if (!YarnConfiguration.timelineServiceEnabled(conf)) {
-      timelineClient = null;
-      timelineV2Client = null;
-      LOG.warn("Timeline service is not enabled");
-    }
-
-    if (cliParser.hasOption("localized_files")) {
-      String localizedFilesArg = cliParser.getOptionValue("localized_files");
-      if (localizedFilesArg.contains(",")) {
-        String[] files = localizedFilesArg.split(",");
-        localizableFiles = Arrays.asList(files);
-      } else {
-        localizableFiles.add(localizedFilesArg);
-      }
-    }
-
-    return true;
-  }
-
-  private void parsePlacementSpecs(String decodedSpec,
-      int globalNumOfContainers) {
-    Map<String, PlacementSpec> pSpecs =
-        PlacementSpec.parse(decodedSpec);
-    this.placementSpecs = new HashMap<>();
-    for (PlacementSpec pSpec : pSpecs.values()) {
-      // Use global num of containers when the spec doesn't specify
-      // source tags. This is allowed when using node-attribute constraints.
-      if (Strings.isNullOrEmpty(pSpec.sourceTag)
-          && pSpec.getNumContainers() == 0
-          && globalNumOfContainers > 0) {
-        pSpec.setNumContainers(globalNumOfContainers);
-      }
-      this.numTotalContainers += pSpec.getNumContainers();
-      this.placementSpecs.put(pSpec.sourceTag, pSpec);
-    }
-  }
-
-  private String getDecodedPlacementSpec(String placementSpecifications) {
-    Base64.Decoder decoder = Base64.getDecoder();
-    byte[] decodedBytes = decoder.decode(
-        placementSpecifications.getBytes(StandardCharsets.UTF_8));
-    String decodedSpec = new String(decodedBytes, StandardCharsets.UTF_8);
-    LOG.info("Decode placement spec: " + decodedSpec);
-    return decodedSpec;
-  }
-
-  /**
-   * Helper function to print usage
-   *
-   * @param opts Parsed command line options
-   */
-  private void printUsage(Options opts) {
-    new HelpFormatter().printHelp("ApplicationMaster", opts);
-  }
-
-  protected void cleanup() {
-    try {
-      appSubmitterUgi.doAs(new PrivilegedExceptionAction<Void>() {
-        @Override
-        public Void run() throws IOException {
-          FileSystem fs = FileSystem.get(conf);
-          Path dst = new Path(homeDirectory,
-              getRelativePath(appName, appId.toString(), ""));
-          fs.delete(dst, true);
-          return null;
-        }
-      });
-    } catch(Exception e) {
-      LOG.warn("Failed to remove application staging directory", e);
-    }
-  }
-
-  /**
-   * Main run function for the application master
-   *
-   * @throws YarnException
-   * @throws IOException
-   */
-  @SuppressWarnings({ "unchecked" })
-  public void run() throws YarnException, IOException, InterruptedException {
-    LOG.info("Starting ApplicationMaster");
-
-    // Note: Credentials, Token, UserGroupInformation, DataOutputBuffer class
-    // are marked as LimitedPrivate
-    Credentials credentials =
-        UserGroupInformation.getCurrentUser().getCredentials();
-    DataOutputBuffer dob = new DataOutputBuffer();
-    credentials.writeTokenStorageToStream(dob);
-    // Now remove the AM->RM token so that containers cannot access it.
-    Iterator<Token<?>> iter = credentials.getAllTokens().iterator();
-    LOG.info("Executing with tokens:");
-    while (iter.hasNext()) {
-      Token<?> token = iter.next();
-      LOG.info(token.toString());
-      if (token.getKind().equals(AMRMTokenIdentifier.KIND_NAME)) {
-        iter.remove();
-      }
-    }
-    allTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());
-
-    // Create appSubmitterUgi and add original tokens to it
-    String appSubmitterUserName =
-        System.getenv(ApplicationConstants.Environment.USER.name());
-    appSubmitterUgi =
-        UserGroupInformation.createRemoteUser(appSubmitterUserName);
-    appSubmitterUgi.addCredentials(credentials);
-
-    AMRMClientAsync.AbstractCallbackHandler allocListener =
-        new RMCallbackHandler();
-    amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);
-    amRMClient.init(conf);
-    amRMClient.start();
-
-    containerListener = createNMCallbackHandler();
-    nmClientAsync = new NMClientAsyncImpl(containerListener);
-    nmClientAsync.init(conf);
-    nmClientAsync.start();
-
-    startTimelineClient(conf);
-    if (timelineServiceV2Enabled) {
-      // need to bind timelineClient
-      amRMClient.registerTimelineV2Client(timelineV2Client);
-      publishApplicationAttemptEventOnTimelineServiceV2(
-          DSEvent.DS_APP_ATTEMPT_START);
-    }
-
-    if (timelineServiceV1Enabled) {
-      publishApplicationAttemptEvent(timelineClient, appAttemptID.toString(),
-          DSEvent.DS_APP_ATTEMPT_START, domainId, appSubmitterUgi);
-    }
-
-    // Setup local RPC Server to accept status requests directly from clients
-    // TODO need to setup a protocol for client to be able to communicate to
-    // the RPC server
-    // TODO use the rpc port info to register with the RM for the client to
-    // send requests to this app master
-
-    // Register self with ResourceManager
-    // This will start heartbeating to the RM
-    appMasterHostname = NetUtils.getHostname();
-    Map<Set<String>, PlacementConstraint> placementConstraintMap = null;
-    if (this.placementSpecs != null) {
-      placementConstraintMap = new HashMap<>();
-      for (PlacementSpec spec : this.placementSpecs.values()) {
-        if (spec.constraint != null) {
-          Set<String> allocationTags = Strings.isNullOrEmpty(spec.sourceTag) ?
-              Collections.emptySet() : Collections.singleton(spec.sourceTag);
-          placementConstraintMap.put(allocationTags, spec.constraint);
-        }
-      }
-    }
-
-    RegisterApplicationMasterResponse response = amRMClient
-        .registerApplicationMaster(appMasterHostname, appMasterRpcPort,
-            appMasterTrackingUrl, placementConstraintMap);
-    resourceProfiles = response.getResourceProfiles();
-    ResourceUtils.reinitializeResources(response.getResourceTypes());
-    // Dump out information about cluster capability as seen by the
-    // resource manager
-    long maxMem = response.getMaximumResourceCapability().getMemorySize();
-    LOG.info("Max mem capability of resources in this cluster " + maxMem);
-    
-    int maxVCores = response.getMaximumResourceCapability().getVirtualCores();
-    LOG.info("Max vcores capability of resources in this cluster " + maxVCores);
-
-    // A resource ask cannot exceed the max.
-    if (containerMemory > maxMem) {
-      LOG.info("Container memory specified above max threshold of cluster."
-          + " Using max value." + ", specified=" + containerMemory + ", max="
-          + maxMem);
-      containerMemory = maxMem;
-    }
-
-    if (containerVirtualCores > maxVCores) {
-      LOG.info("Container virtual cores specified above max threshold of cluster."
-          + " Using max value." + ", specified=" + containerVirtualCores + ", max="
-          + maxVCores);
-      containerVirtualCores = maxVCores;
-    }
-
-    List<Container> previousAMRunningContainers =
-        response.getContainersFromPreviousAttempts();
-    LOG.info(appAttemptID + " received " + previousAMRunningContainers.size()
-      + " previous attempts' running containers on AM registration.");
-    for(Container container: previousAMRunningContainers) {
-      launchedContainers.add(container.getId());
-    }
-    numAllocatedContainers.addAndGet(previousAMRunningContainers.size());
-
-
-    int numTotalContainersToRequest =
-        numTotalContainers - previousAMRunningContainers.size();
-    // Setup ask for containers from RM
-    // Send request for containers to RM
-    // Until we get our fully allocated quota, we keep on polling RM for
-    // containers
-    // Keep looping until all the containers are launched and shell script
-    // executed on them ( regardless of success/failure).
-    if (this.placementSpecs == null) {
-      LOG.info("placementSpecs null");
-      for (int i = 0; i < numTotalContainersToRequest; ++i) {
-        ContainerRequest containerAsk = setupContainerAskForRM();
-        amRMClient.addContainerRequest(containerAsk);
-      }
-    } else {
-      LOG.info("placementSpecs to create req:" + placementSpecs);
-      List<SchedulingRequest> schedReqs = new ArrayList<>();
-      for (PlacementSpec pSpec : this.placementSpecs.values()) {
-        LOG.info("placementSpec :" + pSpec + ", container:" + pSpec
-            .getNumContainers());
-        for (int i = 0; i < pSpec.getNumContainers(); i++) {
-          SchedulingRequest sr = setupSchedulingRequest(pSpec);
-          schedReqs.add(sr);
-        }
-      }
-      amRMClient.addSchedulingRequests(schedReqs);
-    }
-    numRequestedContainers.set(numTotalContainers);
-  }
-
-  @VisibleForTesting
-  void startTimelineClient(final Configuration conf)
-      throws YarnException, IOException, InterruptedException {
-    try {
-      appSubmitterUgi.doAs(new PrivilegedExceptionAction<Void>() {
-        @Override
-        public Void run() throws Exception {
-          if (YarnConfiguration.timelineServiceEnabled(conf)) {
-            timelineServiceV1Enabled =
-                YarnConfiguration.timelineServiceV1Enabled(conf);
-            timelineServiceV2Enabled =
-                YarnConfiguration.timelineServiceV2Enabled(conf);
-            // Creating the Timeline Client
-            if (timelineServiceV1Enabled) {
-              timelineClient = TimelineClient.createTimelineClient();
-              timelineClient.init(conf);
-              timelineClient.start();
-              LOG.info("Timeline service V1 client is enabled");
-            }
-            if (timelineServiceV2Enabled) {
-              timelineV2Client = TimelineV2Client.createTimelineClient(
-                  appAttemptID.getApplicationId());
-              timelineV2Client.init(conf);
-              timelineV2Client.start();
-              LOG.info("Timeline service V2 client is enabled");
-            }
-          } else {
-            timelineClient = null;
-            timelineV2Client = null;
-            LOG.warn("Timeline service is not enabled");
-          }
-          return null;
-        }
-      });
-    } catch (UndeclaredThrowableException e) {
-      throw new YarnException(e.getCause());
-    }
-  }
-
-  @VisibleForTesting
-  NMCallbackHandler createNMCallbackHandler() {
-    return new NMCallbackHandler(this);
-  }
-
-  @VisibleForTesting
-  protected boolean finish() {
-    // wait for completion.
-    while (!done
-        && (numCompletedContainers.get() != numTotalContainers)) {
-      try {
-        Thread.sleep(200);
-      } catch (InterruptedException ex) {}
-    }
-
-    if (timelineServiceV1Enabled) {
-      publishApplicationAttemptEvent(timelineClient, appAttemptID.toString(),
-          DSEvent.DS_APP_ATTEMPT_END, domainId, appSubmitterUgi);
-    }
-
-    if (timelineServiceV2Enabled) {
-      publishApplicationAttemptEventOnTimelineServiceV2(
-          DSEvent.DS_APP_ATTEMPT_END);
-    }
-
-    // Join all launched threads
-    // needed for when we time out
-    // and we need to release containers
-    for (Thread launchThread : launchThreads) {
-      try {
-        launchThread.join(10000);
-      } catch (InterruptedException e) {
-        LOG.info("Exception thrown in thread join: " + e.getMessage());
-        e.printStackTrace();
-      }
-    }
-
-    // When the application completes, it should stop all running containers
-    LOG.info("Application completed. Stopping running containers");
-    nmClientAsync.stop();
-
-    // When the application completes, it should send a finish application
-    // signal to the RM
-    LOG.info("Application completed. Signalling finished to RM");
-
-    FinalApplicationStatus appStatus;
-    boolean success = true;
-    String message = null;
-    if (numCompletedContainers.get() - numFailedContainers.get()
-        >= numTotalContainers) {
-      appStatus = FinalApplicationStatus.SUCCEEDED;
-    } else {
-      appStatus = FinalApplicationStatus.FAILED;
-      message = String.format("Application Failure: desired = %d, " +
-              "completed = %d, allocated = %d, failed = %d, " +
-              "diagnostics = %s", numRequestedContainers.get(),
-          numCompletedContainers.get(), numAllocatedContainers.get(),
-          numFailedContainers.get(), diagnostics);
-      success = false;
-    }
-    try {
-      amRMClient.unregisterApplicationMaster(appStatus, message, null);
-    } catch (YarnException | IOException ex) {
-      LOG.error("Failed to unregister application", ex);
-    }
-    amRMClient.stop();
-
-    // Stop Timeline Client
-    if(timelineServiceV1Enabled) {
-      timelineClient.stop();
-    }
-    if (timelineServiceV2Enabled) {
-      timelineV2Client.stop();
-    }
-
-    return success;
-  }
-
-  public static String getRelativePath(String appName,
-      String appId, String fileDstPath) {
-    return appName + "/" + appId + "/" + fileDstPath;
-  }
-
-  @VisibleForTesting
-  class RMCallbackHandler extends AMRMClientAsync.AbstractCallbackHandler {
-    @SuppressWarnings("unchecked")
-    @Override
-    public void onContainersCompleted(List<ContainerStatus> completedContainers) {
-      LOG.info("Got response from RM for container ask, completedCnt="
-          + completedContainers.size());
-      for (ContainerStatus containerStatus : completedContainers) {
-        String message = appAttemptID + " got container status for containerID="
-            + containerStatus.getContainerId() + ", state="
-            + containerStatus.getState() + ", exitStatus="
-            + containerStatus.getExitStatus() + ", diagnostics="
-            + containerStatus.getDiagnostics();
-        if (containerStatus.getExitStatus() != 0) {
-          LOG.error(message);
-          diagnostics.append(containerStatus.getDiagnostics());
-        } else {
-          LOG.info(message);
-        }
-
-        // non complete containers should not be here
-        assert (containerStatus.getState() == ContainerState.COMPLETE);
-        // ignore containers we know nothing about - probably from a previous
-        // attempt
-        if (!launchedContainers.contains(containerStatus.getContainerId())) {
-          LOG.info("Ignoring completed status of "
-              + containerStatus.getContainerId()
-              + "; unknown container(probably launched by previous attempt)");
-          continue;
-        }
-
-        // increment counters for completed/failed containers
-        int exitStatus = containerStatus.getExitStatus();
-        if (0 != exitStatus) {
-          // container failed
-          if (ContainerExitStatus.ABORTED != exitStatus) {
-            // shell script failed
-            // counts as completed
-            numCompletedContainers.incrementAndGet();
-            numFailedContainers.incrementAndGet();
-          } else {
-            // container was killed by framework, possibly preempted
-            // we should re-try as the container was lost for some reason
-            numAllocatedContainers.decrementAndGet();
-            numRequestedContainers.decrementAndGet();
-            // we do not need to release the container as it would be done
-            // by the RM
-
-            // Ignore these containers if placementspec is enabled
-            // for the time being.
-            if (placementSpecs != null) {
-              numIgnore.incrementAndGet();
-            }
-          }
-        } else {
-          // nothing to do
-          // container completed successfully
-          numCompletedContainers.incrementAndGet();
-          LOG.info("Container completed successfully." + ", containerId="
-              + containerStatus.getContainerId());
-        }
-        if (timelineServiceV2Enabled) {
-          Long containerStartTime =
-              containerStartTimes.get(containerStatus.getContainerId());
-          if (containerStartTime == null) {
-            containerStartTime = SystemClock.getInstance().getTime();
-            containerStartTimes.put(containerStatus.getContainerId(),
-                containerStartTime);
-          }
-          publishContainerEndEventOnTimelineServiceV2(containerStatus,
-              containerStartTime);
-        }
-        if (timelineServiceV1Enabled) {
-          publishContainerEndEvent(timelineClient, containerStatus, domainId,
-              appSubmitterUgi);
-        }
-      }
-
-      // ask for more containers if any failed
-      int askCount = numTotalContainers - numRequestedContainers.get();
-      numRequestedContainers.addAndGet(askCount);
-
-      // Dont bother re-asking if we are using placementSpecs
-      if (placementSpecs == null) {
-        if (askCount > 0) {
-          for (int i = 0; i < askCount; ++i) {
-            ContainerRequest containerAsk = setupContainerAskForRM();
-            amRMClient.addContainerRequest(containerAsk);
-          }
-        }
-      }
-
-      if (numCompletedContainers.get() + numIgnore.get() >=
-          numTotalContainers) {
-        done = true;
-      }
-    }
-
-    @Override
-    public void onContainersAllocated(List<Container> allocatedContainers) {
-      LOG.info("Got response from RM for container ask, allocatedCnt="
-          + allocatedContainers.size());
-      for (Container allocatedContainer : allocatedContainers) {
-        if (numAllocatedContainers.get() == numTotalContainers) {
-          LOG.info("The requested number of containers have been allocated."
-              + " Releasing the extra container allocation from the RM.");
-          amRMClient.releaseAssignedContainer(allocatedContainer.getId());
-        } else {
-          numAllocatedContainers.addAndGet(1);
-          String yarnShellId = Integer.toString(yarnShellIdCounter);
-          yarnShellIdCounter++;
-          LOG.info(
-              "Launching shell command on a new container."
-                  + ", containerId=" + allocatedContainer.getId()
-                  + ", yarnShellId=" + yarnShellId
-                  + ", containerNode="
-                  + allocatedContainer.getNodeId().getHost()
-                  + ":" + allocatedContainer.getNodeId().getPort()
-                  + ", containerNodeURI="
-                  + allocatedContainer.getNodeHttpAddress()
-                  + ", containerResourceMemory"
-                  + allocatedContainer.getResource().getMemorySize()
-                  + ", containerResourceVirtualCores"
-                  + allocatedContainer.getResource().getVirtualCores());
-
-          Thread launchThread =
-              createLaunchContainerThread(allocatedContainer, yarnShellId);
-
-          // launch and start the container on a separate thread to keep
-          // the main thread unblocked
-          // as all containers may not be allocated at one go.
-          launchThreads.add(launchThread);
-          launchedContainers.add(allocatedContainer.getId());
-          launchThread.start();
-
-          // Remove the corresponding request
-          Collection<AMRMClient.ContainerRequest> requests =
-              amRMClient.getMatchingRequests(
-                  allocatedContainer.getAllocationRequestId());
-          if (requests.iterator().hasNext()) {
-            AMRMClient.ContainerRequest request = requests.iterator().next();
-            amRMClient.removeContainerRequest(request);
-          }
-        }
-      }
-    }
-
-    @Override
-    public void onContainersUpdated(
-        List<UpdatedContainer> containers) {
-      for (UpdatedContainer container : containers) {
-        LOG.info("Container {} updated, updateType={}, resource={}, "
-                + "execType={}",
-            container.getContainer().getId(),
-            container.getUpdateType().toString(),
-            container.getContainer().getResource().toString(),
-            container.getContainer().getExecutionType());
-
-        // TODO Remove this line with finalized updateContainer API.
-        // Currently nm client needs to notify the NM to update container
-        // execution type via NMClient#updateContainerResource() or
-        // NMClientAsync#updateContainerResourceAsync() when
-        // auto-update.containers is disabled, but this API is
-        // under evolving and will need to be replaced by a proper new API.
-        nmClientAsync.updateContainerResourceAsync(container.getContainer());
-      }
-    }
-
-    @Override
-    public void onRequestsRejected(List<RejectedSchedulingRequest> rejReqs) {
-      List<SchedulingRequest> reqsToRetry = new ArrayList<>();
-      for (RejectedSchedulingRequest rejReq : rejReqs) {
-        LOG.info("Scheduling Request {} has been rejected. Reason {}",
-            rejReq.getRequest(), rejReq.getReason());
-        reqsToRetry.add(rejReq.getRequest());
-      }
-      totalRetries.addAndGet(-1 * reqsToRetry.size());
-      if (totalRetries.get() <= 0) {
-        LOG.info("Exiting, since retries are exhausted !!");
-        done = true;
-      } else {
-        amRMClient.addSchedulingRequests(reqsToRetry);
-      }
-    }
-
-    @Override public void onShutdownRequest() {
-      if (keepContainersAcrossAttempts) {
-        LOG.info("Shutdown request received. Ignoring since "
-            + "keep_containers_across_application_attempts is enabled");
-      } else{
-        LOG.info("Shutdown request received. Processing since "
-            + "keep_containers_across_application_attempts is disabled");
-        done = true;
-      }
-    }
-
-    @Override
-    public void onNodesUpdated(List<NodeReport> updatedNodes) {}
-
-    @Override
-    public float getProgress() {
-      // set progress to deliver to RM on next heartbeat
-      float progress = (float) numCompletedContainers.get()
-          / numTotalContainers;
-      return progress;
-    }
-
-    @Override
-    public void onError(Throwable e) {
-      LOG.error("Error in RMCallbackHandler: ", e);
-      done = true;
-    }
-  }
-
-  @VisibleForTesting
-  class NMCallbackHandler extends NMClientAsync.AbstractCallbackHandler {
-
-    private ConcurrentMap<ContainerId, Container> containers =
-        new ConcurrentHashMap<ContainerId, Container>();
-    private final ApplicationMaster applicationMaster;
-
-    public NMCallbackHandler(ApplicationMaster applicationMaster) {
-      this.applicationMaster = applicationMaster;
-    }
-
-    public void addContainer(ContainerId containerId, Container container) {
-      containers.putIfAbsent(containerId, container);
-    }
-
-    @Override
-    public void onContainerStopped(ContainerId containerId) {
-      LOG.debug("Succeeded to stop Container {}", containerId);
-      containers.remove(containerId);
-    }
-
-    @Override
-    public void onContainerStatusReceived(ContainerId containerId,
-        ContainerStatus containerStatus) {
-      LOG.debug("Container Status: id={}, status={}", containerId,
-          containerStatus);
-
-      // If promote_opportunistic_after_start is set, automatically promote
-      // opportunistic containers to guaranteed.
-      if (autoPromoteContainers) {
-        if (containerStatus.getState() == ContainerState.RUNNING) {
-          Container container = containers.get(containerId);
-          if (container.getExecutionType() == ExecutionType.OPPORTUNISTIC) {
-            // Promote container
-            LOG.info("Promoting container {} to {}", container.getId(),
-                container.getExecutionType());
-            UpdateContainerRequest updateRequest = UpdateContainerRequest
-                .newInstance(container.getVersion(), container.getId(),
-                    ContainerUpdateType.PROMOTE_EXECUTION_TYPE, null,
-                    ExecutionType.GUARANTEED);
-            amRMClient.requestContainerUpdate(container, updateRequest);
-          }
-        }
-      }
-    }
-
-    @Override
-    public void onContainerStarted(ContainerId containerId,
-        Map<String, ByteBuffer> allServiceResponse) {
-      LOG.debug("Succeeded to start Container {}", containerId);
-      Container container = containers.get(containerId);
-      if (container != null) {
-        applicationMaster.nmClientAsync.getContainerStatusAsync(
-            containerId, container.getNodeId());
-      }
-      if (applicationMaster.timelineServiceV2Enabled) {
-        long startTime = SystemClock.getInstance().getTime();
-        applicationMaster.getContainerStartTimes().put(containerId, startTime);
-        applicationMaster.publishContainerStartEventOnTimelineServiceV2(
-            container, startTime);
-      }
-      if (applicationMaster.timelineServiceV1Enabled) {
-        applicationMaster.publishContainerStartEvent(
-            applicationMaster.timelineClient, container,
-            applicationMaster.domainId, applicationMaster.appSubmitterUgi);
-      }
-    }
-
-    @Override
-    public void onStartContainerError(ContainerId containerId, Throwable t) {
-      LOG.error("Failed to start Container {}", containerId, t);
-      containers.remove(containerId);
-      applicationMaster.numCompletedContainers.incrementAndGet();
-      applicationMaster.numFailedContainers.incrementAndGet();
-      if (timelineServiceV2Enabled) {
-        publishContainerStartFailedEventOnTimelineServiceV2(containerId,
-            t.getMessage());
-      }
-      if (timelineServiceV1Enabled) {
-        publishContainerStartFailedEvent(containerId, t.getMessage());
-      }
-    }
-
-    @Override
-    public void onGetContainerStatusError(
-        ContainerId containerId, Throwable t) {
-      LOG.error("Failed to query the status of Container " + containerId);
-    }
-
-    @Override
-    public void onStopContainerError(ContainerId containerId, Throwable t) {
-      LOG.error("Failed to stop Container " + containerId);
-      containers.remove(containerId);
-    }
-
-    @Deprecated
-    @Override
-    public void onIncreaseContainerResourceError(
-        ContainerId containerId, Throwable t) {}
-
-    @Deprecated
-    @Override
-    public void onContainerResourceIncreased(
-        ContainerId containerId, Resource resource) {}
-
-    @Override
-    public void onUpdateContainerResourceError(
-        ContainerId containerId, Throwable t) {
-    }
-
-    @Override
-    public void onContainerResourceUpdated(ContainerId containerId,
-        Resource resource) {
-    }
-  }
-
-  /**
-   * Thread to connect to the {@link ContainerManagementProtocol} and launch the container
-   * that will execute the shell command.
-   */
-  private class LaunchContainerRunnable implements Runnable {
-
-    // Allocated container
-    private Container container;
-    private String shellId;
-
-    NMCallbackHandler containerListener;
-
-    /**
-     * @param lcontainer Allocated container
-     * @param containerListener Callback handler of the container
-     */
-    public LaunchContainerRunnable(Container lcontainer,
-        NMCallbackHandler containerListener, String shellId) {
-      this.container = lcontainer;
-      this.containerListener = containerListener;
-      this.shellId = shellId;
-    }
-
-    @Override
-    /**
-     * Connects to CM, sets up container launch context 
-     * for shell command and eventually dispatches the container 
-     * start request to the CM. 
-     */
-    public void run() {
-      LOG.info("Setting up container launch container for containerid="
-          + container.getId() + " with shellid=" + shellId);
-
-      // Set the local resources
-      Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();
-
-      // The container for the eventual shell commands needs its own local
-      // resources too.
-      // In this scenario, if a shell script is specified, we need to have it
-      // copied and made available to the container.
-      if (!scriptPath.isEmpty()) {
-        Path renamedScriptPath = null;
-        if (Shell.WINDOWS) {
-          renamedScriptPath = new Path(scriptPath + ".bat");
-        } else {
-          renamedScriptPath = new Path(scriptPath + ".sh");
-        }
-
-        try {
-          // rename the script file based on the underlying OS syntax.
-          renameScriptFile(renamedScriptPath);
-        } catch (Exception e) {
-          LOG.error(
-              "Not able to add suffix (.bat/.sh) to the shell script filename",
-              e);
-          // We know we cannot continue launching the container
-          // so we should release it.
-          numCompletedContainers.incrementAndGet();
-          numFailedContainers.incrementAndGet();
-          return;
-        }
-
-        URL yarnUrl = null;
-        try {
-          yarnUrl = URL.fromURI(new URI(renamedScriptPath.toString()));
-        } catch (URISyntaxException e) {
-          LOG.error("Error when trying to use shell script path specified"
-              + " in env, path=" + renamedScriptPath, e);
-          // A failure scenario on bad input such as invalid shell script path
-          // We know we cannot continue launching the container
-          // so we should release it.
-          // TODO
-          numCompletedContainers.incrementAndGet();
-          numFailedContainers.incrementAndGet();
-          return;
-        }
-        LocalResource shellRsrc = LocalResource.newInstance(yarnUrl,
-          LocalResourceType.FILE, LocalResourceVisibility.APPLICATION,
-          shellScriptPathLen, shellScriptPathTimestamp);
-        localResources.put(Shell.WINDOWS ? EXEC_BAT_SCRIPT_STRING_PATH :
-            EXEC_SHELL_STRING_PATH, shellRsrc);
-        shellCommand = Shell.WINDOWS ? windows_command : linux_bash_command;
-      }
-
-      // Set up localization for the container which runs the command
-      if (localizableFiles.size() > 0) {
-        FileSystem fs;
-        try {
-          fs = FileSystem.get(conf);
-        } catch (IOException e) {
-          numCompletedContainers.incrementAndGet();
-          numFailedContainers.incrementAndGet();
-          throw new UncheckedIOException("Cannot get FileSystem", e);
-        }
-
-        localizableFiles.stream().forEach(fileName -> {
-          try {
-            String relativePath =
-                getRelativePath(appName, appId.toString(), fileName);
-            Path dst =
-                new Path(homeDirectory, relativePath);
-            FileStatus fileStatus = fs.getFileStatus(dst);
-            LocalResource localRes = LocalResource.newInstance(
-                URL.fromURI(dst.toUri()),
-                LocalResourceType.FILE, LocalResourceVisibility.APPLICATION,
-                fileStatus.getLen(), fileStatus.getModificationTime());
-            LOG.info("Setting up file for localization: " + dst);
-            localResources.put(fileName, localRes);
-          } catch (IOException e) {
-            numCompletedContainers.incrementAndGet();
-            numFailedContainers.incrementAndGet();
-            throw new UncheckedIOException(
-                "Error during localization setup", e);
-          }
-        });
-      }
-
-      // Set the necessary command to execute on the allocated container
-      Vector<CharSequence> vargs = new Vector<CharSequence>(5);
-
-      // Set executable command
-      vargs.add(shellCommand);
-      // Set shell script path
-      if (!scriptPath.isEmpty()) {
-        vargs.add(Shell.WINDOWS ? EXEC_BAT_SCRIPT_STRING_PATH
-            : EXEC_SHELL_STRING_PATH);
-      }
-
-      // Set args for the shell command if any
-      vargs.add(shellArgs);
-      // Add log redirect params
-      vargs.add("1>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stdout");
-      vargs.add("2>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/stderr");
-
-      // Get final commmand
-      StringBuilder command = new StringBuilder();
-      for (CharSequence str : vargs) {
-        command.append(str).append(" ");
-      }
-
-      List<String> commands = new ArrayList<String>();
-      commands.add(command.toString());
-
-      // Set up ContainerLaunchContext, setting local resource, environment,
-      // command and token for constructor.
-
-      // Note for tokens: Set up tokens for the container too. Today, for normal
-      // shell commands, the container in distribute-shell doesn't need any
-      // tokens. We are populating them mainly for NodeManagers to be able to
-      // download anyfiles in the distributed file-system. The tokens are
-      // otherwise also useful in cases, for e.g., when one is running a
-      // "hadoop dfs" command inside the distributed shell.
-      Map<String, String> myShellEnv = new HashMap<String, String>(shellEnv);
-      myShellEnv.put(YARN_SHELL_ID, shellId);
-      ContainerRetryContext containerRetryContext =
-          ContainerRetryContext.newInstance(
-              containerRetryPolicy, containerRetryErrorCodes,
-              containerMaxRetries, containrRetryInterval,
-              containerFailuresValidityInterval);
-      ContainerLaunchContext ctx = ContainerLaunchContext.newInstance(
-        localResources, myShellEnv, commands, null, allTokens.duplicate(),
-          null, containerRetryContext);
-      containerListener.addContainer(container.getId(), container);
-      nmClientAsync.startContainerAsync(container, ctx);
-    }
-  }
-
-  private void renameScriptFile(final Path renamedScriptPath)
-      throws IOException, InterruptedException {
-    appSubmitterUgi.doAs(new PrivilegedExceptionAction<Void>() {
-      @Override
-      public Void run() throws IOException {
-        FileSystem fs = renamedScriptPath.getFileSystem(conf);
-        fs.rename(new Path(scriptPath), renamedScriptPath);
-        return null;
-      }
-    });
-    LOG.info("User " + appSubmitterUgi.getUserName()
-        + " added suffix(.sh/.bat) to script file as " + renamedScriptPath);
-  }
-
-  /**
-   * Setup the request that will be sent to the RM for the container ask.
-   *
-   * @return the setup ResourceRequest to be sent to RM
-   */
-  private ContainerRequest setupContainerAskForRM() {
-    // setup requirements for hosts
-    // using * as any host will do for the distributed shell app
-    // set the priority for the request
-    // TODO - what is the range for priority? how to decide?
-    Priority pri = Priority.newInstance(requestPriority);
-
-    // Set up resource type requirements
-    ContainerRequest request = new ContainerRequest(
-        getTaskResourceCapability(),
-        null, null, pri, 0, true, null,
-        ExecutionTypeRequest.newInstance(containerType, enforceExecType),
-        containerResourceProfile);
-    LOG.info("Requested container ask: " + request.toString());
-    return request;
-  }
-
-  private SchedulingRequest setupSchedulingRequest(PlacementSpec spec) {
-    long allocId = allocIdCounter.incrementAndGet();
-    SchedulingRequest sReq = SchedulingRequest.newInstance(
-        allocId, Priority.newInstance(requestPriority),
-        ExecutionTypeRequest.newInstance(),
-        Collections.singleton(spec.sourceTag),
-        ResourceSizing.newInstance(
-            getTaskResourceCapability()), null);
-    sReq.setPlacementConstraint(spec.constraint);
-    LOG.info("Scheduling Request made: " + sReq.toString());
-    return sReq;
-  }
-
-  private boolean fileExist(String filePath) {
-    return new File(filePath).exists();
-  }
-
-  private String readContent(String filePath) throws IOException {
-    try (DataInputStream ds = new DataInputStream(
-        new FileInputStream(filePath))) {
-      return ds.readUTF();
-    }
-  }
-
-  private void publishContainerStartEvent(
-      final TimelineClient timelineClient, final Container container,
-      String domainId, UserGroupInformation ugi) {
-    final TimelineEntity entity = new TimelineEntity();
-    entity.setEntityId(container.getId().toString());
-    entity.setEntityType(DSEntity.DS_CONTAINER.toString());
-    entity.setDomainId(domainId);
-    entity.addPrimaryFilter(USER_TIMELINE_FILTER_NAME, ugi.getShortUserName());
-    entity.addPrimaryFilter(APPID_TIMELINE_FILTER_NAME, container.getId()
-        .getApplicationAttemptId().getApplicationId().toString());
-    TimelineEvent event = new TimelineEvent();
-    event.setTimestamp(System.currentTimeMillis());
-    event.setEventType(DSEvent.DS_CONTAINER_START.toString());
-    event.addEventInfo("Node", container.getNodeId().toString());
-    event.addEventInfo("Resources", container.getResource().toString());
-    entity.addEvent(event);
-
-    try {
-      processTimelineResponseErrors(
-          putContainerEntity(timelineClient,
-              container.getId().getApplicationAttemptId(),
-              entity));
-    } catch (YarnException | IOException | ClientHandlerException e) {
-      LOG.error("Container start event could not be published for "
-          + container.getId().toString(), e);
-    }
-  }
-
-  @VisibleForTesting
-  void publishContainerEndEvent(
-      final TimelineClient timelineClient, ContainerStatus container,
-      String domainId, UserGroupInformation ugi) {
-    final TimelineEntity entity = new TimelineEntity();
-    entity.setEntityId(container.getContainerId().toString());
-    entity.setEntityType(DSEntity.DS_CONTAINER.toString());
-    entity.setDomainId(domainId);
-    entity.addPrimaryFilter(USER_TIMELINE_FILTER_NAME, ugi.getShortUserName());
-    entity.addPrimaryFilter(APPID_TIMELINE_FILTER_NAME,
-        container.getContainerId().getApplicationAttemptId()
-            .getApplicationId().toString());
-    TimelineEvent event = new TimelineEvent();
-    event.setTimestamp(System.currentTimeMillis());
-    event.setEventType(DSEvent.DS_CONTAINER_END.toString());
-    event.addEventInfo("State", container.getState().name());
-    event.addEventInfo("Exit Status", container.getExitStatus());
-    event.addEventInfo(DIAGNOSTICS, container.getDiagnostics());
-    entity.addEvent(event);
-    try {
-      processTimelineResponseErrors(
-          putContainerEntity(timelineClient,
-              container.getContainerId().getApplicationAttemptId(),
-              entity));
-    } catch (YarnException | IOException | ClientHandlerException e) {
-      LOG.error("Container end event could not be published for "
-          + container.getContainerId().toString(), e);
-    }
-  }
-
-  private TimelinePutResponse putContainerEntity(
-      TimelineClient timelineClient, ApplicationAttemptId currAttemptId,
-      TimelineEntity entity)
-      throws YarnException, IOException {
-    if (TimelineUtils.timelineServiceV1_5Enabled(conf)) {
-      TimelineEntityGroupId groupId = TimelineEntityGroupId.newInstance(
-          currAttemptId.getApplicationId(),
-          CONTAINER_ENTITY_GROUP_ID);
-      return timelineClient.putEntities(currAttemptId, groupId, entity);
-    } else {
-      return timelineClient.putEntities(entity);
-    }
-  }
-
-  private void publishApplicationAttemptEvent(
-      final TimelineClient timelineClient, String appAttemptId,
-      DSEvent appEvent, String domainId, UserGroupInformation ugi) {
-    final TimelineEntity entity = new TimelineEntity();
-    entity.setEntityId(appAttemptId);
-    entity.setEntityType(DSEntity.DS_APP_ATTEMPT.toString());
-    entity.setDomainId(domainId);
-    entity.addPrimaryFilter(USER_TIMELINE_FILTER_NAME, ugi.getShortUserName());
-    TimelineEvent event = new TimelineEvent();
-    event.setEventType(appEvent.toString());
-    event.setTimestamp(System.currentTimeMillis());
-    entity.addEvent(event);
-    try {
-      TimelinePutResponse response = timelineClient.putEntities(entity);
-      processTimelineResponseErrors(response);
-    } catch (YarnException | IOException | ClientHandlerException e) {
-      LOG.error("App Attempt "
-          + (appEvent.equals(DSEvent.DS_APP_ATTEMPT_START) ? "start" : "end")
-          + " event could not be published for "
-          + appAttemptID, e);
-    }
-  }
-
-  private TimelinePutResponse processTimelineResponseErrors(
-      TimelinePutResponse response) {
-    List<TimelinePutResponse.TimelinePutError> errors = response.getErrors();
-    if (errors.size() == 0) {
-      LOG.debug("Timeline entities are successfully put");
-    } else {
-      for (TimelinePutResponse.TimelinePutError error : errors) {
-        LOG.error(
-            "Error when publishing entity [" + error.getEntityType() + ","
-                + error.getEntityId() + "], server side error code: "
-                + error.getErrorCode());
-      }
-    }
-    return response;
-  }
-
-  RMCallbackHandler getRMCallbackHandler() {
-    return new RMCallbackHandler();
-  }
-
-  @VisibleForTesting
-  void setAmRMClient(AMRMClientAsync client) {
-    this.amRMClient = client;
-  }
-
-  @VisibleForTesting
-  int getNumCompletedContainers() {
-    return numCompletedContainers.get();
-  }
-
-  @VisibleForTesting
-  boolean getDone() {
-    return done;
-  }
-
-  @VisibleForTesting
-  Thread createLaunchContainerThread(Container allocatedContainer,
-      String shellId) {
-    LaunchContainerRunnable runnableLaunchContainer =
-        new LaunchContainerRunnable(allocatedContainer, containerListener,
-            shellId);
-    return new Thread(runnableLaunchContainer);
-  }
-
-  private void publishContainerStartEventOnTimelineServiceV2(
-      Container container, long startTime) {
-    final org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity
-        entity =
-            new org.apache.hadoop.yarn.api.records.timelineservice.
-            TimelineEntity();
-    entity.setId(container.getId().toString());
-    entity.setType(DSEntity.DS_CONTAINER.toString());
-    entity.setCreatedTime(startTime);
-    entity.addInfo("user", appSubmitterUgi.getShortUserName());
-
-    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent event =
-        new org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent();
-    event.setTimestamp(startTime);
-    event.setId(DSEvent.DS_CONTAINER_START.toString());
-    event.addInfo("Node", container.getNodeId().toString());
-    event.addInfo("Resources", container.getResource().toString());
-    entity.addEvent(event);
-    entity.setIdPrefix(TimelineServiceHelper.invertLong(startTime));
-
-    try {
-      appSubmitterUgi.doAs(new PrivilegedExceptionAction<Object>() {
-        @Override
-        public TimelinePutResponse run() throws Exception {
-          timelineV2Client.putEntitiesAsync(entity);
-          return null;
-        }
-      });
-    } catch (Exception e) {
-      LOG.error("Container start event could not be published for "
-          + container.getId().toString(),
-          e instanceof UndeclaredThrowableException ? e.getCause() : e);
-    }
-  }
-
-  private void publishContainerStartFailedEventOnTimelineServiceV2(
-      final ContainerId containerId, String diagnostics) {
-    final org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity
-        entity = new org.apache.hadoop.yarn.api.records.timelineservice.
-        TimelineEntity();
-    entity.setId(containerId.toString());
-    entity.setType(DSEntity.DS_CONTAINER.toString());
-    entity.addInfo("user", appSubmitterUgi.getShortUserName());
-    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent event =
-        new org.apache.hadoop.yarn.api.records.timelineservice
-            .TimelineEvent();
-    event.setTimestamp(System.currentTimeMillis());
-    event.setId(DSEvent.DS_CONTAINER_END.toString());
-    event.addInfo(DIAGNOSTICS, diagnostics);
-    entity.addEvent(event);
-    try {
-      appSubmitterUgi.doAs((PrivilegedExceptionAction<Object>) () -> {
-        timelineV2Client.putEntitiesAsync(entity);
-        return null;
-      });
-    } catch (Exception e) {
-      LOG.error("Container start failed event could not be published for {}",
-          containerId,
-          e instanceof UndeclaredThrowableException ? e.getCause() : e);
-    }
-  }
-
-  private void publishContainerStartFailedEvent(final ContainerId containerId,
-      String diagnostics) {
-    final TimelineEntity entityV1 = new TimelineEntity();
-    entityV1.setEntityId(containerId.toString());
-    entityV1.setEntityType(DSEntity.DS_CONTAINER.toString());
-    entityV1.setDomainId(domainId);
-    entityV1.addPrimaryFilter(USER_TIMELINE_FILTER_NAME, appSubmitterUgi
-        .getShortUserName());
-    entityV1.addPrimaryFilter(APPID_TIMELINE_FILTER_NAME,
-        containerId.getApplicationAttemptId().getApplicationId().toString());
-
-    TimelineEvent eventV1 = new TimelineEvent();
-    eventV1.setTimestamp(System.currentTimeMillis());
-    eventV1.setEventType(DSEvent.DS_CONTAINER_END.toString());
-    eventV1.addEventInfo(DIAGNOSTICS, diagnostics);
-    entityV1.addEvent(eventV1);
-    try {
-      processTimelineResponseErrors(putContainerEntity(timelineClient,
-          containerId.getApplicationAttemptId(), entityV1));
-    } catch (YarnException | IOException | ClientHandlerException e) {
-      LOG.error("Container end event could not be published for {}",
-          containerId, e);
-    }
-  }
-
-  private void publishContainerEndEventOnTimelineServiceV2(
-      final ContainerStatus container, long containerStartTime) {
-    final org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity
-        entity =
-            new org.apache.hadoop.yarn.api.records.timelineservice.
-            TimelineEntity();
-    entity.setId(container.getContainerId().toString());
-    entity.setType(DSEntity.DS_CONTAINER.toString());
-    //entity.setDomainId(domainId);
-    entity.addInfo("user", appSubmitterUgi.getShortUserName());
-    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent event =
-        new  org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent();
-    event.setTimestamp(System.currentTimeMillis());
-    event.setId(DSEvent.DS_CONTAINER_END.toString());
-    event.addInfo("State", container.getState().name());
-    event.addInfo("Exit Status", container.getExitStatus());
-    event.addInfo(DIAGNOSTICS, container.getDiagnostics());
-    entity.addEvent(event);
-    entity.setIdPrefix(TimelineServiceHelper.invertLong(containerStartTime));
-
-    try {
-      appSubmitterUgi.doAs(new PrivilegedExceptionAction<Object>() {
-        @Override
-        public TimelinePutResponse run() throws Exception {
-          timelineV2Client.putEntitiesAsync(entity);
-          return null;
-        }
-      });
-    } catch (Exception e) {
-      LOG.error("Container end event could not be published for "
-          + container.getContainerId().toString(),
-          e instanceof UndeclaredThrowableException ? e.getCause() : e);
-    }
-  }
-
-  private void publishApplicationAttemptEventOnTimelineServiceV2(
-      DSEvent appEvent) {
-    final org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity
-        entity =
-            new org.apache.hadoop.yarn.api.records.timelineservice.
-            TimelineEntity();
-    entity.setId(appAttemptID.toString());
-    entity.setType(DSEntity.DS_APP_ATTEMPT.toString());
-    long ts = System.currentTimeMillis();
-    if (appEvent == DSEvent.DS_APP_ATTEMPT_START) {
-      entity.setCreatedTime(ts);
-    }
-    entity.addInfo("user", appSubmitterUgi.getShortUserName());
-    org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent event =
-        new org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent();
-    event.setId(appEvent.toString());
-    event.setTimestamp(ts);
-    entity.addEvent(event);
-    entity.setIdPrefix(
-        TimelineServiceHelper.invertLong(appAttemptID.getAttemptId()));
-
-    try {
-      appSubmitterUgi.doAs(new PrivilegedExceptionAction<Object>() {
-        @Override
-        public TimelinePutResponse run() throws Exception {
-          timelineV2Client.putEntitiesAsync(entity);
-          return null;
-        }
-      });
-    } catch (Exception e) {
-      LOG.error("App Attempt "
-          + (appEvent.equals(DSEvent.DS_APP_ATTEMPT_START) ? "start" : "end")
-          + " event could not be published for "
-          + appAttemptID,
-          e instanceof UndeclaredThrowableException ? e.getCause() : e);
-    }
-  }
-
-  private Resource getTaskResourceCapability()
-      throws YarnRuntimeException {
-    if (containerMemory < -1 || containerMemory == 0) {
-      throw new YarnRuntimeException("Value of AM memory '" + containerMemory
-          + "' has to be greater than 0");
-    }
-    if (containerVirtualCores < -1 || containerVirtualCores == 0) {
-      throw new YarnRuntimeException(
-          "Value of AM vcores '" + containerVirtualCores
-              + "' has to be greater than 0");
-    }
-
-    Resource resourceCapability =
-        Resource.newInstance(containerMemory, containerVirtualCores);
-    containerMemory =
-        containerMemory == -1 ? DEFAULT_CONTAINER_MEMORY : containerMemory;
-    containerVirtualCores = containerVirtualCores == -1 ?
-        DEFAULT_CONTAINER_VCORES :
-        containerVirtualCores;
-    resourceCapability.setMemorySize(containerMemory);
-    resourceCapability.setVirtualCores(containerVirtualCores);
-    for (Map.Entry<String, Long> entry : containerResources.entrySet()) {
-      resourceCapability.setResourceValue(entry.getKey(), entry.getValue());
-    }
-
-    return resourceCapability;
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
deleted file mode 100644
index 2987165486e..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Client.java
+++ /dev/null
@@ -1,1429 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.UncheckedIOException;
-import java.nio.ByteBuffer;
-import java.nio.charset.StandardCharsets;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.Vector;
-import java.util.Arrays;
-import java.util.Base64;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.hadoop.thirdparty.com.google.common.base.Joiner;
-
-import org.apache.commons.cli.CommandLine;
-import org.apache.commons.cli.GnuParser;
-import org.apache.commons.cli.HelpFormatter;
-import org.apache.commons.cli.Option;
-import org.apache.commons.cli.Options;
-import org.apache.commons.cli.ParseException;
-import org.apache.commons.lang3.StringUtils;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.security.Credentials;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.security.token.Token;
-import org.apache.hadoop.yarn.api.ApplicationClientProtocol;
-import org.apache.hadoop.yarn.api.ApplicationConstants;
-import org.apache.hadoop.yarn.api.ApplicationConstants.Environment;
-import org.apache.hadoop.yarn.api.protocolrecords.GetNewApplicationResponse;
-import org.apache.hadoop.yarn.api.protocolrecords.KillApplicationRequest;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.ApplicationReport;
-import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;
-import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;
-import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;
-import org.apache.hadoop.yarn.api.records.LocalResource;
-import org.apache.hadoop.yarn.api.records.LocalResourceType;
-import org.apache.hadoop.yarn.api.records.LocalResourceVisibility;
-import org.apache.hadoop.yarn.api.records.LogAggregationContext;
-import org.apache.hadoop.yarn.api.records.NodeReport;
-import org.apache.hadoop.yarn.api.records.NodeState;
-import org.apache.hadoop.yarn.api.records.Priority;
-import org.apache.hadoop.yarn.api.records.QueueACL;
-import org.apache.hadoop.yarn.api.records.QueueInfo;
-import org.apache.hadoop.yarn.api.records.QueueUserACLInfo;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.api.records.ResourceInformation;
-import org.apache.hadoop.yarn.api.records.ResourceRequest;
-import org.apache.hadoop.yarn.api.records.ResourceTypeInfo;
-import org.apache.hadoop.yarn.api.records.URL;
-import org.apache.hadoop.yarn.api.records.YarnApplicationState;
-import org.apache.hadoop.yarn.api.records.YarnClusterMetrics;
-import org.apache.hadoop.yarn.api.records.timeline.TimelineDomain;
-import org.apache.hadoop.yarn.api.records.ExecutionType;
-import org.apache.hadoop.yarn.client.api.TimelineClient;
-import org.apache.hadoop.yarn.client.api.YarnClient;
-import org.apache.hadoop.yarn.client.api.YarnClientApplication;
-import org.apache.hadoop.yarn.client.util.YarnClientUtils;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.exceptions.ResourceNotFoundException;
-import org.apache.hadoop.yarn.exceptions.YARNFeatureNotEnabledException;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.util.DockerClientConfigHandler;
-import org.apache.hadoop.yarn.util.UnitsConversionUtil;
-import org.apache.hadoop.yarn.util.resource.ResourceUtils;
-import org.apache.hadoop.yarn.util.resource.Resources;
-import org.apache.hadoop.yarn.util.timeline.TimelineUtils;
-import org.apache.hadoop.classification.VisibleForTesting;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Client for Distributed Shell application submission to YARN.
- * 
- * <p> The distributed shell client allows an application master to be launched that in turn would run 
- * the provided shell command on a set of containers. </p>
- * 
- * <p>This client is meant to act as an example on how to write yarn-based applications. </p>
- *
- * <p> To submit an application, a client first needs to connect to the <code>ResourceManager</code> 
- * aka ApplicationsManager or ASM via the {@link ApplicationClientProtocol}. The {@link ApplicationClientProtocol} 
- * provides a way for the client to get access to cluster information and to request for a
- * new {@link ApplicationId}. <p>
- * 
- * <p> For the actual job submission, the client first has to create an {@link ApplicationSubmissionContext}. 
- * The {@link ApplicationSubmissionContext} defines the application details such as {@link ApplicationId} 
- * and application name, the priority assigned to the application and the queue
- * to which this application needs to be assigned. In addition to this, the {@link ApplicationSubmissionContext}
- * also defines the {@link ContainerLaunchContext} which describes the <code>Container</code> with which 
- * the {@link ApplicationMaster} is launched. </p>
- * 
- * <p> The {@link ContainerLaunchContext} in this scenario defines the resources to be allocated for the 
- * {@link ApplicationMaster}'s container, the local resources (jars, configuration files) to be made available 
- * and the environment to be set for the {@link ApplicationMaster} and the commands to be executed to run the 
- * {@link ApplicationMaster}. <p>
- * 
- * <p> Using the {@link ApplicationSubmissionContext}, the client submits the application to the 
- * <code>ResourceManager</code> and then monitors the application by requesting the <code>ResourceManager</code> 
- * for an {@link ApplicationReport} at regular time intervals. In case of the application taking too long, the client 
- * kills the application by submitting a {@link KillApplicationRequest} to the <code>ResourceManager</code>. </p>
- *
- */
-@InterfaceAudience.Public
-@InterfaceStability.Unstable
-public class Client {
-
-  private static final Logger LOG = LoggerFactory
-      .getLogger(Client.class);
-
-  private static final int DEFAULT_AM_MEMORY = 100;
-  private static final int DEFAULT_AM_VCORES = 1;
-  private static final int DEFAULT_CONTAINER_MEMORY = 10;
-  private static final int DEFAULT_CONTAINER_VCORES = 1;
-
-  // check the application once per second.
-  private static final int APP_MONITOR_INTERVAL = 1000;
-  
-  // Configuration
-  private Configuration conf;
-  private YarnClient yarnClient;
-  // Application master specific info to register a new Application with RM/ASM
-  private String appName = "";
-  private ApplicationId applicationId;
-  // App master priority
-  private int amPriority = 0;
-  // Queue for App master
-  private String amQueue = "";
-  // Amt. of memory resource to request for to run the App Master
-  private long amMemory = DEFAULT_AM_MEMORY;
-  // Amt. of virtual core resource to request for to run the App Master
-  private int amVCores = DEFAULT_AM_VCORES;
-  // Amount of resources to request to run the App Master
-  private Map<String, Long> amResources = new HashMap<>();
-  // AM resource profile
-  private String amResourceProfile = "";
-
-  // Application master jar file
-  private String appMasterJar = ""; 
-  // Main class to invoke application master
-  private final String appMasterMainClass;
-
-  // Shell command to be executed 
-  private String shellCommand = ""; 
-  // Location of shell script 
-  private String shellScriptPath = ""; 
-  // Args to be passed to the shell command
-  private String[] shellArgs = new String[] {};
-  // Env variables to be setup for the shell command 
-  private Map<String, String> shellEnv = new HashMap<String, String>();
-  // Shell Command Container priority 
-  private int shellCmdPriority = 0;
-
-  // Amt of memory to request for container in which shell script will be executed
-  private long containerMemory = DEFAULT_CONTAINER_MEMORY;
-  // Amt. of virtual cores to request for container in which shell script will be executed
-  private int containerVirtualCores = DEFAULT_CONTAINER_VCORES;
-  // Amt. of resources to request for container
-  // in which shell script will be executed
-  private Map<String, Long> containerResources = new HashMap<>();
-  // container resource profile
-  private String containerResourceProfile = "";
-  // No. of containers in which the shell script needs to be executed
-  private int numContainers = 1;
-  private String nodeLabelExpression = null;
-  // Container type, default GUARANTEED.
-  private ExecutionType containerType = ExecutionType.GUARANTEED;
-  // Whether to auto promote opportunistic containers
-  private boolean autoPromoteContainers = false;
-  // Whether to enforce execution type of containers
-  private boolean enforceExecType = false;
-
-  // Placement specification
-  private String placementSpec = "";
-  // Node Attribute specification
-  private String nodeAttributeSpec = "";
-  // log4j.properties file 
-  // if available, add to local resources and set into classpath 
-  private String log4jPropFile = "";
-  // rolling
-  private String rollingFilesPattern = "";
-
-  // Start time for client
-  private long clientStartTime = System.currentTimeMillis();
-  // Timeout threshold for client. Kill app after time interval expires.
-  private long clientTimeout = 600000;
-
-  // flag to indicate whether to keep containers across application attempts.
-  private boolean keepContainers = false;
-
-  private long attemptFailuresValidityInterval = -1;
-
-  private Vector<CharSequence> containerRetryOptions = new Vector<>(5);
-
-  // Debug flag
-  boolean debugFlag = false;
-
-  // Timeline domain ID
-  private String domainId = null;
-
-  // Flag to indicate whether to create the domain of the given ID
-  private boolean toCreateDomain = false;
-
-  // Timeline domain reader access control
-  private String viewACLs = null;
-
-  // Timeline domain writer access control
-  private String modifyACLs = null;
-
-  private String flowName = null;
-  private String flowVersion = null;
-  private long flowRunId = 0L;
-
-  // Docker client configuration
-  private String dockerClientConfig = null;
-
-  // Application tags
-  private Set<String> applicationTags = new HashSet<>();
-
-  private List<String> filesToLocalize = new ArrayList<>();
-
-  // Command line options
-  private Options opts;
-
-  private final AtomicBoolean stopSignalReceived;
-  private final AtomicBoolean isRunning;
-  private final Object objectLock = new Object();
-
-  private static final String shellCommandPath = "shellCommands";
-  private static final String shellArgsPath = "shellArgs";
-  private static final String appMasterJarPath = "AppMaster.jar";
-  // Hardcoded path to custom log_properties
-  private static final String log4jPath = "log4j.properties";
-
-  public static final String SCRIPT_PATH = "ExecScript";
-
-  /**
-   * @param args Command line arguments 
-   */
-  public static void main(String[] args) {
-    boolean result = false;
-    try {
-      Client client = new Client();
-      LOG.info("Initializing Client");
-      try {
-        boolean doRun = client.init(args);
-        if (!doRun) {
-          System.exit(0);
-        }
-      } catch (IllegalArgumentException e) {
-        System.err.println(e.getLocalizedMessage());
-        client.printUsage();
-        System.exit(-1);
-      }
-      result = client.run();
-    } catch (Throwable t) {
-      LOG.error("Error running Client", t);
-      System.exit(1);
-    }
-    if (result) {
-      LOG.info("Application completed successfully");
-      System.exit(0);
-    } 
-    LOG.error("Application failed to complete successfully");
-    System.exit(2);
-  }
-
-  /**
-   */
-  public Client(Configuration conf) throws Exception  {
-    this(
-      "org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster",
-      conf);
-  }
-
-  Client(String appMasterMainClass, Configuration conf) {
-    this.conf = conf;
-    this.conf.setBoolean(
-        YarnConfiguration.YARN_CLIENT_LOAD_RESOURCETYPES_FROM_SERVER, true);
-    this.appMasterMainClass = appMasterMainClass;
-    yarnClient = YarnClient.createYarnClient();
-    yarnClient.init(conf);
-    opts = new Options();
-    opts.addOption("appname", true, "Application Name. Default value - DistributedShell");
-    opts.addOption("priority", true, "Application Priority. Default 0");
-    opts.addOption("queue", true, "RM Queue in which this application is to be submitted");
-    opts.addOption("timeout", true, "Application timeout in milliseconds");
-    opts.addOption("master_memory", true, "Amount of memory in MB to be requested to run the application master");
-    opts.addOption("master_vcores", true, "Amount of virtual cores " +
-        "to be requested to run the application master");
-    opts.addOption("master_resources", true, "Amount of resources " +
-        "to be requested to run the application master. " +
-        "Specified as resource type=value pairs separated by commas." +
-        "E.g. -master_resources memory-mb=512,vcores=2");
-    opts.addOption("jar", true, "Jar file containing the application master");
-    opts.addOption("master_resource_profile", true, "Resource profile for the application master");
-    opts.addOption("shell_command", true, "Shell command to be executed by " +
-        "the Application Master. Can only specify either --shell_command " +
-        "or --shell_script");
-    opts.addOption("shell_script", true, "Location of the shell script to be " +
-        "executed. Can only specify either --shell_command or --shell_script");
-    opts.addOption("shell_args", true, "Command line args for the shell script." +
-        "Multiple args can be separated by empty space.");
-    opts.getOption("shell_args").setArgs(Option.UNLIMITED_VALUES);
-    opts.addOption("shell_env", true,
-        "Environment for shell script. Specified as env_key=env_val pairs");
-    opts.addOption("shell_cmd_priority", true, "Priority for the shell command containers");
-    opts.addOption("container_type", true,
-        "Container execution type, GUARANTEED or OPPORTUNISTIC");
-    opts.addOption("container_memory", true, "Amount of memory in MB " +
-        "to be requested to run the shell command");
-    opts.addOption("container_vcores", true, "Amount of virtual cores " +
-        "to be requested to run the shell command");
-    opts.addOption("container_resources", true, "Amount of resources " +
-        "to be requested to run the shell command. " +
-        "Specified as resource type=value pairs separated by commas. " +
-        "E.g. -container_resources memory-mb=256,vcores=1");
-    opts.addOption("container_resource_profile", true, "Resource profile for the shell command");
-    opts.addOption("num_containers", true, "No. of containers on which the shell command needs to be executed");
-    opts.addOption("promote_opportunistic_after_start", false,
-        "Flag to indicate whether to automatically promote opportunistic"
-            + " containers to guaranteed.");
-    opts.addOption("enforce_execution_type", false,
-        "Flag to indicate whether to enforce execution type of containers");
-    opts.addOption("log_properties", true, "log4j.properties file");
-    opts.addOption("rolling_log_pattern", true,
-        "pattern for files that should be aggregated in a rolling fashion");
-    opts.addOption("keep_containers_across_application_attempts", false,
-        "Flag to indicate whether to keep containers across application "
-            + "attempts."
-            + " If the flag is true, running containers will not be killed when"
-            + " application attempt fails and these containers will be "
-            + "retrieved by"
-            + " the new application attempt ");
-    opts.addOption("attempt_failures_validity_interval", true,
-      "when attempt_failures_validity_interval in milliseconds is set to > 0," +
-      "the failure number will not take failures which happen out of " +
-      "the validityInterval into failure count. " +
-      "If failure count reaches to maxAppAttempts, " +
-      "the application will be failed.");
-    opts.addOption("debug", false, "Dump out debug information");
-    opts.addOption("domain", true, "ID of the timeline domain where the "
-        + "timeline entities will be put");
-    opts.addOption("view_acls", true, "Users and groups that allowed to "
-        + "view the timeline entities in the given domain");
-    opts.addOption("modify_acls", true, "Users and groups that allowed to "
-        + "modify the timeline entities in the given domain");
-    opts.addOption("create", false, "Flag to indicate whether to create the "
-        + "domain specified with -domain.");
-    opts.addOption("flow_name", true, "Flow name which the distributed shell "
-        + "app belongs to");
-    opts.addOption("flow_version", true, "Flow version which the distributed "
-        + "shell app belongs to");
-    opts.addOption("flow_run_id", true, "Flow run ID which the distributed "
-        + "shell app belongs to");
-    opts.addOption("help", false, "Print usage");
-    opts.addOption("node_label_expression", true,
-        "Node label expression to determine the nodes"
-            + " where all the containers of this application"
-            + " will be allocated, \"\" means containers"
-            + " can be allocated anywhere, if you don't specify the option,"
-            + " default node_label_expression of queue will be used.");
-    opts.addOption("container_retry_policy", true,
-        "Retry policy when container fails to run, "
-            + "0: NEVER_RETRY, 1: RETRY_ON_ALL_ERRORS, "
-            + "2: RETRY_ON_SPECIFIC_ERROR_CODES");
-    opts.addOption("container_retry_error_codes", true,
-        "When retry policy is set to RETRY_ON_SPECIFIC_ERROR_CODES, error "
-            + "codes is specified with this option, "
-            + "e.g. --container_retry_error_codes 1,2,3");
-    opts.addOption("container_max_retries", true,
-        "If container could retry, it specifies max retires");
-    opts.addOption("container_retry_interval", true,
-        "Interval between each retry, unit is milliseconds");
-    opts.addOption("container_failures_validity_interval", true,
-        "Failures which are out of the time window will not be added to"
-            + " the number of container retry attempts");
-    opts.addOption("docker_client_config", true,
-        "The docker client configuration path. The scheme should be supplied"
-            + " (i.e. file:// or hdfs://)."
-            + " Only used when the Docker runtime is enabled and requested.");
-    opts.addOption("placement_spec", true,
-        "Placement specification. Please note, if this option is specified,"
-            + " The \"num_containers\" option will be ignored. All requested"
-            + " containers will be of type GUARANTEED" );
-    opts.addOption("application_tags", true, "Application tags.");
-    opts.addOption("localize_files", true, "List of files, separated by comma"
-        + " to be localized for the command");
-    stopSignalReceived = new AtomicBoolean(false);
-    isRunning = new AtomicBoolean(false);
-  }
-
-  /**
-   */
-  public Client() throws Exception  {
-    this(new YarnConfiguration());
-  }
-
-  /**
-   * Helper function to print out usage
-   */
-  private void printUsage() {
-    new HelpFormatter().printHelp("Client", opts);
-  }
-
-  /**
-   * Parse command line options
-   * @param args Parsed command line options 
-   * @return Whether the init was successful to run the client
-   * @throws ParseException
-   */
-  public boolean init(String[] args) throws ParseException {
-
-    CommandLine cliParser = new GnuParser().parse(opts, args);
-
-    if (args.length == 0) {
-      throw new IllegalArgumentException("No args specified for client to initialize");
-    }
-
-    if (cliParser.hasOption("log_properties")) {
-      String log4jPath = cliParser.getOptionValue("log_properties");
-      try {
-        Log4jPropertyHelper.updateLog4jConfiguration(Client.class, log4jPath);
-      } catch (Exception e) {
-        LOG.warn("Can not set up custom log4j properties. " + e);
-      }
-    }
-
-    if (cliParser.hasOption("rolling_log_pattern")) {
-      rollingFilesPattern = cliParser.getOptionValue("rolling_log_pattern");
-    }
-
-    if (cliParser.hasOption("help")) {
-      printUsage();
-      return false;
-    }
-
-    if (cliParser.hasOption("debug")) {
-      debugFlag = true;
-
-    }
-
-    if (cliParser.hasOption("keep_containers_across_application_attempts")) {
-      LOG.info("keep_containers_across_application_attempts");
-      keepContainers = true;
-    }
-
-    if (cliParser.hasOption("placement_spec")) {
-      placementSpec = cliParser.getOptionValue("placement_spec");
-      // Check if it is parsable
-      PlacementSpec.parse(this.placementSpec);
-    }
-
-    appName = cliParser.getOptionValue("appname", "DistributedShell");
-    amPriority = Integer.parseInt(cliParser.getOptionValue("priority", "0"));
-    amQueue = cliParser.getOptionValue("queue", "default");
-    amMemory =
-        Integer.parseInt(cliParser.getOptionValue("master_memory", "-1"));
-    amVCores =
-        Integer.parseInt(cliParser.getOptionValue("master_vcores", "-1"));
-    if (cliParser.hasOption("master_resources")) {
-      Map<String, Long> masterResources =
-          parseResourcesString(cliParser.getOptionValue("master_resources"));
-      for (Map.Entry<String, Long> entry : masterResources.entrySet()) {
-        if (entry.getKey().equals(ResourceInformation.MEMORY_URI)) {
-          amMemory = entry.getValue();
-        } else if (entry.getKey().equals(ResourceInformation.VCORES_URI)) {
-          amVCores = entry.getValue().intValue();
-        } else {
-          amResources.put(entry.getKey(), entry.getValue());
-        }
-      }
-    }
-    amResourceProfile = cliParser.getOptionValue("master_resource_profile", "");
-
-    if (!cliParser.hasOption("jar")) {
-      throw new IllegalArgumentException("No jar file specified for application master");
-    }
-
-    appMasterJar = cliParser.getOptionValue("jar");
-
-    if (!cliParser.hasOption("shell_command") && !cliParser.hasOption("shell_script")) {
-      throw new IllegalArgumentException(
-          "No shell command or shell script specified to be executed by application master");
-    } else if (cliParser.hasOption("shell_command") && cliParser.hasOption("shell_script")) {
-      throw new IllegalArgumentException("Can not specify shell_command option " +
-          "and shell_script option at the same time");
-    } else if (cliParser.hasOption("shell_command")) {
-      shellCommand = cliParser.getOptionValue("shell_command");
-    } else {
-      shellScriptPath = cliParser.getOptionValue("shell_script");
-    }
-    if (cliParser.hasOption("shell_args")) {
-      shellArgs = cliParser.getOptionValues("shell_args");
-    }
-    if (cliParser.hasOption("shell_env")) { 
-      String envs[] = cliParser.getOptionValues("shell_env");
-      for (String env : envs) {
-        env = env.trim();
-        int index = env.indexOf('=');
-        if (index == -1) {
-          shellEnv.put(env, "");
-          continue;
-        }
-        String key = env.substring(0, index);
-        String val = "";
-        if (index < (env.length()-1)) {
-          val = env.substring(index+1);
-        }
-        shellEnv.put(key, val);
-      }
-    }
-    shellCmdPriority = Integer.parseInt(cliParser.getOptionValue("shell_cmd_priority", "0"));
-
-    if (cliParser.hasOption("container_type")) {
-      String containerTypeStr = cliParser.getOptionValue("container_type");
-      if (Arrays.stream(ExecutionType.values()).noneMatch(
-          executionType -> executionType.toString()
-          .equals(containerTypeStr))) {
-        throw new IllegalArgumentException("Invalid container_type: "
-            + containerTypeStr);
-      }
-      containerType = ExecutionType.valueOf(containerTypeStr);
-    }
-    if (cliParser.hasOption("promote_opportunistic_after_start")) {
-      autoPromoteContainers = true;
-    }
-    if (cliParser.hasOption("enforce_execution_type")) {
-      enforceExecType = true;
-    }
-    containerMemory =
-        Integer.parseInt(cliParser.getOptionValue("container_memory", "-1"));
-    containerVirtualCores =
-        Integer.parseInt(cliParser.getOptionValue("container_vcores", "-1"));
-    if (cliParser.hasOption("container_resources")) {
-      Map<String, Long> resources =
-          parseResourcesString(cliParser.getOptionValue("container_resources"));
-      for (Map.Entry<String, Long> entry : resources.entrySet()) {
-        if (entry.getKey().equals(ResourceInformation.MEMORY_URI)) {
-          containerMemory = entry.getValue();
-        } else if (entry.getKey().equals(ResourceInformation.VCORES_URI)) {
-          containerVirtualCores = entry.getValue().intValue();
-        } else {
-          containerResources.put(entry.getKey(), entry.getValue());
-        }
-      }
-    }
-    containerResourceProfile =
-        cliParser.getOptionValue("container_resource_profile", "");
-    numContainers =
-        Integer.parseInt(cliParser.getOptionValue("num_containers", "1"));
-
-    if (numContainers < 1) {
-      throw new IllegalArgumentException("Invalid no. of containers specified,"
-          + " exiting. Specified numContainer=" + numContainers);
-    }
-    
-    nodeLabelExpression = cliParser.getOptionValue("node_label_expression", null);
-
-    clientTimeout = Integer.parseInt(cliParser.getOptionValue("timeout", "600000"));
-
-    attemptFailuresValidityInterval =
-        Long.parseLong(cliParser.getOptionValue(
-          "attempt_failures_validity_interval", "-1"));
-
-    log4jPropFile = cliParser.getOptionValue("log_properties", "");
-
-    // Get timeline domain options
-    if (cliParser.hasOption("domain")) {
-      domainId = cliParser.getOptionValue("domain");
-      toCreateDomain = cliParser.hasOption("create");
-      if (cliParser.hasOption("view_acls")) {
-        viewACLs = cliParser.getOptionValue("view_acls");
-      }
-      if (cliParser.hasOption("modify_acls")) {
-        modifyACLs = cliParser.getOptionValue("modify_acls");
-      }
-    }
-
-    // Get container retry options
-    if (cliParser.hasOption("container_retry_policy")) {
-      containerRetryOptions.add("--container_retry_policy "
-          + cliParser.getOptionValue("container_retry_policy"));
-    }
-    if (cliParser.hasOption("container_retry_error_codes")) {
-      containerRetryOptions.add("--container_retry_error_codes "
-          + cliParser.getOptionValue("container_retry_error_codes"));
-    }
-    if (cliParser.hasOption("container_max_retries")) {
-      containerRetryOptions.add("--container_max_retries "
-          + cliParser.getOptionValue("container_max_retries"));
-    }
-    if (cliParser.hasOption("container_retry_interval")) {
-      containerRetryOptions.add("--container_retry_interval "
-          + cliParser.getOptionValue("container_retry_interval"));
-    }
-    if (cliParser.hasOption("container_failures_validity_interval")) {
-      containerRetryOptions.add("--container_failures_validity_interval "
-          + cliParser.getOptionValue("container_failures_validity_interval"));
-    }
-
-    if (cliParser.hasOption("flow_name")) {
-      flowName = cliParser.getOptionValue("flow_name");
-    }
-    if (cliParser.hasOption("flow_version")) {
-      flowVersion = cliParser.getOptionValue("flow_version");
-    }
-    if (cliParser.hasOption("flow_run_id")) {
-      try {
-        flowRunId = Long.parseLong(cliParser.getOptionValue("flow_run_id"));
-      } catch (NumberFormatException e) {
-        throw new IllegalArgumentException(
-            "Flow run is not a valid long value", e);
-      }
-    }
-    if (cliParser.hasOption("docker_client_config")) {
-      dockerClientConfig = cliParser.getOptionValue("docker_client_config");
-    }
-
-    if (cliParser.hasOption("application_tags")) {
-      String applicationTagsStr = cliParser.getOptionValue("application_tags");
-      String[] appTags = applicationTagsStr.split(",");
-      for (String appTag : appTags) {
-        this.applicationTags.add(appTag.trim());
-      }
-    }
-
-    if (cliParser.hasOption("localize_files")) {
-      String filesStr = cliParser.getOptionValue("localize_files");
-      if (filesStr.contains(",")) {
-        String[] files = filesStr.split(",");
-        filesToLocalize = Arrays.asList(files);
-      } else {
-        filesToLocalize.add(filesStr);
-      }
-    }
-
-    return true;
-  }
-
-  /**
-   * Main run function for the client
-   * @return true if application completed successfully
-   * @throws IOException
-   * @throws YarnException
-   */
-  public boolean run() throws IOException, YarnException {
-    LOG.info("Running Client");
-    isRunning.set(true);
-    yarnClient.start();
-    // set the client start time.
-    clientStartTime = System.currentTimeMillis();
-
-    YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();
-    LOG.info("Got Cluster metric info from ASM" 
-        + ", numNodeManagers=" + clusterMetrics.getNumNodeManagers());
-
-    List<NodeReport> clusterNodeReports = yarnClient.getNodeReports(
-        NodeState.RUNNING);
-    LOG.info("Got Cluster node info from ASM");
-    for (NodeReport node : clusterNodeReports) {
-      LOG.info("Got node report from ASM for"
-          + ", nodeId=" + node.getNodeId() 
-          + ", nodeAddress=" + node.getHttpAddress()
-          + ", nodeRackName=" + node.getRackName()
-          + ", nodeNumContainers=" + node.getNumContainers());
-    }
-
-    QueueInfo queueInfo = yarnClient.getQueueInfo(this.amQueue);
-    if (queueInfo == null) {
-      throw new IllegalArgumentException(String
-          .format("Queue %s not present in scheduler configuration.",
-              this.amQueue));
-    }
-
-    LOG.info("Queue info"
-        + ", queueName=" + queueInfo.getQueueName()
-        + ", queueCurrentCapacity=" + queueInfo.getCurrentCapacity()
-        + ", queueMaxCapacity=" + queueInfo.getMaximumCapacity()
-        + ", queueApplicationCount=" + queueInfo.getApplications().size()
-        + ", queueChildQueueCount=" + queueInfo.getChildQueues().size());
-
-    List<QueueUserACLInfo> listAclInfo = yarnClient.getQueueAclsInfo();
-    for (QueueUserACLInfo aclInfo : listAclInfo) {
-      for (QueueACL userAcl : aclInfo.getUserAcls()) {
-        LOG.info("User ACL Info for Queue"
-            + ", queueName=" + aclInfo.getQueueName()
-            + ", userAcl=" + userAcl.name());
-      }
-    }
-
-    if (domainId != null && domainId.length() > 0 && toCreateDomain) {
-      prepareTimelineDomain();
-    }
-
-    Map<String, Resource> profiles;
-    try {
-      profiles = yarnClient.getResourceProfiles();
-    } catch (YARNFeatureNotEnabledException re) {
-      profiles = null;
-    }
-
-    List<String> appProfiles = new ArrayList<>(2);
-    appProfiles.add(amResourceProfile);
-    appProfiles.add(containerResourceProfile);
-    for (String appProfile : appProfiles) {
-      if (appProfile != null && !appProfile.isEmpty()) {
-        if (profiles == null) {
-          String message = "Resource profiles is not enabled";
-          LOG.error(message);
-          throw new IOException(message);
-        }
-        if (!profiles.containsKey(appProfile)) {
-          String message = "Unknown resource profile '" + appProfile
-              + "'. Valid resource profiles are " + profiles.keySet();
-          LOG.error(message);
-          throw new IOException(message);
-        }
-      }
-    }
-
-    // Get a new application id
-    YarnClientApplication app = yarnClient.createApplication();
-    GetNewApplicationResponse appResponse = app.getNewApplicationResponse();
-    // TODO get min/max resource capabilities from RM and change memory ask if needed
-    // If we do not have min/max, we may not be able to correctly request 
-    // the required resources from the RM for the app master
-    // Memory ask has to be a multiple of min and less than max. 
-    // Dump out information about cluster capability as seen by the resource manager
-    long maxMem = appResponse.getMaximumResourceCapability().getMemorySize();
-    LOG.info("Max mem capability of resources in this cluster " + maxMem);
-
-    // A resource ask cannot exceed the max. 
-    if (amMemory > maxMem) {
-      LOG.info("AM memory specified above max threshold of cluster. Using max value."
-          + ", specified=" + amMemory
-          + ", max=" + maxMem);
-      amMemory = maxMem;
-    }
-
-    int maxVCores = appResponse.getMaximumResourceCapability().getVirtualCores();
-    LOG.info("Max virtual cores capability of resources in this cluster " + maxVCores);
-    
-    if (amVCores > maxVCores) {
-      LOG.info("AM virtual cores specified above max threshold of cluster. " 
-          + "Using max value." + ", specified=" + amVCores 
-          + ", max=" + maxVCores);
-      amVCores = maxVCores;
-    }
-    
-    // set the application name
-    ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();
-    applicationId = appContext.getApplicationId();
-
-    // Set up resource type requirements
-    // For now, both memory and vcores are supported, so we set memory and
-    // vcores requirements
-    List<ResourceTypeInfo> resourceTypes = yarnClient.getResourceTypeInfo();
-    setAMResourceCapability(appContext, profiles, resourceTypes);
-    setContainerResources(profiles, resourceTypes);
-
-    appContext.setKeepContainersAcrossApplicationAttempts(keepContainers);
-    appContext.setApplicationName(appName);
-
-    if (attemptFailuresValidityInterval >= 0) {
-      appContext
-        .setAttemptFailuresValidityInterval(attemptFailuresValidityInterval);
-    }
-
-    Set<String> tags = new HashSet<String>();
-    if (applicationTags != null) {
-      tags.addAll(applicationTags);
-    }
-    if (flowName != null) {
-      tags.add(TimelineUtils.generateFlowNameTag(flowName));
-    }
-    if (flowVersion != null) {
-      tags.add(TimelineUtils.generateFlowVersionTag(flowVersion));
-    }
-    if (flowRunId != 0) {
-      tags.add(TimelineUtils.generateFlowRunIdTag(flowRunId));
-    }
-    appContext.setApplicationTags(tags);
-
-    // set local resources for the application master
-    // local files or archives as needed
-    // In this scenario, the jar file for the application master is part of the local resources
-    Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();
-
-    LOG.info("Copy App Master jar from local filesystem and add to local environment");
-    // Copy the application master jar to the filesystem 
-    // Create a local resource to point to the destination jar path 
-    FileSystem fs = FileSystem.get(conf);
-    addToLocalResources(fs, appMasterJar, appMasterJarPath,
-        applicationId.toString(), localResources, null);
-
-    // Set the log4j properties if needed 
-    if (!log4jPropFile.isEmpty()) {
-      addToLocalResources(fs, log4jPropFile, log4jPath,
-          applicationId.toString(), localResources, null);
-    }
-
-    // Process local files for localization
-    // Here we just upload the files, the AM
-    // will set up localization later.
-    StringBuilder localizableFiles = new StringBuilder();
-    filesToLocalize.stream().forEach(path -> {
-      File f = new File(path);
-
-      if (!f.exists()) {
-        throw new UncheckedIOException(
-            new IOException(path + " does not exist"));
-      }
-
-      if (!f.canRead()) {
-        throw new UncheckedIOException(
-            new IOException(path + " cannot be read"));
-      }
-
-      if (f.isDirectory()) {
-        throw new UncheckedIOException(
-          new IOException(path + " is a directory"));
-      }
-
-      try {
-        String fileName = f.getName();
-        uploadFile(fs, path, fileName, applicationId.toString());
-        if (localizableFiles.length() == 0) {
-          localizableFiles.append(fileName);
-        } else {
-          localizableFiles.append(",").append(fileName);
-        }
-      } catch (IOException e) {
-        throw new UncheckedIOException("Cannot upload file: " + path, e);
-      }
-    });
-
-    // The shell script has to be made available on the final container(s)
-    // where it will be executed. 
-    // To do this, we need to first copy into the filesystem that is visible 
-    // to the yarn framework. 
-    // We do not need to set this as a local resource for the application 
-    // master as the application master does not need it.
-    String hdfsShellScriptLocation = ""; 
-    long hdfsShellScriptLen = 0;
-    long hdfsShellScriptTimestamp = 0;
-    if (!shellScriptPath.isEmpty()) {
-      Path shellSrc = new Path(shellScriptPath);
-      String shellPathSuffix =
-          ApplicationMaster.getRelativePath(appName,
-              applicationId.toString(),
-              SCRIPT_PATH);
-      Path shellDst =
-          new Path(fs.getHomeDirectory(), shellPathSuffix);
-      fs.copyFromLocalFile(false, true, shellSrc, shellDst);
-      hdfsShellScriptLocation = shellDst.toUri().toString(); 
-      FileStatus shellFileStatus = fs.getFileStatus(shellDst);
-      hdfsShellScriptLen = shellFileStatus.getLen();
-      hdfsShellScriptTimestamp = shellFileStatus.getModificationTime();
-    }
-
-    if (!shellCommand.isEmpty()) {
-      addToLocalResources(fs, null, shellCommandPath, applicationId.toString(),
-          localResources, shellCommand);
-    }
-
-    if (shellArgs.length > 0) {
-      addToLocalResources(fs, null, shellArgsPath, applicationId.toString(),
-          localResources, StringUtils.join(shellArgs, " "));
-    }
-
-    // Set the necessary security tokens as needed
-    //amContainer.setContainerTokens(containerToken);
-
-    // Set the env variables to be setup in the env where the application master will be run
-    LOG.info("Set the environment for the application master");
-    Map<String, String> env = new HashMap<String, String>();
-
-    // put location of shell script into env
-    // using the env info, the application master will create the correct local resource for the 
-    // eventual containers that will be launched to execute the shell scripts
-    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLOCATION, hdfsShellScriptLocation);
-    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTTIMESTAMP, Long.toString(hdfsShellScriptTimestamp));
-    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLEN, Long.toString(hdfsShellScriptLen));
-    if (domainId != null && domainId.length() > 0) {
-      env.put(DSConstants.DISTRIBUTEDSHELLTIMELINEDOMAIN, domainId);
-    }
-
-    // Add AppMaster.jar location to classpath
-    // At some point we should not be required to add 
-    // the hadoop specific classpaths to the env. 
-    // It should be provided out of the box. 
-    // For now setting all required classpaths including
-    // the classpath to "." for the application jar
-    StringBuilder classPathEnv = new StringBuilder(Environment.CLASSPATH.$$())
-      .append(ApplicationConstants.CLASS_PATH_SEPARATOR).append("./*");
-    for (String c : conf.getStrings(
-        YarnConfiguration.YARN_APPLICATION_CLASSPATH,
-        YarnConfiguration.DEFAULT_YARN_CROSS_PLATFORM_APPLICATION_CLASSPATH)) {
-      classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR)
-          .append(c.trim());
-    }
-    classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR).append(
-      "./log4j.properties");
-
-    // add the runtime classpath needed for tests to work
-    if (conf.getBoolean(YarnConfiguration.IS_MINI_YARN_CLUSTER, false)) {
-      classPathEnv.append(ApplicationConstants.CLASS_PATH_SEPARATOR)
-          .append(System.getProperty("java.class.path"));
-    }
-
-    env.put("CLASSPATH", classPathEnv.toString());
-
-    // Set the necessary command to execute the application master 
-    Vector<CharSequence> vargs = new Vector<CharSequence>(30);
-
-    // Set java executable command 
-    LOG.info("Setting up app master command");
-    // Need extra quote here because JAVA_HOME might contain space on Windows,
-    // e.g. C:/Program Files/Java...
-    vargs.add("\"" + Environment.JAVA_HOME.$$() + "/bin/java\"");
-    // Set Xmx based on am memory size
-    vargs.add("-Xmx" + amMemory + "m");
-    // JDK17 support
-    vargs.add(ApplicationConstants.JVM_ADD_OPENS_VAR);
-    // Set class name 
-    vargs.add(appMasterMainClass);
-    // Set params for Application Master
-    if (containerType != null) {
-      vargs.add("--container_type " + String.valueOf(containerType));
-    }
-    if (autoPromoteContainers) {
-      vargs.add("--promote_opportunistic_after_start");
-    }
-    if (enforceExecType) {
-      vargs.add("--enforce_execution_type");
-    }
-    if (containerMemory > 0) {
-      vargs.add("--container_memory " + String.valueOf(containerMemory));
-    }
-    if (containerVirtualCores > 0) {
-      vargs.add("--container_vcores " + String.valueOf(containerVirtualCores));
-    }
-    if (!containerResources.isEmpty()) {
-      Joiner.MapJoiner joiner = Joiner.on(',').withKeyValueSeparator("=");
-      vargs.add("--container_resources " + joiner.join(containerResources));
-    }
-    if (containerResourceProfile != null && !containerResourceProfile
-        .isEmpty()) {
-      vargs.add("--container_resource_profile " + containerResourceProfile);
-    }
-    vargs.add("--num_containers " + String.valueOf(numContainers));
-    if (placementSpec != null && placementSpec.length() > 0) {
-      // Encode the spec to avoid passing special chars via shell arguments.
-      String encodedSpec = Base64.getEncoder()
-          .encodeToString(placementSpec.getBytes(StandardCharsets.UTF_8));
-      LOG.info("Encode placement spec: " + encodedSpec);
-      vargs.add("--placement_spec " + encodedSpec);
-    }
-    if (null != nodeLabelExpression) {
-      appContext.setNodeLabelExpression(nodeLabelExpression);
-    }
-    vargs.add("--priority " + String.valueOf(shellCmdPriority));
-
-    if (keepContainers) {
-      vargs.add("--keep_containers_across_application_attempts");
-    }
-    for (Map.Entry<String, String> entry : shellEnv.entrySet()) {
-      vargs.add("--shell_env " + entry.getKey() + "=" + entry.getValue());
-    }
-    if (debugFlag) {
-      vargs.add("--debug");
-    }
-    if (localizableFiles.length() > 0) {
-      vargs.add("--localized_files " + localizableFiles.toString());
-    }
-    vargs.add("--appname " + appName);
-
-    vargs.add("--homedir " + fs.getHomeDirectory());
-
-    vargs.addAll(containerRetryOptions);
-
-    vargs.add("1>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/AppMaster.stdout");
-    vargs.add("2>" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + "/AppMaster.stderr");
-
-    // Get final commmand
-    StringBuilder command = new StringBuilder();
-    for (CharSequence str : vargs) {
-      command.append(str).append(" ");
-    }
-
-    LOG.info("Completed setting up app master command " + command.toString());
-    List<String> commands = new ArrayList<String>();
-    commands.add(command.toString());
-
-    // Set up the container launch context for the application master
-    ContainerLaunchContext amContainer = ContainerLaunchContext.newInstance(
-      localResources, env, commands, null, null, null);
-
-    // Service data is a binary blob that can be passed to the application
-    // Not needed in this scenario
-    // amContainer.setServiceData(serviceData);
-
-    // Setup security tokens
-    Credentials rmCredentials = null;
-    if (UserGroupInformation.isSecurityEnabled()) {
-      // Note: Credentials class is marked as LimitedPrivate for HDFS and MapReduce
-      rmCredentials = new Credentials();
-      String tokenRenewer = YarnClientUtils.getRmPrincipal(conf);
-      if (tokenRenewer == null || tokenRenewer.length() == 0) {
-        throw new IOException(
-          "Can't get Master Kerberos principal for the RM to use as renewer");
-      }
-
-      // For now, only getting tokens for the default file-system.
-      final Token<?> tokens[] =
-          fs.addDelegationTokens(tokenRenewer, rmCredentials);
-      if (tokens != null) {
-        for (Token<?> token : tokens) {
-          LOG.info("Got dt for " + fs.getUri() + "; " + token);
-        }
-      }
-    }
-
-    // Add the docker client config credentials if supplied.
-    Credentials dockerCredentials = null;
-    if (dockerClientConfig != null) {
-      dockerCredentials =
-          DockerClientConfigHandler.readCredentialsFromConfigFile(
-              new Path(dockerClientConfig), conf, applicationId.toString());
-    }
-
-    if (rmCredentials != null || dockerCredentials != null) {
-      DataOutputBuffer dob = new DataOutputBuffer();
-      if (rmCredentials != null) {
-        rmCredentials.writeTokenStorageToStream(dob);
-      }
-      if (dockerCredentials != null) {
-        dockerCredentials.writeTokenStorageToStream(dob);
-      }
-      ByteBuffer tokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());
-      amContainer.setTokens(tokens);
-    }
-
-    appContext.setAMContainerSpec(amContainer);
-
-    // Set the priority for the application master
-    // TODO - what is the range for priority? how to decide? 
-    Priority pri = Priority.newInstance(amPriority);
-    appContext.setPriority(pri);
-
-    // Set the queue to which this application is to be submitted in the RM
-    appContext.setQueue(amQueue);
-
-    specifyLogAggregationContext(appContext);
-
-    // Submit the application to the applications manager
-    // SubmitApplicationResponse submitResp = applicationsManager.submitApplication(appRequest);
-    // Ignore the response as either a valid response object is returned on success 
-    // or an exception thrown to denote some form of a failure
-    LOG.info("Submitting application to ASM");
-
-    yarnClient.submitApplication(appContext);
-
-    // TODO
-    // Try submitting the same request again
-    // app submission failure?
-
-    // Monitor the application
-    return monitorApplication(applicationId);
-
-  }
-
-  @VisibleForTesting
-  void specifyLogAggregationContext(ApplicationSubmissionContext appContext) {
-    if (!rollingFilesPattern.isEmpty()) {
-      LogAggregationContext logAggregationContext = LogAggregationContext
-          .newInstance(null, null, rollingFilesPattern, "");
-      appContext.setLogAggregationContext(logAggregationContext);
-    }
-  }
-
-  /**
-   * Monitor the submitted application for completion. 
-   * Kill application if time expires. 
-   * @param appId Application Id of application to be monitored
-   * @return true if application completed successfully
-   * @throws YarnException
-   * @throws IOException
-   */
-  private boolean monitorApplication(ApplicationId appId)
-      throws YarnException, IOException {
-
-    boolean res = false;
-    boolean needForceKill = false;
-    while (isRunning.get()) {
-      // Check app status every 1 second.
-      try {
-        synchronized (objectLock) {
-          objectLock.wait(APP_MONITOR_INTERVAL);
-        }
-        needForceKill = stopSignalReceived.get();
-      } catch (InterruptedException e) {
-        LOG.warn("Thread sleep in monitoring loop interrupted");
-        // if the application is to be killed when client times out;
-        // then set needForceKill to true
-        break;
-      } finally {
-        if (needForceKill) {
-          break;
-        }
-      }
-
-      // Get application report for the appId we are interested in 
-      ApplicationReport report = yarnClient.getApplicationReport(appId);
-
-      LOG.info("Got application report from ASM for"
-          + ", appId=" + appId.getId()
-          + ", clientToAMToken=" + report.getClientToAMToken()
-          + ", appDiagnostics=" + report.getDiagnostics()
-          + ", appMasterHost=" + report.getHost()
-          + ", appQueue=" + report.getQueue()
-          + ", appMasterRpcPort=" + report.getRpcPort()
-          + ", appStartTime=" + report.getStartTime()
-          + ", yarnAppState=" + report.getYarnApplicationState().toString()
-          + ", distributedFinalState=" + report.getFinalApplicationStatus().toString()
-          + ", appTrackingUrl=" + report.getTrackingUrl()
-          + ", appUser=" + report.getUser());
-
-      YarnApplicationState state = report.getYarnApplicationState();
-      FinalApplicationStatus dsStatus = report.getFinalApplicationStatus();
-      if (YarnApplicationState.FINISHED == state) {
-        if (FinalApplicationStatus.SUCCEEDED == dsStatus) {
-          LOG.info("Application has completed successfully. "
-                  + "Breaking monitoring loop");
-          res = true;
-        } else {
-          LOG.info("Application did finished unsuccessfully. "
-                  + "YarnState={}, DSFinalStatus={}. Breaking monitoring loop",
-              state, dsStatus);
-        }
-        break;
-      } else if (YarnApplicationState.KILLED == state
-          || YarnApplicationState.FAILED == state) {
-        LOG.info("Application did not finish. YarnState={}, DSFinalStatus={}. "
-                + "Breaking monitoring loop", state, dsStatus);
-        break;
-      }
-
-      // The value equal or less than 0 means no timeout
-      if (clientTimeout > 0
-          && System.currentTimeMillis() > (clientStartTime + clientTimeout)) {
-        LOG.info("Reached client specified timeout for application. " +
-            "Killing application");
-        needForceKill = true;
-        break;
-      }
-    }
-
-    if (needForceKill) {
-      forceKillApplication(appId);
-    }
-
-    isRunning.set(false);
-
-    return res;
-  }
-
-  /**
-   * Kill a submitted application by sending a call to the ASM
-   * @param appId Application Id to be killed. 
-   * @throws YarnException
-   * @throws IOException
-   */
-  private void forceKillApplication(ApplicationId appId)
-      throws YarnException, IOException {
-    // TODO clarify whether multiple jobs with the same app id can be submitted and be running at 
-    // the same time. 
-    // If yes, can we kill a particular attempt only?
-
-    // Response can be ignored as it is non-null on success or 
-    // throws an exception in case of failures
-    yarnClient.killApplication(appId);
-  }
-
-  private void addToLocalResources(FileSystem fs, String fileSrcPath,
-      String fileDstPath, String appId, Map<String, LocalResource> localResources,
-      String resources) throws IOException {
-    String suffix =
-        ApplicationMaster.getRelativePath(appName, appId, fileDstPath);
-    Path dst =
-        new Path(fs.getHomeDirectory(), suffix);
-    if (fileSrcPath == null) {
-      try (FSDataOutputStream ostream = FileSystem.create(fs, dst,
-          new FsPermission((short) 0710))) {
-        ostream.writeUTF(resources);
-      }
-    } else {
-      fs.copyFromLocalFile(new Path(fileSrcPath), dst);
-    }
-    FileStatus scFileStatus = fs.getFileStatus(dst);
-    LocalResource scRsrc =
-        LocalResource.newInstance(
-            URL.fromURI(dst.toUri()),
-            LocalResourceType.FILE, LocalResourceVisibility.APPLICATION,
-            scFileStatus.getLen(), scFileStatus.getModificationTime());
-    localResources.put(fileDstPath, scRsrc);
-  }
-
-  private void uploadFile(FileSystem fs, String fileSrcPath,
-      String fileDstPath, String appId) throws IOException {
-    String relativePath =
-        ApplicationMaster.getRelativePath(appName, appId, fileDstPath);
-    Path dst =
-        new Path(fs.getHomeDirectory(), relativePath);
-    LOG.info("Uploading file: " + fileSrcPath + " to " + dst);
-    fs.copyFromLocalFile(new Path(fileSrcPath), dst);
-  }
-
-  @VisibleForTesting
-  ApplicationId getAppId() {
-    return applicationId;
-  }
-
-  private void prepareTimelineDomain() {
-    TimelineClient timelineClient = null;
-    if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED,
-        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_ENABLED)) {
-      timelineClient = TimelineClient.createTimelineClient();
-      timelineClient.init(conf);
-      timelineClient.start();
-    } else {
-      LOG.warn("Cannot put the domain " + domainId +
-          " because the timeline service is not enabled");
-      return;
-    }
-    try {
-      //TODO: we need to check and combine the existing timeline domain ACLs,
-      //but let's do it once we have client java library to query domains.
-      TimelineDomain domain = new TimelineDomain();
-      domain.setId(domainId);
-      domain.setReaders(
-          viewACLs != null && viewACLs.length() > 0 ? viewACLs : " ");
-      domain.setWriters(
-          modifyACLs != null && modifyACLs.length() > 0 ? modifyACLs : " ");
-      timelineClient.putDomain(domain);
-      LOG.info("Put the timeline domain: " +
-          TimelineUtils.dumpTimelineRecordtoJSON(domain));
-    } catch (Exception e) {
-      LOG.error("Error when putting the timeline domain", e);
-    } finally {
-      timelineClient.stop();
-    }
-  }
-
-  private void setAMResourceCapability(ApplicationSubmissionContext appContext,
-      Map<String, Resource> profiles, List<ResourceTypeInfo> resourceTypes)
-      throws IllegalArgumentException, IOException, YarnException {
-    if (amMemory < -1 || amMemory == 0) {
-      throw new IllegalArgumentException("Invalid memory specified for"
-          + " application master, exiting. Specified memory=" + amMemory);
-    }
-    if (amVCores < -1 || amVCores == 0) {
-      throw new IllegalArgumentException("Invalid virtual cores specified for"
-          + " application master, exiting. " +
-          "Specified virtual cores=" + amVCores);
-    }
-    Resource capability = Resource.newInstance(0, 0);
-
-    if (!amResourceProfile.isEmpty()) {
-      if (!profiles.containsKey(amResourceProfile)) {
-        throw new IllegalArgumentException(
-            "Failed to find specified resource profile for application master="
-                + amResourceProfile);
-      }
-      capability = Resources.clone(profiles.get(amResourceProfile));
-    }
-
-    if (appContext.getAMContainerResourceRequests() == null) {
-      List<ResourceRequest> amResourceRequests = new ArrayList<ResourceRequest>();
-      amResourceRequests
-          .add(ResourceRequest.newInstance(Priority.newInstance(amPriority),
-              "*", Resources.clone(Resources.none()), 1));
-      appContext.setAMContainerResourceRequests(amResourceRequests);
-    }
-
-    validateResourceTypes(amResources.keySet(), resourceTypes);
-    for (Map.Entry<String, Long> entry : amResources.entrySet()) {
-      capability.setResourceValue(entry.getKey(), entry.getValue());
-    }
-    // set amMemory because it's used to set Xmx param
-    if (amMemory == -1) {
-      amMemory = DEFAULT_AM_MEMORY;
-      LOG.warn("AM Memory not specified, use " + DEFAULT_AM_MEMORY
-          + " mb as AM memory");
-    }
-    if (amVCores == -1) {
-      amVCores = DEFAULT_AM_VCORES;
-      LOG.warn("AM vcore not specified, use " + DEFAULT_AM_VCORES
-          + " mb as AM vcores");
-    }
-    capability.setMemorySize(amMemory);
-    capability.setVirtualCores(amVCores);
-    appContext.getAMContainerResourceRequests().get(0).setCapability(
-        capability);
-    LOG.warn("AM Resource capability=" + capability);
-  }
-
-  private void setContainerResources(Map<String, Resource> profiles,
-      List<ResourceTypeInfo> resourceTypes) throws IllegalArgumentException {
-    if (containerMemory < -1 || containerMemory == 0) {
-      throw new IllegalArgumentException("Container memory '" +
-          containerMemory + "' has to be greated than 0");
-    }
-    if (containerVirtualCores < -1 || containerVirtualCores == 0) {
-      throw new IllegalArgumentException("Container vcores '" +
-          containerVirtualCores + "' has to be greated than 0");
-    }
-    validateResourceTypes(containerResources.keySet(), resourceTypes);
-    if (profiles == null) {
-      containerMemory = containerMemory == -1 ?
-          DEFAULT_CONTAINER_MEMORY : containerMemory;
-      containerVirtualCores = containerVirtualCores == -1 ?
-          DEFAULT_CONTAINER_VCORES : containerVirtualCores;
-    }
-  }
-
-  private void validateResourceTypes(Iterable<String> resourceNames,
-      List<ResourceTypeInfo> resourceTypes) {
-    for (String resourceName : resourceNames) {
-      if (!resourceTypes.stream().anyMatch(e ->
-          e.getName().equals(resourceName))) {
-        throw new ResourceNotFoundException("Unknown resource: " +
-            resourceName);
-      }
-    }
-  }
-
-  static Map<String, Long> parseResourcesString(String resourcesStr) {
-    Map<String, Long> resources = new HashMap<>();
-
-    // Ignore the grouping "[]"
-    if (resourcesStr.startsWith("[")) {
-      resourcesStr = resourcesStr.substring(1);
-    }
-    if (resourcesStr.endsWith("]")) {
-      resourcesStr = resourcesStr.substring(0, resourcesStr.length() - 1);
-    }
-
-    for (String resource : resourcesStr.trim().split(",")) {
-      resource = resource.trim();
-      if (!resource.matches("^[^=]+=\\d+\\s?\\w*$")) {
-        throw new IllegalArgumentException("\"" + resource + "\" is not a " +
-            "valid resource type/amount pair. " +
-            "Please provide key=amount pairs separated by commas.");
-      }
-      String[] splits = resource.split("=");
-      String key = splits[0], value = splits[1];
-      String units = ResourceUtils.getUnits(value);
-      String valueWithoutUnit = value.substring(
-          0, value.length() - units.length()).trim();
-      Long resourceValue = Long.valueOf(valueWithoutUnit);
-      if (!units.isEmpty()) {
-        resourceValue = UnitsConversionUtil.convert(units, "Mi", resourceValue);
-      }
-      if (key.equals("memory")) {
-        key = ResourceInformation.MEMORY_URI;
-      }
-      resources.put(key, resourceValue);
-    }
-    return resources;
-  }
-
-  @VisibleForTesting
-  protected void sendStopSignal() {
-    LOG.info("Sending stop Signal to Client");
-    stopSignalReceived.set(true);
-    synchronized (objectLock) {
-      objectLock.notifyAll();
-    }
-    int waitCount = 0;
-    LOG.info("Waiting for Client to exit loop");
-    while (isRunning.get()) {
-      try {
-        Thread.sleep(50);
-      } catch (InterruptedException ie) {
-        // do nothing
-      } finally {
-        if (++waitCount > 2000) {
-          break;
-        }
-      }
-    }
-    LOG.info("Stopping yarnClient within the DS Client");
-    yarnClient.stop();
-    LOG.info("done stopping Client");
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
deleted file mode 100644
index fbaf2d47fa1..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DSConstants.java
+++ /dev/null
@@ -1,52 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Constants used in both Client and Application Master
- */
-@InterfaceAudience.Public
-@InterfaceStability.Unstable
-public class DSConstants {
-
-  /**
-   * Environment key name pointing to the shell script's location
-   */
-  public static final String DISTRIBUTEDSHELLSCRIPTLOCATION = "DISTRIBUTEDSHELLSCRIPTLOCATION";
-
-  /**
-   * Environment key name denoting the file timestamp for the shell script. 
-   * Used to validate the local resource. 
-   */
-  public static final String DISTRIBUTEDSHELLSCRIPTTIMESTAMP = "DISTRIBUTEDSHELLSCRIPTTIMESTAMP";
-
-  /**
-   * Environment key name denoting the file content length for the shell script. 
-   * Used to validate the local resource. 
-   */
-  public static final String DISTRIBUTEDSHELLSCRIPTLEN = "DISTRIBUTEDSHELLSCRIPTLEN";
-
-  /**
-   * Environment key name denoting the timeline domain ID.
-   */
-  public static final String DISTRIBUTEDSHELLTIMELINEDOMAIN = "DISTRIBUTEDSHELLTIMELINEDOMAIN";
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellTimelinePlugin.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellTimelinePlugin.java
deleted file mode 100644
index 89eea4b67a9..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellTimelinePlugin.java
+++ /dev/null
@@ -1,78 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.ContainerId;
-import org.apache.hadoop.yarn.api.records.timeline.TimelineEntityGroupId;
-import org.apache.hadoop.yarn.server.timeline.NameValuePair;
-import org.apache.hadoop.yarn.server.timeline.TimelineEntityGroupPlugin;
-
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.Set;
-import java.util.SortedSet;
-
-/**
- * Timeline v1.5 reader plugin for YARN distributed shell. It tranlsates an
- * incoming getEntity request to a set of related timeline entity groups, via
- * the information provided in the primary filter or entity id field.
- */
-public class DistributedShellTimelinePlugin extends TimelineEntityGroupPlugin {
-
-  @Override
-  public Set<TimelineEntityGroupId> getTimelineEntityGroupId(String entityType,
-      NameValuePair primaryFilter, Collection<NameValuePair> secondaryFilters) {
-    if (ApplicationMaster.DSEntity.DS_CONTAINER.toString().equals(entityType)) {
-      if (primaryFilter == null) {
-        return null;
-      }
-      return toEntityGroupId(primaryFilter.getValue().toString());
-    }
-    return null;
-  }
-
-  @Override
-  public Set<TimelineEntityGroupId> getTimelineEntityGroupId(String entityId,
-      String entityType) {
-    if (ApplicationMaster.DSEntity.DS_CONTAINER.toString().equals(entityType)) {
-      ContainerId containerId = ContainerId.fromString(entityId);
-      ApplicationId appId = containerId.getApplicationAttemptId()
-          .getApplicationId();
-      return toEntityGroupId(appId.toString());
-    }
-    return null;
-  }
-
-  @Override
-  public Set<TimelineEntityGroupId> getTimelineEntityGroupId(String entityType,
-      SortedSet<String> entityIds, Set<String> eventTypes) {
-    // Right now this method is not used by TimelineEntityGroupPlugin
-    return null;
-  }
-
-  private Set<TimelineEntityGroupId> toEntityGroupId(String strAppId) {
-    ApplicationId appId = ApplicationId.fromString(strAppId);
-    TimelineEntityGroupId groupId = TimelineEntityGroupId.newInstance(
-        appId, ApplicationMaster.CONTAINER_ENTITY_GROUP_ID);
-    Set<TimelineEntityGroupId> result = new HashSet<>();
-    result.add(groupId);
-    return result;
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Log4jPropertyHelper.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Log4jPropertyHelper.java
deleted file mode 100644
index 0301a6880f8..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/Log4jPropertyHelper.java
+++ /dev/null
@@ -1,48 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.io.FileInputStream;
-import java.io.InputStream;
-import java.util.Map.Entry;
-import java.util.Properties;
-
-import org.apache.log4j.LogManager;
-import org.apache.log4j.PropertyConfigurator;
-
-public class Log4jPropertyHelper {
-
-  public static void updateLog4jConfiguration(Class<?> targetClass,
-      String log4jPath) throws Exception {
-    Properties customProperties = new Properties();
-    try (
-        FileInputStream fs = new FileInputStream(log4jPath);
-        InputStream is = targetClass.getResourceAsStream("/log4j.properties")) {
-      customProperties.load(fs);
-      Properties originalProperties = new Properties();
-      originalProperties.load(is);
-      for (Entry<Object, Object> entry : customProperties.entrySet()) {
-        originalProperties.setProperty(entry.getKey().toString(), entry
-            .getValue().toString());
-      }
-      LogManager.resetConfiguration();
-      PropertyConfigurator.configure(originalProperties);
-    }
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/PlacementSpec.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/PlacementSpec.java
deleted file mode 100644
index 7c6daf6d21b..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/PlacementSpec.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import org.apache.hadoop.yarn.api.resource.PlacementConstraint;
-import org.apache.hadoop.yarn.util.constraint.PlacementConstraintParseException;
-import org.apache.hadoop.yarn.util.constraint.PlacementConstraintParser;
-import org.apache.hadoop.yarn.util.constraint.PlacementConstraintParser.SourceTags;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Class encapsulating a SourceTag, number of container and a Placement
- * Constraint.
- */
-public class PlacementSpec {
-
-  private static final Logger LOG =
-      LoggerFactory.getLogger(PlacementSpec.class);
-
-  public final String sourceTag;
-  public final PlacementConstraint constraint;
-  private int numContainers;
-
-  public PlacementSpec(String sourceTag, int numContainers,
-      PlacementConstraint constraint) {
-    this.sourceTag = sourceTag;
-    this.numContainers = numContainers;
-    this.constraint = constraint;
-  }
-
-  /**
-   * Get the number of container for this spec.
-   * @return container count
-   */
-  public int getNumContainers() {
-    return numContainers;
-  }
-
-  /**
-   * Set number of containers for this spec.
-   * @param numContainers number of containers.
-   */
-  public void setNumContainers(int numContainers) {
-    this.numContainers = numContainers;
-  }
-
-  // Placement specification should be of the form:
-  // PlacementSpec => ""|KeyVal;PlacementSpec
-  // KeyVal => SourceTag=Constraint
-  // SourceTag => String
-  // Constraint => NumContainers|
-  //               NumContainers,"in",Scope,TargetTag|
-  //               NumContainers,"notin",Scope,TargetTag|
-  //               NumContainers,"cardinality",Scope,TargetTag,MinCard,MaxCard
-  // NumContainers => int (number of containers)
-  // Scope => "NODE"|"RACK"
-  // TargetTag => String (Target Tag)
-  // MinCard => int (min cardinality - needed if ConstraintType == cardinality)
-  // MaxCard => int (max cardinality - needed if ConstraintType == cardinality)
-
-  /**
-   * Parser to convert a string representation of a placement spec to mapping
-   * from source tag to Placement Constraint.
-   *
-   * @param specs Placement spec.
-   * @return Mapping from source tag to placement constraint.
-   */
-  public static Map<String, PlacementSpec> parse(String specs)
-      throws IllegalArgumentException {
-    LOG.info("Parsing Placement Specs: [{}]", specs);
-
-    Map<String, PlacementSpec> pSpecs = new HashMap<>();
-    Map<SourceTags, PlacementConstraint> parsed;
-    try {
-      parsed = PlacementConstraintParser.parsePlacementSpec(specs);
-      for (Map.Entry<SourceTags, PlacementConstraint> entry :
-          parsed.entrySet()) {
-        LOG.info("Parsed source tag: {}, number of allocations: {}",
-            entry.getKey().getTag(), entry.getKey().getNumOfAllocations());
-        if (entry.getValue() != null) {
-          LOG.info("Parsed constraint: {}", entry.getValue()
-              .getConstraintExpr().getClass().getSimpleName());
-        } else {
-          LOG.info("Parsed constraint Empty");
-        }
-        pSpecs.put(entry.getKey().getTag(), new PlacementSpec(
-            entry.getKey().getTag(),
-            entry.getKey().getNumOfAllocations(),
-            entry.getValue()));
-      }
-      return pSpecs;
-    } catch (PlacementConstraintParseException e) {
-      throw new IllegalArgumentException(
-          "Invalid placement spec: " + specs, e);
-    }
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/package-info.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/package-info.java
deleted file mode 100644
index 299a2867486..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/package-info.java
+++ /dev/null
@@ -1,19 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
\ No newline at end of file
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/ContainerLaunchFailAppMaster.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/ContainerLaunchFailAppMaster.java
deleted file mode 100644
index 1d41c0d4e56..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/ContainerLaunchFailAppMaster.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.nio.ByteBuffer;
-import java.util.Map;
-
-import org.apache.hadoop.yarn.api.records.ContainerId;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class ContainerLaunchFailAppMaster extends ApplicationMaster {
-
-  private static final Logger LOG =
-    LoggerFactory.getLogger(ContainerLaunchFailAppMaster.class);
-
-  public ContainerLaunchFailAppMaster() {
-    super();
-  }
-
-  @Override
-  NMCallbackHandler createNMCallbackHandler() {
-    return new FailContainerLaunchNMCallbackHandler(this);
-  }
-
-  class FailContainerLaunchNMCallbackHandler
-    extends ApplicationMaster.NMCallbackHandler {
-
-    public FailContainerLaunchNMCallbackHandler(
-      ApplicationMaster applicationMaster) {
-      super(applicationMaster);
-    }
-
-    @Override
-    public void onContainerStarted(ContainerId containerId,
-                                   Map<String, ByteBuffer> allServiceResponse) {
-      super.onStartContainerError(containerId,
-        new RuntimeException("Inject Container Launch failure"));
-    }
-
-  }
-
-  public static void main(String[] args) {
-    boolean result = false;
-    try {
-      ContainerLaunchFailAppMaster appMaster =
-        new ContainerLaunchFailAppMaster();
-      LOG.info("Initializing ApplicationMaster");
-      boolean doRun = appMaster.init(args);
-      if (!doRun) {
-        System.exit(0);
-      }
-      appMaster.run();
-      result = appMaster.finish();
-    } catch (Throwable t) {
-      LOG.error("Error running ApplicationMaster", t);
-      System.exit(1);
-    }
-    if (result) {
-      LOG.info("Application Master completed successfully. exiting");
-      System.exit(0);
-    } else {
-      LOG.info("Application Master failed. exiting");
-      System.exit(2);
-    }
-  }
-
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellBaseTest.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellBaseTest.java
deleted file mode 100644
index 28cdf8f8223..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/DistributedShellBaseTest.java
+++ /dev/null
@@ -1,607 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.net.URL;
-import java.nio.file.Files;
-import java.nio.file.Paths;
-import java.nio.file.StandardCopyOption;
-import java.util.List;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicReference;
-import java.util.function.Supplier;
-
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Rule;
-import org.junit.rules.TemporaryFolder;
-import org.junit.rules.TestName;
-import org.junit.rules.Timeout;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileContext;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.hdfs.HdfsConfiguration;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.net.ServerSocketUtil;
-import org.apache.hadoop.test.GenericTestUtils;
-import org.apache.hadoop.util.JarFinder;
-import org.apache.hadoop.util.Shell;
-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
-import org.apache.hadoop.yarn.api.records.ApplicationAttemptReport;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.ApplicationReport;
-import org.apache.hadoop.yarn.api.records.ContainerReport;
-import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;
-import org.apache.hadoop.yarn.api.records.YarnApplicationState;
-import org.apache.hadoop.yarn.api.records.timeline.TimelineDomain;
-import org.apache.hadoop.yarn.api.records.timeline.TimelineEntities;
-import org.apache.hadoop.yarn.api.records.timeline.TimelineEntity;
-import org.apache.hadoop.yarn.client.api.YarnClient;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.server.MiniYARNCluster;
-import org.apache.hadoop.yarn.server.nodemanager.NodeManager;
-import org.apache.hadoop.yarn.server.resourcemanager.RMContext;
-import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;
-import org.apache.hadoop.yarn.server.timeline.NameValuePair;
-import org.apache.hadoop.yarn.util.LinuxResourceCalculatorPlugin;
-import org.apache.hadoop.yarn.util.ProcfsBasedProcessTree;
-
-/**
- * Base class for testing DistributedShell features.
- */
-public abstract class DistributedShellBaseTest {
-  protected static final int MIN_ALLOCATION_MB = 128;
-  protected static final int NUM_DATA_NODES = 1;
-  protected static final int TEST_TIME_OUT = 160000;
-  // set the timeout of the yarnClient to be 95% of the globalTimeout.
-  protected static final int TEST_TIME_WINDOW_EXPIRE =
-      (TEST_TIME_OUT * 90) / 100;
-  private static final Logger LOG =
-      LoggerFactory.getLogger(DistributedShellBaseTest.class);
-  private static final String APP_MASTER_JAR =
-      JarFinder.getJar(ApplicationMaster.class);
-  private static final int NUM_NMS = 1;
-  // set the timeout of the yarnClient to be 95% of the globalTimeout.
-  private static final String YARN_CLIENT_TIMEOUT =
-      String.valueOf(TEST_TIME_WINDOW_EXPIRE);
-  private static final String[] COMMON_ARGS = {
-      "--jar",
-      APP_MASTER_JAR,
-      "--timeout",
-      YARN_CLIENT_TIMEOUT,
-      "--appname",
-      ""
-  };
-  private static MiniDFSCluster hdfsCluster = null;
-  private static MiniYARNCluster yarnCluster = null;
-  private static String yarnSiteBackupPath = null;
-  private static String yarnSitePath = null;
-  @Rule
-  public Timeout globalTimeout = new Timeout(TEST_TIME_OUT,
-      TimeUnit.MILLISECONDS);
-  @Rule
-  public TemporaryFolder tmpFolder = new TemporaryFolder();
-  @Rule
-  public TestName name = new TestName();
-  private Client dsClient;
-  private YarnConfiguration conf = null;
-  // location of the filesystem timeline writer for timeline service v.2
-  private String timelineV2StorageDir = null;
-
-  @BeforeClass
-  public static void setupUnitTests() throws Exception {
-    URL url = Thread.currentThread().getContextClassLoader().getResource(
-        "yarn-site.xml");
-    if (url == null) {
-      throw new RuntimeException(
-          "Could not find 'yarn-site.xml' dummy file in classpath");
-    }
-    // backup the original yarn-site file.
-    yarnSitePath = url.getPath();
-    yarnSiteBackupPath = url.getPath() + "-backup";
-    Files.copy(Paths.get(yarnSitePath),
-        Paths.get(yarnSiteBackupPath),
-        StandardCopyOption.COPY_ATTRIBUTES,
-        StandardCopyOption.REPLACE_EXISTING);
-  }
-
-  @AfterClass
-  public static void tearDownUnitTests() throws Exception {
-    // shutdown the clusters.
-    shutdownYarnCluster();
-    shutdownHdfsCluster();
-    if (yarnSitePath == null || yarnSiteBackupPath == null) {
-      return;
-    }
-    // restore the original yarn-site file.
-    if (Files.exists(Paths.get(yarnSiteBackupPath))) {
-      Files.move(Paths.get(yarnSiteBackupPath), Paths.get(yarnSitePath),
-          StandardCopyOption.REPLACE_EXISTING);
-    }
-  }
-
-  /**
-   * Utility function to merge two String arrays to form a new String array for
-   * our arguments.
-   *
-   * @param args the first set of the arguments.
-   * @param newArgs the second set of the arguments.
-   * @return a String array consists of {args, newArgs}
-   */
-  protected static String[] mergeArgs(String[] args, String[] newArgs) {
-    int length = args.length + newArgs.length;
-    String[] result = new String[length];
-    System.arraycopy(args, 0, result, 0, args.length);
-    System.arraycopy(newArgs, 0, result, args.length, newArgs.length);
-    return result;
-  }
-
-  protected static String[] createArguments(Supplier<String> testNameProvider,
-      String... args) {
-    String[] res = mergeArgs(COMMON_ARGS, args);
-    // set the application name so we can track down which command is running.
-    res[COMMON_ARGS.length - 1] = testNameProvider.get();
-    return res;
-  }
-
-  protected static String getSleepCommand(int sec) {
-    // Windows doesn't have a sleep command, ping -n does the trick
-    return Shell.WINDOWS ? "ping -n " + (sec + 1) + " 127.0.0.1 >nul"
-        : "sleep " + sec;
-  }
-
-  protected static String getListCommand() {
-    return Shell.WINDOWS ? "dir" : "ls";
-  }
-
-  protected static String getCatCommand() {
-    return Shell.WINDOWS ? "type" : "cat";
-  }
-
-  protected static void shutdownYarnCluster() {
-    if (yarnCluster != null) {
-      try {
-        yarnCluster.stop();
-      } finally {
-        yarnCluster = null;
-      }
-    }
-  }
-
-  protected static void shutdownHdfsCluster() {
-    if (hdfsCluster != null) {
-      try {
-        hdfsCluster.shutdown();
-      } finally {
-        hdfsCluster = null;
-      }
-    }
-  }
-
-  public String getTimelineV2StorageDir() {
-    return timelineV2StorageDir;
-  }
-
-  public void setTimelineV2StorageDir() throws Exception {
-    timelineV2StorageDir = tmpFolder.newFolder().getAbsolutePath();
-  }
-
-  @Before
-  public void setup() throws Exception {
-    setupInternal(NUM_NMS, new YarnConfiguration());
-  }
-
-  @After
-  public void tearDown() throws IOException {
-    cleanUpDFSClient();
-    FileContext fsContext = FileContext.getLocalFSFileContext();
-    fsContext
-        .delete(
-            new Path(conf.get(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_PATH)),
-            true);
-    shutdownYarnCluster();
-    shutdownHdfsCluster();
-  }
-
-  protected String[] createArgumentsWithAppName(String... args) {
-    return createArguments(() -> generateAppName(), args);
-  }
-
-  protected void waitForContainersLaunch(YarnClient client, int nContainers,
-      AtomicReference<ApplicationAttemptReport> appAttemptReportRef,
-      AtomicReference<List<ContainerReport>> containersListRef,
-      AtomicReference<ApplicationAttemptId> appAttemptIdRef,
-      AtomicReference<Throwable> thrownErrorRef) throws Exception {
-    GenericTestUtils.waitFor(() -> {
-      try {
-        List<ApplicationReport> apps = client.getApplications();
-        if (apps == null || apps.isEmpty()) {
-          return false;
-        }
-        ApplicationId appId = apps.get(0).getApplicationId();
-        List<ApplicationAttemptReport> appAttempts =
-            client.getApplicationAttempts(appId);
-        if (appAttempts == null || appAttempts.isEmpty()) {
-          return false;
-        }
-        ApplicationAttemptId attemptId =
-            appAttempts.get(0).getApplicationAttemptId();
-        List<ContainerReport> containers = client.getContainers(attemptId);
-        if (containers == null || containers.size() < nContainers) {
-          return false;
-        }
-        containersListRef.set(containers);
-        appAttemptIdRef.set(attemptId);
-        appAttemptReportRef.set(appAttempts.get(0));
-      } catch (Exception e) {
-        LOG.error("Exception waiting for Containers Launch", e);
-        thrownErrorRef.set(e);
-      }
-      return true;
-    }, 10, TEST_TIME_WINDOW_EXPIRE);
-  }
-
-  protected abstract void customizeConfiguration(YarnConfiguration config)
-      throws Exception;
-
-  protected String[] appendFlowArgsForTestDSShell(String[] args,
-      boolean defaultFlow) {
-    return args;
-  }
-
-  protected String[] appendDomainArgsForTestDSShell(String[] args,
-      boolean haveDomain) {
-    String[] result = args;
-    if (haveDomain) {
-      String[] domainArgs = {
-          "--domain",
-          "TEST_DOMAIN",
-          "--view_acls",
-          "reader_user reader_group",
-          "--modify_acls",
-          "writer_user writer_group",
-          "--create"
-      };
-      result = mergeArgs(args, domainArgs);
-    }
-    return result;
-  }
-
-  protected Client setAndGetDSClient(Configuration config) throws Exception {
-    dsClient = new Client(config);
-    return dsClient;
-  }
-
-  protected Client setAndGetDSClient(String appMasterMainClass,
-      Configuration config) throws Exception {
-    dsClient = new Client(appMasterMainClass, config);
-    return dsClient;
-  }
-
-  protected void baseTestDSShell(boolean haveDomain, boolean defaultFlow)
-      throws Exception {
-    String[] baseArgs = createArgumentsWithAppName(
-        "--num_containers",
-        "2",
-        "--shell_command",
-        getListCommand(),
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1");
-    String[] domainArgs = appendDomainArgsForTestDSShell(baseArgs, haveDomain);
-    String[] args = appendFlowArgsForTestDSShell(domainArgs, defaultFlow);
-
-    LOG.info("Initializing DS Client");
-    YarnClient yarnClient;
-    dsClient = setAndGetDSClient(new Configuration(yarnCluster.getConfig()));
-    boolean initSuccess = dsClient.init(args);
-    Assert.assertTrue(initSuccess);
-    LOG.info("Running DS Client");
-    final AtomicBoolean result = new AtomicBoolean(false);
-    Thread t = new Thread(() -> {
-      try {
-        result.set(dsClient.run());
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    });
-    t.start();
-
-    yarnClient = YarnClient.createYarnClient();
-    yarnClient.init(new Configuration(yarnCluster.getConfig()));
-    yarnClient.start();
-
-    AtomicInteger waitResult = new AtomicInteger(0);
-    AtomicReference<ApplicationId> appIdRef =
-        new AtomicReference<>(null);
-    AtomicReference<ApplicationReport> appReportRef =
-        new AtomicReference<>(null);
-    GenericTestUtils.waitFor(() -> {
-      try {
-        List<ApplicationReport> apps = yarnClient.getApplications();
-        if (apps.size() == 0) {
-          return false;
-        }
-        ApplicationReport appReport = apps.get(0);
-        appReportRef.set(appReport);
-        appIdRef.set(appReport.getApplicationId());
-        if (appReport.getHost().equals("N/A")) {
-          return false;
-        }
-        if (appReport.getRpcPort() == -1) {
-          waitResult.set(1);
-        }
-        if (appReport.getYarnApplicationState() == YarnApplicationState.FINISHED
-            && appReport.getFinalApplicationStatus() !=
-            FinalApplicationStatus.UNDEFINED) {
-          return true;
-        }
-      } catch (Exception e) {
-        LOG.error("Exception get application from Yarn Client", e);
-        waitResult.set(2);
-      }
-      return waitResult.get() != 0;
-    }, 10, TEST_TIME_WINDOW_EXPIRE);
-    t.join();
-    if (waitResult.get() == 2) {
-      // Exception was raised
-      Assert.fail("Exception in getting application report. Failed");
-    }
-    if (waitResult.get() == 1) {
-      Assert.assertEquals("Failed waiting for expected rpc port to be -1.",
-          -1, appReportRef.get().getRpcPort());
-    }
-    checkTimeline(appIdRef.get(), defaultFlow, haveDomain, appReportRef.get());
-  }
-
-  protected void baseTestDSShell(boolean haveDomain) throws Exception {
-    baseTestDSShell(haveDomain, true);
-  }
-
-  protected void checkTimeline(ApplicationId appId,
-      boolean defaultFlow, boolean haveDomain,
-      ApplicationReport appReport) throws Exception {
-    TimelineDomain domain = null;
-    if (haveDomain) {
-      domain = yarnCluster.getApplicationHistoryServer()
-          .getTimelineStore().getDomain("TEST_DOMAIN");
-      Assert.assertNotNull(domain);
-      Assert.assertEquals("reader_user reader_group", domain.getReaders());
-      Assert.assertEquals("writer_user writer_group", domain.getWriters());
-    }
-    TimelineEntities entitiesAttempts = yarnCluster
-        .getApplicationHistoryServer()
-        .getTimelineStore()
-        .getEntities(ApplicationMaster.DSEntity.DS_APP_ATTEMPT.toString(),
-            null, null, null, null, null, null, null, null, null);
-    Assert.assertNotNull(entitiesAttempts);
-    Assert.assertEquals(1, entitiesAttempts.getEntities().size());
-    Assert.assertEquals(2, entitiesAttempts.getEntities().get(0).getEvents()
-        .size());
-    Assert.assertEquals(entitiesAttempts.getEntities().get(0).getEntityType(),
-        ApplicationMaster.DSEntity.DS_APP_ATTEMPT.toString());
-    Assert.assertEquals(haveDomain ? domain.getId() : "DEFAULT",
-        entitiesAttempts.getEntities().get(0).getDomainId());
-    String currAttemptEntityId =
-        entitiesAttempts.getEntities().get(0).getEntityId();
-    ApplicationAttemptId attemptId = ApplicationAttemptId.fromString(
-        currAttemptEntityId);
-    NameValuePair primaryFilter = new NameValuePair(
-        ApplicationMaster.APPID_TIMELINE_FILTER_NAME,
-        attemptId.getApplicationId().toString());
-    TimelineEntities entities = yarnCluster
-        .getApplicationHistoryServer()
-        .getTimelineStore()
-        .getEntities(ApplicationMaster.DSEntity.DS_CONTAINER.toString(), null,
-            null, null, null, null, primaryFilter, null, null, null);
-    Assert.assertNotNull(entities);
-    Assert.assertEquals(2, entities.getEntities().size());
-    Assert.assertEquals(entities.getEntities().get(0).getEntityType(),
-        ApplicationMaster.DSEntity.DS_CONTAINER.toString());
-
-    String entityId = entities.getEntities().get(0).getEntityId();
-    TimelineEntity entity =
-        yarnCluster.getApplicationHistoryServer().getTimelineStore()
-            .getEntity(entityId,
-                ApplicationMaster.DSEntity.DS_CONTAINER.toString(), null);
-    Assert.assertNotNull(entity);
-    Assert.assertEquals(entityId, entity.getEntityId());
-    Assert.assertEquals(haveDomain ? domain.getId() : "DEFAULT",
-        entities.getEntities().get(0).getDomainId());
-  }
-
-  protected String[] createArgsWithPostFix(int index, String... args) {
-    String[] res = mergeArgs(COMMON_ARGS, args);
-    // set the application name so we can track down which command is running.
-    res[COMMON_ARGS.length - 1] = generateAppName(String.format("%03d",
-        index));
-    return res;
-  }
-
-  protected String generateAppName() {
-    return generateAppName(null);
-  }
-
-  protected String generateAppName(String postFix) {
-    return name.getMethodName().replaceFirst("test", "")
-        .concat(postFix == null ? "" : "-" + postFix);
-  }
-
-  protected void setUpHDFSCluster() throws IOException {
-    if (hdfsCluster == null) {
-      HdfsConfiguration hdfsConfig = new HdfsConfiguration();
-      hdfsCluster = new MiniDFSCluster.Builder(hdfsConfig)
-          .numDataNodes(NUM_DATA_NODES).build();
-      hdfsCluster.waitActive();
-    }
-  }
-
-  protected void setUpYarnCluster(int numNodeManagers,
-      YarnConfiguration yarnConfig) throws Exception {
-    if (yarnCluster != null) {
-      return;
-    }
-    yarnCluster =
-        new MiniYARNCluster(getClass().getSimpleName(), 1, numNodeManagers,
-            1, 1);
-    yarnCluster.init(yarnConfig);
-    yarnCluster.start();
-    // wait for the node managers to register.
-    waitForNMsToRegister();
-    conf.set(
-        YarnConfiguration.TIMELINE_SERVICE_WEBAPP_ADDRESS,
-        MiniYARNCluster.getHostname() + ":"
-            + yarnCluster.getApplicationHistoryServer().getPort());
-    Configuration yarnClusterConfig = yarnCluster.getConfig();
-    yarnClusterConfig.set(YarnConfiguration.YARN_APPLICATION_CLASSPATH,
-        new File(yarnSitePath).getParent());
-    // write the document to a buffer (not directly to the file, as that
-    // can cause the file being written to get read -which will then fail.
-    ByteArrayOutputStream bytesOut = new ByteArrayOutputStream();
-    yarnClusterConfig.writeXml(bytesOut);
-    bytesOut.close();
-    // write the bytes to the file in the classpath
-    OutputStream os = new FileOutputStream(yarnSitePath);
-    os.write(bytesOut.toByteArray());
-    os.close();
-  }
-
-  protected void setupInternal(int numNodeManagers,
-      YarnConfiguration yarnConfig) throws Exception {
-    LOG.info("========== Setting UP UnitTest {}#{} ==========",
-        getClass().getCanonicalName(), name.getMethodName());
-    LOG.info("Starting up YARN cluster. Timeline version {}",
-        getTimelineVersion());
-    conf = yarnConfig;
-    conf.setInt(YarnConfiguration.RM_SCHEDULER_MINIMUM_ALLOCATION_MB,
-        MIN_ALLOCATION_MB);
-    // reduce the tearDown waiting time
-    conf.setLong(YarnConfiguration.DISPATCHER_DRAIN_EVENTS_TIMEOUT, 1000);
-    conf.setLong(YarnConfiguration.NM_LOG_RETAIN_SECONDS, 500);
-    conf.set("yarn.log.dir", "target");
-    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);
-    // mark if we need to launch the v1 timeline server
-    // disable aux-service based timeline aggregators
-    conf.set(YarnConfiguration.NM_AUX_SERVICES, "");
-    conf.setBoolean(YarnConfiguration.SYSTEM_METRICS_PUBLISHER_ENABLED, true);
-
-    conf.set(YarnConfiguration.NM_VMEM_PMEM_RATIO, "8");
-    conf.setBoolean(YarnConfiguration.NODE_LABELS_ENABLED, true);
-    conf.set("mapreduce.jobhistory.address",
-        "0.0.0.0:" + ServerSocketUtil.getPort(10021, 10));
-    // Enable ContainersMonitorImpl
-    conf.set(YarnConfiguration.NM_CONTAINER_MON_RESOURCE_CALCULATOR,
-        LinuxResourceCalculatorPlugin.class.getName());
-    conf.set(YarnConfiguration.NM_CONTAINER_MON_PROCESS_TREE,
-        ProcfsBasedProcessTree.class.getName());
-    conf.setBoolean(YarnConfiguration.NM_PMEM_CHECK_ENABLED, true);
-    conf.setBoolean(YarnConfiguration.NM_VMEM_CHECK_ENABLED, true);
-    conf.setBoolean(
-        YarnConfiguration.YARN_MINICLUSTER_CONTROL_RESOURCE_MONITORING, true);
-    conf.setBoolean(YarnConfiguration.RM_SYSTEM_METRICS_PUBLISHER_ENABLED,
-        true);
-    conf.setBoolean(
-        YarnConfiguration.OPPORTUNISTIC_CONTAINER_ALLOCATION_ENABLED, true);
-    conf.setInt(YarnConfiguration.NM_OPPORTUNISTIC_CONTAINERS_MAX_QUEUE_LENGTH,
-        10);
-    conf.set(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_HANDLER,
-        YarnConfiguration.PROCESSOR_RM_PLACEMENT_CONSTRAINTS_HANDLER);
-    // ATS version specific settings
-    conf.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION,
-        getTimelineVersion());
-    // setup the configuration of relevant for each TimelineService version.
-    customizeConfiguration(conf);
-    // setup the yarn cluster.
-    setUpYarnCluster(numNodeManagers, conf);
-  }
-
-  protected NodeManager getNodeManager(int index) {
-    return yarnCluster.getNodeManager(index);
-  }
-
-  protected MiniYARNCluster getYarnCluster() {
-    return yarnCluster;
-  }
-
-  protected void setConfiguration(String key, String value) {
-    conf.set(key, value);
-  }
-
-  protected Configuration getYarnClusterConfiguration() {
-    return yarnCluster.getConfig();
-  }
-
-  protected Configuration getConfiguration() {
-    return conf;
-  }
-
-  protected ResourceManager getResourceManager() {
-    return yarnCluster.getResourceManager();
-  }
-
-  protected ResourceManager getResourceManager(int index) {
-    return yarnCluster.getResourceManager(index);
-  }
-
-  protected Client getDSClient() {
-    return dsClient;
-  }
-
-  protected void resetDSClient() {
-    dsClient = null;
-  }
-
-  protected abstract float getTimelineVersion();
-
-  protected void cleanUpDFSClient() {
-    if (getDSClient() != null) {
-      getDSClient().sendStopSignal();
-      resetDSClient();
-    }
-  }
-
-  private void waitForNMsToRegister() throws Exception {
-    GenericTestUtils.waitFor(() -> {
-      RMContext rmContext = yarnCluster.getResourceManager().getRMContext();
-      return (rmContext.getRMNodes().size() >= NUM_NMS);
-    }, 100, 60000);
-  }
-
-  protected MiniDFSCluster getHDFSCluster() {
-    return hdfsCluster;
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestClient.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestClient.java
deleted file mode 100644
index 074f2e8422b..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestClient.java
+++ /dev/null
@@ -1,42 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import org.junit.Test;
-
-import java.util.HashMap;
-import java.util.Map;
-
-import static org.junit.Assert.assertEquals;
-
-public class TestClient {
-  @Test
-  public void testParseResourcesString() {
-    // Setup
-    final Map<String, Long> expectedResult = new HashMap<>();
-    expectedResult.put("memory-mb", 3072L);
-    expectedResult.put("vcores", 1L);
-
-    // Run the test
-    final Map<String, Long> result = Client.parseResourcesString("[memory-mb=3072,vcores=1]");
-
-    // Verify the results
-    assertEquals(expectedResult, result);
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSAppMaster.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSAppMaster.java
deleted file mode 100644
index af095bc9187..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSAppMaster.java
+++ /dev/null
@@ -1,252 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
-import org.apache.hadoop.yarn.api.records.Container;
-import org.apache.hadoop.yarn.api.records.ContainerExitStatus;
-import org.apache.hadoop.yarn.api.records.ContainerId;
-import org.apache.hadoop.yarn.api.records.ContainerState;
-import org.apache.hadoop.yarn.api.records.ContainerStatus;
-import org.apache.hadoop.yarn.api.records.NodeId;
-import org.apache.hadoop.yarn.api.records.Priority;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.client.api.AMRMClient;
-import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;
-import org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl;
-import org.apache.hadoop.yarn.server.timelineservice.storage.TimelineWriter;
-import org.apache.hadoop.yarn.server.utils.BuilderUtils;
-import org.junit.Assert;
-import org.junit.Test;
-import org.mockito.ArgumentMatchers;
-import org.mockito.Mockito;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-/**
- * A bunch of tests to make sure that the container allocations
- * and releases occur correctly.
- */
-public class TestDSAppMaster {
-
-  static class TestAppMaster extends ApplicationMaster {
-    private int threadsLaunched = 0;
-    public List<String> yarnShellIds = new ArrayList<String>();
-
-    @Override
-    protected Thread createLaunchContainerThread(Container allocatedContainer,
-        String shellId) {
-      threadsLaunched++;
-      launchedContainers.add(allocatedContainer.getId());
-      yarnShellIds.add(shellId);
-      return new Thread();
-    }
-
-    void setNumTotalContainers(int numTotalContainers) {
-      this.numTotalContainers = numTotalContainers;
-    }
-
-    int getAllocatedContainers() {
-      return this.numAllocatedContainers.get();
-    }
-
-    @Override
-    void startTimelineClient(final Configuration conf) throws YarnException,
-        IOException, InterruptedException {
-      timelineClient = null;
-    }
-  }
-
-  @SuppressWarnings("unchecked")
-  @Test
-  public void testDSAppMasterAllocateHandler() throws Exception {
-
-    TestAppMaster master = new TestAppMaster();
-    int targetContainers = 2;
-    AMRMClientAsync mockClient = Mockito.mock(AMRMClientAsync.class);
-    master.setAmRMClient(mockClient);
-    master.setNumTotalContainers(targetContainers);
-    Mockito.doNothing().when(mockClient)
-        .addContainerRequest(ArgumentMatchers.any(
-            AMRMClient.ContainerRequest.class));
-
-    ApplicationMaster.RMCallbackHandler handler = master.getRMCallbackHandler();
-
-    List<Container> containers = new ArrayList<>(1);
-    ContainerId id1 = BuilderUtils.newContainerId(1, 1, 1, 1);
-    containers.add(generateContainer(id1));
-
-    master.numRequestedContainers.set(targetContainers);
-
-    // first allocate a single container, everything should be fine
-    handler.onContainersAllocated(containers);
-    Assert.assertEquals("Wrong container allocation count", 1,
-        master.getAllocatedContainers());
-    Assert.assertEquals("Incorrect number of threads launched", 1,
-        master.threadsLaunched);
-    Assert.assertEquals("Incorrect YARN Shell IDs",
-        Arrays.asList("1"), master.yarnShellIds);
-
-    // now send 3 extra containers
-    containers.clear();
-    ContainerId id2 = BuilderUtils.newContainerId(1, 1, 1, 2);
-    containers.add(generateContainer(id2));
-    ContainerId id3 = BuilderUtils.newContainerId(1, 1, 1, 3);
-    containers.add(generateContainer(id3));
-    ContainerId id4 = BuilderUtils.newContainerId(1, 1, 1, 4);
-    containers.add(generateContainer(id4));
-    handler.onContainersAllocated(containers);
-    Assert.assertEquals("Wrong final container allocation count", 2,
-        master.getAllocatedContainers());
-
-    Assert.assertEquals("Incorrect number of threads launched", 2,
-        master.threadsLaunched);
-
-    Assert.assertEquals("Incorrect YARN Shell IDs",
-        Arrays.asList("1", "2"), master.yarnShellIds);
-    // make sure we handle completion events correctly
-    List<ContainerStatus> status = new ArrayList<>();
-    status.add(generateContainerStatus(id1, ContainerExitStatus.SUCCESS));
-    status.add(generateContainerStatus(id2, ContainerExitStatus.SUCCESS));
-    status.add(generateContainerStatus(id3, ContainerExitStatus.ABORTED));
-    status.add(generateContainerStatus(id4, ContainerExitStatus.ABORTED));
-    handler.onContainersCompleted(status);
-
-    Assert.assertEquals("Unexpected number of completed containers",
-        targetContainers, master.getNumCompletedContainers());
-    Assert.assertTrue("Master didn't finish containers as expected",
-        master.getDone());
-
-    // test for events from containers we know nothing about
-    // these events should be ignored
-    status = new ArrayList<>();
-    ContainerId id5 = BuilderUtils.newContainerId(1, 1, 1, 5);
-    status.add(generateContainerStatus(id5, ContainerExitStatus.ABORTED));
-    Assert.assertEquals("Unexpected number of completed containers",
-        targetContainers, master.getNumCompletedContainers());
-    Assert.assertTrue("Master didn't finish containers as expected",
-        master.getDone());
-    status.add(generateContainerStatus(id5, ContainerExitStatus.SUCCESS));
-    Assert.assertEquals("Unexpected number of completed containers",
-        targetContainers, master.getNumCompletedContainers());
-    Assert.assertTrue("Master didn't finish containers as expected",
-        master.getDone());
-  }
-
-  private Container generateContainer(ContainerId cid) {
-    return Container.newInstance(cid, NodeId.newInstance("host", 5000),
-      "host:80", Resource.newInstance(1024, 1), Priority.newInstance(0),
-      null);
-  }
-
-  private ContainerStatus
-      generateContainerStatus(ContainerId id, int exitStatus) {
-    return ContainerStatus.newInstance(id, ContainerState.COMPLETE, "",
-      exitStatus);
-  }
-
-  @Test
-  public void testTimelineClientInDSAppMasterV1() throws Exception {
-    runTimelineClientInDSAppMaster(true, false);
-  }
-
-  @Test
-  public void testTimelineClientInDSAppMasterV2() throws Exception {
-    runTimelineClientInDSAppMaster(false, true);
-  }
-
-  @Test
-  public void testTimelineClientInDSAppMasterV1V2() throws Exception {
-    runTimelineClientInDSAppMaster(true, true);
-  }
-
-  @Test
-  public void testTimelineClientInDSAppMasterDisabled() throws Exception {
-    runTimelineClientInDSAppMaster(false, false);
-  }
-
-  private void runTimelineClientInDSAppMaster(boolean v1Enabled,
-      boolean v2Enabled) throws Exception {
-    ApplicationMaster appMaster = createAppMasterWithStartedTimelineService(
-        v1Enabled, v2Enabled);
-    validateAppMasterTimelineService(v1Enabled, v2Enabled, appMaster);
-  }
-
-  private void validateAppMasterTimelineService(boolean v1Enabled,
-      boolean v2Enabled, ApplicationMaster appMaster) {
-    if (v1Enabled) {
-      Assert.assertEquals(appMaster.appSubmitterUgi,
-          ((TimelineClientImpl)appMaster.timelineClient).getUgi());
-    } else {
-      Assert.assertNull(appMaster.timelineClient);
-    }
-    if (v2Enabled) {
-      Assert.assertNotNull(appMaster.timelineV2Client);
-    } else {
-      Assert.assertNull(appMaster.timelineV2Client);
-    }
-  }
-
-  private ApplicationMaster createAppMasterWithStartedTimelineService(
-      boolean v1Enabled, boolean v2Enabled) throws Exception {
-    ApplicationMaster appMaster = new ApplicationMaster();
-    appMaster.appSubmitterUgi = UserGroupInformation
-        .createUserForTesting("foo", new String[] {"bar"});
-    Configuration conf = this.getTimelineServiceConf(v1Enabled, v2Enabled);
-    ApplicationId appId = ApplicationId.newInstance(1L, 1);
-    appMaster.appAttemptID = ApplicationAttemptId.newInstance(appId, 1);
-    appMaster.startTimelineClient(conf);
-    return appMaster;
-  }
-
-  private Configuration getTimelineServiceConf(boolean v1Enabled,
-      boolean v2Enabled) {
-    Configuration conf = new YarnConfiguration(new Configuration(false));
-    Assert.assertFalse(YarnConfiguration.timelineServiceEnabled(conf));
-
-    if (v1Enabled || v2Enabled) {
-      conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);
-    }
-
-    if (v1Enabled) {
-      conf.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, 1.0f);
-    }
-
-    if (v2Enabled) {
-      conf.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, 2.0f);
-      conf.setClass(YarnConfiguration.TIMELINE_SERVICE_WRITER_CLASS,
-          FileSystemTimelineWriterImpl.class, TimelineWriter.class);
-    }
-
-    if (v1Enabled && v2Enabled) {
-      conf.set(YarnConfiguration.TIMELINE_SERVICE_VERSION, "1.0");
-      conf.set(YarnConfiguration.TIMELINE_SERVICE_VERSIONS, "1.0,2.0f");
-    }
-    return conf;
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSFailedAppMaster.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSFailedAppMaster.java
deleted file mode 100644
index 4f424f930b2..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSFailedAppMaster.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.io.IOException;
-
-import org.apache.hadoop.yarn.exceptions.YarnException;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class TestDSFailedAppMaster extends ApplicationMaster {
-
-  private static final Logger LOG = LoggerFactory
-      .getLogger(TestDSFailedAppMaster.class);
-
-  @Override
-  public void run() throws YarnException, IOException, InterruptedException {
-    super.run();
-
-    // for the 2nd attempt.
-    if (appAttemptID.getAttemptId() == 2) {
-      // should reuse the earlier running container, so numAllocatedContainers
-      // should be set to 1. And should ask no more containers, so
-      // numRequestedContainers should be the same as numTotalContainers.
-      // The only container is the container requested by the AM in the first
-      // attempt.
-      if (numAllocatedContainers.get() != 1
-          || numRequestedContainers.get() != numTotalContainers) {
-        LOG.info("NumAllocatedContainers is " + numAllocatedContainers.get()
-            + " and NumRequestedContainers is " + numAllocatedContainers.get()
-            + ".Application Master failed. exiting");
-        System.exit(200);
-      }
-    }
-  }
-
-  public static void main(String[] args) {
-    boolean result = false;
-    try {
-      TestDSFailedAppMaster appMaster = new TestDSFailedAppMaster();
-      boolean doRun = appMaster.init(args);
-      if (!doRun) {
-        System.exit(0);
-      }
-      appMaster.run();
-      if (appMaster.appAttemptID.getAttemptId() == 1) {
-        try {
-          // sleep some time, wait for the AM to launch a container.
-          Thread.sleep(3000);
-        } catch (InterruptedException e) {}
-        // fail the first am.
-        System.exit(100);
-      }
-      result = appMaster.finish();
-    } catch (Throwable t) {
-      System.exit(1);
-    }
-    if (result) {
-      LOG.info("Application Master completed successfully. exiting");
-      System.exit(0);
-    } else {
-      LOG.info("Application Master failed. exiting");
-      System.exit(2);
-    }
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSSleepingAppMaster.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSSleepingAppMaster.java
deleted file mode 100644
index ae25ece1f82..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSSleepingAppMaster.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-public class TestDSSleepingAppMaster extends ApplicationMaster {
-
-  private static final Logger LOG = LoggerFactory
-      .getLogger(TestDSSleepingAppMaster.class);
-  private static final long SLEEP_TIME = 5000;
-
-  public static void main(String[] args) {
-    boolean result = false;
-    TestDSSleepingAppMaster appMaster = new TestDSSleepingAppMaster();
-    try {
-      boolean doRun = appMaster.init(args);
-      if (!doRun) {
-        System.exit(0);
-      }
-      appMaster.run();
-      if (appMaster.appAttemptID.getAttemptId() <= 2) {
-        try {
-          // sleep some time
-          Thread.sleep(SLEEP_TIME);
-        } catch (InterruptedException e) {}
-        // fail the first am.
-        System.exit(100);
-      }
-      result = appMaster.finish();
-    } catch (Throwable t) {
-      System.exit(1);
-    } finally {
-      if (appMaster != null) {
-        appMaster.cleanup();
-      }
-    }
-    if (result) {
-      LOG.info("Application Master completed successfully. exiting");
-      System.exit(0);
-    } else {
-      LOG.info("Application Master failed. exiting");
-      System.exit(2);
-    }
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV10.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV10.java
deleted file mode 100644
index 15dc1cb04ee..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV10.java
+++ /dev/null
@@ -1,843 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileReader;
-import java.io.IOException;
-import java.io.PrintWriter;
-import java.io.UncheckedIOException;
-import java.net.URI;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.concurrent.atomic.AtomicReference;
-
-import org.junit.Assert;
-import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.commons.cli.MissingArgumentException;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.CommonConfigurationKeysPublic;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.test.GenericTestUtils;
-import org.apache.hadoop.test.LambdaTestUtils;
-import org.apache.hadoop.util.Shell;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;
-import org.apache.hadoop.yarn.api.records.ContainerState;
-import org.apache.hadoop.yarn.api.records.ContainerStatus;
-import org.apache.hadoop.yarn.api.records.LogAggregationContext;
-import org.apache.hadoop.yarn.client.api.impl.DirectTimelineWriter;
-import org.apache.hadoop.yarn.client.api.impl.TestTimelineClient;
-import org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl;
-import org.apache.hadoop.yarn.client.api.impl.TimelineWriter;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.exceptions.ResourceNotFoundException;
-import org.apache.hadoop.yarn.server.utils.BuilderUtils;
-import org.apache.hadoop.yarn.util.Records;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertTrue;
-import static org.mockito.Mockito.mock;
-import static org.mockito.Mockito.spy;
-import static org.mockito.Mockito.when;
-
-/**
- * Unit tests implementations for distributed shell on TimeLineV1.
- */
-public class TestDSTimelineV10 extends DistributedShellBaseTest {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TestDSTimelineV10.class);
-
-  @Override
-  protected float getTimelineVersion() {
-    return 1.0f;
-  }
-
-  @Override
-  protected void cleanUpDFSClient() {
-
-  }
-
-  @Test
-  public void testDSShellWithDomain() throws Exception {
-    baseTestDSShell(true);
-  }
-
-  @Test
-  public void testDSShellWithoutDomain() throws Exception {
-    baseTestDSShell(false);
-  }
-
-  @Test
-  public void testDSRestartWithPreviousRunningContainers() throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getSleepCommand(8),
-        "--master_memory",
-        "512",
-        "--container_memory",
-        "128",
-        "--keep_containers_across_application_attempts"
-    );
-
-    LOG.info("Initializing DS Client");
-    setAndGetDSClient(TestDSFailedAppMaster.class.getName(),
-        new Configuration(getYarnClusterConfiguration()));
-
-    getDSClient().init(args);
-
-    LOG.info("Running DS Client");
-    boolean result = getDSClient().run();
-    LOG.info("Client run completed. Result={}", result);
-    // application should succeed
-    Assert.assertTrue(result);
-  }
-
-  /*
-   * The sleeping period in TestDSSleepingAppMaster is set as 5 seconds.
-   * Set attempt_failures_validity_interval as 2.5 seconds. It will check
-   * how many attempt failures for previous 2.5 seconds.
-   * The application is expected to be successful.
-   */
-  @Test
-  public void testDSAttemptFailuresValidityIntervalSuccess() throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getSleepCommand(8),
-        "--master_memory",
-        "512",
-        "--container_memory",
-        "128",
-        "--attempt_failures_validity_interval",
-        "2500"
-    );
-
-    LOG.info("Initializing DS Client");
-    Configuration config = getYarnClusterConfiguration();
-    config.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS, 2);
-    setAndGetDSClient(TestDSSleepingAppMaster.class.getName(),
-        new Configuration(config));
-
-    getDSClient().init(args);
-
-    LOG.info("Running DS Client");
-    boolean result = getDSClient().run();
-
-    LOG.info("Client run completed. Result=" + result);
-    // application should succeed
-    Assert.assertTrue(result);
-  }
-
-  /*
-   * The sleeping period in TestDSSleepingAppMaster is set as 5 seconds.
-   * Set attempt_failures_validity_interval as 15 seconds. It will check
-   * how many attempt failure for previous 15 seconds.
-   * The application is expected to be fail.
-   */
-  @Test
-  public void testDSAttemptFailuresValidityIntervalFailed() throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getSleepCommand(8),
-        "--master_memory",
-        "512",
-        "--container_memory",
-        "128",
-        "--attempt_failures_validity_interval",
-        "15000"
-    );
-
-    LOG.info("Initializing DS Client");
-    Configuration config = getYarnClusterConfiguration();
-    config.setInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS, 2);
-    setAndGetDSClient(TestDSSleepingAppMaster.class.getName(),
-        new Configuration(config));
-
-    getDSClient().init(args);
-
-    LOG.info("Running DS Client");
-    boolean result = getDSClient().run();
-
-    LOG.info("Client run completed. Result=" + result);
-    // application should be failed
-    Assert.assertFalse(result);
-  }
-
-  @Test
-  public void testDSShellWithCustomLogPropertyFile() throws Exception {
-    final File basedir = getBaseDirForTest();
-    final File tmpDir = new File(basedir, "tmpDir");
-    tmpDir.mkdirs();
-    final File customLogProperty = new File(tmpDir, "custom_log4j.properties");
-    if (customLogProperty.exists()) {
-      customLogProperty.delete();
-    }
-    if (!customLogProperty.createNewFile()) {
-      Assert.fail("Can not create custom log4j property file.");
-    }
-    PrintWriter fileWriter = new PrintWriter(customLogProperty);
-    // set the output to DEBUG level
-    fileWriter.write("log4j.rootLogger=debug,stdout");
-    fileWriter.close();
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "3",
-        "--shell_command",
-        "echo",
-        "--shell_args",
-        "HADOOP",
-        "--log_properties",
-        customLogProperty.getAbsolutePath(),
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1"
-    );
-
-    // Before run the DS, the default the log level is INFO
-    final Logger LOG_Client =
-        LoggerFactory.getLogger(Client.class);
-    Assert.assertTrue(LOG_Client.isInfoEnabled());
-    Assert.assertFalse(LOG_Client.isDebugEnabled());
-    final Logger LOG_AM = LoggerFactory.getLogger(ApplicationMaster.class);
-    Assert.assertTrue(LOG_AM.isInfoEnabled());
-    Assert.assertFalse(LOG_AM.isDebugEnabled());
-
-    LOG.info("Initializing DS Client");
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    boolean initSuccess = getDSClient().init(args);
-    Assert.assertTrue(initSuccess);
-
-    LOG.info("Running DS Client");
-    boolean result = getDSClient().run();
-    LOG.info("Client run completed. Result=" + result);
-    Assert.assertTrue(verifyContainerLog(3, null, true, "DEBUG") > 10);
-    //After DS is finished, the log level should be DEBUG
-    Assert.assertTrue(LOG_Client.isInfoEnabled());
-    Assert.assertTrue(LOG_Client.isDebugEnabled());
-    Assert.assertTrue(LOG_AM.isInfoEnabled());
-    Assert.assertTrue(LOG_AM.isDebugEnabled());
-  }
-
-  @Test
-  public void testSpecifyingLogAggregationContext() throws Exception {
-    String regex = ".*(foo|bar)\\d";
-    String[] args = createArgumentsWithAppName(
-        "--shell_command",
-        "echo",
-        "--rolling_log_pattern",
-        regex
-    );
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    Assert.assertTrue(getDSClient().init(args));
-
-    ApplicationSubmissionContext context =
-        Records.newRecord(ApplicationSubmissionContext.class);
-    getDSClient().specifyLogAggregationContext(context);
-    LogAggregationContext logContext = context.getLogAggregationContext();
-    assertEquals(logContext.getRolledLogsIncludePattern(), regex);
-    assertTrue(logContext.getRolledLogsExcludePattern().isEmpty());
-  }
-
-  @Test
-  public void testDSShellWithMultipleArgs() throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "4",
-        "--shell_command",
-        "echo",
-        "--shell_args",
-        "HADOOP YARN MAPREDUCE HDFS",
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1"
-    );
-
-    LOG.info("Initializing DS Client");
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    boolean initSuccess = getDSClient().init(args);
-    Assert.assertTrue(initSuccess);
-    LOG.info("Running DS Client");
-
-    boolean result = getDSClient().run();
-    LOG.info("Client run completed. Result=" + result);
-    List<String> expectedContent = new ArrayList<>();
-    expectedContent.add("HADOOP YARN MAPREDUCE HDFS");
-    verifyContainerLog(4, expectedContent, false, "");
-  }
-
-  @Test
-  public void testDSShellWithShellScript() throws Exception {
-    final File basedir = getBaseDirForTest();
-    final File tmpDir = new File(basedir, "tmpDir");
-    tmpDir.mkdirs();
-    final File customShellScript = new File(tmpDir, "custom_script.sh");
-    if (customShellScript.exists()) {
-      customShellScript.delete();
-    }
-    if (!customShellScript.createNewFile()) {
-      Assert.fail("Can not create custom shell script file.");
-    }
-    PrintWriter fileWriter = new PrintWriter(customShellScript);
-    // set the output to DEBUG level
-    fileWriter.write("echo testDSShellWithShellScript");
-    fileWriter.close();
-    LOG.info(customShellScript.getAbsolutePath());
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_script",
-        customShellScript.getAbsolutePath(),
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1"
-    );
-
-    LOG.info("Initializing DS Client");
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    Assert.assertTrue(getDSClient().init(args));
-    LOG.info("Running DS Client");
-    assertTrue(getDSClient().run());
-    List<String> expectedContent = new ArrayList<>();
-    expectedContent.add("testDSShellWithShellScript");
-    verifyContainerLog(1, expectedContent, false, "");
-  }
-
-  @Test
-  public void testDSShellWithInvalidArgs() throws Exception {
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    int appNameCounter = 0;
-    LOG.info("Initializing DS Client with no args");
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        "No args",
-        () -> getDSClient().init(new String[]{}));
-
-    LOG.info("Initializing DS Client with no jar file");
-    String[] noJarArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "2",
-        "--shell_command",
-        getListCommand(),
-        "--master_memory",
-        "512",
-        "--container_memory",
-        "128"
-    );
-    String[] argsNoJar = Arrays.copyOfRange(noJarArgs, 2, noJarArgs.length);
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        "No jar",
-        () -> getDSClient().init(argsNoJar));
-
-    LOG.info("Initializing DS Client with no shell command");
-    String[] noShellCmdArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "2",
-        "--master_memory",
-        "512",
-        "--container_memory",
-        "128"
-    );
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        "No shell command",
-        () -> getDSClient().init(noShellCmdArgs));
-
-    LOG.info("Initializing DS Client with invalid no. of containers");
-
-    String[] numContainersArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "-1",
-        "--shell_command",
-        getListCommand(),
-        "--master_memory",
-        "512",
-        "--container_memory",
-        "128"
-    );
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        "Invalid no. of containers",
-        () -> getDSClient().init(numContainersArgs));
-
-    LOG.info("Initializing DS Client with invalid no. of vcores");
-
-    String[] vCoresArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "2",
-        "--shell_command",
-        getListCommand(),
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "-2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1"
-    );
-    getDSClient().init(vCoresArgs);
-
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        "Invalid virtual cores specified",
-        () -> {
-          getDSClient().init(vCoresArgs);
-          getDSClient().run();
-        });
-
-    LOG.info("Initializing DS Client with --shell_command and --shell_script");
-
-    String[] scriptAndCmdArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "2",
-        "--shell_command",
-        getListCommand(),
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1",
-        "--shell_script",
-        "test.sh"
-    );
-
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        "Can not specify shell_command option and shell_script option at "
-            + "the same time",
-        () -> getDSClient().init(scriptAndCmdArgs));
-
-    LOG.info(
-        "Initializing DS Client without --shell_command and --shell_script");
-
-    String[] noShellCmdNoScriptArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "2",
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1"
-    );
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        "No shell command or shell script specified "
-            + "to be executed by application master",
-        () -> getDSClient().init(noShellCmdNoScriptArgs));
-
-    LOG.info("Initializing DS Client with invalid container_type argument");
-    String[] invalidTypeArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "2",
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1",
-        "--shell_command",
-        "date",
-        "--container_type",
-        "UNSUPPORTED_TYPE"
-    );
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        "Invalid container_type: UNSUPPORTED_TYPE",
-        () -> getDSClient().init(invalidTypeArgs));
-
-    String[] invalidMemArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getListCommand(),
-        "--master_resources",
-        "memory-mb=invalid"
-    );
-    LambdaTestUtils.intercept(IllegalArgumentException.class,
-        () -> getDSClient().init(invalidMemArgs));
-
-    String[] invalidMasterResArgs = createArgsWithPostFix(appNameCounter++,
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getListCommand(),
-        "--master_resources"
-    );
-    LambdaTestUtils.intercept(MissingArgumentException.class,
-        () -> getDSClient().init(invalidMasterResArgs));
-  }
-
-  @Test
-  public void testDSTimelineClientWithConnectionRefuse() throws Exception {
-    ApplicationMaster am = new ApplicationMaster();
-    final AtomicReference<TimelineWriter> spyTimelineWriterRef =
-        new AtomicReference<>(null);
-    TimelineClientImpl client = new TimelineClientImpl() {
-      @Override
-      protected TimelineWriter createTimelineWriter(Configuration conf,
-          UserGroupInformation authUgi, com.sun.jersey.api.client.Client client,
-          URI resURI) throws IOException {
-        TimelineWriter timelineWriter =
-            new DirectTimelineWriter(authUgi, client, resURI);
-        spyTimelineWriterRef.set(spy(timelineWriter));
-        return spyTimelineWriterRef.get();
-      }
-    };
-    client.init(getConfiguration());
-    client.start();
-    TestTimelineClient.mockEntityClientResponse(spyTimelineWriterRef.get(),
-        null, false, true);
-    try {
-      UserGroupInformation ugi = mock(UserGroupInformation.class);
-      when(ugi.getShortUserName()).thenReturn("user1");
-      // verify no ClientHandlerException get thrown out.
-      am.publishContainerEndEvent(client, ContainerStatus.newInstance(
-          BuilderUtils.newContainerId(1, 1, 1, 1), ContainerState.COMPLETE, "",
-          1), "domainId", ugi);
-    } finally {
-      client.stop();
-    }
-  }
-
-  @Test
-  public void testContainerLaunchFailureHandling() throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "2",
-        "--shell_command",
-        getListCommand(),
-        "--master_memory",
-        "512",
-        "--container_memory",
-        "128"
-    );
-
-    LOG.info("Initializing DS Client");
-    setAndGetDSClient(ContainerLaunchFailAppMaster.class.getName(),
-        new Configuration(getYarnClusterConfiguration()));
-    Assert.assertTrue(getDSClient().init(args));
-    LOG.info("Running DS Client");
-    Assert.assertFalse(getDSClient().run());
-  }
-
-  @Test
-  public void testDebugFlag() throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "2",
-        "--shell_command",
-        getListCommand(),
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1",
-        "--debug"
-    );
-
-    LOG.info("Initializing DS Client");
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    Assert.assertTrue(getDSClient().init(args));
-    LOG.info("Running DS Client");
-    Assert.assertTrue(getDSClient().run());
-  }
-
-  private int verifyContainerLog(int containerNum,
-      List<String> expectedContent, boolean count, String expectedWord) {
-    File logFolder =
-        new File(getNodeManager(0).getConfig()
-            .get(YarnConfiguration.NM_LOG_DIRS,
-                YarnConfiguration.DEFAULT_NM_LOG_DIRS));
-
-    File[] listOfFiles = logFolder.listFiles();
-    Assert.assertNotNull(listOfFiles);
-    int currentContainerLogFileIndex = -1;
-    for (int i = listOfFiles.length - 1; i >= 0; i--) {
-      if (listOfFiles[i].listFiles().length == containerNum + 1) {
-        currentContainerLogFileIndex = i;
-        break;
-      }
-    }
-    Assert.assertTrue(currentContainerLogFileIndex != -1);
-    File[] containerFiles =
-        listOfFiles[currentContainerLogFileIndex].listFiles();
-
-    int numOfWords = 0;
-    for (File containerFile : containerFiles) {
-      if (containerFile == null) {
-        continue;
-      }
-      for (File output : containerFile.listFiles()) {
-        if (output.getName().trim().contains("stdout")) {
-          List<String> stdOutContent = new ArrayList<>();
-          try (BufferedReader br = new BufferedReader(new FileReader(output))) {
-            String sCurrentLine;
-
-            int numOfline = 0;
-            while ((sCurrentLine = br.readLine()) != null) {
-              if (count) {
-                if (sCurrentLine.contains(expectedWord)) {
-                  numOfWords++;
-                }
-              } else if (output.getName().trim().equals("stdout")) {
-                if (!Shell.WINDOWS) {
-                  Assert.assertEquals("The current is" + sCurrentLine,
-                      expectedContent.get(numOfline), sCurrentLine.trim());
-                  numOfline++;
-                } else {
-                  stdOutContent.add(sCurrentLine.trim());
-                }
-              }
-            }
-            /* By executing bat script using cmd /c,
-             * it will output all contents from bat script first
-             * It is hard for us to do check line by line
-             * Simply check whether output from bat file contains
-             * all the expected messages
-             */
-            if (Shell.WINDOWS && !count
-                && output.getName().trim().equals("stdout")) {
-              Assert.assertTrue(stdOutContent.containsAll(expectedContent));
-            }
-          } catch (IOException e) {
-            LOG.error("Exception reading the buffer", e);
-          }
-        }
-      }
-    }
-    return numOfWords;
-  }
-
-  @Test
-  public void testDistributedShellResourceProfiles() throws Exception {
-    int appNameCounter = 0;
-    String[][] args = {
-        createArgsWithPostFix(appNameCounter++,
-            "--num_containers", "1", "--shell_command",
-            getListCommand(), "--container_resource_profile",
-            "maximum"),
-        createArgsWithPostFix(appNameCounter++,
-            "--num_containers", "1", "--shell_command",
-            getListCommand(), "--master_resource_profile",
-            "default"),
-        createArgsWithPostFix(appNameCounter++,
-            "--num_containers", "1", "--shell_command",
-            getListCommand(), "--master_resource_profile",
-            "default", "--container_resource_profile", "maximum"),
-    };
-
-    for (int i = 0; i < args.length; ++i) {
-      LOG.info("Initializing DS Client[{}]", i);
-      setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-      Assert.assertTrue(getDSClient().init(args[i]));
-      LOG.info("Running DS Client[{}]", i);
-      LambdaTestUtils.intercept(Exception.class,
-          () -> getDSClient().run());
-    }
-  }
-
-  @Test
-  public void testDSShellWithOpportunisticContainers() throws Exception {
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "2",
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1",
-        "--shell_command",
-        "date",
-        "--container_type",
-        "OPPORTUNISTIC"
-    );
-    assertTrue(getDSClient().init(args));
-    assertTrue(getDSClient().run());
-  }
-
-  @Test(expected = ResourceNotFoundException.class)
-  public void testDistributedShellAMResourcesWithUnknownResource()
-      throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getListCommand(),
-        "--master_resources",
-        "unknown-resource=5"
-    );
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    assertTrue(getDSClient().init(args));
-    getDSClient().run();
-  }
-
-  @Test(expected = IllegalArgumentException.class)
-  public void testDistributedShellNonExistentQueue()
-      throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getListCommand(),
-        "--queue",
-        "non-existent-queue"
-    );
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    assertTrue(getDSClient().init(args));
-    getDSClient().run();
-  }
-
-  @Test
-  public void testDistributedShellWithSingleFileLocalization()
-      throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getCatCommand(),
-        "--localize_files",
-        "./src/test/resources/a.txt",
-        "--shell_args",
-        "a.txt"
-    );
-
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    assertTrue(getDSClient().init(args));
-    assertTrue("Client exited with an error", getDSClient().run());
-  }
-
-  @Test
-  public void testDistributedShellWithMultiFileLocalization()
-      throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getCatCommand(),
-        "--localize_files",
-        "./src/test/resources/a.txt,./src/test/resources/b.txt",
-        "--shell_args",
-        "a.txt b.txt"
-    );
-
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    assertTrue(getDSClient().init(args));
-    assertTrue("Client exited with an error", getDSClient().run());
-  }
-
-  @Test(expected = UncheckedIOException.class)
-  public void testDistributedShellWithNonExistentFileLocalization()
-      throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getCatCommand(),
-        "--localize_files",
-        "/non/existing/path/file.txt",
-        "--shell_args",
-        "file.txt"
-    );
-
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    assertTrue(getDSClient().init(args));
-    assertTrue(getDSClient().run());
-  }
-
-  @Test
-  public void testDistributedShellCleanup()
-      throws Exception {
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "1",
-        "--shell_command",
-        getListCommand()
-    );
-    Configuration config = new Configuration(getYarnClusterConfiguration());
-    setAndGetDSClient(config);
-
-    assertTrue(getDSClient().init(args));
-    assertTrue(getDSClient().run());
-    ApplicationId appId = getDSClient().getAppId();
-    String relativePath =
-        ApplicationMaster.getRelativePath(generateAppName(),
-            appId.toString(), "");
-    FileSystem fs1 = FileSystem.get(config);
-    Path path = new Path(fs1.getHomeDirectory(), relativePath);
-
-    GenericTestUtils.waitFor(() -> {
-      try {
-        return !fs1.exists(path);
-      } catch (IOException e) {
-        return false;
-      }
-    }, 10, 60000);
-
-    assertFalse("Distributed Shell Cleanup failed", fs1.exists(path));
-  }
-
-  @Override
-  protected void customizeConfiguration(
-      YarnConfiguration config) throws Exception {
-    config.set(CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY,
-        CommonConfigurationKeysPublic.FS_DEFAULT_NAME_DEFAULT);
-  }
-
-  private static File getBaseDirForTest() {
-    return new File("target", TestDSTimelineV10.class.getName());
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV15.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV15.java
deleted file mode 100644
index 634bac4df43..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV15.java
+++ /dev/null
@@ -1,100 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.util.concurrent.atomic.AtomicReference;
-
-import org.junit.Assert;
-import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RemoteIterator;
-import org.apache.hadoop.test.GenericTestUtils;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.ApplicationReport;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.server.timeline.PluginStoreTestUtils;
-
-/**
- * Unit tests implementations for distributed shell on TimeLineV1.5.
- */
-public class TestDSTimelineV15 extends DistributedShellBaseTest {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TestDSTimelineV15.class);
-
-  @Override
-  protected float getTimelineVersion() {
-    return 1.5f;
-  }
-
-  @Override
-  protected void customizeConfiguration(
-      YarnConfiguration config) throws Exception {
-    setUpHDFSCluster();
-    PluginStoreTestUtils.prepareFileSystemForPluginStore(
-        getHDFSCluster().getFileSystem());
-    PluginStoreTestUtils.prepareConfiguration(config, getHDFSCluster());
-    config.set(YarnConfiguration.TIMELINE_SERVICE_ENTITY_GROUP_PLUGIN_CLASSES,
-        DistributedShellTimelinePlugin.class.getName());
-  }
-
-  @Override
-  protected void checkTimeline(ApplicationId appId,
-      boolean defaultFlow, boolean haveDomain,
-      ApplicationReport appReport) throws Exception {
-    long scanInterval = getConfiguration().getLong(
-        YarnConfiguration
-            .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS,
-        YarnConfiguration
-            .TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_SCAN_INTERVAL_SECONDS_DEFAULT
-    );
-    Path doneDir = new Path(
-        YarnConfiguration.TIMELINE_SERVICE_ENTITYGROUP_FS_STORE_DONE_DIR_DEFAULT
-    );
-    // Wait till the data is moved to done dir, or timeout and fail
-    AtomicReference<Exception> exceptionRef = new AtomicReference<>(null);
-    GenericTestUtils.waitFor(() -> {
-      try {
-        RemoteIterator<FileStatus> iterApps =
-            getHDFSCluster().getFileSystem().listStatusIterator(doneDir);
-        return (iterApps.hasNext());
-      } catch (Exception e) {
-        exceptionRef.set(e);
-        LOG.error("Exception listing Done Dir", e);
-        return true;
-      }
-    }, scanInterval * 2, TEST_TIME_WINDOW_EXPIRE);
-    Assert.assertNull("Exception in getting listing status",
-        exceptionRef.get());
-    super.checkTimeline(appId, defaultFlow, haveDomain, appReport);
-  }
-
-  @Test
-  public void testDSShellWithDomain() throws Exception {
-    baseTestDSShell(true);
-  }
-
-  @Test
-  public void testDSShellWithoutDomain() throws Exception {
-    baseTestDSShell(false);
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV20.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV20.java
deleted file mode 100644
index caf9d3b8de7..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSTimelineV20.java
+++ /dev/null
@@ -1,484 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileReader;
-import java.io.IOException;
-import java.util.List;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.atomic.AtomicReference;
-
-import org.junit.Assert;
-import org.junit.Assume;
-import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.test.GenericTestUtils;
-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;
-import org.apache.hadoop.yarn.api.records.ApplicationAttemptReport;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.ApplicationReport;
-import org.apache.hadoop.yarn.api.records.ContainerId;
-import org.apache.hadoop.yarn.api.records.ContainerReport;
-import org.apache.hadoop.yarn.api.records.ExecutionType;
-import org.apache.hadoop.yarn.api.records.Resource;
-import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;
-import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntityType;
-import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent;
-import org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.DSEvent;
-import org.apache.hadoop.yarn.client.api.YarnClient;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.server.metrics.AppAttemptMetricsConstants;
-import org.apache.hadoop.yarn.server.metrics.ApplicationMetricsConstants;
-import org.apache.hadoop.yarn.server.metrics.ContainerMetricsConstants;
-import org.apache.hadoop.yarn.server.timelineservice.collector.PerNodeTimelineCollectorsAuxService;
-import org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineReaderImpl;
-import org.apache.hadoop.yarn.server.timelineservice.storage.FileSystemTimelineWriterImpl;
-import org.apache.hadoop.yarn.util.timeline.TimelineUtils;
-
-/**
- * Unit tests implementations for distributed shell on TimeLineV2.
- */
-public class TestDSTimelineV20 extends DistributedShellBaseTest {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TestDSTimelineV20.class);
-  private static final String TIMELINE_AUX_SERVICE_NAME = "timeline_collector";
-
-  @Override
-  protected float getTimelineVersion() {
-    return 2.0f;
-  }
-
-  @Override
-  protected void customizeConfiguration(
-      YarnConfiguration config) throws Exception {
-    // disable v1 timeline server since we no longer have a server here
-    // enable aux-service based timeline aggregators
-    config.set(YarnConfiguration.NM_AUX_SERVICES, TIMELINE_AUX_SERVICE_NAME);
-    config.set(YarnConfiguration.NM_AUX_SERVICES + "." +
-            TIMELINE_AUX_SERVICE_NAME + ".class",
-        PerNodeTimelineCollectorsAuxService.class.getName());
-    config.setClass(YarnConfiguration.TIMELINE_SERVICE_WRITER_CLASS,
-        FileSystemTimelineWriterImpl.class,
-        org.apache.hadoop.yarn.server.timelineservice.storage.
-            TimelineWriter.class);
-    setTimelineV2StorageDir();
-    // set the file system timeline writer storage directory
-    config.set(FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_DIR_ROOT,
-        getTimelineV2StorageDir());
-  }
-
-  @Test
-  public void testDSShellWithEnforceExecutionType() throws Exception {
-    YarnClient yarnClient = null;
-    AtomicReference<Throwable> thrownError = new AtomicReference<>(null);
-    AtomicReference<List<ContainerReport>> containersListRef =
-        new AtomicReference<>(null);
-    AtomicReference<ApplicationAttemptId> appAttemptIdRef =
-        new AtomicReference<>(null);
-    AtomicReference<ApplicationAttemptReport> appAttemptReportRef =
-        new AtomicReference<>(null);
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "2",
-        "--master_memory",
-        "512",
-        "--master_vcores",
-        "2",
-        "--container_memory",
-        "128",
-        "--container_vcores",
-        "1",
-        "--shell_command",
-        getListCommand(),
-        "--container_type",
-        "OPPORTUNISTIC",
-        "--enforce_execution_type"
-    );
-    try {
-      setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-      getDSClient().init(args);
-      Thread dsClientRunner = new Thread(() -> {
-        try {
-          getDSClient().run();
-        } catch (Exception e) {
-          throw new RuntimeException(e);
-        }
-      });
-      dsClientRunner.start();
-
-      yarnClient = YarnClient.createYarnClient();
-      yarnClient.init(new Configuration(getYarnClusterConfiguration()));
-      yarnClient.start();
-
-      // expecting three containers including the AM container.
-      waitForContainersLaunch(yarnClient, 3, appAttemptReportRef,
-          containersListRef, appAttemptIdRef, thrownError);
-      if (thrownError.get() != null) {
-        Assert.fail(thrownError.get().getMessage());
-      }
-      ContainerId amContainerId = appAttemptReportRef.get().getAMContainerId();
-      for (ContainerReport container : containersListRef.get()) {
-        if (!container.getContainerId().equals(amContainerId)) {
-          Assert.assertEquals(container.getExecutionType(),
-              ExecutionType.OPPORTUNISTIC);
-        }
-      }
-    } catch (Exception e) {
-      LOG.error("Job execution with enforce execution type failed.", e);
-      Assert.fail("Exception. " + e.getMessage());
-    } finally {
-      if (yarnClient != null) {
-        yarnClient.stop();
-      }
-    }
-  }
-
-  @Test
-  public void testDistributedShellWithResources() throws Exception {
-    doTestDistributedShellWithResources(false);
-  }
-
-  @Test
-  public void testDistributedShellWithResourcesWithLargeContainers()
-      throws Exception {
-    doTestDistributedShellWithResources(true);
-  }
-
-  private void doTestDistributedShellWithResources(boolean largeContainers)
-      throws Exception {
-    AtomicReference<Throwable> thrownExceptionRef =
-        new AtomicReference<>(null);
-    AtomicReference<List<ContainerReport>> containersListRef =
-        new AtomicReference<>(null);
-    AtomicReference<ApplicationAttemptId> appAttemptIdRef =
-        new AtomicReference<>(null);
-    AtomicReference<ApplicationAttemptReport> appAttemptReportRef =
-        new AtomicReference<>(null);
-    Resource clusterResource = getYarnCluster().getResourceManager()
-        .getResourceScheduler().getClusterResource();
-    String masterMemoryString = "1 Gi";
-    String containerMemoryString = "512 Mi";
-    long[] memVars = {1024, 512};
-    YarnClient yarnClient = null;
-    Assume.assumeTrue("The cluster doesn't have enough memory for this test",
-        clusterResource.getMemorySize() >= memVars[0] + memVars[1]);
-    Assume.assumeTrue("The cluster doesn't have enough cores for this test",
-        clusterResource.getVirtualCores() >= 2);
-    if (largeContainers) {
-      memVars[0] = clusterResource.getMemorySize() * 2 / 3;
-      memVars[0] = memVars[0] - memVars[0] % MIN_ALLOCATION_MB;
-      masterMemoryString = memVars[0] + "Mi";
-      memVars[1] = clusterResource.getMemorySize() / 3;
-      memVars[1] = memVars[1] - memVars[1] % MIN_ALLOCATION_MB;
-      containerMemoryString = String.valueOf(memVars[1]);
-    }
-
-    String[] args = createArgumentsWithAppName(
-        "--num_containers",
-        "2",
-        "--shell_command",
-        getListCommand(),
-        "--master_resources",
-        "memory=" + masterMemoryString + ",vcores=1",
-        "--container_resources",
-        "memory=" + containerMemoryString + ",vcores=1"
-    );
-
-    LOG.info("Initializing DS Client");
-    setAndGetDSClient(new Configuration(getYarnClusterConfiguration()));
-    Assert.assertTrue(getDSClient().init(args));
-    LOG.info("Running DS Client");
-    final AtomicBoolean result = new AtomicBoolean(false);
-    Thread dsClientRunner = new Thread(() -> {
-      try {
-        result.set(getDSClient().run());
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    });
-    dsClientRunner.start();
-    try {
-      yarnClient = YarnClient.createYarnClient();
-      yarnClient.init(new Configuration(getYarnClusterConfiguration()));
-      yarnClient.start();
-      // expecting two containers.
-      waitForContainersLaunch(yarnClient, 2, appAttemptReportRef,
-          containersListRef, appAttemptIdRef, thrownExceptionRef);
-      if (thrownExceptionRef.get() != null) {
-        Assert.fail(thrownExceptionRef.get().getMessage());
-      }
-      ContainerId amContainerId = appAttemptReportRef.get().getAMContainerId();
-      ContainerReport report = yarnClient.getContainerReport(amContainerId);
-      Resource masterResource = report.getAllocatedResource();
-      Assert.assertEquals(memVars[0], masterResource.getMemorySize());
-      Assert.assertEquals(1, masterResource.getVirtualCores());
-      for (ContainerReport container : containersListRef.get()) {
-        if (!container.getContainerId().equals(amContainerId)) {
-          Resource containerResource = container.getAllocatedResource();
-          Assert.assertEquals(memVars[1],
-              containerResource.getMemorySize());
-          Assert.assertEquals(1, containerResource.getVirtualCores());
-        }
-      }
-    } finally {
-      LOG.info("Signaling Client to Stop");
-      if (yarnClient != null) {
-        LOG.info("Stopping yarnClient service");
-        yarnClient.stop();
-      }
-    }
-  }
-
-  @Test
-  public void testDSShellWithoutDomain() throws Exception {
-    baseTestDSShell(false);
-  }
-
-  @Test
-  public void testDSShellWithoutDomainDefaultFlow() throws Exception {
-    baseTestDSShell(false, true);
-  }
-
-  @Test
-  public void testDSShellWithoutDomainCustomizedFlow() throws Exception {
-    baseTestDSShell(false, false);
-  }
-
-  @Override
-  protected String[] appendFlowArgsForTestDSShell(String[] args,
-      boolean defaultFlow) {
-    if (!defaultFlow) {
-      String[] flowArgs = {
-          "--flow_name",
-          "test_flow_name",
-          "--flow_version",
-          "test_flow_version",
-          "--flow_run_id",
-          "12345678"
-      };
-      args = mergeArgs(args, flowArgs);
-    }
-    return args;
-  }
-
-  @Override
-  protected void checkTimeline(ApplicationId appId, boolean defaultFlow,
-      boolean haveDomain, ApplicationReport appReport) throws Exception {
-    LOG.info("Started {}#checkTimeline()", getClass().getCanonicalName());
-    // For PoC check using the file-based timeline writer (YARN-3264)
-    String tmpRoot = getTimelineV2StorageDir() + File.separator + "entities"
-        + File.separator;
-
-    File tmpRootFolder = new File(tmpRoot);
-    try {
-      Assert.assertTrue(tmpRootFolder.isDirectory());
-      String basePath = tmpRoot +
-          YarnConfiguration.DEFAULT_RM_CLUSTER_ID + File.separator +
-          UserGroupInformation.getCurrentUser().getShortUserName() +
-          (defaultFlow ?
-              File.separator + appReport.getName() + File.separator +
-                  TimelineUtils.DEFAULT_FLOW_VERSION + File.separator +
-                  appReport.getStartTime() + File.separator :
-              File.separator + "test_flow_name" + File.separator +
-                  "test_flow_version" + File.separator + "12345678" +
-                  File.separator) +
-          appId.toString();
-      LOG.info("basePath for appId {}: {}", appId, basePath);
-      // for this test, we expect DS_APP_ATTEMPT AND DS_CONTAINER dirs
-
-      // Verify DS_APP_ATTEMPT entities posted by the client
-      // there will be at least one attempt, look for that file
-      String appTimestampFileName =
-          String.format("appattempt_%d_000%d_000001%s",
-              appId.getClusterTimestamp(), appId.getId(),
-              FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_EXTENSION);
-      File dsAppAttemptEntityFile = verifyEntityTypeFileExists(basePath,
-          "DS_APP_ATTEMPT", appTimestampFileName);
-      // Check if required events are published and same idprefix is sent for
-      // on each publish.
-      verifyEntityForTimeline(dsAppAttemptEntityFile,
-          DSEvent.DS_APP_ATTEMPT_START.toString(), 1, 1, 0, true);
-      // to avoid race condition of testcase, at least check 40 times with
-      // sleep of 50ms
-      verifyEntityForTimeline(dsAppAttemptEntityFile,
-          DSEvent.DS_APP_ATTEMPT_END.toString(), 1, 40, 50, true);
-
-      // Verify DS_CONTAINER entities posted by the client.
-      String containerTimestampFileName =
-          String.format("container_%d_000%d_01_000002.thist",
-              appId.getClusterTimestamp(), appId.getId());
-      File dsContainerEntityFile = verifyEntityTypeFileExists(basePath,
-          "DS_CONTAINER", containerTimestampFileName);
-      // Check if required events are published and same idprefix is sent for
-      // on each publish.
-      verifyEntityForTimeline(dsContainerEntityFile,
-          DSEvent.DS_CONTAINER_START.toString(), 1, 1, 0, true);
-      // to avoid race condition of testcase, at least check 40 times with
-      // sleep of 50ms.
-      verifyEntityForTimeline(dsContainerEntityFile,
-          DSEvent.DS_CONTAINER_END.toString(), 1, 40, 50, true);
-
-      // Verify NM posting container metrics info.
-      String containerMetricsTimestampFileName =
-          String.format("container_%d_000%d_01_000001%s",
-              appId.getClusterTimestamp(), appId.getId(),
-              FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_EXTENSION);
-      File containerEntityFile = verifyEntityTypeFileExists(basePath,
-          TimelineEntityType.YARN_CONTAINER.toString(),
-          containerMetricsTimestampFileName);
-      verifyEntityForTimeline(containerEntityFile,
-          ContainerMetricsConstants.CREATED_EVENT_TYPE, 1, 1, 0, true);
-
-      // to avoid race condition of testcase, at least check 40 times with
-      // sleep of 50ms
-      verifyEntityForTimeline(containerEntityFile,
-          ContainerMetricsConstants.FINISHED_EVENT_TYPE, 1, 40, 50, true);
-
-      // Verify RM posting Application life cycle Events are getting published
-      String appMetricsTimestampFileName =
-          String.format("application_%d_000%d%s",
-              appId.getClusterTimestamp(), appId.getId(),
-              FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_EXTENSION);
-      File appEntityFile =
-          verifyEntityTypeFileExists(basePath,
-              TimelineEntityType.YARN_APPLICATION.toString(),
-              appMetricsTimestampFileName);
-      // No need to check idprefix for app.
-      verifyEntityForTimeline(appEntityFile,
-          ApplicationMetricsConstants.CREATED_EVENT_TYPE, 1, 1, 0, false);
-
-      // to avoid race condition of testcase, at least check 40 times with
-      // sleep of 50ms
-      verifyEntityForTimeline(appEntityFile,
-          ApplicationMetricsConstants.FINISHED_EVENT_TYPE, 1, 40, 50, false);
-
-      // Verify RM posting AppAttempt life cycle Events are getting published
-      String appAttemptMetricsTimestampFileName =
-          String.format("appattempt_%d_000%d_000001%s",
-              appId.getClusterTimestamp(), appId.getId(),
-              FileSystemTimelineWriterImpl.TIMELINE_SERVICE_STORAGE_EXTENSION);
-
-      File appAttemptEntityFile =
-          verifyEntityTypeFileExists(basePath,
-              TimelineEntityType.YARN_APPLICATION_ATTEMPT.toString(),
-              appAttemptMetricsTimestampFileName);
-      verifyEntityForTimeline(appAttemptEntityFile,
-          AppAttemptMetricsConstants.REGISTERED_EVENT_TYPE, 1, 1, 0, true);
-      verifyEntityForTimeline(appAttemptEntityFile,
-          AppAttemptMetricsConstants.FINISHED_EVENT_TYPE, 1, 1, 0, true);
-    } finally {
-      try {
-        FileUtils.deleteDirectory(tmpRootFolder.getParentFile());
-      } catch (Exception ex) {
-        // the recursive delete can throw an exception when one of the file
-        // does not exist.
-        LOG.warn("Exception deleting a file/subDirectory: {}", ex.getMessage());
-      }
-    }
-  }
-
-  /**
-   * Checks the events and idprefix published for an entity.
-   *
-   * @param entityFile Entity file.
-   * @param expectedEvent Expected event Id.
-   * @param numOfExpectedEvent Number of expected occurrences of expected event
-   *                           id.
-   * @param checkTimes Number of times to check.
-   * @param sleepTime Sleep time for each iteration.
-   * @param checkIdPrefix Whether to check idprefix.
-   * @throws IOException if entity file reading fails.
-   * @throws InterruptedException if sleep is interrupted.
-   */
-  private void verifyEntityForTimeline(File entityFile, String expectedEvent,
-      long numOfExpectedEvent, int checkTimes, long sleepTime,
-      boolean checkIdPrefix) throws Exception  {
-    AtomicReference<Throwable> thrownExceptionRef = new AtomicReference<>(null);
-    GenericTestUtils.waitFor(() -> {
-      String strLine;
-      long actualCount = 0;
-      long idPrefix = -1;
-      try (BufferedReader reader =
-               new BufferedReader(new FileReader(entityFile))) {
-        while ((strLine = reader.readLine()) != null) {
-          String entityLine = strLine.trim();
-          if (entityLine.isEmpty()) {
-            continue;
-          }
-          if (entityLine.contains(expectedEvent)) {
-            actualCount++;
-          }
-          if (expectedEvent.equals(DSEvent.DS_CONTAINER_END.toString())
-              && entityLine.contains(expectedEvent)) {
-            TimelineEntity entity = FileSystemTimelineReaderImpl.
-                getTimelineRecordFromJSON(entityLine, TimelineEntity.class);
-            TimelineEvent event = entity.getEvents().pollFirst();
-            Assert.assertNotNull(event);
-            Assert.assertTrue("diagnostics",
-                event.getInfo().containsKey(ApplicationMaster.DIAGNOSTICS));
-          }
-          if (checkIdPrefix) {
-            TimelineEntity entity = FileSystemTimelineReaderImpl.
-                getTimelineRecordFromJSON(entityLine, TimelineEntity.class);
-            Assert.assertTrue("Entity ID prefix expected to be > 0",
-                entity.getIdPrefix() > 0);
-            if (idPrefix == -1) {
-              idPrefix = entity.getIdPrefix();
-            } else {
-              Assert.assertEquals(
-                  "Entity ID prefix should be same across each publish of "
-                      + "same entity", idPrefix, entity.getIdPrefix());
-            }
-          }
-        }
-      } catch (Throwable e) {
-        LOG.error("Exception is waiting on application report", e);
-        thrownExceptionRef.set(e);
-        return true;
-      }
-      return (numOfExpectedEvent == actualCount);
-    }, sleepTime, (checkTimes + 1) * sleepTime);
-
-    if (thrownExceptionRef.get() != null) {
-      Assert.fail("verifyEntityForTimeline failed "
-          + thrownExceptionRef.get().getMessage());
-    }
-  }
-
-  private File verifyEntityTypeFileExists(String basePath, String entityType,
-      String entityFileName) {
-    String outputDirPathForEntity =
-        basePath + File.separator + entityType + File.separator;
-    LOG.info("verifyEntityTypeFileExists output path for entityType {}: {}",
-        entityType, outputDirPathForEntity);
-    File outputDirForEntity = new File(outputDirPathForEntity);
-    Assert.assertTrue(outputDirForEntity.isDirectory());
-    String entityFilePath = outputDirPathForEntity + entityFileName;
-    File entityFile = new File(entityFilePath);
-    Assert.assertTrue(entityFile.exists());
-    return entityFile;
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSWithMultipleNodeManager.java b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSWithMultipleNodeManager.java
deleted file mode 100644
index 19f04237f09..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/java/org/apache/hadoop/yarn/applications/distributedshell/TestDSWithMultipleNodeManager.java
+++ /dev/null
@@ -1,503 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.yarn.applications.distributedshell;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Set;
-import java.util.concurrent.ConcurrentMap;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicReference;
-
-import org.junit.After;
-import org.junit.AfterClass;
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Rule;
-import org.junit.Test;
-import org.junit.rules.TestName;
-import org.junit.rules.Timeout;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.test.GenericTestUtils;
-import org.apache.hadoop.thirdparty.com.google.common.collect.ImmutableMap;
-import org.apache.hadoop.yarn.api.records.ApplicationId;
-import org.apache.hadoop.yarn.api.records.NodeId;
-import org.apache.hadoop.yarn.client.api.YarnClient;
-import org.apache.hadoop.yarn.conf.YarnConfiguration;
-import org.apache.hadoop.yarn.server.nodemanager.NodeManager;
-import org.apache.hadoop.yarn.server.resourcemanager.RMContext;
-import org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager;
-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;
-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;
-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;
-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler;
-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration;
-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.ResourceUsageMultiNodeLookupPolicy;
-import org.apache.hadoop.yarn.util.resource.DominantResourceCalculator;
-
-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.PREFIX;
-
-/**
- * Test for Distributed Shell With Multiple Node Managers.
- * Parameter 0 tests with Single Node Placement and
- * parameter 1 tests with Multiple Node Placement.
- */
-@RunWith(value = Parameterized.class)
-public class TestDSWithMultipleNodeManager {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(TestDSWithMultipleNodeManager.class);
-
-  private static final int NUM_NMS = 2;
-  private static final String POLICY_CLASS_NAME =
-      ResourceUsageMultiNodeLookupPolicy.class.getName();
-  private final Boolean multiNodePlacementEnabled;
-  @Rule
-  public TestName name = new TestName();
-  @Rule
-  public Timeout globalTimeout =
-      new Timeout(DistributedShellBaseTest.TEST_TIME_OUT,
-          TimeUnit.MILLISECONDS);
-  private DistributedShellBaseTest distShellTest;
-  private Client dsClient;
-
-  public TestDSWithMultipleNodeManager(Boolean multiNodePlacementEnabled) {
-    this.multiNodePlacementEnabled = multiNodePlacementEnabled;
-  }
-
-  @Parameterized.Parameters
-  public static Collection<Boolean> getParams() {
-    return Arrays.asList(false, true);
-  }
-
-  private YarnConfiguration getConfiguration(
-      boolean multiNodePlacementConfigs) {
-    YarnConfiguration conf = new YarnConfiguration();
-    if (multiNodePlacementConfigs) {
-      conf.set(CapacitySchedulerConfiguration.RESOURCE_CALCULATOR_CLASS,
-          DominantResourceCalculator.class.getName());
-      conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,
-          ResourceScheduler.class);
-      conf.set(CapacitySchedulerConfiguration.MULTI_NODE_SORTING_POLICIES,
-          "resource-based");
-      conf.set(CapacitySchedulerConfiguration.MULTI_NODE_SORTING_POLICY_NAME,
-          "resource-based");
-      String policyName =
-          CapacitySchedulerConfiguration.MULTI_NODE_SORTING_POLICY_NAME
-          + ".resource-based" + ".class";
-      conf.set(policyName, POLICY_CLASS_NAME);
-      conf.setBoolean(
-          CapacitySchedulerConfiguration.MULTI_NODE_PLACEMENT_ENABLED, true);
-    }
-    return conf;
-  }
-
-  @BeforeClass
-  public static void setupUnitTests() throws Exception {
-    TestDSTimelineV10.setupUnitTests();
-  }
-
-  @AfterClass
-  public static void tearDownUnitTests() throws Exception {
-    TestDSTimelineV10.tearDownUnitTests();
-  }
-
-  @Before
-  public void setup() throws Exception {
-    distShellTest = new TestDSTimelineV10();
-    distShellTest.setupInternal(NUM_NMS,
-        getConfiguration(multiNodePlacementEnabled));
-  }
-
-  @After
-  public void tearDown() throws Exception {
-    if (dsClient != null) {
-      dsClient.sendStopSignal();
-      dsClient = null;
-    }
-    if (distShellTest != null) {
-      distShellTest.tearDown();
-      distShellTest = null;
-    }
-  }
-
-  private void initializeNodeLabels() throws IOException {
-    RMContext rmContext = distShellTest.getResourceManager(0).getRMContext();
-    // Setup node labels
-    RMNodeLabelsManager labelsMgr = rmContext.getNodeLabelManager();
-    Set<String> labels = new HashSet<>();
-    labels.add("x");
-    labelsMgr.addToCluserNodeLabelsWithDefaultExclusivity(labels);
-
-    // Setup queue access to node labels
-    distShellTest.setConfiguration(PREFIX + "root.accessible-node-labels", "x");
-    distShellTest.setConfiguration(
-        PREFIX + "root.accessible-node-labels.x.capacity", "100");
-    distShellTest.setConfiguration(
-        PREFIX + "root.default.accessible-node-labels", "x");
-    distShellTest.setConfiguration(PREFIX
-        + "root.default.accessible-node-labels.x.capacity", "100");
-
-    rmContext.getScheduler().reinitialize(distShellTest.getConfiguration(),
-        rmContext);
-
-    // Fetch node-ids from yarn cluster
-    NodeId[] nodeIds = new NodeId[NUM_NMS];
-    for (int i = 0; i < NUM_NMS; i++) {
-      NodeManager mgr = distShellTest.getNodeManager(i);
-      nodeIds[i] = mgr.getNMContext().getNodeId();
-    }
-
-    // Set label x to NM[1]
-    labelsMgr.addLabelsToNode(ImmutableMap.of(nodeIds[1], labels));
-  }
-
-  @Test
-  public void testDSShellWithNodeLabelExpression() throws Exception {
-    NMContainerMonitor containerMonitorRunner = null;
-    initializeNodeLabels();
-
-    try {
-      // Start NMContainerMonitor
-      containerMonitorRunner = new NMContainerMonitor();
-      containerMonitorRunner.start();
-
-      // Submit a job which will sleep for 60 sec
-      String[] args =
-          DistributedShellBaseTest.createArguments(() -> generateAppName(),
-              "--num_containers",
-              "4",
-              "--shell_command",
-              "sleep",
-              "--shell_args",
-              "15",
-              "--master_memory",
-              "512",
-              "--master_vcores",
-              "2",
-              "--container_memory",
-              "128",
-              "--container_vcores",
-              "1",
-              "--node_label_expression",
-              "x"
-          );
-
-      LOG.info("Initializing DS Client");
-      dsClient =
-          new Client(
-              new Configuration(distShellTest.getYarnClusterConfiguration()));
-      Assert.assertTrue(dsClient.init(args));
-      LOG.info("Running DS Client");
-      boolean result = dsClient.run();
-      LOG.info("Client run completed. Result={}", result);
-
-      containerMonitorRunner.stopMonitoring();
-
-      // Check maximum number of containers on each NMs
-      int[] maxRunningContainersOnNMs =
-          containerMonitorRunner.getMaxRunningContainersReport();
-      // Check no container allocated on NM[0]
-      Assert.assertEquals(0, maxRunningContainersOnNMs[0]);
-      // Check there are some containers allocated on NM[1]
-      Assert.assertTrue(maxRunningContainersOnNMs[1] > 0);
-    } finally {
-      if (containerMonitorRunner != null) {
-        containerMonitorRunner.stopMonitoring();
-        containerMonitorRunner.join();
-      }
-    }
-  }
-
-  @Test
-  public void testDistributedShellWithPlacementConstraint()
-      throws Exception {
-    NMContainerMonitor containerMonitorRunner = null;
-    String[] args =
-        DistributedShellBaseTest.createArguments(() -> generateAppName(),
-            "1",
-            "--shell_command",
-            DistributedShellBaseTest.getSleepCommand(15),
-            "--placement_spec",
-            "zk(1),NOTIN,NODE,zk:spark(1),NOTIN,NODE,zk"
-        );
-    try {
-      containerMonitorRunner = new NMContainerMonitor();
-      containerMonitorRunner.start();
-
-      LOG.info("Initializing DS Client with args {}", Arrays.toString(args));
-      dsClient =
-          new Client(
-              new Configuration(distShellTest.getYarnClusterConfiguration()));
-      Assert.assertTrue(dsClient.init(args));
-      LOG.info("Running DS Client");
-      boolean result = dsClient.run();
-      LOG.info("Client run completed. Result={}", result);
-
-      containerMonitorRunner.stopMonitoring();
-
-      ConcurrentMap<ApplicationId, RMApp> apps =
-          distShellTest.getResourceManager().getRMContext().getRMApps();
-      RMApp app = apps.values().iterator().next();
-      RMAppAttempt appAttempt = app.getAppAttempts().values().iterator().next();
-      NodeId masterNodeId = appAttempt.getMasterContainer().getNodeId();
-      NodeManager nm1 = distShellTest.getNodeManager(0);
-
-      int[] expectedNMsCount = new int[]{1, 1};
-      if (nm1.getNMContext().getNodeId().equals(masterNodeId)) {
-        expectedNMsCount[0]++;
-      } else {
-        expectedNMsCount[1]++;
-      }
-
-      int[] maxRunningContainersOnNMs =
-          containerMonitorRunner.getMaxRunningContainersReport();
-      Assert.assertEquals(expectedNMsCount[0], maxRunningContainersOnNMs[0]);
-      Assert.assertEquals(expectedNMsCount[1], maxRunningContainersOnNMs[1]);
-    } finally {
-      if (containerMonitorRunner != null) {
-        containerMonitorRunner.stopMonitoring();
-        containerMonitorRunner.join();
-      }
-    }
-  }
-
-  @Test
-  public void testDistributedShellWithAllocationTagNamespace()
-      throws Exception {
-    NMContainerMonitor containerMonitorRunner = null;
-    Client clientB = null;
-    YarnClient yarnClient = null;
-
-    String[] argsA =
-        DistributedShellBaseTest.createArguments(() -> generateAppName("001"),
-            "--shell_command",
-            DistributedShellBaseTest.getSleepCommand(30),
-            "--placement_spec",
-            "bar(1),notin,node,bar"
-        );
-    String[] argsB =
-        DistributedShellBaseTest.createArguments(() -> generateAppName("002"),
-            "1",
-            "--shell_command",
-            DistributedShellBaseTest.getListCommand(),
-            "--placement_spec",
-            "foo(3),notin,node,all/bar"
-        );
-
-    try {
-      containerMonitorRunner = new NMContainerMonitor();
-      containerMonitorRunner.start();
-      dsClient =
-          new Client(
-              new Configuration(distShellTest.getYarnClusterConfiguration()));
-      dsClient.init(argsA);
-      Thread dsClientRunner = new Thread(() -> {
-        try {
-          dsClient.run();
-        } catch (Exception e) {
-          throw new RuntimeException(e);
-        }
-      });
-      dsClientRunner.start();
-
-      NodeId taskContainerNodeIdA;
-      ConcurrentMap<ApplicationId, RMApp> apps;
-      AtomicReference<RMApp> appARef = new AtomicReference<>(null);
-      AtomicReference<NodeId> masterContainerNodeIdARef =
-          new AtomicReference<>(null);
-      int[] expectedNMCounts = new int[]{0, 0};
-
-      waitForExpectedNMsCount(expectedNMCounts, appARef,
-          masterContainerNodeIdARef);
-
-      NodeId nodeA = distShellTest.getNodeManager(0).getNMContext().
-          getNodeId();
-      NodeId nodeB = distShellTest.getNodeManager(1).getNMContext().
-          getNodeId();
-      Assert.assertEquals(2, (expectedNMCounts[0] + expectedNMCounts[1]));
-      if (expectedNMCounts[0] != expectedNMCounts[1]) {
-        taskContainerNodeIdA = masterContainerNodeIdARef.get();
-      } else {
-        taskContainerNodeIdA =
-            masterContainerNodeIdARef.get().equals(nodeA) ? nodeB : nodeA;
-      }
-
-      clientB =
-          new Client(
-              new Configuration(distShellTest.getYarnClusterConfiguration()));
-      clientB.init(argsB);
-      Assert.assertTrue(clientB.run());
-      containerMonitorRunner.stopMonitoring();
-      apps = distShellTest.getResourceManager().getRMContext().getRMApps();
-      Iterator<RMApp> it = apps.values().iterator();
-      RMApp appB = it.next();
-      if (appARef.get().equals(appB)) {
-        appB = it.next();
-      }
-      LOG.info("Allocation Tag NameSpace Applications are={} and {}",
-          appARef.get().getApplicationId(), appB.getApplicationId());
-
-      RMAppAttempt appAttemptB =
-          appB.getAppAttempts().values().iterator().next();
-      NodeId masterContainerNodeIdB =
-          appAttemptB.getMasterContainer().getNodeId();
-
-      if (nodeA.equals(masterContainerNodeIdB)) {
-        expectedNMCounts[0]++;
-      } else {
-        expectedNMCounts[1]++;
-      }
-      if (nodeA.equals(taskContainerNodeIdA)) {
-        expectedNMCounts[1] += 3;
-      } else {
-        expectedNMCounts[0] += 3;
-      }
-      int[] maxRunningContainersOnNMs =
-          containerMonitorRunner.getMaxRunningContainersReport();
-      Assert.assertEquals(expectedNMCounts[0], maxRunningContainersOnNMs[0]);
-      Assert.assertEquals(expectedNMCounts[1], maxRunningContainersOnNMs[1]);
-
-      try {
-        yarnClient = YarnClient.createYarnClient();
-        yarnClient.init(
-            new Configuration(distShellTest.getYarnClusterConfiguration()));
-        yarnClient.start();
-        yarnClient.killApplication(appARef.get().getApplicationId());
-      } catch (Exception e) {
-        // Ignore Exception while killing a job
-        LOG.warn("Exception killing the job: {}", e.getMessage());
-      }
-    } finally {
-      if (yarnClient != null) {
-        yarnClient.stop();
-      }
-      if (clientB != null) {
-        clientB.sendStopSignal();
-      }
-      if (containerMonitorRunner != null) {
-        containerMonitorRunner.stopMonitoring();
-        containerMonitorRunner.join();
-      }
-    }
-  }
-
-  protected String generateAppName() {
-    return generateAppName(null);
-  }
-
-  protected String generateAppName(String postFix) {
-    return name.getMethodName().replaceFirst("test", "")
-        .concat(postFix == null ? "" : "-" + postFix);
-  }
-
-  private void waitForExpectedNMsCount(int[] expectedNMCounts,
-      AtomicReference<RMApp> appARef,
-      AtomicReference<NodeId> masterContainerNodeIdARef) throws Exception {
-    GenericTestUtils.waitFor(() -> {
-      if ((expectedNMCounts[0] + expectedNMCounts[1]) < 2) {
-        expectedNMCounts[0] =
-            distShellTest.getNodeManager(0).getNMContext()
-                .getContainers().size();
-        expectedNMCounts[1] =
-            distShellTest.getNodeManager(1).getNMContext()
-                .getContainers().size();
-        return false;
-      }
-      ConcurrentMap<ApplicationId, RMApp> appIDsMap =
-          distShellTest.getResourceManager().getRMContext().getRMApps();
-      if (appIDsMap.isEmpty()) {
-        return false;
-      }
-      appARef.set(appIDsMap.values().iterator().next());
-      if (appARef.get().getAppAttempts().isEmpty()) {
-        return false;
-      }
-      RMAppAttempt appAttemptA =
-          appARef.get().getAppAttempts().values().iterator().next();
-      if (appAttemptA.getMasterContainer() == null) {
-        return false;
-      }
-      masterContainerNodeIdARef.set(
-          appAttemptA.getMasterContainer().getNodeId());
-      return true;
-    }, 10, 60000);
-  }
-
-  /**
-   * Monitor containers running on NMs.
-   */
-  class NMContainerMonitor extends Thread {
-    // The interval of milliseconds of sampling (500ms)
-    private final static int SAMPLING_INTERVAL_MS = 500;
-
-    // The maximum number of containers running on each NMs
-    private final int[] maxRunningContainersOnNMs = new int[NUM_NMS];
-    private final Object quitSignal = new Object();
-    private volatile boolean isRunning = true;
-
-    @Override
-    public void run() {
-      while (isRunning) {
-        for (int i = 0; i < NUM_NMS; i++) {
-          int nContainers =
-              distShellTest.getNodeManager(i).getNMContext()
-                  .getContainers().size();
-          if (nContainers > maxRunningContainersOnNMs[i]) {
-            maxRunningContainersOnNMs[i] = nContainers;
-          }
-        }
-        synchronized (quitSignal) {
-          try {
-            if (!isRunning) {
-              break;
-            }
-            quitSignal.wait(SAMPLING_INTERVAL_MS);
-          } catch (InterruptedException e) {
-            LOG.warn("NMContainerMonitor interrupted");
-            isRunning = false;
-            break;
-          }
-        }
-      }
-    }
-
-    public int[] getMaxRunningContainersReport() {
-      return maxRunningContainersOnNMs;
-    }
-
-    public void stopMonitoring() {
-      if (!isRunning) {
-        return;
-      }
-      synchronized (quitSignal) {
-        isRunning = false;
-        quitSignal.notifyAll();
-      }
-    }
-  }
-}
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/a.txt b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/a.txt
deleted file mode 100644
index 231a9db3768..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/a.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-#
-# Sample file for testing
-
-aaaaaaaaaaaaaaaaaa
\ No newline at end of file
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/b.txt b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/b.txt
deleted file mode 100644
index 86464314564..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/b.txt
+++ /dev/null
@@ -1,15 +0,0 @@
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-#
-# Sample file for testing
-
-bbbbbbbbbbbbbbbbbbbb
\ No newline at end of file
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/log4j.properties b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/log4j.properties
deleted file mode 100644
index 81a3f6ad5d2..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/log4j.properties
+++ /dev/null
@@ -1,19 +0,0 @@
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-
-# log4j configuration used during build and unit tests
-
-log4j.rootLogger=info,stdout
-log4j.threshold=ALL
-log4j.appender.stdout=org.apache.log4j.ConsoleAppender
-log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
-log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p [%t] %c{2} (%F:%M(%L)) - %m%n
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/yarn-site.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/yarn-site.xml
deleted file mode 100644
index 9660ace4688..00000000000
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/test/resources/yarn-site.xml
+++ /dev/null
@@ -1,21 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-<configuration>
-  <!-- Dummy (invalid) config file to be overwriten by TestDistributedShell with MiniCluster configuration. -->
-</configuration>
-
-
diff --git a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/pom.xml b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
index 2b7285f4989..16b2787393c 100644
--- a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
+++ b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/pom.xml
@@ -34,7 +34,6 @@
   <!-- Do not add dependencies here, add them to the POM of the leaf module -->
 
   <modules>
-    <module>hadoop-yarn-applications-distributedshell</module>
     <module>hadoop-yarn-applications-unmanaged-am-launcher</module>
     <module>hadoop-yarn-services</module>
     <module>hadoop-yarn-applications-mawo</module>
