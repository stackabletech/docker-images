Entirely remove hadoop-gridmix

From: Lars Francke <git@lars-francke.de>


---
 .../src/main/resources/assemblies/hadoop-tools.xml |   15 
 hadoop-project/pom.xml                             |    5 
 hadoop-project/src/site/site.xml                   |    1 
 .../dev-support/findbugs-exclude.xml               |   40 -
 hadoop-tools/hadoop-gridmix/pom.xml                |  199 ----
 .../hadoop/mapred/gridmix/AvgRecordFactory.java    |  123 --
 .../hadoop/mapred/gridmix/ClusterSummarizer.java   |  116 --
 .../mapred/gridmix/CompressionEmulationUtil.java   |  600 ------------
 .../mapred/gridmix/DistributedCacheEmulator.java   |  551 -----------
 .../hadoop/mapred/gridmix/EchoUserResolver.java    |   57 -
 .../hadoop/mapred/gridmix/ExecutionSummarizer.java |  320 ------
 .../org/apache/hadoop/mapred/gridmix/FilePool.java |  301 ------
 .../apache/hadoop/mapred/gridmix/FileQueue.java    |  103 --
 .../apache/hadoop/mapred/gridmix/GenerateData.java |  412 --------
 .../mapred/gridmix/GenerateDistCacheData.java      |  267 -----
 .../org/apache/hadoop/mapred/gridmix/Gridmix.java  |  809 ----------------
 .../apache/hadoop/mapred/gridmix/GridmixJob.java   |  526 -----------
 .../mapred/gridmix/GridmixJobSubmissionPolicy.java |   90 --
 .../apache/hadoop/mapred/gridmix/GridmixKey.java   |  301 ------
 .../hadoop/mapred/gridmix/GridmixRecord.java       |  272 ------
 .../apache/hadoop/mapred/gridmix/GridmixSplit.java |  148 ---
 .../apache/hadoop/mapred/gridmix/InputStriper.java |  137 ---
 .../mapred/gridmix/IntermediateRecordFactory.java  |  110 --
 .../apache/hadoop/mapred/gridmix/JobCreator.java   |  135 ---
 .../apache/hadoop/mapred/gridmix/JobFactory.java   |  276 ------
 .../apache/hadoop/mapred/gridmix/JobMonitor.java   |  294 ------
 .../apache/hadoop/mapred/gridmix/JobSubmitter.java |  225 -----
 .../org/apache/hadoop/mapred/gridmix/LoadJob.java  |  663 -------------
 .../apache/hadoop/mapred/gridmix/LoadSplit.java    |  180 ----
 .../apache/hadoop/mapred/gridmix/Progressive.java  |   25 -
 .../hadoop/mapred/gridmix/PseudoLocalFs.java       |  338 -------
 .../hadoop/mapred/gridmix/RandomAlgorithms.java    |  209 ----
 .../mapred/gridmix/RandomTextDataGenerator.java    |  147 ---
 .../hadoop/mapred/gridmix/ReadRecordFactory.java   |   85 --
 .../hadoop/mapred/gridmix/RecordFactory.java       |   40 -
 .../hadoop/mapred/gridmix/ReplayJobFactory.java    |  128 ---
 .../mapred/gridmix/RoundRobinUserResolver.java     |  141 ---
 .../hadoop/mapred/gridmix/SerialJobFactory.java    |  182 ----
 .../org/apache/hadoop/mapred/gridmix/SleepJob.java |  412 --------
 .../apache/hadoop/mapred/gridmix/StatListener.java |   32 -
 .../apache/hadoop/mapred/gridmix/Statistics.java   |  405 --------
 .../hadoop/mapred/gridmix/StressJobFactory.java    |  610 ------------
 .../mapred/gridmix/SubmitterUserResolver.java      |   59 -
 .../apache/hadoop/mapred/gridmix/Summarizer.java   |   75 --
 .../apache/hadoop/mapred/gridmix/UserResolver.java |   65 -
 .../CumulativeCpuUsageEmulatorPlugin.java          |  324 -------
 .../resourceusage/ResourceUsageEmulatorPlugin.java |   63 -
 .../resourceusage/ResourceUsageMatcher.java        |  111 --
 .../TotalHeapUsageEmulatorPlugin.java              |  279 ------
 .../src/main/shellprofile.d/hadoop-gridmix.sh      |   36 -
 .../hadoop-gridmix/src/site/markdown/GridMix.md.vm |  807 ----------------
 .../hadoop-gridmix/src/site/resources/css/site.css |   30 -
 .../hadoop/mapred/gridmix/CommonJobTest.java       |  385 --------
 .../hadoop/mapred/gridmix/DebugJobFactory.java     |  106 --
 .../hadoop/mapred/gridmix/DebugJobProducer.java    |  311 ------
 .../gridmix/DummyResourceCalculatorPlugin.java     |  154 ---
 .../hadoop/mapred/gridmix/GridmixTestUtils.java    |  120 --
 .../gridmix/TestCompressionEmulationUtils.java     |  586 ------------
 .../mapred/gridmix/TestDistCacheEmulation.java     |  430 ---------
 .../apache/hadoop/mapred/gridmix/TestFilePool.java |  189 ----
 .../hadoop/mapred/gridmix/TestFileQueue.java       |  143 ---
 .../hadoop/mapred/gridmix/TestGridMixClasses.java  |  989 --------------------
 .../mapred/gridmix/TestGridmixMemoryEmulation.java |  457 ---------
 .../hadoop/mapred/gridmix/TestGridmixRecord.java   |  278 ------
 .../mapred/gridmix/TestGridmixSubmission.java      |  203 ----
 .../hadoop/mapred/gridmix/TestGridmixSummary.java  |  391 --------
 .../hadoop/mapred/gridmix/TestHighRamJob.java      |  194 ----
 .../apache/hadoop/mapred/gridmix/TestLoadJob.java  |   82 --
 .../hadoop/mapred/gridmix/TestPseudoLocalFs.java   |  233 -----
 .../hadoop/mapred/gridmix/TestRandomAlgorithm.java |  132 ---
 .../gridmix/TestRandomTextDataGenerator.java       |   84 --
 .../hadoop/mapred/gridmix/TestRecordFactory.java   |   81 --
 .../mapred/gridmix/TestResourceUsageEmulators.java |  614 ------------
 .../apache/hadoop/mapred/gridmix/TestSleepJob.java |  142 ---
 .../hadoop/mapred/gridmix/TestUserResolve.java     |  172 ---
 .../src/test/resources/data/wordcount.json         |  414 --------
 .../src/test/resources/data/wordcount.json.gz      |  Bin
 .../src/test/resources/data/wordcount2.json        |  828 -----------------
 hadoop-tools/hadoop-tools-dist/pom.xml             |    5 
 hadoop-tools/pom.xml                               |    1 
 80 files changed, 19603 deletions(-)
 delete mode 100644 hadoop-tools/hadoop-gridmix/dev-support/findbugs-exclude.xml
 delete mode 100644 hadoop-tools/hadoop-gridmix/pom.xml
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
 delete mode 100755 hadoop-tools/hadoop-gridmix/src/main/shellprofile.d/hadoop-gridmix.sh
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/site/markdown/GridMix.md.vm
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/site/resources/css/site.css
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/CommonJobTest.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DummyResourceCalculatorPlugin.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestDistCacheEmulation.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFilePool.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridMixClasses.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSubmission.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestLoadJob.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestSleepJob.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json.gz
 delete mode 100644 hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount2.json

diff --git a/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml b/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
index 5d59374cbf0..f99ebc66e8a 100644
--- a/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
+++ b/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
@@ -81,21 +81,6 @@
         <include>*-sources.jar</include>
       </includes>
     </fileSet>
-    <fileSet>
-      <directory>../hadoop-gridmix/target</directory>
-      <outputDirectory>/share/hadoop/${hadoop.component}/sources</outputDirectory>
-      <includes>
-        <include>*-sources.jar</include>
-      </includes>
-    </fileSet>
-    <fileSet>
-      <directory>../hadoop-gridmix/src/main/shellprofile.d</directory>
-      <includes>
-        <include>*</include>
-      </includes>
-      <outputDirectory>/libexec/shellprofile.d</outputDirectory>
-      <fileMode>0755</fileMode>
-    </fileSet>
     <fileSet>
       <directory>../hadoop-rumen/target</directory>
       <outputDirectory>/share/hadoop/${hadoop.component}/sources</outputDirectory>
diff --git a/hadoop-project/pom.xml b/hadoop-project/pom.xml
index 9d6f373a9a8..6015bb3ef10 100644
--- a/hadoop-project/pom.xml
+++ b/hadoop-project/pom.xml
@@ -630,11 +630,6 @@
         <artifactId>hadoop-mapreduce-examples</artifactId>
         <version>${hadoop.version}</version>
       </dependency>
-      <dependency>
-        <groupId>org.apache.hadoop</groupId>
-        <artifactId>hadoop-gridmix</artifactId>
-        <version>${hadoop.version}</version>
-      </dependency>
 
       <dependency>
         <groupId>org.apache.hadoop</groupId>
diff --git a/hadoop-project/src/site/site.xml b/hadoop-project/src/site/site.xml
index c42c8756bd9..05c8f152a4d 100644
--- a/hadoop-project/src/site/site.xml
+++ b/hadoop-project/src/site/site.xml
@@ -193,7 +193,6 @@
       <item name="Hadoop Streaming" href="hadoop-streaming/HadoopStreaming.html"/>
       <item name="DistCp" href="hadoop-distcp/DistCp.html"/>
       <item name="HDFS Federation Balance" href="hadoop-federation-balance/HDFSFederationBalance.html"/>
-      <item name="GridMix" href="hadoop-gridmix/GridMix.html"/>
       <item name="Rumen" href="hadoop-rumen/Rumen.html"/>
       <item name="Resource Estimator Service" href="hadoop-resourceestimator/ResourceEstimator.html"/>
       <item name="Scheduler Load Simulator" href="hadoop-sls/SchedulerLoadSimulator.html"/>
diff --git a/hadoop-tools/hadoop-gridmix/dev-support/findbugs-exclude.xml b/hadoop-tools/hadoop-gridmix/dev-support/findbugs-exclude.xml
deleted file mode 100644
index cb043c4330e..00000000000
--- a/hadoop-tools/hadoop-gridmix/dev-support/findbugs-exclude.xml
+++ /dev/null
@@ -1,40 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-<FindBugsFilter>
-
- <!-- Ignore some irrelevant serialization warnings -->
-  <Match>
-    <Class name="org.apache.hadoop.mapred.gridmix.GridmixRecord$Comparator" />
-    <Bug pattern="SE_COMPARATOR_SHOULD_BE_SERIALIZABLE" />
-  </Match>
-
-  <!-- The called methods actually might throw Exceptions -->
-  <Match>
-    <Class name="org.apache.hadoop.mapred.gridmix.ExecutionSummarizer"/>
-    <Method name="processJobState"/>
-    <Bug pattern="REC_CATCH_EXCEPTION"/>
-    <Bug code="REC"/>
-  </Match>
-
-  <!-- This has been done knowingly and meant to fool JVM so that it doesn't optimize code -->
-  <Match>
-    <Class name="org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin$DefaultCpuUsageEmulator"/>
-    <Field name="returnValue"/>
-    <Bug pattern="URF_UNREAD_PUBLIC_OR_PROTECTED_FIELD"/>
-  </Match>
-</FindBugsFilter>
diff --git a/hadoop-tools/hadoop-gridmix/pom.xml b/hadoop-tools/hadoop-gridmix/pom.xml
deleted file mode 100644
index 85d9254578d..00000000000
--- a/hadoop-tools/hadoop-gridmix/pom.xml
+++ /dev/null
@@ -1,199 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.hadoop</groupId>
-    <artifactId>hadoop-project</artifactId>
-    <version>3.4.0</version>
-    <relativePath>../../hadoop-project</relativePath>
-  </parent>
-  <artifactId>hadoop-gridmix</artifactId>
-  <version>3.4.0</version>
-  <description>Apache Hadoop Gridmix</description>
-  <name>Apache Hadoop Gridmix</name>
-  <packaging>jar</packaging>
-
-  <properties>
-    <hadoop.log.dir>${project.build.directory}/log</hadoop.log.dir>
-  </properties>
-
-  <dependencies>
-    <dependency>
-      <groupId>junit</groupId>
-      <artifactId>junit</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-annotations</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-hs</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-core</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-mapreduce-client-jobclient</artifactId>
-      <scope>test</scope>
-      <type>test-jar</type>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-rumen</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs-client</artifactId>
-      <scope>provided</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-common</artifactId>
-      <scope>test</scope>
-      <type>test-jar</type>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs</artifactId>
-      <scope>test</scope>
-      <exclusions>
-        <exclusion>
-          <groupId>org.ow2.asm</groupId>
-          <artifactId>asm-commons</artifactId>
-        </exclusion>
-      </exclusions>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-hdfs</artifactId>
-      <scope>test</scope>
-      <type>test-jar</type>
-    </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-yarn-server-tests</artifactId>
-      <type>test-jar</type>
-      <scope>test</scope>
-    </dependency>
-     <dependency>
-      <groupId>org.mockito</groupId>
-      <artifactId>mockito-core</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.bouncycastle</groupId>
-      <artifactId>bcprov-jdk15on</artifactId>
-      <scope>test</scope>
-    </dependency>
-    <dependency>
-      <groupId>org.bouncycastle</groupId>
-      <artifactId>bcpkix-jdk15on</artifactId>
-      <scope>test</scope>
-    </dependency>
-  </dependencies>
-
-  <build>
-    <plugins>
-       <plugin>
-        <groupId>com.github.spotbugs</groupId>
-        <artifactId>spotbugs-maven-plugin</artifactId>
-         <configuration>
-          <xmlOutput>true</xmlOutput>
-          <excludeFilterFile>${basedir}/dev-support/findbugs-exclude.xml</excludeFilterFile>
-          <effort>Max</effort>
-        </configuration>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-antrun-plugin</artifactId>
-        <executions>
-          <execution>
-            <id>create-log-dir</id>
-            <phase>process-test-resources</phase>
-            <goals>
-              <goal>run</goal>
-            </goals>
-            <configuration>
-              <target>
-                <delete dir="${test.build.data}"/>
-                <mkdir dir="${test.build.data}"/>
-                <mkdir dir="${hadoop.log.dir}"/>
-              </target>
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.rat</groupId>
-        <artifactId>apache-rat-plugin</artifactId>
-        <configuration>
-        <excludes>
-        <exclude>src/test/resources/data/*</exclude>
-        </excludes>
-        </configuration>
-      </plugin>
-      <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-jar-plugin</artifactId>
-         <configuration>
-          <archive>
-           <manifest>
-            <mainClass>org.apache.hadoop.mapred.gridmix.Gridmix</mainClass>
-           </manifest>
-         </archive>
-        </configuration>
-       </plugin>
-       <plugin>
-        <groupId>org.apache.maven.plugins</groupId>
-        <artifactId>maven-dependency-plugin</artifactId>
-        <executions>
-          <execution>
-            <id>deplist</id>
-            <phase>compile</phase>
-            <goals>
-              <goal>list</goal>
-            </goals>
-            <configuration>
-              <!-- referenced by a built-in command -->
-              <outputFile>${project.basedir}/target/hadoop-tools-deps/${project.artifactId}.tools-builtin.txt</outputFile>
-            </configuration>
-          </execution>
-        </executions>
-      </plugin>
-    </plugins>
-  </build>
-</project>
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
deleted file mode 100644
index 9ba6e9a7ad1..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/AvgRecordFactory.java
+++ /dev/null
@@ -1,123 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * Given byte and record targets, emit roughly equal-sized records satisfying
- * the contract.
- */
-class AvgRecordFactory extends RecordFactory {
-
-  /**
-   * Percentage of record for key data.
-   */
-  public static final String GRIDMIX_KEY_FRC = "gridmix.key.fraction";
-  public static final String GRIDMIX_MISSING_REC_SIZE = 
-    "gridmix.missing.rec.size";
-
-
-  private final long targetBytes;
-  private final long targetRecords;
-  private final long step;
-  private final int avgrec;
-  private final int keyLen;
-  private long accBytes = 0L;
-  private long accRecords = 0L;
-  private int unspilledBytes = 0;
-  private int minSpilledBytes = 0;
-
-  /**
-   * @param targetBytes Expected byte count.
-   * @param targetRecords Expected record count.
-   * @param conf Used to resolve edge cases @see #GRIDMIX_KEY_FRC
-   */
-  public AvgRecordFactory(long targetBytes, long targetRecords,
-      Configuration conf) {
-    this(targetBytes, targetRecords, conf, 0);
-  }
-  
-  /**
-   * @param minSpilledBytes Minimum amount of data expected per record
-   */
-  public AvgRecordFactory(long targetBytes, long targetRecords,
-      Configuration conf, int minSpilledBytes) {
-    this.targetBytes = targetBytes;
-    this.targetRecords = targetRecords <= 0 && this.targetBytes >= 0
-      ? Math.max(1,
-          this.targetBytes / conf.getInt(GRIDMIX_MISSING_REC_SIZE, 64 * 1024))
-      : targetRecords;
-    final long tmp = this.targetBytes / this.targetRecords;
-    step = this.targetBytes - this.targetRecords * tmp;
-    avgrec = (int) Math.min(Integer.MAX_VALUE, tmp + 1);
-    keyLen = Math.max(1,
-        (int)(tmp * Math.min(1.0f, conf.getFloat(GRIDMIX_KEY_FRC, 0.1f))));
-    this.minSpilledBytes = minSpilledBytes;
-  }
-
-  @Override
-  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
-    if (accBytes >= targetBytes) {
-      return false;
-    }
-    final int reclen = accRecords++ >= step ? avgrec - 1 : avgrec;
-    final int len = (int) Math.min(targetBytes - accBytes, reclen);
-    
-    unspilledBytes += len;
-    
-    // len != reclen?
-    if (key != null) {
-      if (unspilledBytes < minSpilledBytes && accRecords < targetRecords) {
-        key.setSize(1);
-        val.setSize(1);
-        accBytes += key.getSize() + val.getSize();
-        unspilledBytes -= (key.getSize() + val.getSize());
-      } else {
-        key.setSize(keyLen);
-        val.setSize(unspilledBytes - key.getSize());
-        accBytes += unspilledBytes;
-        unspilledBytes = 0;
-      }
-    } else {
-      if (unspilledBytes < minSpilledBytes && accRecords < targetRecords) {
-        val.setSize(1);
-        accBytes += val.getSize();
-        unspilledBytes -= val.getSize();
-      } else {
-        val.setSize(unspilledBytes);
-        accBytes += unspilledBytes;
-        unspilledBytes = 0;
-      }
-    }
-    return true;
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    return Math.min(1.0f, accBytes / ((float)targetBytes));
-  }
-
-  @Override
-  public void close() throws IOException {
-    // noop
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
deleted file mode 100644
index 16e4c3a3503..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ClusterSummarizer.java
+++ /dev/null
@@ -1,116 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.lang3.time.FastDateFormat;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.CommonConfigurationKeys;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-
-/**
- * Summarizes the Hadoop cluster used in this {@link Gridmix} run. 
- * Statistics that are reported are
- * <ul>
- *   <li>Total number of active trackers in the cluster</li>
- *   <li>Total number of blacklisted trackers in the cluster</li>
- *   <li>Max map task capacity of the cluster</li>
- *   <li>Max reduce task capacity of the cluster</li>
- * </ul>
- * 
- * Apart from these statistics, {@link JobTracker} and {@link FileSystem} 
- * addresses are also recorded in the summary.
- */
-class ClusterSummarizer implements StatListener<ClusterStats> {
-  static final Logger LOG = LoggerFactory.getLogger(ClusterSummarizer.class);
-  
-  private int numBlacklistedTrackers;
-  private int numActiveTrackers;
-  private int maxMapTasks;
-  private int maxReduceTasks;
-  private String jobTrackerInfo = Summarizer.NA;
-  private String namenodeInfo = Summarizer.NA;
-  
-  @Override
-  @SuppressWarnings("deprecation")
-  public void update(ClusterStats item) {
-    try {
-      numBlacklistedTrackers = item.getStatus().getBlacklistedTrackers();
-      numActiveTrackers = item.getStatus().getTaskTrackers();
-      maxMapTasks = item.getStatus().getMaxMapTasks();
-      maxReduceTasks = item.getStatus().getMaxReduceTasks();
-    } catch (Exception e) {
-      long time = System.currentTimeMillis();
-      LOG.info("Error in processing cluster status at " 
-               + FastDateFormat.getInstance().format(time));
-    }
-  }
-  
-  /**
-   * Summarizes the cluster used for this {@link Gridmix} run.
-   */
-  @Override
-  public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append("Cluster Summary:-");
-    builder.append("\nJobTracker: ").append(getJobTrackerInfo());
-    builder.append("\nFileSystem: ").append(getNamenodeInfo());
-    builder.append("\nNumber of blacklisted trackers: ")
-           .append(getNumBlacklistedTrackers());
-    builder.append("\nNumber of active trackers: ")
-           .append(getNumActiveTrackers());
-    builder.append("\nMax map task capacity: ")
-           .append(getMaxMapTasks());
-    builder.append("\nMax reduce task capacity: ").append(getMaxReduceTasks());
-    builder.append("\n\n");
-    return builder.toString();
-  }
-  
-  void start(Configuration conf) {
-    jobTrackerInfo = conf.get(JTConfig.JT_IPC_ADDRESS);
-    namenodeInfo = conf.getTrimmed(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY);
-  }
-  
-  // Getters
-  protected int getNumBlacklistedTrackers() {
-    return numBlacklistedTrackers;
-  }
-  
-  protected int getNumActiveTrackers() {
-    return numActiveTrackers;
-  }
-  
-  protected int getMaxMapTasks() {
-    return maxMapTasks;
-  }
-  
-  protected int getMaxReduceTasks() {
-    return maxReduceTasks;
-  }
-  
-  protected String getJobTrackerInfo() {
-    return jobTrackerInfo;
-  }
-  
-  protected String getNamenodeInfo() {
-    return namenodeInfo;
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
deleted file mode 100644
index 99c621a3e92..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/CompressionEmulationUtil.java
+++ /dev/null
@@ -1,600 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.OutputStream;
-import java.nio.charset.Charset;
-import java.nio.charset.StandardCharsets;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.compress.CodecPool;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionCodecFactory;
-import org.apache.hadoop.io.compress.CompressionInputStream;
-import org.apache.hadoop.io.compress.Decompressor;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Utils;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-import org.apache.hadoop.mapred.gridmix.GenerateData.GenDataFormat;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.util.StringUtils;
-
-/**
- * This is a utility class for all the compression related modules.
- */
-class CompressionEmulationUtil {
-  static final Logger LOG = LoggerFactory.getLogger(CompressionEmulationUtil.class);
-  
-  /**
-   * Enable compression usage in GridMix runs.
-   */
-  private static final String COMPRESSION_EMULATION_ENABLE = 
-    "gridmix.compression-emulation.enable";
-  
-  /**
-   * Enable input data decompression.
-   */
-  private static final String INPUT_DECOMPRESSION_EMULATION_ENABLE = 
-    "gridmix.compression-emulation.input-decompression.enable";
-  
-  /**
-   * Configuration property for setting the compression ratio for map input 
-   * data.
-   */
-  private static final String GRIDMIX_MAP_INPUT_COMPRESSION_RATIO = 
-    "gridmix.compression-emulation.map-input.decompression-ratio";
-  
-  /**
-   * Configuration property for setting the compression ratio of map output.
-   */
-  private static final String GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO = 
-    "gridmix.compression-emulation.map-output.compression-ratio";
-  
-  /**
-   * Configuration property for setting the compression ratio of job output.
-   */
-  private static final String GRIDMIX_JOB_OUTPUT_COMPRESSION_RATIO = 
-    "gridmix.compression-emulation.job-output.compression-ratio";
-  
-  /**
-   * Default compression ratio.
-   */
-  static final float DEFAULT_COMPRESSION_RATIO = 0.5F;
-  
-  private static final CompressionRatioLookupTable COMPRESSION_LOOKUP_TABLE = 
-    new CompressionRatioLookupTable();
-
-  private static final Charset charsetUTF8 = StandardCharsets.UTF_8;
-
-  /**
-   * This is a {@link Mapper} implementation for generating random text data.
-   * It uses {@link RandomTextDataGenerator} for generating text data and the
-   * output files are compressed.
-   */
-  public static class RandomTextDataMapper
-  extends Mapper<NullWritable, LongWritable, Text, Text> {
-    private RandomTextDataGenerator rtg;
-
-    @Override
-    protected void setup(Context context)
-        throws IOException, InterruptedException {
-      Configuration conf = context.getConfiguration();
-      int listSize = 
-        RandomTextDataGenerator.getRandomTextDataGeneratorListSize(conf);
-      int wordSize = 
-        RandomTextDataGenerator.getRandomTextDataGeneratorWordSize(conf);
-      rtg = new RandomTextDataGenerator(listSize, wordSize);
-    }
-    
-    /**
-     * Emits random words sequence of desired size. Note that the desired output
-     * size is passed as the value parameter to this map.
-     */
-    @Override
-    public void map(NullWritable key, LongWritable value, Context context)
-    throws IOException, InterruptedException {
-      //TODO Control the extra data written ..
-      //TODO Should the key\tvalue\n be considered for measuring size?
-      //     Can counters like BYTES_WRITTEN be used? What will be the value of
-      //     such counters in LocalJobRunner?
-      for (long bytes = value.get(); bytes > 0;) {
-        String randomKey = rtg.getRandomWord();
-        String randomValue = rtg.getRandomWord();
-        context.write(new Text(randomKey), new Text(randomValue));
-        bytes -= (randomValue.getBytes(charsetUTF8).length +
-            randomKey.getBytes(charsetUTF8).length);
-      }
-    }
-  }
-  
-  /**
-   * Configure the {@link Job} for enabling compression emulation.
-   */
-  static void configure(final Job job) throws IOException, InterruptedException,
-                                              ClassNotFoundException {
-    // set the random text mapper
-    job.setMapperClass(RandomTextDataMapper.class);
-    job.setNumReduceTasks(0);
-    job.setMapOutputKeyClass(Text.class);
-    job.setMapOutputValueClass(Text.class);
-    job.setInputFormatClass(GenDataFormat.class);
-    job.setJarByClass(GenerateData.class);
-
-    // set the output compression true
-    FileOutputFormat.setCompressOutput(job, true);
-    try {
-      FileInputFormat.addInputPath(job, new Path("ignored"));
-    } catch (IOException e) {
-      LOG.error("Error while adding input path ", e);
-    }
-  }
-
-  /**
-   * This is the lookup table for mapping compression ratio to the size of the 
-   * word in the {@link RandomTextDataGenerator}'s dictionary. 
-   * 
-   * Note that this table is computed (empirically) using a dictionary of 
-   * default length i.e {@value RandomTextDataGenerator#DEFAULT_LIST_SIZE}.
-   */
-  private static class CompressionRatioLookupTable {
-    private static Map<Float, Integer> map = new HashMap<Float, Integer>(60);
-    private static final float MIN_RATIO = 0.07F;
-    private static final float MAX_RATIO = 0.68F;
-    
-    // add the empirically obtained data points in the lookup table
-    CompressionRatioLookupTable() {
-      map.put(.07F,30);
-      map.put(.08F,25);
-      map.put(.09F,60);
-      map.put(.10F,20);
-      map.put(.11F,70);
-      map.put(.12F,15);
-      map.put(.13F,80);
-      map.put(.14F,85);
-      map.put(.15F,90);
-      map.put(.16F,95);
-      map.put(.17F,100);
-      map.put(.18F,105);
-      map.put(.19F,110);
-      map.put(.20F,115);
-      map.put(.21F,120);
-      map.put(.22F,125);
-      map.put(.23F,130);
-      map.put(.24F,140);
-      map.put(.25F,145);
-      map.put(.26F,150);
-      map.put(.27F,155);
-      map.put(.28F,160);
-      map.put(.29F,170);
-      map.put(.30F,175);
-      map.put(.31F,180);
-      map.put(.32F,190);
-      map.put(.33F,195);
-      map.put(.34F,205);
-      map.put(.35F,215);
-      map.put(.36F,225);
-      map.put(.37F,230);
-      map.put(.38F,240);
-      map.put(.39F,250);
-      map.put(.40F,260);
-      map.put(.41F,270);
-      map.put(.42F,280);
-      map.put(.43F,295);
-      map.put(.44F,310);
-      map.put(.45F,325);
-      map.put(.46F,335);
-      map.put(.47F,355);
-      map.put(.48F,375);
-      map.put(.49F,395);
-      map.put(.50F,420);
-      map.put(.51F,440);
-      map.put(.52F,465);
-      map.put(.53F,500);
-      map.put(.54F,525);
-      map.put(.55F,550);
-      map.put(.56F,600);
-      map.put(.57F,640);
-      map.put(.58F,680);
-      map.put(.59F,734);
-      map.put(.60F,813);
-      map.put(.61F,905);
-      map.put(.62F,1000);
-      map.put(.63F,1055);
-      map.put(.64F,1160);
-      map.put(.65F,1355);
-      map.put(.66F,1510);
-      map.put(.67F,1805);
-      map.put(.68F,2170);
-    }
-    
-    /**
-     * Returns the size of the word in {@link RandomTextDataGenerator}'s 
-     * dictionary that can generate text with the desired compression ratio.
-     * 
-     * @throws RuntimeException If ratio is less than {@value #MIN_RATIO} or 
-     *                          greater than {@value #MAX_RATIO}.
-     */
-    int getWordSizeForRatio(float ratio) {
-      ratio = standardizeCompressionRatio(ratio);
-      if (ratio >= MIN_RATIO && ratio <= MAX_RATIO) {
-        return map.get(ratio);
-      } else {
-        throw new RuntimeException("Compression ratio should be in the range [" 
-          + MIN_RATIO + "," + MAX_RATIO + "]. Configured compression ratio is " 
-          + ratio + ".");
-      }
-    }
-  }
-  
-  /**
-   * Setup the data generator's configuration to generate compressible random 
-   * text data with the desired compression ratio.
-   * Note that the compression ratio, if configured, will set the 
-   * {@link RandomTextDataGenerator}'s list-size and word-size based on 
-   * empirical values using the compression ratio set in the configuration. 
-   * 
-   * Hence to achieve the desired compression ratio, 
-   * {@link RandomTextDataGenerator}'s list-size will be set to the default 
-   * value i.e {@value RandomTextDataGenerator#DEFAULT_LIST_SIZE}.
-   */
-  static void setupDataGeneratorConfig(Configuration conf) {
-    boolean compress = isCompressionEmulationEnabled(conf);
-    if (compress) {
-      float ratio = getMapInputCompressionEmulationRatio(conf);
-      LOG.info("GridMix is configured to generate compressed input data with "
-               + " a compression ratio of " + ratio);
-      int wordSize = COMPRESSION_LOOKUP_TABLE.getWordSizeForRatio(ratio);
-      RandomTextDataGenerator.setRandomTextDataGeneratorWordSize(conf, 
-                                                                 wordSize);
-
-      // since the compression ratios are computed using the default value of 
-      // list size
-      RandomTextDataGenerator.setRandomTextDataGeneratorListSize(conf, 
-          RandomTextDataGenerator.DEFAULT_LIST_SIZE);
-    }
-  }
-  
-  /**
-   * Returns a {@link RandomTextDataGenerator} that generates random 
-   * compressible text with the desired compression ratio.
-   */
-  static RandomTextDataGenerator getRandomTextDataGenerator(float ratio, 
-                                                            long seed) {
-    int wordSize = COMPRESSION_LOOKUP_TABLE.getWordSizeForRatio(ratio);
-    RandomTextDataGenerator rtg = 
-      new RandomTextDataGenerator(RandomTextDataGenerator.DEFAULT_LIST_SIZE, 
-            seed, wordSize);
-    return rtg;
-  }
-  
-  /** Publishes compression related data statistics. Following statistics are
-   * published
-   * <ul>
-   *   <li>Total compressed input data size</li>
-   *   <li>Number of compressed input data files</li>
-   *   <li>Compression Ratio</li>
-   *   <li>Text data dictionary size</li>
-   *   <li>Random text word size</li>
-   * </ul>
-   */
-  static DataStatistics publishCompressedDataStatistics(Path inputDir, 
-                          Configuration conf, long uncompressedDataSize) 
-  throws IOException {
-    FileSystem fs = inputDir.getFileSystem(conf);
-    CompressionCodecFactory compressionCodecs = 
-      new CompressionCodecFactory(conf);
-
-    // iterate over compressed files and sum up the compressed file sizes
-    long compressedDataSize = 0;
-    int numCompressedFiles = 0;
-    // obtain input data file statuses
-    FileStatus[] outFileStatuses = 
-      fs.listStatus(inputDir, new Utils.OutputFileUtils.OutputFilesFilter());
-    for (FileStatus status : outFileStatuses) {
-      // check if the input file is compressed
-      if (compressionCodecs != null) {
-        CompressionCodec codec = compressionCodecs.getCodec(status.getPath());
-        if (codec != null) {
-          ++numCompressedFiles;
-          compressedDataSize += status.getLen();
-        }
-      }
-    }
-
-    LOG.info("Gridmix is configured to use compressed input data.");
-    // publish the input data size
-    LOG.info("Total size of compressed input data : " 
-             + StringUtils.humanReadableInt(compressedDataSize));
-    LOG.info("Total number of compressed input data files : " 
-             + numCompressedFiles);
-
-    if (numCompressedFiles == 0) {
-      throw new RuntimeException("No compressed file found in the input" 
-          + " directory : " + inputDir.toString() + ". To enable compression"
-          + " emulation, run Gridmix either with "
-          + " an input directory containing compressed input file(s) or" 
-          + " use the -generate option to (re)generate it. If compression"
-          + " emulation is not desired, disable it by setting '" 
-          + COMPRESSION_EMULATION_ENABLE + "' to 'false'.");
-    }
-    
-    // publish compression ratio only if its generated in this gridmix run
-    if (uncompressedDataSize > 0) {
-      // compute the compression ratio
-      double ratio = ((double)compressedDataSize) / uncompressedDataSize;
-
-      // publish the compression ratio
-      LOG.info("Input Data Compression Ratio : " + ratio);
-    }
-    
-    return new DataStatistics(compressedDataSize, numCompressedFiles, true);
-  }
-  
-  /**
-   * Enables/Disables compression emulation.
-   * @param conf Target configuration where the parameter 
-   * {@value #COMPRESSION_EMULATION_ENABLE} will be set. 
-   * @param val The value to be set.
-   */
-  static void setCompressionEmulationEnabled(Configuration conf, boolean val) {
-    conf.setBoolean(COMPRESSION_EMULATION_ENABLE, val);
-  }
-  
-  /**
-   * Checks if compression emulation is enabled or not. Default is {@code true}.
-   */
-  static boolean isCompressionEmulationEnabled(Configuration conf) {
-    return conf.getBoolean(COMPRESSION_EMULATION_ENABLE, true);
-  }
-  
-  /**
-   * Enables/Disables input decompression emulation.
-   * @param conf Target configuration where the parameter 
-   * {@value #INPUT_DECOMPRESSION_EMULATION_ENABLE} will be set. 
-   * @param val The value to be set.
-   */
-  static void setInputCompressionEmulationEnabled(Configuration conf, 
-                                                  boolean val) {
-    conf.setBoolean(INPUT_DECOMPRESSION_EMULATION_ENABLE, val);
-  }
-  
-  /**
-   * Check if input decompression emulation is enabled or not. 
-   * Default is {@code false}.
-   */
-  static boolean isInputCompressionEmulationEnabled(Configuration conf) {
-    return conf.getBoolean(INPUT_DECOMPRESSION_EMULATION_ENABLE, false);
-  }
-  
-  /**
-   * Set the map input data compression ratio in the given conf.
-   */
-  static void setMapInputCompressionEmulationRatio(Configuration conf, 
-                                                   float ratio) {
-    conf.setFloat(GRIDMIX_MAP_INPUT_COMPRESSION_RATIO, ratio);
-  }
-  
-  /**
-   * Get the map input data compression ratio using the given configuration.
-   * If the compression ratio is not set in the configuration then use the 
-   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
-   */
-  static float getMapInputCompressionEmulationRatio(Configuration conf) {
-    return conf.getFloat(GRIDMIX_MAP_INPUT_COMPRESSION_RATIO, 
-                         DEFAULT_COMPRESSION_RATIO);
-  }
-  
-  /**
-   * Set the map output data compression ratio in the given configuration.
-   */
-  static void setMapOutputCompressionEmulationRatio(Configuration conf, 
-                                                    float ratio) {
-    conf.setFloat(GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO, ratio);
-  }
-  
-  /**
-   * Get the map output data compression ratio using the given configuration.
-   * If the compression ratio is not set in the configuration then use the 
-   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
-   */
-  static float getMapOutputCompressionEmulationRatio(Configuration conf) {
-    return conf.getFloat(GRIDMIX_MAP_OUTPUT_COMPRESSION_RATIO, 
-                         DEFAULT_COMPRESSION_RATIO);
-  }
-  
-  /**
-   * Set the job output data compression ratio in the given configuration.
-   */
-  static void setJobOutputCompressionEmulationRatio(Configuration conf, 
-                                                    float ratio) {
-    conf.setFloat(GRIDMIX_JOB_OUTPUT_COMPRESSION_RATIO, ratio);
-  }
-  
-  /**
-   * Get the job output data compression ratio using the given configuration.
-   * If the compression ratio is not set in the configuration then use the 
-   * default value i.e {@value #DEFAULT_COMPRESSION_RATIO}.
-   */
-  static float getJobOutputCompressionEmulationRatio(Configuration conf) {
-    return conf.getFloat(GRIDMIX_JOB_OUTPUT_COMPRESSION_RATIO, 
-                         DEFAULT_COMPRESSION_RATIO);
-  }
-  
-  /**
-   * Standardize the compression ratio i.e round off the compression ratio to
-   * only 2 significant digits.
-   */
-  static float standardizeCompressionRatio(float ratio) {
-    // round off to 2 significant digits
-    int significant = (int)Math.round(ratio * 100);
-    return ((float)significant)/100;
-  }
-  
-  /**
-   * Returns a {@link InputStream} for a file that might be compressed.
-   */
-  static InputStream getPossiblyDecompressedInputStream(Path file, 
-                                                        Configuration conf,
-                                                        long offset)
-  throws IOException {
-    FileSystem fs = file.getFileSystem(conf);
-    if (isCompressionEmulationEnabled(conf)
-        && isInputCompressionEmulationEnabled(conf)) {
-      CompressionCodecFactory compressionCodecs = 
-        new CompressionCodecFactory(conf);
-      CompressionCodec codec = compressionCodecs.getCodec(file);
-      if (codec != null) {
-        Decompressor decompressor = CodecPool.getDecompressor(codec);
-        if (decompressor != null) {
-          CompressionInputStream in = 
-            codec.createInputStream(fs.open(file), decompressor);
-          //TODO Seek doesnt work with compressed input stream. 
-          //     Use SplittableCompressionCodec?
-          return (InputStream)in;
-        }
-      }
-    }
-    FSDataInputStream in = fs.open(file);
-    in.seek(offset);
-    return (InputStream)in;
-  }
-  
-  /**
-   * Returns a {@link OutputStream} for a file that might need 
-   * compression.
-   */
-  static OutputStream getPossiblyCompressedOutputStream(Path file, 
-                                                        Configuration conf)
-  throws IOException {
-    FileSystem fs = file.getFileSystem(conf);
-    JobConf jConf = new JobConf(conf);
-    if (org.apache.hadoop.mapred.FileOutputFormat.getCompressOutput(jConf)) {
-      // get the codec class
-      Class<? extends CompressionCodec> codecClass =
-        org.apache.hadoop.mapred.FileOutputFormat
-                                .getOutputCompressorClass(jConf, 
-                                                          GzipCodec.class);
-      // get the codec implementation
-      CompressionCodec codec = ReflectionUtils.newInstance(codecClass, conf);
-
-      // add the appropriate extension
-      file = file.suffix(codec.getDefaultExtension());
-
-      if (isCompressionEmulationEnabled(conf)) {
-        FSDataOutputStream fileOut = fs.create(file, false);
-        return new DataOutputStream(codec.createOutputStream(fileOut));
-      }
-    }
-    return fs.create(file, false);
-  }
-  
-  /**
-   * Extracts compression/decompression related configuration parameters from 
-   * the source configuration to the target configuration.
-   */
-  static void configureCompressionEmulation(Configuration source, 
-                                            Configuration target) {
-    // enable output compression
-    target.setBoolean(FileOutputFormat.COMPRESS, 
-        source.getBoolean(FileOutputFormat.COMPRESS, false));
-
-    // set the job output compression codec
-    String jobOutputCompressionCodec = 
-      source.get(FileOutputFormat.COMPRESS_CODEC);
-    if (jobOutputCompressionCodec != null) {
-      target.set(FileOutputFormat.COMPRESS_CODEC, jobOutputCompressionCodec);
-    }
-
-    // set the job output compression type
-    String jobOutputCompressionType = 
-      source.get(FileOutputFormat.COMPRESS_TYPE);
-    if (jobOutputCompressionType != null) {
-      target.set(FileOutputFormat.COMPRESS_TYPE, jobOutputCompressionType);
-    }
-
-    // enable map output compression
-    target.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS,
-        source.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false));
-
-    // set the map output compression codecs
-    String mapOutputCompressionCodec = 
-      source.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC);
-    if (mapOutputCompressionCodec != null) {
-      target.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, 
-                 mapOutputCompressionCodec);
-    }
-
-    // enable input decompression
-    //TODO replace with mapInputBytes and hdfsBytesRead
-    Path[] inputs = 
-      org.apache.hadoop.mapred.FileInputFormat
-         .getInputPaths(new JobConf(source));
-    boolean needsCompressedInput = false;
-    CompressionCodecFactory compressionCodecs = 
-      new CompressionCodecFactory(source);
-    for (Path input : inputs) {
-      CompressionCodec codec = compressionCodecs.getCodec(input);
-      if (codec != null) {
-        needsCompressedInput = true;
-      }
-    }
-    setInputCompressionEmulationEnabled(target, needsCompressedInput);
-  }
-
-  /**
-   * Get the uncompressed input bytes count from the given possibly compressed
-   * input bytes count.
-   * @param possiblyCompressedInputBytes input bytes count. This is compressed
-   *        input size if compression emulation is on.
-   * @param conf configuration of the Gridmix simulated job
-   * @return uncompressed input bytes count. Compute this in case if compressed
-   *         input was used
-   */
-  static long getUncompressedInputBytes(long possiblyCompressedInputBytes,
-                                        Configuration conf) {
-    long uncompressedInputBytes = possiblyCompressedInputBytes;
-
-    if (CompressionEmulationUtil.isInputCompressionEmulationEnabled(conf)) {
-      float inputCompressionRatio =
-          CompressionEmulationUtil.getMapInputCompressionEmulationRatio(conf);
-      uncompressedInputBytes /= inputCompressionRatio;
-    }
-    return uncompressedInputBytes;
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
deleted file mode 100644
index a6f986ce260..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/DistributedCacheEmulator.java
+++ /dev/null
@@ -1,551 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configuration.DeprecationDelta;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsAction;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.MD5Hash;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants;
-
-import java.io.IOException;
-import java.net.URI;
-import java.net.URISyntaxException;
-import java.nio.charset.Charset;
-import java.nio.charset.StandardCharsets;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-
-/**
- * Emulation of Distributed Cache Usage in gridmix.
- * <br> Emulation of Distributed Cache Load in gridmix will put load on
- * TaskTrackers and affects execution time of tasks because of localization of
- * distributed cache files by TaskTrackers.
- * <br> Gridmix creates distributed cache files for simulated jobs by launching
- * a MapReduce job {@link GenerateDistCacheData} in advance i.e. before
- * launching simulated jobs.
- * <br> The distributed cache file paths used in the original cluster are mapped
- * to unique file names in the simulated cluster.
- * <br> All HDFS-based distributed cache files generated by gridmix are
- * public distributed cache files. But Gridmix makes sure that load incurred due
- * to localization of private distributed cache files on the original cluster
- * is also faithfully simulated. Gridmix emulates the load due to private
- * distributed cache files by mapping private distributed cache files of
- * different users in the original cluster to different public distributed cache
- * files in the simulated cluster.
- *
- * <br> The configuration properties like
- * {@link MRJobConfig#CACHE_FILES}, {@link MRJobConfig#CACHE_FILE_VISIBILITIES},
- * {@link MRJobConfig#CACHE_FILES_SIZES} and
- * {@link MRJobConfig#CACHE_FILE_TIMESTAMPS} obtained from trace are used to
- *  decide
- * <li> file size of each distributed cache file to be generated
- * <li> whether a distributed cache file is already seen in this trace file
- * <li> whether a distributed cache file was considered public or private.
- * <br>
- * <br> Gridmix configures these generated files as distributed cache files for
- * the simulated jobs.
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-class DistributedCacheEmulator {
-  private static final Logger LOG =
-      LoggerFactory.getLogger(DistributedCacheEmulator.class);
-
-  static final long AVG_BYTES_PER_MAP = 128 * 1024 * 1024L;// 128MB
-
-  private Path distCachePath;
-
-  /**
-   * Map between simulated cluster's distributed cache file paths and their
-   * file sizes. Unique distributed cache files are entered into this map.
-   * 2 distributed cache files are considered same if and only if their
-   * file paths, visibilities and timestamps are same.
-   */
-  private Map<String, Long> distCacheFiles = new HashMap<String, Long>();
-
-  /**
-   * Configuration property for whether gridmix should emulate
-   * distributed cache usage or not. Default value is true.
-   */
-  static final String GRIDMIX_EMULATE_DISTRIBUTEDCACHE =
-      "gridmix.distributed-cache-emulation.enable";
-
-  // Whether to emulate distributed cache usage or not
-  boolean emulateDistributedCache = true;
-
-  // Whether to generate distributed cache data or not
-  boolean generateDistCacheData = false;
-
-  Configuration conf; // gridmix configuration
-
-  private static final Charset charsetUTF8 = StandardCharsets.UTF_8;
-
-  // Pseudo local file system where local FS based distributed cache files are
-  // created by gridmix.
-  FileSystem pseudoLocalFs = null;
-
-  {
-    // Need to handle deprecation of these MapReduce-internal configuration
-    // properties as MapReduce doesn't handle their deprecation.
-    Configuration.addDeprecations(new DeprecationDelta[] {
-      new DeprecationDelta("mapred.cache.files.filesizes",
-          MRJobConfig.CACHE_FILES_SIZES),
-      new DeprecationDelta("mapred.cache.files.visibilities",
-          MRJobConfig.CACHE_FILE_VISIBILITIES)
-    });
-  }
-
-  /**
-   * @param conf gridmix configuration
-   * @param ioPath &lt;ioPath&gt;/distributedCache/ is the gridmix Distributed
-   *               Cache directory
-   */
-  public DistributedCacheEmulator(Configuration conf, Path ioPath) {
-    this.conf = conf;
-    distCachePath = new Path(ioPath, "distributedCache");
-    this.conf.setClass("fs.pseudo.impl", PseudoLocalFs.class, FileSystem.class);
-  }
-
-  /**
-   * This is to be called before any other method of DistributedCacheEmulator.
-   * <br> Checks if emulation of distributed cache load is needed and is feasible.
-   *  Sets the flags generateDistCacheData and emulateDistributedCache to the
-   *  appropriate values.
-   * <br> Gridmix does not emulate distributed cache load if
-   * <ol><li> the specific gridmix job type doesn't need emulation of
-   * distributed cache load OR
-   * <li> the trace is coming from a stream instead of file OR
-   * <li> the distributed cache dir where distributed cache data is to be
-   * generated by gridmix is on local file system OR
-   * <li> execute permission is not there for any of the ascendant directories
-   * of &lt;ioPath&gt; till root. This is because for emulation of distributed
-   * cache load, distributed cache files created under
-   * &lt;ioPath/distributedCache/&gt; should be considered by hadoop
-   * as public distributed cache files.
-   * <li> creation of pseudo local file system fails.</ol>
-   * <br> For (2), (3), (4) and (5), generation of distributed cache data
-   * is also disabled.
-   * 
-   * @param traceIn trace file path. If this is '-', then trace comes from the
-   *                stream stdin.
-   * @param jobCreator job creator of gridmix jobs of a specific type
-   * @param generate  true if -generate option was specified
-   * @throws IOException
-   */
-  void init(String traceIn, JobCreator jobCreator, boolean generate)
-      throws IOException {
-    emulateDistributedCache = jobCreator.canEmulateDistCacheLoad()
-        && conf.getBoolean(GRIDMIX_EMULATE_DISTRIBUTEDCACHE, true);
-    generateDistCacheData = generate;
-
-    if (generateDistCacheData || emulateDistributedCache) {
-      if ("-".equals(traceIn)) {// trace is from stdin
-        LOG.warn("Gridmix will not emulate Distributed Cache load because "
-            + "the input trace source is a stream instead of file.");
-        emulateDistributedCache = generateDistCacheData = false;
-      } else if (FileSystem.getLocal(conf).getUri().getScheme().equals(
-          distCachePath.toUri().getScheme())) {// local FS
-        LOG.warn("Gridmix will not emulate Distributed Cache load because "
-            + "<iopath> provided is on local file system.");
-        emulateDistributedCache = generateDistCacheData = false;
-      } else {
-        // Check if execute permission is there for all the ascendant
-        // directories of distCachePath till root.
-        FileSystem fs = FileSystem.get(conf);
-        Path cur = distCachePath.getParent();
-        while (cur != null) {
-          if (cur.toString().length() > 0) {
-            FsPermission perm = fs.getFileStatus(cur).getPermission();
-            if (!perm.getOtherAction().and(FsAction.EXECUTE).equals(
-                FsAction.EXECUTE)) {
-              LOG.warn("Gridmix will not emulate Distributed Cache load "
-                  + "because the ascendant directory (of distributed cache "
-                  + "directory) " + cur + " doesn't have execute permission "
-                  + "for others.");
-              emulateDistributedCache = generateDistCacheData = false;
-              break;
-            }
-          }
-          cur = cur.getParent();
-        }
-      }
-    }
-
-    // Check if pseudo local file system can be created
-    try {
-      pseudoLocalFs = FileSystem.get(new URI("pseudo:///"), conf);
-    } catch (URISyntaxException e) {
-      LOG.warn("Gridmix will not emulate Distributed Cache load because "
-          + "creation of pseudo local file system failed.");
-      e.printStackTrace();
-      emulateDistributedCache = generateDistCacheData = false;
-      return;
-    }
-  }
-
-  /**
-   * @return true if gridmix should emulate distributed cache load
-   */
-  boolean shouldEmulateDistCacheLoad() {
-    return emulateDistributedCache;
-  }
-
-  /**
-   * @return true if gridmix should generate distributed cache data
-   */
-  boolean shouldGenerateDistCacheData() {
-    return generateDistCacheData;
-  }
-
-  /**
-   * @return the distributed cache directory path
-   */
-  Path getDistributedCacheDir() {
-    return distCachePath;
-  }
-
-  /**
-   * Create distributed cache directories.
-   * Also create a file that contains the list of distributed cache files
-   * that will be used as distributed cache files for all the simulated jobs.
-   * @param jsp job story producer for the trace
-   * @return exit code
-   * @throws IOException
-   */
-  int setupGenerateDistCacheData(JobStoryProducer jsp)
-      throws IOException {
-
-    createDistCacheDirectory();
-    return buildDistCacheFilesList(jsp);
-  }
-
-  /**
-   * Create distributed cache directory where distributed cache files will be
-   * created by the MapReduce job {@link GenerateDistCacheData#JOB_NAME}.
-   * @throws IOException
-   */
-  private void createDistCacheDirectory() throws IOException {
-    FileSystem fs = FileSystem.get(conf);
-    FileSystem.mkdirs(fs, distCachePath, new FsPermission((short) 0777));
-  }
-
-  /**
-   * Create the list of unique distributed cache files needed for all the
-   * simulated jobs and write the list to a special file.
-   * @param jsp job story producer for the trace
-   * @return exit code
-   * @throws IOException
-   */
-  private int buildDistCacheFilesList(JobStoryProducer jsp) throws IOException {
-    // Read all the jobs from the trace file and build the list of unique
-    // distributed cache files.
-    JobStory jobStory;
-    while ((jobStory = jsp.getNextJob()) != null) {
-      if (jobStory.getOutcome() == Pre21JobHistoryConstants.Values.SUCCESS && 
-         jobStory.getSubmissionTime() >= 0) {
-        updateHDFSDistCacheFilesList(jobStory);
-      }
-    }
-    jsp.close();
-
-    return writeDistCacheFilesList();
-  }
-
-  /**
-   * For the job to be simulated, identify the needed distributed cache files by
-   * mapping original cluster's distributed cache file paths to the simulated cluster's
-   * paths and add these paths in the map {@code distCacheFiles}.
-   *<br>
-   * JobStory should contain distributed cache related properties like
-   * <li> {@link MRJobConfig#CACHE_FILES}
-   * <li> {@link MRJobConfig#CACHE_FILE_VISIBILITIES}
-   * <li> {@link MRJobConfig#CACHE_FILES_SIZES}
-   * <li> {@link MRJobConfig#CACHE_FILE_TIMESTAMPS}
-   * <li> {@link MRJobConfig#CLASSPATH_FILES}
-   *
-   * <li> {@link MRJobConfig#CACHE_ARCHIVES}
-   * <li> {@link MRJobConfig#CACHE_ARCHIVES_VISIBILITIES}
-   * <li> {@link MRJobConfig#CACHE_ARCHIVES_SIZES}
-   * <li> {@link MRJobConfig#CACHE_ARCHIVES_TIMESTAMPS}
-   * <li> {@link MRJobConfig#CLASSPATH_ARCHIVES}
-   *
-   * <li> {@link MRJobConfig#CACHE_SYMLINK}
-   *
-   * @param jobdesc JobStory of original job obtained from trace
-   * @throws IOException
-   */
-  void updateHDFSDistCacheFilesList(JobStory jobdesc) throws IOException {
-
-    // Map original job's distributed cache file paths to simulated cluster's
-    // paths, to be used by this simulated job.
-    JobConf jobConf = jobdesc.getJobConf();
-
-    String[] files = jobConf.getStrings(MRJobConfig.CACHE_FILES);
-    if (files != null) {
-
-      String[] fileSizes = jobConf.getStrings(MRJobConfig.CACHE_FILES_SIZES);
-      String[] visibilities =
-        jobConf.getStrings(MRJobConfig.CACHE_FILE_VISIBILITIES);
-      String[] timeStamps =
-        jobConf.getStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS);
-
-      FileSystem fs = FileSystem.get(conf);
-      String user = jobConf.getUser();
-      for (int i = 0; i < files.length; i++) {
-        // Check if visibilities are available because older hadoop versions
-        // didn't have public, private Distributed Caches separately.
-        boolean visibility =
-            (visibilities == null) || Boolean.parseBoolean(visibilities[i]);
-        if (isLocalDistCacheFile(files[i], user, visibility)) {
-          // local FS based distributed cache file.
-          // Create this file on the pseudo local FS on the fly (i.e. when the
-          // simulated job is submitted).
-          continue;
-        }
-        // distributed cache file on hdfs
-        String mappedPath = mapDistCacheFilePath(files[i], timeStamps[i],
-                                                 visibility, user);
-
-        // No need to add a distributed cache file path to the list if
-        // (1) the mapped path is already there in the list OR
-        // (2) the file with the mapped path already exists.
-        // In any of the above 2 cases, file paths, timestamps, file sizes and
-        // visibilities match. File sizes should match if file paths and
-        // timestamps match because single file path with single timestamp
-        // should correspond to a single file size.
-        if (distCacheFiles.containsKey(mappedPath) ||
-            fs.exists(new Path(mappedPath))) {
-          continue;
-        }
-        distCacheFiles.put(mappedPath, Long.valueOf(fileSizes[i]));
-      }
-    }
-  }
-
-  /**
-   * Check if the file path provided was constructed by MapReduce for a
-   * distributed cache file on local file system.
-   * @param filePath path of the distributed cache file
-   * @param user job submitter of the job for which &lt;filePath&gt; is a
-   *             distributed cache file
-   * @param visibility <code>true</code> for public distributed cache file
-   * @return true if the path provided is of a local file system based
-   *              distributed cache file
-   */
-  static boolean isLocalDistCacheFile(String filePath, String user,
-                                       boolean visibility) {
-    return (!visibility && filePath.contains(user + "/.staging"));
-  }
-
-  /**
-   * Map the HDFS based distributed cache file path from original cluster to
-   * a unique file name on the simulated cluster.
-   * <br> Unique  distributed file names on simulated cluster are generated
-   * using original cluster's <li>file path, <li>timestamp and <li> the
-   * job-submitter for private distributed cache file.
-   * <br> This implies that if on original cluster, a single HDFS file
-   * considered as two private distributed cache files for two jobs of
-   * different users, then the corresponding simulated jobs will have two
-   * different files of the same size in public distributed cache, one for each
-   * user. Both these simulated jobs will not share these distributed cache
-   * files, thus leading to the same load as seen in the original cluster.
-   * @param file distributed cache file path
-   * @param timeStamp time stamp of dist cachce file
-   * @param isPublic true if this distributed cache file is a public
-   *                 distributed cache file
-   * @param user job submitter on original cluster
-   * @return the mapped path on simulated cluster
-   */
-  private String mapDistCacheFilePath(String file, String timeStamp,
-      boolean isPublic, String user) {
-    String id = file + timeStamp;
-    if (!isPublic) {
-      // consider job-submitter for private distributed cache file
-      id = id.concat(user);
-    }
-    return new Path(distCachePath, MD5Hash.digest(id).toString()).toUri()
-               .getPath();
-  }
-
-  /**
-   * Write the list of distributed cache files in the decreasing order of
-   * file sizes into the sequence file. This file will be input to the job
-   * {@link GenerateDistCacheData}.
-   * Also validates if -generate option is missing and distributed cache files
-   * are missing.
-   * @return exit code
-   * @throws IOException
-   */
-  private int writeDistCacheFilesList()
-      throws IOException {
-    // Sort the distributed cache files in the decreasing order of file sizes.
-    List dcFiles = new ArrayList(distCacheFiles.entrySet());
-    Collections.sort(dcFiles, new Comparator() {
-      public int compare(Object dc1, Object dc2) {
-        return ((Comparable) ((Map.Entry) (dc2)).getValue())
-            .compareTo(((Map.Entry) (dc1)).getValue());
-      }
-    });
-
-    // write the sorted distributed cache files to the sequence file
-    FileSystem fs = FileSystem.get(conf);
-    Path distCacheFilesList = new Path(distCachePath, "_distCacheFiles.txt");
-    conf.set(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_LIST,
-        distCacheFilesList.toString());
-    SequenceFile.Writer src_writer = SequenceFile.createWriter(fs, conf,
-        distCacheFilesList, LongWritable.class, BytesWritable.class,
-        SequenceFile.CompressionType.NONE);
-
-    // Total number of unique distributed cache files
-    int fileCount = dcFiles.size();
-    long byteCount = 0;// Total size of all distributed cache files
-    long bytesSync = 0;// Bytes after previous sync;used to add sync marker
-
-    for (Iterator it = dcFiles.iterator(); it.hasNext();) {
-      Map.Entry entry = (Map.Entry)it.next();
-      LongWritable fileSize =
-          new LongWritable(Long.parseLong(entry.getValue().toString()));
-      BytesWritable filePath =
-          new BytesWritable(
-          entry.getKey().toString().getBytes(charsetUTF8));
-
-      byteCount += fileSize.get();
-      bytesSync += fileSize.get();
-      if (bytesSync > AVG_BYTES_PER_MAP) {
-        src_writer.sync();
-        bytesSync = fileSize.get();
-      }
-      src_writer.append(fileSize, filePath);
-    }
-    if (src_writer != null) {
-      src_writer.close();
-    }
-    // Set delete on exit for 'dist cache files list' as it is not needed later.
-    fs.deleteOnExit(distCacheFilesList);
-
-    conf.setInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, fileCount);
-    conf.setLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, byteCount);
-    LOG.info("Number of HDFS based distributed cache files to be generated is "
-        + fileCount + ". Total size of HDFS based distributed cache files "
-        + "to be generated is " + byteCount);
-
-    if (!shouldGenerateDistCacheData() && fileCount > 0) {
-      LOG.error("Missing " + fileCount + " distributed cache files under the "
-          + " directory\n" + distCachePath + "\nthat are needed for gridmix"
-          + " to emulate distributed cache load. Either use -generate\noption"
-          + " to generate distributed cache data along with input data OR "
-          + "disable\ndistributed cache emulation by configuring '"
-          + DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE
-          + "' to false.");
-      return Gridmix.MISSING_DIST_CACHE_FILES_ERROR;
-    }
-    return 0;
-  }
-
-  /**
-   * If gridmix needs to emulate distributed cache load, then configure
-   * distributed cache files of a simulated job by mapping the original
-   * cluster's distributed cache file paths to the simulated cluster's paths and
-   * setting these mapped paths in the job configuration of the simulated job.
-   * <br>
-   * Configure local FS based distributed cache files through the property
-   * "tmpfiles" and hdfs based distributed cache files through the property
-   * {@link MRJobConfig#CACHE_FILES}.
-   * @param conf configuration for the simulated job to be run
-   * @param jobConf job configuration of original cluster's job, obtained from
-   *                trace
-   * @throws IOException
-   */
-  void configureDistCacheFiles(Configuration conf, JobConf jobConf)
-      throws IOException {
-    if (shouldEmulateDistCacheLoad()) {
-
-      String[] files = jobConf.getStrings(MRJobConfig.CACHE_FILES);
-      if (files != null) {
-        // hdfs based distributed cache files to be configured for simulated job
-        List<String> cacheFiles = new ArrayList<String>();
-        // local FS based distributed cache files to be configured for
-        // simulated job
-        List<String> localCacheFiles = new ArrayList<String>();
-
-        String[] visibilities =
-          jobConf.getStrings(MRJobConfig.CACHE_FILE_VISIBILITIES);
-        String[] timeStamps =
-          jobConf.getStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS);
-        String[] fileSizes = jobConf.getStrings(MRJobConfig.CACHE_FILES_SIZES);
-
-        String user = jobConf.getUser();
-        for (int i = 0; i < files.length; i++) {
-          // Check if visibilities are available because older hadoop versions
-          // didn't have public, private Distributed Caches separately.
-          boolean visibility =
-              (visibilities == null) || Boolean.parseBoolean(visibilities[i]);
-          if (isLocalDistCacheFile(files[i], user, visibility)) {
-            // local FS based distributed cache file.
-            // Create this file on the pseudo local FS.
-            String fileId = MD5Hash.digest(files[i] + timeStamps[i]).toString();
-            long fileSize = Long.parseLong(fileSizes[i]);
-            Path mappedLocalFilePath =
-                PseudoLocalFs.generateFilePath(fileId, fileSize)
-                    .makeQualified(pseudoLocalFs.getUri(),
-                                   pseudoLocalFs.getWorkingDirectory());
-            pseudoLocalFs.create(mappedLocalFilePath);
-            localCacheFiles.add(mappedLocalFilePath.toUri().toString());
-          } else {
-            // hdfs based distributed cache file.
-            // Get the mapped HDFS path on simulated cluster
-            String mappedPath = mapDistCacheFilePath(files[i], timeStamps[i],
-                                                     visibility, user);
-            cacheFiles.add(mappedPath);
-          }
-        }
-        if (cacheFiles.size() > 0) {
-          // configure hdfs based distributed cache files for simulated job
-          conf.setStrings(MRJobConfig.CACHE_FILES,
-                          cacheFiles.toArray(new String[cacheFiles.size()]));
-        }
-        if (localCacheFiles.size() > 0) {
-          // configure local FS based distributed cache files for simulated job
-          conf.setStrings("tmpfiles", localCacheFiles.toArray(
-                                        new String[localCacheFiles.size()]));
-        }
-      }
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
deleted file mode 100644
index 43f54bc96e9..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/EchoUserResolver.java
+++ /dev/null
@@ -1,57 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.net.URI;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Echos the UGI offered.
- */
-public class EchoUserResolver implements UserResolver {
-  public static final Logger LOG = LoggerFactory.getLogger(Gridmix.class);
-
-  public EchoUserResolver() {
-    LOG.info(" Current user resolver is EchoUserResolver ");
-  }
-
-  public synchronized boolean setTargetUsers(URI userdesc, Configuration conf)
-  throws IOException {
-    return false;
-  }
-
-  public synchronized UserGroupInformation getTargetUgi(
-    UserGroupInformation ugi) {
-    return ugi;
-  }
-
-  /**
-   * {@inheritDoc}
-   * <br><br>
-   * Since {@link EchoUserResolver} simply returns the user's name passed as
-   * the argument, it doesn't need a target list of users.
-   */
-  public boolean needsTargetUsersList() {
-    return false;
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
deleted file mode 100644
index 9ecd9e8e5da..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ExecutionSummarizer.java
+++ /dev/null
@@ -1,320 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.apache.commons.lang3.time.FastDateFormat;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.MD5Hash;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.util.StringUtils;
-
-/**
- * Summarizes a {@link Gridmix} run. Statistics that are reported are
- * <ul>
- *   <li>Total number of jobs in the input trace</li>
- *   <li>Trace signature</li>
- *   <li>Total number of jobs processed from the input trace</li>
- *   <li>Total number of jobs submitted</li>
- *   <li>Total number of successful and failed jobs</li>
- *   <li>Total number of map/reduce tasks launched</li>
- *   <li>Gridmix start & end time</li>
- *   <li>Total time for the Gridmix run (data-generation and simulation)</li>
- *   <li>Gridmix Configuration (i.e job-type, submission-type, resolver)</li>
- * </ul>
- */
-class ExecutionSummarizer implements StatListener<JobStats> {
-  static final Logger LOG = LoggerFactory.getLogger(ExecutionSummarizer.class);
-  private static final FastDateFormat UTIL = FastDateFormat.getInstance();
-  
-  private int numJobsInInputTrace;
-  private int totalSuccessfulJobs;
-  private int totalFailedJobs;
-  private int totalLostJobs;
-  private int totalMapTasksLaunched;
-  private int totalReduceTasksLaunched;
-  private long totalSimulationTime;
-  private long totalRuntime;
-  private final String commandLineArgs;
-  private long startTime;
-  private long endTime;
-  private long simulationStartTime;
-  private String inputTraceLocation;
-  private String inputTraceSignature;
-  private String jobSubmissionPolicy;
-  private String resolver;
-  private DataStatistics dataStats;
-  private String expectedDataSize;
-  
-  /**
-   * Basic constructor initialized with the runtime arguments. 
-   */
-  ExecutionSummarizer(String[] args) {
-    startTime = System.currentTimeMillis();
-    // flatten the args string and store it
-    commandLineArgs = 
-      org.apache.commons.lang3.StringUtils.join(args, ' ');
-  }
-  
-  /**
-   * Default constructor. 
-   */
-  ExecutionSummarizer() {
-    startTime = System.currentTimeMillis();
-    commandLineArgs = Summarizer.NA; 
-  }
-  
-  void start(Configuration conf) {
-    simulationStartTime = System.currentTimeMillis();
-  }
-  
-  private void processJobState(JobStats stats) {
-    Job job = stats.getJob();
-    try {
-      if (job.isSuccessful()) {
-        ++totalSuccessfulJobs;
-      } else {
-        ++totalFailedJobs;
-      }
-    } catch (Exception e) {
-      // this behavior is consistent with job-monitor which marks the job as 
-      // complete (lost) if the status polling bails out
-      ++totalLostJobs; 
-    }
-  }
-  
-  private void processJobTasks(JobStats stats) {
-    totalMapTasksLaunched += stats.getNoOfMaps();
-    totalReduceTasksLaunched += stats.getNoOfReds();
-  }
-  
-  private void process(JobStats stats) {
-    // process the job run state
-    processJobState(stats);
-
-    // process the tasks information
-    processJobTasks(stats);
-  }
-  
-  @Override
-  public void update(JobStats item) {
-    // process only if the simulation has started
-    if (simulationStartTime > 0) {
-      process(item);
-      totalSimulationTime = 
-        System.currentTimeMillis() - getSimulationStartTime();
-    }
-  }
-  
-  // Generates a signature for the trace file based on
-  //   - filename
-  //   - modification time
-  //   - file length
-  //   - owner
-  protected static String getTraceSignature(String input) throws IOException {
-    Path inputPath = new Path(input);
-    FileSystem fs = inputPath.getFileSystem(new Configuration());
-    FileStatus status = fs.getFileStatus(inputPath);
-    Path qPath = fs.makeQualified(status.getPath());
-    String traceID = status.getModificationTime() + qPath.toString()
-                     + status.getOwner() + status.getLen();
-    return MD5Hash.digest(traceID).toString();
-  }
-  
-  @SuppressWarnings("unchecked")
-  void finalize(JobFactory factory, String inputPath, long dataSize, 
-                UserResolver userResolver, DataStatistics stats,
-                Configuration conf) 
-  throws IOException {
-    numJobsInInputTrace = factory.numJobsInTrace;
-    endTime = System.currentTimeMillis();
-     if ("-".equals(inputPath)) {
-      inputTraceLocation = Summarizer.NA;
-      inputTraceSignature = Summarizer.NA;
-    } else {
-      Path inputTracePath = new Path(inputPath);
-      FileSystem fs = inputTracePath.getFileSystem(conf);
-      inputTraceLocation = fs.makeQualified(inputTracePath).toString();
-      inputTraceSignature = getTraceSignature(inputPath);
-    }
-    jobSubmissionPolicy = Gridmix.getJobSubmissionPolicy(conf).name();
-    resolver = userResolver.getClass().getName();
-    if (dataSize > 0) {
-      expectedDataSize = StringUtils.humanReadableInt(dataSize);
-    } else {
-      expectedDataSize = Summarizer.NA;
-    }
-    dataStats = stats;
-    totalRuntime = System.currentTimeMillis() - getStartTime();
-  }
-  
-  /**
-   * Summarizes the current {@link Gridmix} run.
-   */
-  @Override
-  public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append("Execution Summary:-");
-    builder.append("\nInput trace: ").append(getInputTraceLocation());
-    builder.append("\nInput trace signature: ")
-           .append(getInputTraceSignature());
-    builder.append("\nTotal number of jobs in trace: ")
-           .append(getNumJobsInTrace());
-    builder.append("\nExpected input data size: ")
-           .append(getExpectedDataSize());
-    builder.append("\nInput data statistics: ")
-           .append(getInputDataStatistics());
-    builder.append("\nTotal number of jobs processed: ")
-           .append(getNumSubmittedJobs());
-    builder.append("\nTotal number of successful jobs: ")
-           .append(getNumSuccessfulJobs());
-    builder.append("\nTotal number of failed jobs: ")
-           .append(getNumFailedJobs());
-    builder.append("\nTotal number of lost jobs: ")
-           .append(getNumLostJobs());
-    builder.append("\nTotal number of map tasks launched: ")
-           .append(getNumMapTasksLaunched());
-    builder.append("\nTotal number of reduce task launched: ")
-           .append(getNumReduceTasksLaunched());
-    builder.append("\nGridmix start time: ")
-           .append(UTIL.format(getStartTime()));
-    builder.append("\nGridmix end time: ").append(UTIL.format(getEndTime()));
-    builder.append("\nGridmix simulation start time: ")
-           .append(UTIL.format(getStartTime()));
-    builder.append("\nGridmix runtime: ")
-           .append(StringUtils.formatTime(getRuntime()));
-    builder.append("\nTime spent in initialization (data-gen etc): ")
-           .append(StringUtils.formatTime(getInitTime()));
-    builder.append("\nTime spent in simulation: ")
-           .append(StringUtils.formatTime(getSimulationTime()));
-    builder.append("\nGridmix configuration parameters: ")
-           .append(getCommandLineArgsString());
-    builder.append("\nGridmix job submission policy: ")
-           .append(getJobSubmissionPolicy());
-    builder.append("\nGridmix resolver: ").append(getUserResolver());
-    builder.append("\n\n");
-    return builder.toString();
-  }
-  
-  // Gets the stringified version of DataStatistics
-  static String stringifyDataStatistics(DataStatistics stats) {
-    if (stats != null) {
-      StringBuffer buffer = new StringBuffer();
-      String compressionStatus = stats.isDataCompressed() 
-                                 ? "Compressed" 
-                                 : "Uncompressed";
-      buffer.append(compressionStatus).append(" input data size: ");
-      buffer.append(StringUtils.humanReadableInt(stats.getDataSize()));
-      buffer.append(", ");
-      buffer.append("Number of files: ").append(stats.getNumFiles());
-
-      return buffer.toString();
-    } else {
-      return Summarizer.NA;
-    }
-  }
-  
-  // Getters
-  protected String getExpectedDataSize() {
-    return expectedDataSize;
-  }
-  
-  protected String getUserResolver() {
-    return resolver;
-  }
-  
-  protected String getInputDataStatistics() {
-    return stringifyDataStatistics(dataStats);
-  }
-  
-  protected String getInputTraceSignature() {
-    return inputTraceSignature;
-  }
-  
-  protected String getInputTraceLocation() {
-    return inputTraceLocation;
-  }
-  
-  protected int getNumJobsInTrace() {
-    return numJobsInInputTrace;
-  }
-  
-  protected int getNumSuccessfulJobs() {
-    return totalSuccessfulJobs;
-  }
-  
-  protected int getNumFailedJobs() {
-    return totalFailedJobs;
-  }
-  
-  protected int getNumLostJobs() {
-    return totalLostJobs;
-  }
-  
-  protected int getNumSubmittedJobs() {
-    return totalSuccessfulJobs + totalFailedJobs + totalLostJobs;
-  }
-  
-  protected int getNumMapTasksLaunched() {
-    return totalMapTasksLaunched;
-  }
-  
-  protected int getNumReduceTasksLaunched() {
-    return totalReduceTasksLaunched;
-  }
-  
-  protected long getStartTime() {
-    return startTime;
-  }
-  
-  protected long getEndTime() {
-    return endTime;
-  }
-  
-  protected long getInitTime() {
-    return simulationStartTime - startTime;
-  }
-  
-  protected long getSimulationStartTime() {
-    return simulationStartTime;
-  }
-  
-  protected long getSimulationTime() {
-    return totalSimulationTime;
-  }
-  
-  protected long getRuntime() {
-    return totalRuntime;
-  }
-  
-  protected String getCommandLineArgsString() {
-    return commandLineArgs;
-  }
-  
-  protected String getJobSubmissionPolicy() {
-    return jobSubmissionPolicy;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java
deleted file mode 100644
index 9a0cca380bc..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FilePool.java
+++ /dev/null
@@ -1,301 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import java.util.Arrays;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.Iterator;
-import java.util.Map;
-import java.util.Random;
-import java.util.concurrent.locks.ReadWriteLock;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.BlockLocation;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.mapred.gridmix.RandomAlgorithms.Selector;
-
-/**
- * Class for caching a pool of input data to be used by synthetic jobs for
- * simulating read traffic.
- */
-class FilePool {
-
-  public static final Logger LOG = LoggerFactory.getLogger(FilePool.class);
-
-  /**
-   * The minimum file size added to the pool. Default 128MiB.
-   */
-  public static final String GRIDMIX_MIN_FILE = "gridmix.min.file.size";
-
-  /**
-   * The maximum size for files added to the pool. Defualts to 100TiB.
-   */
-  public static final String GRIDMIX_MAX_TOTAL = "gridmix.max.total.scan";
-
-  private Node root;
-  private final Path path;
-  private final FileSystem fs;
-  private final Configuration conf;
-  private final ReadWriteLock updateLock;
-
-  /**
-   * Initialize a filepool under the path provided, but do not populate the
-   * cache.
-   */
-  public FilePool(Configuration conf, Path input) throws IOException {
-    root = null;
-    this.conf = conf;
-    this.path = input;
-    this.fs = path.getFileSystem(conf);
-    updateLock = new ReentrantReadWriteLock();
-  }
-
-  /**
-   * Gather a collection of files at least as large as minSize.
-   * @return The total size of files returned.
-   */
-  public long getInputFiles(long minSize, Collection<FileStatus> files)
-      throws IOException {
-    updateLock.readLock().lock();
-    try {
-      return root.selectFiles(minSize, files);
-    } finally {
-      updateLock.readLock().unlock();
-    }
-  }
-
-  /**
-   * (Re)generate cache of input FileStatus objects.
-   */
-  public void refresh() throws IOException {
-    updateLock.writeLock().lock();
-    try {
-      root = new InnerDesc(fs, fs.getFileStatus(path),
-        new MinFileFilter(conf.getLong(GRIDMIX_MIN_FILE, 128 * 1024 * 1024),
-                          conf.getLong(GRIDMIX_MAX_TOTAL, 100L * (1L << 40))));
-      if (0 == root.getSize()) {
-        throw new IOException("Found no satisfactory file in " + path);
-      }
-    } finally {
-      updateLock.writeLock().unlock();
-    }
-  }
-
-  /**
-   * Get a set of locations for the given file.
-   */
-  public BlockLocation[] locationsFor(FileStatus stat, long start, long len)
-      throws IOException {
-    // TODO cache
-    return fs.getFileBlockLocations(stat, start, len);
-  }
-
-  static abstract class Node {
-
-    protected final static Random rand = new Random();
-
-    /**
-     * Total size of files and directories under the current node.
-     */
-    abstract long getSize();
-
-    /**
-     * Return a set of files whose cumulative size is at least
-     * <tt>targetSize</tt>.
-     * TODO Clearly size is not the only criterion, e.g. refresh from
-     * generated data without including running task output, tolerance
-     * for permission issues, etc.
-     */
-    abstract long selectFiles(long targetSize, Collection<FileStatus> files)
-        throws IOException;
-  }
-
-  /**
-   * Files in current directory of this Node.
-   */
-  static class LeafDesc extends Node {
-    final long size;
-    final ArrayList<FileStatus> curdir;
-
-    LeafDesc(ArrayList<FileStatus> curdir, long size) {
-      this.size = size;
-      this.curdir = curdir;
-    }
-
-    @Override
-    public long getSize() {
-      return size;
-    }
-
-    @Override
-    public long selectFiles(long targetSize, Collection<FileStatus> files)
-        throws IOException {
-      if (targetSize >= getSize()) {
-        files.addAll(curdir);
-        return getSize();
-      }
-
-      Selector selector = new Selector(curdir.size(), (double) targetSize
-          / getSize(), rand);
-      
-      ArrayList<Integer> selected = new ArrayList<Integer>();
-      long ret = 0L;
-      do {
-        int index = selector.next();
-        selected.add(index);
-        ret += curdir.get(index).getLen();
-      } while (ret < targetSize);
-
-      for (Integer i : selected) {
-        files.add(curdir.get(i));
-      }
-
-      return ret;
-    }
-  }
-
-  /**
-   * A subdirectory of the current Node.
-   */
-  static class InnerDesc extends Node {
-    final long size;
-    final double[] dist;
-    final Node[] subdir;
-
-    private static final Comparator<Node> nodeComparator =
-      new Comparator<Node>() {
-          public int compare(Node n1, Node n2) {
-            return n1.getSize() < n2.getSize() ? -1
-                 : n1.getSize() > n2.getSize() ? 1 : 0;
-          }
-    };
-
-    InnerDesc(final FileSystem fs, FileStatus thisDir, MinFileFilter filter)
-        throws IOException {
-      long fileSum = 0L;
-      final ArrayList<FileStatus> curFiles = new ArrayList<FileStatus>();
-      final ArrayList<FileStatus> curDirs = new ArrayList<FileStatus>();
-      for (FileStatus stat : fs.listStatus(thisDir.getPath())) {
-        if (stat.isDirectory()) {
-          curDirs.add(stat);
-        } else if (filter.accept(stat)) {
-          curFiles.add(stat);
-          fileSum += stat.getLen();
-        }
-      }
-      ArrayList<Node> subdirList = new ArrayList<Node>();
-      if (!curFiles.isEmpty()) {
-        subdirList.add(new LeafDesc(curFiles, fileSum));
-      }
-      for (Iterator<FileStatus> i = curDirs.iterator();
-          !filter.done() && i.hasNext();) {
-        // add subdirectories
-        final Node d = new InnerDesc(fs, i.next(), filter);
-        final long dSize = d.getSize();
-        if (dSize > 0) {
-          fileSum += dSize;
-          subdirList.add(d);
-        }
-      }
-      size = fileSum;
-      LOG.debug(size + " bytes in " + thisDir.getPath());
-      subdir = subdirList.toArray(new Node[subdirList.size()]);
-      Arrays.sort(subdir, nodeComparator);
-      dist = new double[subdir.length];
-      for (int i = dist.length - 1; i > 0; --i) {
-        fileSum -= subdir[i].getSize();
-        dist[i] = fileSum / (1.0 * size);
-      }
-    }
-
-    @Override
-    public long getSize() {
-      return size;
-    }
-
-    @Override
-    public long selectFiles(long targetSize, Collection<FileStatus> files)
-        throws IOException {
-      long ret = 0L;
-      if (targetSize >= getSize()) {
-        // request larger than all subdirs; add everything
-        for (Node n : subdir) {
-          long added = n.selectFiles(targetSize, files);
-          ret += added;
-          targetSize -= added;
-        }
-        return ret;
-      }
-
-      // can satisfy request in proper subset of contents
-      // select random set, weighted by size
-      final HashSet<Node> sub = new HashSet<Node>();
-      do {
-        assert sub.size() < subdir.length;
-        final double r = rand.nextDouble();
-        int pos = Math.abs(Arrays.binarySearch(dist, r) + 1) - 1;
-        while (sub.contains(subdir[pos])) {
-          pos = (pos + 1) % subdir.length;
-        }
-        long added = subdir[pos].selectFiles(targetSize, files);
-        ret += added;
-        targetSize -= added;
-        sub.add(subdir[pos]);
-      } while (targetSize > 0);
-      return ret;
-    }
-  }
-
-  /**
-   * Filter enforcing the minFile/maxTotal parameters of the scan.
-   */
-  private static class MinFileFilter {
-
-    private long totalScan;
-    private final long minFileSize;
-
-    public MinFileFilter(long minFileSize, long totalScan) {
-      this.minFileSize = minFileSize;
-      this.totalScan = totalScan;
-    }
-    public boolean done() {
-      return totalScan <= 0;
-    }
-    public boolean accept(FileStatus stat) {
-      final boolean done = done();
-      if (!done && stat.getLen() >= minFileSize) {
-        totalScan -= stat.getLen();
-        return true;
-      }
-      return false;
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
deleted file mode 100644
index 2e4222c89c6..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/FileQueue.java
+++ /dev/null
@@ -1,103 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.InputStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-/**
- * Given a {@link org.apache.hadoop.mapreduce.lib.input.CombineFileSplit},
- * circularly read through each input source.
- */
-class FileQueue extends InputStream {
-
-  private int idx = -1;
-  private long curlen = -1L;
-  private InputStream input;
-  private final byte[] z = new byte[1];
-  private final Path[] paths;
-  private final long[] lengths;
-  private final long[] startoffset;
-  private final Configuration conf;
-
-  /**
-   * @param split Description of input sources.
-   * @param conf Used to resolve FileSystem instances.
-   */
-  public FileQueue(CombineFileSplit split, Configuration conf)
-      throws IOException {
-    this.conf = conf;
-    paths = split.getPaths();
-    startoffset = split.getStartOffsets();
-    lengths = split.getLengths();
-    nextSource();
-  }
-
-  protected void nextSource() throws IOException {
-    if (0 == paths.length) {
-      return;
-    }
-    if (input != null) {
-      input.close();
-    }
-    idx = (idx + 1) % paths.length;
-    curlen = lengths[idx];
-    final Path file = paths[idx];
-    input = 
-      CompressionEmulationUtil.getPossiblyDecompressedInputStream(file, 
-                                 conf, startoffset[idx]);
-  }
-
-  @Override
-  public int read() throws IOException {
-    final int tmp = read(z);
-    return tmp == -1 ? -1 : (0xFF & z[0]);
-  }
-
-  @Override
-  public int read(byte[] b) throws IOException {
-    return read(b, 0, b.length);
-  }
-
-  @Override
-  public int read(byte[] b, int off, int len) throws IOException {
-    int kvread = 0;
-    while (kvread < len) {
-      if (curlen <= 0) {
-        nextSource();
-        continue;
-      }
-      final int srcRead = (int) Math.min(len - kvread, curlen);
-      IOUtils.readFully(input, b, kvread, srcRead);
-      curlen -= srcRead;
-      kvread += srcRead;
-    }
-    return kvread;
-  }
-
-  @Override
-  public void close() throws IOException {
-    input.close();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
deleted file mode 100644
index 41e937fe3c8..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateData.java
+++ /dev/null
@@ -1,412 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.OutputStream;
-import java.security.PrivilegedExceptionAction;
-import java.util.Arrays;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.LocatedFileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PathFilter;
-import org.apache.hadoop.fs.RemoteIterator;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Utils;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.util.StringUtils;
-
-// TODO can replace with form of GridmixJob
-class GenerateData extends GridmixJob {
-
-  /**
-   * Total bytes to write.
-   */
-  public static final String GRIDMIX_GEN_BYTES = "gridmix.gen.bytes";
-
-  /**
-   * Maximum size per file written.
-   */
-  public static final String GRIDMIX_GEN_CHUNK = "gridmix.gen.bytes.per.file";
-
-  /**
-   * Size of writes to output file.
-   */
-  public static final String GRIDMIX_VAL_BYTES = "gendata.val.bytes";
-
-  /**
-   * Status reporting interval, in megabytes.
-   */
-  public static final String GRIDMIX_GEN_INTERVAL = "gendata.interval.mb";
-
-  /**
-   * Blocksize of generated data.
-   */
-  public static final String GRIDMIX_GEN_BLOCKSIZE = "gridmix.gen.blocksize";
-
-  /**
-   * Replication of generated data.
-   */
-  public static final String GRIDMIX_GEN_REPLICATION = "gridmix.gen.replicas";
-  static final String JOB_NAME = "GRIDMIX_GENERATE_INPUT_DATA";
-
-  public GenerateData(Configuration conf, Path outdir, long genbytes)
-      throws IOException {
-    super(conf, 0L, JOB_NAME);
-    job.getConfiguration().setLong(GRIDMIX_GEN_BYTES, genbytes);
-    FileOutputFormat.setOutputPath(job, outdir);
-  }
-
-  /**
-   * Represents the input data characteristics.
-   */
-  static class DataStatistics {
-    private long dataSize;
-    private long numFiles;
-    private boolean isDataCompressed;
-    
-    DataStatistics(long dataSize, long numFiles, boolean isCompressed) {
-      this.dataSize = dataSize;
-      this.numFiles = numFiles;
-      this.isDataCompressed = isCompressed;
-    }
-    
-    long getDataSize() {
-      return dataSize;
-    }
-    
-    long getNumFiles() {
-      return numFiles;
-    }
-    
-    boolean isDataCompressed() {
-      return isDataCompressed;
-    }
-  }
-  
-  /**
-   * Publish the data statistics.
-   */
-  static DataStatistics publishDataStatistics(Path inputDir, long genBytes, 
-                                              Configuration conf) 
-  throws IOException {
-    if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)) {
-      return CompressionEmulationUtil.publishCompressedDataStatistics(inputDir, 
-                                        conf, genBytes);
-    } else {
-      return publishPlainDataStatistics(conf, inputDir);
-    }
-  }
-  
-  static DataStatistics publishPlainDataStatistics(Configuration conf, 
-                                                   Path inputDir) 
-  throws IOException {
-    FileSystem fs = inputDir.getFileSystem(conf);
-
-    // obtain input data file statuses
-    long dataSize = 0;
-    long fileCount = 0;
-    RemoteIterator<LocatedFileStatus> iter = fs.listFiles(inputDir, true);
-    PathFilter filter = new Utils.OutputFileUtils.OutputFilesFilter();
-    while (iter.hasNext()) {
-      LocatedFileStatus lStatus = iter.next();
-      if (filter.accept(lStatus.getPath())) {
-        dataSize += lStatus.getLen();
-        ++fileCount;
-      }
-    }
-
-    // publish the plain data statistics
-    LOG.info("Total size of input data : " 
-             + StringUtils.humanReadableInt(dataSize));
-    LOG.info("Total number of input data files : " + fileCount);
-    
-    return new DataStatistics(dataSize, fileCount, false);
-  }
-  
-  @Override
-  public Job call() throws IOException, InterruptedException,
-                           ClassNotFoundException {
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-    ugi.doAs( new PrivilegedExceptionAction <Job>() {
-       public Job run() throws IOException, ClassNotFoundException,
-                               InterruptedException {
-         // check if compression emulation is enabled
-         if (CompressionEmulationUtil
-             .isCompressionEmulationEnabled(job.getConfiguration())) {
-           CompressionEmulationUtil.configure(job);
-         } else {
-           configureRandomBytesDataGenerator();
-         }
-         job.submit();
-         return job;
-       }
-       
-       private void configureRandomBytesDataGenerator() {
-        job.setMapperClass(GenDataMapper.class);
-        job.setNumReduceTasks(0);
-        job.setMapOutputKeyClass(NullWritable.class);
-        job.setMapOutputValueClass(BytesWritable.class);
-        job.setInputFormatClass(GenDataFormat.class);
-        job.setOutputFormatClass(RawBytesOutputFormat.class);
-        job.setJarByClass(GenerateData.class);
-        try {
-          FileInputFormat.addInputPath(job, new Path("ignored"));
-        } catch (IOException e) {
-          LOG.error("Error while adding input path ", e);
-        }
-      }
-    });
-    return job;
-  }
-  
-  @Override
-  protected boolean canEmulateCompression() {
-    return false;
-  }
-
-  public static class GenDataMapper
-      extends Mapper<NullWritable,LongWritable,NullWritable,BytesWritable> {
-
-    private BytesWritable val;
-    private final Random r = new Random();
-
-    @Override
-    protected void setup(Context context)
-        throws IOException, InterruptedException {
-      val = new BytesWritable(new byte[
-          context.getConfiguration().getInt(GRIDMIX_VAL_BYTES, 1024 * 1024)]);
-    }
-
-    @Override
-    public void map(NullWritable key, LongWritable value, Context context)
-        throws IOException, InterruptedException {
-      for (long bytes = value.get(); bytes > 0; bytes -= val.getLength()) {
-        r.nextBytes(val.getBytes());
-        val.setSize((int)Math.min(val.getLength(), bytes));
-        context.write(key, val);
-      }
-    }
-
-  }
-
-  static class GenDataFormat extends InputFormat<NullWritable,LongWritable> {
-
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      final JobClient client =
-        new JobClient(new JobConf(jobCtxt.getConfiguration()));
-      ClusterStatus stat = client.getClusterStatus(true);
-      final long toGen =
-        jobCtxt.getConfiguration().getLong(GRIDMIX_GEN_BYTES, -1);
-      if (toGen < 0) {
-        throw new IOException("Invalid/missing generation bytes: " + toGen);
-      }
-      final int nTrackers = stat.getTaskTrackers();
-      final long bytesPerTracker = toGen / nTrackers;
-      final ArrayList<InputSplit> splits = new ArrayList<InputSplit>(nTrackers);
-      final Pattern trackerPattern = Pattern.compile("tracker_([^:]*):.*");
-      final Matcher m = trackerPattern.matcher("");
-      for (String tracker : stat.getActiveTrackerNames()) {
-        m.reset(tracker);
-        if (!m.find()) {
-          System.err.println("Skipping node: " + tracker);
-          continue;
-        }
-        final String name = m.group(1);
-        splits.add(new GenSplit(bytesPerTracker, new String[] { name }));
-      }
-      return splits;
-    }
-
-    @Override
-    public RecordReader<NullWritable,LongWritable> createRecordReader(
-        InputSplit split, final TaskAttemptContext taskContext)
-        throws IOException {
-      return new RecordReader<NullWritable,LongWritable>() {
-        long written = 0L;
-        long write = 0L;
-        long RINTERVAL;
-        long toWrite;
-        final NullWritable key = NullWritable.get();
-        final LongWritable val = new LongWritable();
-
-        @Override
-        public void initialize(InputSplit split, TaskAttemptContext ctxt)
-            throws IOException, InterruptedException {
-          toWrite = split.getLength();
-          RINTERVAL = ctxt.getConfiguration().getInt(
-              GRIDMIX_GEN_INTERVAL, 10) << 20;
-        }
-        @Override
-        public boolean nextKeyValue() throws IOException {
-          written += write;
-          write = Math.min(toWrite - written, RINTERVAL);
-          val.set(write);
-          return written < toWrite;
-        }
-        @Override
-        public float getProgress() throws IOException {
-          return written / ((float)toWrite);
-        }
-        @Override
-        public NullWritable getCurrentKey() { return key; }
-        @Override
-        public LongWritable getCurrentValue() { return val; }
-        @Override
-        public void close() throws IOException {
-          taskContext.setStatus("Wrote " + toWrite);
-        }
-      };
-    }
-  }
-
-  static class GenSplit extends InputSplit implements Writable {
-    private long bytes;
-    private int nLoc;
-    private String[] locations;
-
-    public GenSplit() { }
-    public GenSplit(long bytes, String[] locations) {
-      this(bytes, locations.length, locations);
-    }
-    public GenSplit(long bytes, int nLoc, String[] locations) {
-      this.bytes = bytes;
-      this.nLoc = nLoc;
-      this.locations = Arrays.copyOf(locations, nLoc);
-    }
-    @Override
-    public long getLength() {
-      return bytes;
-    }
-    @Override
-    public String[] getLocations() {
-      return locations;
-    }
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      bytes = in.readLong();
-      nLoc = in.readInt();
-      if (null == locations || locations.length < nLoc) {
-        locations = new String[nLoc];
-      }
-      for (int i = 0; i < nLoc; ++i) {
-        locations[i] = Text.readString(in);
-      }
-    }
-    @Override
-    public void write(DataOutput out) throws IOException {
-      out.writeLong(bytes);
-      out.writeInt(nLoc);
-      for (int i = 0; i < nLoc; ++i) {
-        Text.writeString(out, locations[i]);
-      }
-    }
-  }
-
-  static class RawBytesOutputFormat
-      extends FileOutputFormat<NullWritable,BytesWritable> {
-
-    @Override
-    public RecordWriter<NullWritable,BytesWritable> getRecordWriter(
-        TaskAttemptContext job) throws IOException {
-
-      return new ChunkWriter(getDefaultWorkFile(job, ""),
-          job.getConfiguration());
-    }
-
-    static class ChunkWriter extends RecordWriter<NullWritable,BytesWritable> {
-      private final Path outDir;
-      private final FileSystem fs;
-      private final int blocksize;
-      private final short replicas;
-      private final FsPermission genPerms = new FsPermission((short) 0777);
-      private final long maxFileBytes;
-
-      private long accFileBytes = 0L;
-      private long fileIdx = -1L;
-      private OutputStream fileOut = null;
-
-      public ChunkWriter(Path outDir, Configuration conf) throws IOException {
-        this.outDir = outDir;
-        fs = outDir.getFileSystem(conf);
-        blocksize = conf.getInt(GRIDMIX_GEN_BLOCKSIZE, 1 << 28);
-        replicas = (short) conf.getInt(GRIDMIX_GEN_REPLICATION, 3);
-        maxFileBytes = conf.getLong(GRIDMIX_GEN_CHUNK, 1L << 30);
-        nextDestination();
-      }
-      private void nextDestination() throws IOException {
-        if (fileOut != null) {
-          fileOut.close();
-        }
-        fileOut = fs.create(new Path(outDir, "segment-" + (++fileIdx)),
-                            genPerms, false, 64 * 1024, replicas, 
-                            blocksize, null);
-        accFileBytes = 0L;
-      }
-      @Override
-      public void write(NullWritable key, BytesWritable value)
-          throws IOException {
-        int written = 0;
-        final int total = value.getLength();
-        while (written < total) {
-          if (accFileBytes >= maxFileBytes) {
-            nextDestination();
-          }
-          final int write = (int)
-            Math.min(total - written, maxFileBytes - accFileBytes);
-          fileOut.write(value.getBytes(), written, write);
-          written += write;
-          accFileBytes += write;
-        }
-      }
-      @Override
-      public void close(TaskAttemptContext ctxt) throws IOException {
-        fileOut.close();
-      }
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
deleted file mode 100644
index aa191629cf1..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GenerateDistCacheData.java
+++ /dev/null
@@ -1,267 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.nio.charset.Charset;
-import java.nio.charset.StandardCharsets;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.SequenceFile;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
-import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
-import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
-import org.apache.hadoop.security.UserGroupInformation;
-
-/**
- * GridmixJob that generates distributed cache files.
- * {@link GenerateDistCacheData} expects a list of distributed cache files to be
- * generated as input. This list is expected to be stored as a sequence file
- * and the filename is expected to be configured using
- * {@code gridmix.distcache.file.list}.
- * This input file contains the list of distributed cache files and their sizes.
- * For each record (i.e. file size and file path) in this input file,
- * a file with the specific file size at the specific path is created.
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-class GenerateDistCacheData extends GridmixJob {
-
-  /**
-   * Number of distributed cache files to be created by gridmix
-   */
-  static final String GRIDMIX_DISTCACHE_FILE_COUNT =
-      "gridmix.distcache.file.count";
-  /**
-   * Total number of bytes to be written to the distributed cache files by
-   * gridmix. i.e. Sum of sizes of all unique distributed cache files to be
-   * created by gridmix.
-   */
-  static final String GRIDMIX_DISTCACHE_BYTE_COUNT =
-      "gridmix.distcache.byte.count";
-  /**
-   * The special file created(and used) by gridmix, that contains the list of
-   * unique distributed cache files that are to be created and their sizes.
-   */
-  static final String GRIDMIX_DISTCACHE_FILE_LIST =
-      "gridmix.distcache.file.list";
-  static final String JOB_NAME = "GRIDMIX_GENERATE_DISTCACHE_DATA";
-
-  /**
-   * Create distributed cache file with the permissions 0644.
-   * Since the private distributed cache directory doesn't have execute
-   * permission for others, it is OK to set read permission for others for
-   * the files under that directory and still they will become 'private'
-   * distributed cache files on the simulated cluster.
-   */
-  static final short GRIDMIX_DISTCACHE_FILE_PERM = 0644;
-
-  private static final Charset charsetUTF8 = StandardCharsets.UTF_8;
-
-  public GenerateDistCacheData(Configuration conf) throws IOException {
-    super(conf, 0L, JOB_NAME);
-  }
-
-  @Override
-  public Job call() throws IOException, InterruptedException,
-                           ClassNotFoundException {
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-    ugi.doAs( new PrivilegedExceptionAction <Job>() {
-       public Job run() throws IOException, ClassNotFoundException,
-                               InterruptedException {
-        job.setMapperClass(GenDCDataMapper.class);
-        job.setNumReduceTasks(0);
-        job.setMapOutputKeyClass(NullWritable.class);
-        job.setMapOutputValueClass(BytesWritable.class);
-        job.setInputFormatClass(GenDCDataFormat.class);
-        job.setOutputFormatClass(NullOutputFormat.class);
-        job.setJarByClass(GenerateDistCacheData.class);
-        try {
-          FileInputFormat.addInputPath(job, new Path("ignored"));
-        } catch (IOException e) {
-          LOG.error("Error while adding input path ", e);
-        }
-        job.submit();
-        return job;
-      }
-    });
-    return job;
-  }
-
-  @Override
-  protected boolean canEmulateCompression() {
-    return false;
-  }
-
-  public static class GenDCDataMapper
-      extends Mapper<LongWritable, BytesWritable, NullWritable, BytesWritable> {
-
-    private BytesWritable val;
-    private final Random r = new Random();
-    private FileSystem fs;
-
-    @Override
-    protected void setup(Context context)
-        throws IOException, InterruptedException {
-      val = new BytesWritable(new byte[context.getConfiguration().getInt(
-              GenerateData.GRIDMIX_VAL_BYTES, 1024 * 1024)]);
-      fs = FileSystem.get(context.getConfiguration());
-    }
-
-    // Create one distributed cache file with the needed file size.
-    // key is distributed cache file size and
-    // value is distributed cache file path.
-    @Override
-    public void map(LongWritable key, BytesWritable value, Context context)
-        throws IOException, InterruptedException {
-
-      String fileName = new String(value.getBytes(), 0,
-          value.getLength(), charsetUTF8);
-      Path path = new Path(fileName);
-
-      FSDataOutputStream dos =
-          FileSystem.create(fs, path, new FsPermission(GRIDMIX_DISTCACHE_FILE_PERM));
-
-      int size = 0;
-      for (long bytes = key.get(); bytes > 0; bytes -= size) {
-        r.nextBytes(val.getBytes());
-        size = (int)Math.min(val.getLength(), bytes);
-        dos.write(val.getBytes(), 0, size);// Write to distCache file
-      }
-      dos.close();
-    }
-  }
-
-  /**
-   * InputFormat for GenerateDistCacheData.
-   * Input to GenerateDistCacheData is the special file(in SequenceFile format)
-   * that contains the list of distributed cache files to be generated along
-   * with their file sizes.
-   */
-  static class GenDCDataFormat
-      extends InputFormat<LongWritable, BytesWritable> {
-
-    // Split the special file that contains the list of distributed cache file
-    // paths and their file sizes such that each split corresponds to
-    // approximately same amount of distributed cache data to be generated.
-    // Consider numTaskTrackers * numMapSlotsPerTracker as the number of maps
-    // for this job, if there is lot of data to be generated.
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      final JobConf jobConf = new JobConf(jobCtxt.getConfiguration());
-      final JobClient client = new JobClient(jobConf);
-      ClusterStatus stat = client.getClusterStatus(true);
-      int numTrackers = stat.getTaskTrackers();
-      final int fileCount = jobConf.getInt(GRIDMIX_DISTCACHE_FILE_COUNT, -1);
-
-      // Total size of distributed cache files to be generated
-      final long totalSize = jobConf.getLong(GRIDMIX_DISTCACHE_BYTE_COUNT, -1);
-      // Get the path of the special file
-      String distCacheFileList = jobConf.get(GRIDMIX_DISTCACHE_FILE_LIST);
-      if (fileCount < 0 || totalSize < 0 || distCacheFileList == null) {
-        throw new RuntimeException("Invalid metadata: #files (" + fileCount
-            + "), total_size (" + totalSize + "), filelisturi ("
-            + distCacheFileList + ")");
-      }
-
-      Path sequenceFile = new Path(distCacheFileList);
-      FileSystem fs = sequenceFile.getFileSystem(jobConf);
-      FileStatus srcst = fs.getFileStatus(sequenceFile);
-      // Consider the number of TTs * mapSlotsPerTracker as number of mappers.
-      int numMapSlotsPerTracker = jobConf.getInt(TTConfig.TT_MAP_SLOTS, 2);
-      int numSplits = numTrackers * numMapSlotsPerTracker;
-
-      List<InputSplit> splits = new ArrayList<InputSplit>(numSplits);
-      LongWritable key = new LongWritable();
-      BytesWritable value = new BytesWritable();
-
-      // Average size of data to be generated by each map task
-      final long targetSize = Math.max(totalSize / numSplits,
-                                DistributedCacheEmulator.AVG_BYTES_PER_MAP);
-      long splitStartPosition = 0L;
-      long splitEndPosition = 0L;
-      long acc = 0L;
-      long bytesRemaining = srcst.getLen();
-      SequenceFile.Reader reader = null;
-      try {
-        reader = new SequenceFile.Reader(fs, sequenceFile, jobConf);
-        while (reader.next(key, value)) {
-
-          // If adding this file would put this split past the target size,
-          // cut the last split and put this file in the next split.
-          if (acc + key.get() > targetSize && acc != 0) {
-            long splitSize = splitEndPosition - splitStartPosition;
-            splits.add(new FileSplit(
-                sequenceFile, splitStartPosition, splitSize, (String[])null));
-            bytesRemaining -= splitSize;
-            splitStartPosition = splitEndPosition;
-            acc = 0L;
-          }
-          acc += key.get();
-          splitEndPosition = reader.getPosition();
-        }
-      } finally {
-        if (reader != null) {
-          reader.close();
-        }
-      }
-      if (bytesRemaining != 0) {
-        splits.add(new FileSplit(
-            sequenceFile, splitStartPosition, bytesRemaining, (String[])null));
-      }
-
-      return splits;
-    }
-
-    /**
-     * Returns a reader for this split of the distributed cache file list.
-     */
-    @Override
-    public RecordReader<LongWritable, BytesWritable> createRecordReader(
-        InputSplit split, final TaskAttemptContext taskContext)
-        throws IOException, InterruptedException {
-      return new SequenceFileRecordReader<LongWritable, BytesWritable>();
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
deleted file mode 100644
index e476223cf1e..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Gridmix.java
+++ /dev/null
@@ -1,809 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.PrintStream;
-import java.net.URI;
-import java.security.PrivilegedExceptionAction;
-import java.util.List;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.conf.Configured;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FsShell;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.util.ExitUtil;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.util.StringUtils;
-import org.apache.hadoop.util.Tool;
-import org.apache.hadoop.util.ToolRunner;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.ZombieJobProducer;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Driver class for the Gridmix3 benchmark. Gridmix accepts a timestamped
- * stream (trace) of job/task descriptions. For each job in the trace, the
- * client will submit a corresponding, synthetic job to the target cluster at
- * the rate in the original trace. The intent is to provide a benchmark that
- * can be configured and extended to closely match the measured resource
- * profile of actual, production loads.
- */
-public class Gridmix extends Configured implements Tool {
-
-  public static final Logger LOG = LoggerFactory.getLogger(Gridmix.class);
-
-  /**
-   * Output (scratch) directory for submitted jobs. Relative paths are
-   * resolved against the path provided as input and absolute paths remain
-   * independent of it. The default is &quot;gridmix&quot;.
-   */
-  public static final String GRIDMIX_OUT_DIR = "gridmix.output.directory";
-
-  /**
-   * Number of submitting threads at the client and upper bound for
-   * in-memory split data. Submitting threads precompute InputSplits for
-   * submitted jobs. This limits the number of splits held in memory waiting
-   * for submission and also permits parallel computation of split data.
-   */
-  public static final String GRIDMIX_SUB_THR = "gridmix.client.submit.threads";
-
-  /**
-   * The depth of the queue of job descriptions. Before splits are computed,
-   * a queue of pending descriptions is stored in memoory. This parameter
-   * limits the depth of that queue.
-   */
-  public static final String GRIDMIX_QUE_DEP =
-    "gridmix.client.pending.queue.depth";
-
-  /**
-   * Multiplier to accelerate or decelerate job submission. As a crude means of
-   * sizing a job trace to a cluster, the time separating two jobs is
-   * multiplied by this factor.
-   */
-  public static final String GRIDMIX_SUB_MUL = "gridmix.submit.multiplier";
-
-  /**
-   * Class used to resolve users in the trace to the list of target users
-   * on the cluster.
-   */
-  public static final String GRIDMIX_USR_RSV = "gridmix.user.resolve.class";
-
-  /**
-   * The configuration key which determines the duration for which the 
-   * job-monitor sleeps while polling for job status.
-   * This value should be specified in milliseconds.
-   */
-  public static final String GRIDMIX_JOBMONITOR_SLEEPTIME_MILLIS = 
-    "gridmix.job-monitor.sleep-time-ms";
-  
-  /**
-   * Default value for {@link #GRIDMIX_JOBMONITOR_SLEEPTIME_MILLIS}.
-   */
-  public static final int GRIDMIX_JOBMONITOR_SLEEPTIME_MILLIS_DEFAULT = 500;
-  
-  /**
-   * The configuration key which determines the total number of job-status
-   * monitoring threads.
-   */
-  public static final String GRIDMIX_JOBMONITOR_THREADS = 
-    "gridmix.job-monitor.thread-count";
-  
-  /**
-   * Default value for {@link #GRIDMIX_JOBMONITOR_THREADS}.
-   */
-  public static final int GRIDMIX_JOBMONITOR_THREADS_DEFAULT = 1;
-  
-  /**
-   * Configuration property set in simulated job's configuration whose value is
-   * set to the corresponding original job's name. This is not configurable by
-   * gridmix user.
-   */
-  public static final String ORIGINAL_JOB_NAME =
-      "gridmix.job.original-job-name";
-  /**
-   * Configuration property set in simulated job's configuration whose value is
-   * set to the corresponding original job's id. This is not configurable by
-   * gridmix user.
-   */
-  public static final String ORIGINAL_JOB_ID = "gridmix.job.original-job-id";
-
-  private DistributedCacheEmulator distCacheEmulator;
-
-  // Submit data structures
-  private JobFactory factory;
-  private JobSubmitter submitter;
-  private JobMonitor monitor;
-  private Statistics statistics;
-  private Summarizer summarizer;
-
-  // Shutdown hook
-  private final Shutdown sdh = new Shutdown();
-
-  /** Error while parsing/analyzing the arguments to Gridmix */
-  static final int ARGS_ERROR = 1;
-  /** Error while trying to start/setup the Gridmix run */
-  static final int STARTUP_FAILED_ERROR = 2;
-  /**
-   * If at least 1 distributed cache file is missing in the expected
-   * distributed cache dir, Gridmix cannot proceed with emulation of
-   * distributed cache load.
-   */
-  static final int MISSING_DIST_CACHE_FILES_ERROR = 3;
-
-
-  Gridmix(String[] args) {
-    summarizer = new Summarizer(args);
-  }
-  
-  public Gridmix() {
-    summarizer = new Summarizer();
-  }
-  
-  // Get the input data directory for Gridmix. Input directory is 
-  // <io-path>/input
-  static Path getGridmixInputDataPath(Path ioPath) {
-    return new Path(ioPath, "input");
-  }
-  
-  /**
-   * Write random bytes at the path &lt;inputDir&gt; if needed.
-   * @see org.apache.hadoop.mapred.gridmix.GenerateData
-   * @return exit status
-   */
-  protected int writeInputData(long genbytes, Path inputDir)
-      throws IOException, InterruptedException {
-    if (genbytes > 0) {
-      final Configuration conf = getConf();
-
-      if (inputDir.getFileSystem(conf).exists(inputDir)) {
-        LOG.error("Gridmix input data directory {} already exists " +
-            "when -generate option is used.", inputDir);
-        return STARTUP_FAILED_ERROR;
-      }
-
-      // configure the compression ratio if needed
-      CompressionEmulationUtil.setupDataGeneratorConfig(conf);
-    
-      final GenerateData genData = new GenerateData(conf, inputDir, genbytes);
-      LOG.info("Generating {} of test data...",
-          StringUtils.TraditionalBinaryPrefix.long2String(genbytes, "", 1));
-      launchGridmixJob(genData);
-    
-      FsShell shell = new FsShell(conf);
-      try {
-        LOG.info("Changing the permissions for inputPath {}", inputDir);
-        shell.run(new String[] {"-chmod","-R","777", inputDir.toString()});
-      } catch (Exception e) {
-        LOG.error("Couldnt change the file permissions " , e);
-        throw new IOException(e);
-      }
-
-      LOG.info("Input data generation successful.");
-    }
-
-    return 0;
-  }
-
-  /**
-   * Write random bytes in the distributed cache files that will be used by all
-   * simulated jobs of current gridmix run, if files are to be generated.
-   * Do this as part of the MapReduce job {@link GenerateDistCacheData#JOB_NAME}
-   * @see org.apache.hadoop.mapred.gridmix.GenerateDistCacheData
-   */
-  protected void writeDistCacheData(Configuration conf)
-      throws IOException, InterruptedException {
-    int fileCount =
-        conf.getInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, -1);
-    if (fileCount > 0) {// generate distributed cache files
-      final GridmixJob genDistCacheData = new GenerateDistCacheData(conf);
-      LOG.info("Generating distributed cache data of size " + conf.getLong(
-          GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, -1));
-      launchGridmixJob(genDistCacheData);
-    }
-  }
-
-  // Launch Input/DistCache Data Generation job and wait for completion
-  void launchGridmixJob(GridmixJob job)
-      throws IOException, InterruptedException {
-    submitter.add(job);
-
-    // TODO add listeners, use for job dependencies
-    try {
-      while (!job.isSubmitted()) {
-        try {
-            Thread.sleep(100); // sleep
-          } catch (InterruptedException ie) {}
-      }
-      // wait for completion
-      job.getJob().waitForCompletion(false);
-    } catch (ClassNotFoundException e) {
-      throw new IOException("Internal error", e);
-    }
-    if (!job.getJob().isSuccessful()) {
-      throw new IOException(job.getJob().getJobName() + " job failed!");
-    }
-  }
-
-  /**
-   * Create an appropriate {@code JobStoryProducer} object for the
-   * given trace.
-   * 
-   * @param traceIn the path to the trace file. The special path
-   * &quot;-&quot; denotes the standard input stream.
-   *
-   * @param conf the configuration to be used.
-   *
-   * @throws IOException if there was an error.
-   */
-  protected JobStoryProducer createJobStoryProducer(String traceIn,
-      Configuration conf) throws IOException {
-    if ("-".equals(traceIn)) {
-      return new ZombieJobProducer(System.in, null);
-    }
-    return new ZombieJobProducer(new Path(traceIn), null, conf);
-  }
-
-  // get the gridmix job submission policy
-  protected static GridmixJobSubmissionPolicy getJobSubmissionPolicy(
-                                                Configuration conf) {
-    return GridmixJobSubmissionPolicy.getPolicy(conf, 
-                                        GridmixJobSubmissionPolicy.STRESS);
-  }
-  
-  /**
-   * Create each component in the pipeline and start it.
-   * @param conf Configuration data, no keys specific to this context
-   * @param traceIn Either a Path to the trace data or &quot;-&quot; for
-   *                stdin
-   * @param ioPath &lt;ioPath&gt;/input/ is the dir from which input data is
-   *               read and &lt;ioPath&gt;/distributedCache/ is the gridmix
-   *               distributed cache directory.
-   * @param scratchDir Path into which job output is written
-   * @param startFlag Semaphore for starting job trace pipeline
-   */
-  private void startThreads(Configuration conf, String traceIn, Path ioPath,
-      Path scratchDir, CountDownLatch startFlag, UserResolver userResolver)
-      throws IOException {
-    try {
-      Path inputDir = getGridmixInputDataPath(ioPath);
-      GridmixJobSubmissionPolicy policy = getJobSubmissionPolicy(conf);
-      LOG.info(" Submission policy is " + policy.name());
-      statistics = new Statistics(conf, policy.getPollingInterval(), startFlag);
-      monitor = createJobMonitor(statistics, conf);
-      int noOfSubmitterThreads = 
-        (policy == GridmixJobSubmissionPolicy.SERIAL) 
-        ? 1
-        : Runtime.getRuntime().availableProcessors() + 1;
-
-      int numThreads = conf.getInt(GRIDMIX_SUB_THR, noOfSubmitterThreads);
-      int queueDep = conf.getInt(GRIDMIX_QUE_DEP, 5);
-      submitter = createJobSubmitter(monitor, numThreads, queueDep,
-                                     new FilePool(conf, inputDir), userResolver, 
-                                     statistics);
-      distCacheEmulator = new DistributedCacheEmulator(conf, ioPath);
-
-      factory = createJobFactory(submitter, traceIn, scratchDir, conf, 
-                                 startFlag, userResolver);
-      factory.jobCreator.setDistCacheEmulator(distCacheEmulator);
-
-      if (policy == GridmixJobSubmissionPolicy.SERIAL) {
-        statistics.addJobStatsListeners(factory);
-      } else {
-        statistics.addClusterStatsObservers(factory);
-      }
-
-      // add the gridmix run summarizer to the statistics
-      statistics.addJobStatsListeners(summarizer.getExecutionSummarizer());
-      statistics.addClusterStatsObservers(summarizer.getClusterSummarizer());
-      
-      monitor.start();
-      submitter.start();
-    }catch(Exception e) {
-      LOG.error(" Exception at start " ,e);
-      throw new IOException(e);
-    }
-   }
-
-  protected JobMonitor createJobMonitor(Statistics stats, Configuration conf) 
-  throws IOException {
-    int delay = conf.getInt(GRIDMIX_JOBMONITOR_SLEEPTIME_MILLIS, 
-                            GRIDMIX_JOBMONITOR_SLEEPTIME_MILLIS_DEFAULT);
-    int numThreads = conf.getInt(GRIDMIX_JOBMONITOR_THREADS, 
-                                 GRIDMIX_JOBMONITOR_THREADS_DEFAULT);
-    return new JobMonitor(delay, TimeUnit.MILLISECONDS, stats, numThreads);
-  }
-
-  protected JobSubmitter createJobSubmitter(JobMonitor monitor, int threads,
-      int queueDepth, FilePool pool, UserResolver resolver, 
-      Statistics statistics) throws IOException {
-    return new JobSubmitter(monitor, threads, queueDepth, pool, statistics);
-  }
-
-  protected JobFactory createJobFactory(JobSubmitter submitter, String traceIn,
-      Path scratchDir, Configuration conf, CountDownLatch startFlag, 
-      UserResolver resolver)
-      throws IOException {
-     return GridmixJobSubmissionPolicy.getPolicy(
-       conf, GridmixJobSubmissionPolicy.STRESS).createJobFactory(
-       submitter, createJobStoryProducer(traceIn, conf), scratchDir, conf,
-       startFlag, resolver);
-  }
-
-  private static UserResolver userResolver;
-
-  public UserResolver getCurrentUserResolver() {
-    return userResolver;
-  }
-  
-  public int run(final String[] argv) throws IOException, InterruptedException {
-    int val = -1;
-    final Configuration conf = getConf();
-    UserGroupInformation.setConfiguration(conf);
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-
-    val = ugi.doAs(new PrivilegedExceptionAction<Integer>() {
-      public Integer run() throws Exception {
-        return runJob(conf, argv);
-      }
-    });
-    
-    // print the gridmix summary if the run was successful
-    if (val == 0) {
-        // print the run summary
-        System.out.print("\n\n");
-        System.out.println(summarizer.toString());
-    }
-    
-    return val; 
-  }
-
-  private int runJob(Configuration conf, String[] argv)
-    throws IOException, InterruptedException {
-    if (argv.length < 2) {
-      LOG.error("Too few arguments to Gridmix.\n");
-      printUsage(System.err);
-      return ARGS_ERROR;
-    }
-
-    long genbytes = -1L;
-    String traceIn = null;
-    Path ioPath = null;
-    URI userRsrc = null;
-    try {
-      userResolver = ReflectionUtils.newInstance(conf.getClass(GRIDMIX_USR_RSV, 
-                       SubmitterUserResolver.class, UserResolver.class), conf);
-
-      for (int i = 0; i < argv.length - 2; ++i) {
-        if ("-generate".equals(argv[i])) {
-          genbytes = StringUtils.TraditionalBinaryPrefix.string2long(argv[++i]);
-          if (genbytes <= 0) {
-            LOG.error("size of input data to be generated specified using "
-                      + "-generate option should be nonnegative.\n");
-            return ARGS_ERROR;
-          }
-        } else if ("-users".equals(argv[i])) {
-          userRsrc = new URI(argv[++i]);
-        } else {
-          LOG.error("Unknown option " + argv[i] + " specified.\n");
-          printUsage(System.err);
-          return ARGS_ERROR;
-        }
-      }
-
-      if (userResolver.needsTargetUsersList()) {
-        if (userRsrc != null) {
-          if (!userResolver.setTargetUsers(userRsrc, conf)) {
-            LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
-          }
-        } else {
-          LOG.error(userResolver.getClass()
-              + " needs target user list. Use -users option.\n");
-          printUsage(System.err);
-          return ARGS_ERROR;
-        }
-      } else if (userRsrc != null) {
-        LOG.warn("Ignoring the user resource '" + userRsrc + "'.");
-      }
-
-      ioPath = new Path(argv[argv.length - 2]);
-      traceIn = argv[argv.length - 1];
-    } catch (Exception e) {
-      LOG.error(e.toString() + "\n");
-      if (LOG.isDebugEnabled()) {
-        e.printStackTrace();
-      }
-
-      printUsage(System.err);
-      return ARGS_ERROR;
-    }
-
-    // Create <ioPath> with 777 permissions
-    final FileSystem inputFs = ioPath.getFileSystem(conf);
-    ioPath = inputFs.makeQualified(ioPath);
-    boolean succeeded = false;
-    try {
-      succeeded = FileSystem.mkdirs(inputFs, ioPath,
-                                    new FsPermission((short)0777));
-    } catch(IOException e) {
-      // No need to emit this exception message
-    } finally {
-      if (!succeeded) {
-        LOG.error("Failed creation of <ioPath> directory " + ioPath + "\n");
-        return STARTUP_FAILED_ERROR;
-      }
-    }
-
-    return start(conf, traceIn, ioPath, genbytes, userResolver);
-  }
-
-  /**
-   * 
-   * @param conf gridmix configuration
-   * @param traceIn trace file path(if it is '-', then trace comes from the
-   *                stream stdin)
-   * @param ioPath Working directory for gridmix. GenerateData job
-   *               will generate data in the directory &lt;ioPath&gt;/input/ and
-   *               distributed cache data is generated in the directory
-   *               &lt;ioPath&gt;/distributedCache/, if -generate option is
-   *               specified.
-   * @param genbytes size of input data to be generated under the directory
-   *                 &lt;ioPath&gt;/input/
-   * @param userResolver gridmix user resolver
-   * @return exit code
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  int start(Configuration conf, String traceIn, Path ioPath, long genbytes,
-      UserResolver userResolver)
-      throws IOException, InterruptedException {
-    DataStatistics stats = null;
-    InputStream trace = null;
-    int exitCode = 0;
-
-    try {
-      Path scratchDir = new Path(ioPath, conf.get(GRIDMIX_OUT_DIR, "gridmix"));
-
-      // add shutdown hook for SIGINT, etc.
-      Runtime.getRuntime().addShutdownHook(sdh);
-      CountDownLatch startFlag = new CountDownLatch(1);
-      try {
-        // Create, start job submission threads
-        startThreads(conf, traceIn, ioPath, scratchDir, startFlag,
-                     userResolver);
-        
-        Path inputDir = getGridmixInputDataPath(ioPath);
-        
-        // Write input data if specified
-        exitCode = writeInputData(genbytes, inputDir);
-        if (exitCode != 0) {
-          return exitCode;
-        }
-
-        // publish the data statistics
-        stats = GenerateData.publishDataStatistics(inputDir, genbytes, conf);
-        
-        // scan input dir contents
-        submitter.refreshFilePool();
-
-        boolean shouldGenerate = (genbytes > 0);
-        // set up the needed things for emulation of various loads
-        exitCode = setupEmulation(conf, traceIn, scratchDir, ioPath,
-                                  shouldGenerate);
-        if (exitCode != 0) {
-          return exitCode;
-        }
-
-        // start the summarizer
-        summarizer.start(conf);
-        
-        factory.start();
-        statistics.start();
-      } catch (Throwable e) {
-        LOG.error("Startup failed. " + e.toString() + "\n");
-        LOG.debug("Startup failed", e);
-        if (factory != null) factory.abort(); // abort pipeline
-        exitCode = STARTUP_FAILED_ERROR;
-      } finally {
-        // signal for factory to start; sets start time
-        startFlag.countDown();
-      }
-      if (factory != null) {
-        // wait for input exhaustion
-        factory.join(Long.MAX_VALUE);
-        final Throwable badTraceException = factory.error();
-        if (null != badTraceException) {
-          LOG.error("Error in trace", badTraceException);
-          throw new IOException("Error in trace", badTraceException);
-        }
-        // wait for pending tasks to be submitted
-        submitter.shutdown();
-        submitter.join(Long.MAX_VALUE);
-        // wait for running tasks to complete
-        monitor.shutdown();
-        monitor.join(Long.MAX_VALUE);
-
-        statistics.shutdown();
-        statistics.join(Long.MAX_VALUE);
-
-      }
-    } finally {
-      if (factory != null) {
-        summarizer.finalize(factory, traceIn, genbytes, userResolver, stats, 
-                            conf);
-      }
-      IOUtils.cleanupWithLogger(LOG, trace);
-    }
-    return exitCode;
-  }
-
-  /**
-   * Create gridmix output directory. Setup things for emulation of
-   * various loads, if needed.
-   * @param conf gridmix configuration
-   * @param traceIn trace file path(if it is '-', then trace comes from the
-   *                stream stdin)
-   * @param scratchDir gridmix output directory
-   * @param ioPath Working directory for gridmix.
-   * @param generate true if -generate option was specified
-   * @return exit code
-   * @throws IOException
-   * @throws InterruptedException 
-   */
-  private int setupEmulation(Configuration conf, String traceIn,
-      Path scratchDir, Path ioPath, boolean generate)
-      throws IOException, InterruptedException {
-    // create scratch directory(output directory of gridmix)
-    final FileSystem scratchFs = scratchDir.getFileSystem(conf);
-    FileSystem.mkdirs(scratchFs, scratchDir, new FsPermission((short) 0777));
-
-    // Setup things needed for emulation of distributed cache load
-    return setupDistCacheEmulation(conf, traceIn, ioPath, generate);
-    // Setup emulation of other loads like CPU load, Memory load
-  }
-
-  /**
-   * Setup gridmix for emulation of distributed cache load. This includes
-   * generation of distributed cache files, if needed.
-   * @param conf gridmix configuration
-   * @param traceIn trace file path(if it is '-', then trace comes from the
-   *                stream stdin)
-   * @param ioPath &lt;ioPath&gt;/input/ is the dir where input data (a) exists
-   *               or (b) is generated. &lt;ioPath&gt;/distributedCache/ is the
-   *               folder where distributed cache data (a) exists or (b) is to be
-   *               generated by gridmix.
-   * @param generate true if -generate option was specified
-   * @return exit code
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  private int setupDistCacheEmulation(Configuration conf, String traceIn,
-      Path ioPath, boolean generate) throws IOException, InterruptedException {
-    distCacheEmulator.init(traceIn, factory.jobCreator, generate);
-    int exitCode = 0;
-    if (distCacheEmulator.shouldGenerateDistCacheData() ||
-        distCacheEmulator.shouldEmulateDistCacheLoad()) {
-
-      JobStoryProducer jsp = createJobStoryProducer(traceIn, conf);
-      exitCode = distCacheEmulator.setupGenerateDistCacheData(jsp);
-      if (exitCode == 0) {
-        // If there are files to be generated, run a MapReduce job to generate
-        // these distributed cache files of all the simulated jobs of this trace.
-        writeDistCacheData(conf);
-      }
-    }
-    return exitCode;
-  }
-
-  /**
-   * Handles orderly shutdown by requesting that each component in the
-   * pipeline abort its progress, waiting for each to exit and killing
-   * any jobs still running on the cluster.
-   */
-  class Shutdown extends Thread {
-
-    static final long FAC_SLEEP = 1000;
-    static final long SUB_SLEEP = 4000;
-    static final long MON_SLEEP = 15000;
-
-    private void killComponent(Component<?> component, long maxwait) {
-      if (component == null) {
-        return;
-      }
-      component.abort();
-      try {
-        component.join(maxwait);
-      } catch (InterruptedException e) {
-        LOG.warn("Interrupted waiting for " + component);
-      }
-
-    }
-
-    @Override
-    public void run() {
-      LOG.info("Exiting...");
-      try {
-        killComponent(factory, FAC_SLEEP);   // read no more tasks
-        killComponent(submitter, SUB_SLEEP); // submit no more tasks
-        killComponent(monitor, MON_SLEEP);   // process remaining jobs here
-        killComponent(statistics,MON_SLEEP);
-      } finally {
-        if (monitor == null) {
-          return;
-        }
-        List<JobStats> remainingJobs = monitor.getRemainingJobs();
-        if (remainingJobs.isEmpty()) {
-          return;
-        }
-        LOG.info("Killing running jobs...");
-        for (JobStats stats : remainingJobs) {
-          Job job = stats.getJob();
-          try {
-            if (!job.isComplete()) {
-              job.killJob();
-              LOG.info("Killed " + job.getJobName() + " (" + job.getJobID() + ")");
-            } else {
-              if (job.isSuccessful()) {
-                monitor.onSuccess(job);
-              } else {
-                monitor.onFailure(job);
-              }
-            }
-          } catch (IOException e) {
-            LOG.warn("Failure killing " + job.getJobName(), e);
-          } catch (Exception e) {
-            LOG.error("Unexpected exception", e);
-          }
-        }
-        LOG.info("Done.");
-      }
-    }
-
-  }
-
-  public static void main(String[] argv) throws Exception {
-    int res = -1;
-    try {
-      res = ToolRunner.run(new Configuration(), new Gridmix(argv), argv);
-    } finally {
-      ExitUtil.terminate(res);
-    }
-  }
-
-  private String getEnumValues(Enum<?>[] e) {
-    StringBuilder sb = new StringBuilder();
-    String sep = "";
-    for (Enum<?> v : e) {
-      sb.append(sep);
-      sb.append(v.name());
-      sep = "|";
-    }
-    return sb.toString();
-  }
-  
-  private String getJobTypes() {
-    return getEnumValues(JobCreator.values());
-  }
-  
-  private String getSubmissionPolicies() {
-    return getEnumValues(GridmixJobSubmissionPolicy.values());
-  }
-  
-  protected void printUsage(PrintStream out) {
-    ToolRunner.printGenericCommandUsage(out);
-    out.println("Usage: gridmix [-generate <MiB>] [-users URI] [-Dname=value ...] <iopath> <trace>");
-    out.println("  e.g. gridmix -generate 100m foo -");
-    out.println("Options:");
-    out.println("   -generate <MiB> : Generate input data of size MiB under "
-        + "<iopath>/input/ and generate\n\t\t     distributed cache data under "
-        + "<iopath>/distributedCache/.");
-    out.println("   -users <usersResourceURI> : URI that contains the users list.");
-    out.println("Configuration parameters:");
-    out.println("   General parameters:");
-    out.printf("       %-48s : Output directory%n", GRIDMIX_OUT_DIR);
-    out.printf("       %-48s : Submitting threads%n", GRIDMIX_SUB_THR);
-    out.printf("       %-48s : Queued job desc%n", GRIDMIX_QUE_DEP);
-    out.printf("       %-48s : User resolution class%n", GRIDMIX_USR_RSV);
-    out.printf("       %-48s : Job types (%s)%n", JobCreator.GRIDMIX_JOB_TYPE, getJobTypes());
-    out.println("   Parameters related to job submission:");    
-    out.printf("       %-48s : Default queue%n",
-        GridmixJob.GRIDMIX_DEFAULT_QUEUE);
-    out.printf("       %-48s : Enable/disable using queues in trace%n",
-        GridmixJob.GRIDMIX_USE_QUEUE_IN_TRACE);
-    out.printf("       %-48s : Job submission policy (%s)%n",
-        GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, getSubmissionPolicies());
-    out.println("   Parameters specific for LOADJOB:");
-    out.printf("       %-48s : Key fraction of rec%n",
-        AvgRecordFactory.GRIDMIX_KEY_FRC);
-    out.println("   Parameters specific for SLEEPJOB:");
-    out.printf("       %-48s : Whether to ignore reduce tasks%n",
-        SleepJob.SLEEPJOB_MAPTASK_ONLY);
-    out.printf("       %-48s : Number of fake locations for map tasks%n",
-        JobCreator.SLEEPJOB_RANDOM_LOCATIONS);
-    out.printf("       %-48s : Maximum map task runtime in mili-sec%n",
-        SleepJob.GRIDMIX_SLEEP_MAX_MAP_TIME);
-    out.printf("       %-48s : Maximum reduce task runtime in mili-sec (merge+reduce)%n",
-        SleepJob.GRIDMIX_SLEEP_MAX_REDUCE_TIME);
-    out.println("   Parameters specific for STRESS submission throttling policy:");
-    out.printf("       %-48s : jobs vs task-tracker ratio%n",
-        StressJobFactory.CONF_MAX_JOB_TRACKER_RATIO);
-    out.printf("       %-48s : maps vs map-slot ratio%n",
-        StressJobFactory.CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO);
-    out.printf("       %-48s : reduces vs reduce-slot ratio%n",
-        StressJobFactory.CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO);
-    out.printf("       %-48s : map-slot share per job%n",
-        StressJobFactory.CONF_MAX_MAPSLOT_SHARE_PER_JOB);
-    out.printf("       %-48s : reduce-slot share per job%n",
-        StressJobFactory.CONF_MAX_REDUCESLOT_SHARE_PER_JOB);
-   }
-
-  /**
-   * Components in the pipeline must support the following operations for
-   * orderly startup and shutdown.
-   */
-  interface Component<T> {
-
-    /**
-     * Accept an item into this component from an upstream component. If
-     * shutdown or abort have been called, this may fail, depending on the
-     * semantics for the component.
-     */
-    void add(T item) throws InterruptedException;
-
-    /**
-     * Attempt to start the service.
-     */
-    void start();
-
-    /**
-     * Wait until the service completes. It is assumed that either a
-     * {@link #shutdown} or {@link #abort} has been requested.
-     */
-    void join(long millis) throws InterruptedException;
-
-    /**
-     * Shut down gracefully, finishing all pending work. Reject new requests.
-     */
-    void shutdown();
-
-    /**
-     * Shut down immediately, aborting any work in progress and discarding
-     * all pending work. It is legal to store pending work for another
-     * thread to process.
-     */
-    void abort();
-  }
-  // it is need for tests
-  protected Summarizer getSummarizer() {
-    return summarizer;
-  }
-  
-}
-
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
deleted file mode 100644
index 325c15c9971..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJob.java
+++ /dev/null
@@ -1,526 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Formatter;
-import java.util.List;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.Delayed;
-import java.util.concurrent.TimeUnit;
-import java.security.PrivilegedExceptionAction;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.MRConfig;
-import org.apache.hadoop.mapreduce.Partitioner;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import static org.apache.hadoop.tools.rumen.datatypes.util.MapReduceJobPropertiesParser.extractMaxHeapOpts;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Synthetic job generated from a trace description.
- */
-abstract class GridmixJob implements Callable<Job>, Delayed {
-
-  // Gridmix job name format is GRIDMIX<6 digit sequence number>
-  public static final String JOB_NAME_PREFIX = "GRIDMIX";
-  public static final Logger LOG = LoggerFactory.getLogger(GridmixJob.class);
-
-  private static final ThreadLocal<Formatter> nameFormat =
-    new ThreadLocal<Formatter>() {
-      @Override
-      protected Formatter initialValue() {
-        final StringBuilder sb =
-            new StringBuilder(JOB_NAME_PREFIX.length() + 6);
-        sb.append(JOB_NAME_PREFIX);
-        return new Formatter(sb);
-      }
-    };
-
-  private boolean submitted;
-  protected final int seq;
-  protected final Path outdir;
-  protected final Job job;
-  protected final JobStory jobdesc;
-  protected final UserGroupInformation ugi;
-  protected final long submissionTimeNanos;
-  private static final ConcurrentHashMap<Integer,List<InputSplit>> descCache =
-     new ConcurrentHashMap<Integer,List<InputSplit>>();
-  protected static final String GRIDMIX_JOB_SEQ = "gridmix.job.seq";
-  protected static final String GRIDMIX_USE_QUEUE_IN_TRACE = 
-      "gridmix.job-submission.use-queue-in-trace";
-  protected static final String GRIDMIX_DEFAULT_QUEUE = 
-      "gridmix.job-submission.default-queue";
-  // configuration key to enable/disable High-Ram feature emulation
-  static final String GRIDMIX_HIGHRAM_EMULATION_ENABLE = 
-    "gridmix.highram-emulation.enable";
-  // configuration key to enable/disable task jvm options
-  static final String GRIDMIX_TASK_JVM_OPTIONS_ENABLE = 
-    "gridmix.task.jvm-options.enable";
-
-  private static void setJobQueue(Job job, String queue) {
-    if (queue != null) {
-      job.getConfiguration().set(MRJobConfig.QUEUE_NAME, queue);
-    }
-  }
-  
-  public GridmixJob(final Configuration conf, long submissionMillis,
-      final JobStory jobdesc, Path outRoot, UserGroupInformation ugi, 
-      final int seq) throws IOException {
-    this.ugi = ugi;
-    this.jobdesc = jobdesc;
-    this.seq = seq;
-
-    ((StringBuilder)nameFormat.get().out()).setLength(JOB_NAME_PREFIX.length());
-    try {
-      job = this.ugi.doAs(new PrivilegedExceptionAction<Job>() {
-        public Job run() throws IOException {
-
-          String jobId = null == jobdesc.getJobID() 
-                         ? "<unknown>" 
-                         : jobdesc.getJobID().toString();
-          Job ret = Job.getInstance(conf, nameFormat.get().format("%06d", seq)
-                                          .toString());
-          ret.getConfiguration().setInt(GRIDMIX_JOB_SEQ, seq);
-
-          ret.getConfiguration().set(Gridmix.ORIGINAL_JOB_ID, jobId);
-          ret.getConfiguration().set(Gridmix.ORIGINAL_JOB_NAME,
-                                     jobdesc.getName());
-          if (conf.getBoolean(GRIDMIX_USE_QUEUE_IN_TRACE, false)) {
-            setJobQueue(ret, jobdesc.getQueueName());
-          } else {
-            setJobQueue(ret, conf.get(GRIDMIX_DEFAULT_QUEUE));
-          }
-
-          // check if the job can emulate compression
-          if (canEmulateCompression()) {
-            // set the compression related configs if compression emulation is
-            // enabled
-            if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)) {
-              CompressionEmulationUtil.configureCompressionEmulation(
-                  jobdesc.getJobConf(), ret.getConfiguration());
-            }
-          }
-          
-          // configure high ram properties if enabled
-          if (conf.getBoolean(GRIDMIX_HIGHRAM_EMULATION_ENABLE, true)) {
-            configureHighRamProperties(jobdesc.getJobConf(), 
-                                       ret.getConfiguration());
-          }
-          
-          // configure task jvm options if enabled
-          // this knob can be turned off if there is a mismatch between the
-          // target (simulation) cluster and the original cluster. Such a 
-          // mismatch can result in job failures (due to memory issues) on the 
-          // target (simulated) cluster.
-          //
-          // TODO If configured, scale the original task's JVM (heap related)
-          //      options to suit the target (simulation) cluster
-          if (conf.getBoolean(GRIDMIX_TASK_JVM_OPTIONS_ENABLE, true)) {
-            configureTaskJVMOptions(jobdesc.getJobConf(), 
-                                    ret.getConfiguration());
-          }
-          
-          return ret;
-        }
-      });
-    } catch (InterruptedException e) {
-      throw new IOException(e);
-    }
-
-    submissionTimeNanos = TimeUnit.NANOSECONDS.convert(
-        submissionMillis, TimeUnit.MILLISECONDS);
-    outdir = new Path(outRoot, "" + seq);
-  }
-  
-  @SuppressWarnings("deprecation")
-  protected static void configureTaskJVMOptions(Configuration originalJobConf,
-                                                Configuration simulatedJobConf){
-    // Get the heap related java opts used for the original job and set the 
-    // same for the simulated job.
-    //    set task task heap options
-    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
-                                   JobConf.MAPRED_TASK_JAVA_OPTS);
-    //  set map task heap options
-    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
-                                   MRJobConfig.MAP_JAVA_OPTS);
-
-    //  set reduce task heap options
-    configureTaskJVMMaxHeapOptions(originalJobConf, simulatedJobConf, 
-                                   MRJobConfig.REDUCE_JAVA_OPTS);
-  }
-  
-  // Configures the task's max heap options using the specified key
-  private static void configureTaskJVMMaxHeapOptions(Configuration srcConf, 
-                                                     Configuration destConf,
-                                                     String key) {
-    String srcHeapOpts = srcConf.get(key);
-    if (srcHeapOpts != null) {
-      List<String> srcMaxOptsList = new ArrayList<String>();
-      // extract the max heap options and ignore the rest
-      extractMaxHeapOpts(srcHeapOpts, srcMaxOptsList, 
-                         new ArrayList<String>());
-      if (srcMaxOptsList.size() > 0) {
-        List<String> destOtherOptsList = new ArrayList<String>();
-        // extract the other heap options and ignore the max options in the 
-        // destination configuration
-        String destHeapOpts = destConf.get(key);
-        if (destHeapOpts != null) {
-          extractMaxHeapOpts(destHeapOpts, new ArrayList<String>(), 
-                             destOtherOptsList);
-        }
-        
-        // the source configuration might have some task level max heap opts set
-        // remove these opts from the destination configuration and replace
-        // with the options set in the original configuration
-        StringBuilder newHeapOpts = new StringBuilder();
-        
-        for (String otherOpt : destOtherOptsList) {
-          newHeapOpts.append(otherOpt).append(" ");
-        }
-        
-        for (String opts : srcMaxOptsList) {
-          newHeapOpts.append(opts).append(" ");
-        }
-        
-        // set the final heap opts 
-        destConf.set(key, newHeapOpts.toString().trim());
-      }
-    }
-  }
-
-  // Scales the desired job-level configuration parameter. This API makes sure 
-  // that the ratio of the job level configuration parameter to the cluster 
-  // level configuration parameter is maintained in the simulated run. Hence 
-  // the values are scaled from the original cluster's configuration to the 
-  // simulated cluster's configuration for higher emulation accuracy.
-  // This kind of scaling is useful for memory parameters.
-  private static void scaleConfigParameter(Configuration sourceConf, 
-                        Configuration destConf, String clusterValueKey, 
-                        String jobValueKey, long defaultValue) {
-    long simulatedClusterDefaultValue = 
-           destConf.getLong(clusterValueKey, defaultValue);
-    
-    long originalClusterDefaultValue = 
-           sourceConf.getLong(clusterValueKey, defaultValue);
-    
-    long originalJobValue = 
-           sourceConf.getLong(jobValueKey, defaultValue);
-    
-    double scaleFactor = (double)originalJobValue/originalClusterDefaultValue;
-    
-    long simulatedJobValue = (long)(scaleFactor * simulatedClusterDefaultValue);
-    
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("For the job configuration parameter '" + jobValueKey 
-                + "' and the cluster configuration parameter '" 
-                + clusterValueKey + "', the original job's configuration value"
-                + " is scaled from '" + originalJobValue + "' to '" 
-                + simulatedJobValue + "' using the default (unit) value of "
-                + "'" + originalClusterDefaultValue + "' for the original "
-                + " cluster and '" + simulatedClusterDefaultValue + "' for the"
-                + " simulated cluster.");
-    }
-    
-    destConf.setLong(jobValueKey, simulatedJobValue);
-  }
-  
-  // Checks if the scaling of original job's memory parameter value is 
-  // valid
-  @SuppressWarnings("deprecation")
-  private static boolean checkMemoryUpperLimits(String jobKey, String limitKey,  
-                                                Configuration conf, 
-                                                boolean convertLimitToMB) {
-    if (conf.get(limitKey) != null) {
-      long limit = conf.getLong(limitKey, JobConf.DISABLED_MEMORY_LIMIT);
-      // scale only if the max memory limit is set.
-      if (limit >= 0) {
-        if (convertLimitToMB) {
-          limit /= (1024 * 1024); //Converting to MB
-        }
-        
-        long scaledConfigValue = 
-               conf.getLong(jobKey, JobConf.DISABLED_MEMORY_LIMIT);
-        
-        // check now
-        if (scaledConfigValue > limit) {
-          throw new RuntimeException("Simulated job's configuration" 
-              + " parameter '" + jobKey + "' got scaled to a value '" 
-              + scaledConfigValue + "' which exceeds the upper limit of '" 
-              + limit + "' defined for the simulated cluster by the key '" 
-              + limitKey + "'. To disable High-Ram feature emulation, set '" 
-              + GRIDMIX_HIGHRAM_EMULATION_ENABLE + "' to 'false'.");
-        }
-        return true;
-      }
-    }
-    return false;
-  }
-  
-  // Check if the parameter scaling does not exceed the cluster limits.
-  @SuppressWarnings("deprecation")
-  private static void validateTaskMemoryLimits(Configuration conf, 
-                        String jobKey, String clusterMaxKey) {
-    if (!checkMemoryUpperLimits(jobKey, 
-        JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, conf, true)) {
-      checkMemoryUpperLimits(jobKey, clusterMaxKey, conf, false);
-    }
-  }
-
-  /**
-   * Sets the high ram job properties in the simulated job's configuration.
-   */
-  @SuppressWarnings("deprecation")
-  static void configureHighRamProperties(Configuration sourceConf, 
-                                         Configuration destConf) {
-    // set the memory per map task
-    scaleConfigParameter(sourceConf, destConf, 
-                         MRConfig.MAPMEMORY_MB, MRJobConfig.MAP_MEMORY_MB, 
-                         MRJobConfig.DEFAULT_MAP_MEMORY_MB);
-    
-    // validate and fail early
-    validateTaskMemoryLimits(destConf, MRJobConfig.MAP_MEMORY_MB, 
-                             JTConfig.JT_MAX_MAPMEMORY_MB);
-    
-    // set the memory per reduce task
-    scaleConfigParameter(sourceConf, destConf, 
-                         MRConfig.REDUCEMEMORY_MB, MRJobConfig.REDUCE_MEMORY_MB,
-                         MRJobConfig.DEFAULT_REDUCE_MEMORY_MB);
-    // validate and fail early
-    validateTaskMemoryLimits(destConf, MRJobConfig.REDUCE_MEMORY_MB, 
-                             JTConfig.JT_MAX_REDUCEMEMORY_MB);
-  }
-  
-  /**
-   * Indicates whether this {@link GridmixJob} supports compression emulation.
-   */
-  protected abstract boolean canEmulateCompression();
-  
-  protected GridmixJob(final Configuration conf, long submissionMillis, 
-                       final String name) throws IOException {
-    submissionTimeNanos = TimeUnit.NANOSECONDS.convert(
-        submissionMillis, TimeUnit.MILLISECONDS);
-    jobdesc = null;
-    outdir = null;
-    seq = -1;
-    ugi = UserGroupInformation.getCurrentUser();
-
-    try {
-      job = this.ugi.doAs(new PrivilegedExceptionAction<Job>() {
-        public Job run() throws IOException {
-          Job ret = Job.getInstance(conf, name);
-          ret.getConfiguration().setInt(GRIDMIX_JOB_SEQ, seq);
-          setJobQueue(ret, conf.get(GRIDMIX_DEFAULT_QUEUE));
-          return ret;
-        }
-      });
-    } catch (InterruptedException e) {
-      throw new IOException(e);
-    }
-  }
-
-  public UserGroupInformation getUgi() {
-    return ugi;
-  }
-
-  public String toString() {
-    return job.getJobName();
-  }
-
-  public long getDelay(TimeUnit unit) {
-    return unit.convert(submissionTimeNanos - System.nanoTime(),
-        TimeUnit.NANOSECONDS);
-  }
-
-  @Override
-  public int compareTo(Delayed other) {
-    if (this == other) {
-      return 0;
-    }
-    if (other instanceof GridmixJob) {
-      final long otherNanos = ((GridmixJob)other).submissionTimeNanos;
-      if (otherNanos < submissionTimeNanos) {
-        return 1;
-      }
-      if (otherNanos > submissionTimeNanos) {
-        return -1;
-      }
-      return id() - ((GridmixJob)other).id();
-    }
-    final long diff =
-      getDelay(TimeUnit.NANOSECONDS) - other.getDelay(TimeUnit.NANOSECONDS);
-    return 0 == diff ? 0 : (diff > 0 ? 1 : -1);
-  }
-
-  @Override
-  public boolean equals(Object other) {
-    if (this == other) {
-      return true;
-    }
-    // not possible unless job is cloned; all jobs should be unique
-    return other instanceof GridmixJob && id() == ((GridmixJob)other).id();
-  }
-
-  @Override
-  public int hashCode() {
-    return id();
-  }
-
-  int id() {
-    return seq;
-  }
-
-  Job getJob() {
-    return job;
-  }
-
-  JobStory getJobDesc() {
-    return jobdesc;
-  }
-
-  void setSubmitted() {
-    submitted = true;
-  }
-  
-  boolean isSubmitted() {
-    return submitted;
-  }
-  
-  static void pushDescription(int seq, List<InputSplit> splits) {
-    if (null != descCache.putIfAbsent(seq, splits)) {
-      throw new IllegalArgumentException("Description exists for id " + seq);
-    }
-  }
-
-  static List<InputSplit> pullDescription(JobContext jobCtxt) {
-    return pullDescription(GridmixJob.getJobSeqId(jobCtxt));
-  }
-  
-  static List<InputSplit> pullDescription(int seq) {
-    return descCache.remove(seq);
-  }
-
-  static void clearAll() {
-    descCache.clear();
-  }
-
-  void buildSplits(FilePool inputDir) throws IOException {
-
-  }
-  static int getJobSeqId(JobContext job) {
-    return job.getConfiguration().getInt(GRIDMIX_JOB_SEQ,-1);
-  }
-
-  public static class DraftPartitioner<V> extends Partitioner<GridmixKey,V> {
-    public int getPartition(GridmixKey key, V value, int numReduceTasks) {
-      return key.getPartition();
-    }
-  }
-
-  public static class SpecGroupingComparator
-      implements RawComparator<GridmixKey> {
-    private final DataInputBuffer di = new DataInputBuffer();
-    private final byte[] reset = di.getData();
-    @Override
-    public int compare(GridmixKey g1, GridmixKey g2) {
-      final byte t1 = g1.getType();
-      final byte t2 = g2.getType();
-      if (t1 == GridmixKey.REDUCE_SPEC ||
-          t2 == GridmixKey.REDUCE_SPEC) {
-        return t1 - t2;
-      }
-      assert t1 == GridmixKey.DATA;
-      assert t2 == GridmixKey.DATA;
-      return g1.compareTo(g2);
-    }
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      try {
-        final int ret;
-        di.reset(b1, s1, l1);
-        final int x1 = WritableUtils.readVInt(di);
-        di.reset(b2, s2, l2);
-        final int x2 = WritableUtils.readVInt(di);
-        final int t1 = b1[s1 + x1];
-        final int t2 = b2[s2 + x2];
-        if (t1 == GridmixKey.REDUCE_SPEC ||
-            t2 == GridmixKey.REDUCE_SPEC) {
-          ret = t1 - t2;
-        } else {
-          assert t1 == GridmixKey.DATA;
-          assert t2 == GridmixKey.DATA;
-          ret =
-            WritableComparator.compareBytes(b1, s1, x1, b2, s2, x2);
-        }
-        di.reset(reset, 0, 0);
-        return ret;
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-  }
-
-  static class RawBytesOutputFormat<K>
-      extends FileOutputFormat<K,GridmixRecord> {
-
-    @Override
-    public RecordWriter<K,GridmixRecord> getRecordWriter(
-        TaskAttemptContext job) throws IOException {
-
-      Path file = getDefaultWorkFile(job, "");
-      final DataOutputStream fileOut;
-
-      fileOut = 
-        new DataOutputStream(CompressionEmulationUtil
-            .getPossiblyCompressedOutputStream(file, job.getConfiguration()));
-
-      return new RecordWriter<K,GridmixRecord>() {
-        @Override
-        public void write(K ignored, GridmixRecord value)
-            throws IOException {
-          // Let the Gridmix record fill itself.
-          value.write(fileOut);
-        }
-        @Override
-        public void close(TaskAttemptContext ctxt) throws IOException {
-          fileOut.close();
-        }
-      };
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
deleted file mode 100644
index b8035386d16..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixJobSubmissionPolicy.java
+++ /dev/null
@@ -1,90 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
-
-import java.util.concurrent.CountDownLatch;
-import java.io.IOException;
-import org.apache.hadoop.util.StringUtils;
-
-enum GridmixJobSubmissionPolicy {
-
-  REPLAY("REPLAY", 320000) {
-    @Override
-    public JobFactory<ClusterStats> createJobFactory(
-      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
-      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
-      throws IOException {
-      return new ReplayJobFactory(
-        submitter, producer, scratchDir, conf, startFlag, userResolver);
-    }
-  },
-
-  STRESS("STRESS", 5000) {
-    @Override
-    public JobFactory<ClusterStats> createJobFactory(
-      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
-      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
-      throws IOException {
-      return new StressJobFactory(
-        submitter, producer, scratchDir, conf, startFlag, userResolver);
-    }
-  },
-
-  SERIAL("SERIAL", 0) {
-    @Override
-    public JobFactory<JobStats> createJobFactory(
-      JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
-      Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
-      throws IOException {
-      return new SerialJobFactory(
-        submitter, producer, scratchDir, conf, startFlag, userResolver);
-    }
-  };
-
-  public static final String JOB_SUBMISSION_POLICY =
-    "gridmix.job-submission.policy";
-
-  private final String name;
-  private final int pollingInterval;
-
-  GridmixJobSubmissionPolicy(String name, int pollingInterval) {
-    this.name = name;
-    this.pollingInterval = pollingInterval;
-  }
-
-  public abstract JobFactory createJobFactory(
-    JobSubmitter submitter, JobStoryProducer producer, Path scratchDir,
-    Configuration conf, CountDownLatch startFlag, UserResolver userResolver)
-    throws IOException;
-
-  public int getPollingInterval() {
-    return pollingInterval;
-  }
-
-  public static GridmixJobSubmissionPolicy getPolicy(
-    Configuration conf, GridmixJobSubmissionPolicy defaultPolicy) {
-    String policy = conf.get(JOB_SUBMISSION_POLICY, defaultPolicy.name());
-    return valueOf(StringUtils.toUpperCase(policy));
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
deleted file mode 100644
index e03e1b9d3cc..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixKey.java
+++ /dev/null
@@ -1,301 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-
-class GridmixKey extends GridmixRecord {
-  static final byte REDUCE_SPEC = 0;
-  static final byte DATA = 1;
-
-  static final int META_BYTES = 1;
-
-  private byte type;
-  private int partition; // NOT serialized
-  private Spec spec = new Spec();
-
-  GridmixKey() {
-    this(DATA, 1, 0L);
-  }
-  GridmixKey(byte type, int size, long seed) {
-    super(size, seed);
-    this.type = type;
-    // setting type may change pcnt random bytes
-    setSize(size);
-  }
-
-  @Override
-  public int getSize() {
-    switch (type) {
-      case REDUCE_SPEC:
-        return super.getSize() + spec.getSize() + META_BYTES;
-      case DATA:
-        return super.getSize() + META_BYTES;
-      default:
-        throw new IllegalStateException("Invalid type: " + type);
-    }
-  }
-
-  @Override
-  public void setSize(int size) {
-    switch (type) {
-      case REDUCE_SPEC:
-        super.setSize(size - (META_BYTES + spec.getSize()));
-        break;
-      case DATA:
-        super.setSize(size - META_BYTES);
-        break;
-      default:
-        throw new IllegalStateException("Invalid type: " + type);
-    }
-  }
-
-  /**
-   * Partition is not serialized.
-   */
-  public int getPartition() {
-    return partition;
-  }
-  public void setPartition(int partition) {
-    this.partition = partition;
-  }
-
-  public long getReduceInputRecords() {
-    assert REDUCE_SPEC == getType();
-    return spec.rec_in;
-  }
-  public void setReduceInputRecords(long rec_in) {
-    assert REDUCE_SPEC == getType();
-    final int origSize = getSize();
-    spec.rec_in = rec_in;
-    setSize(origSize);
-  }
-
-  public long getReduceOutputRecords() {
-    assert REDUCE_SPEC == getType();
-    return spec.rec_out;
-  }
-  public void setReduceOutputRecords(long rec_out) {
-    assert REDUCE_SPEC == getType();
-    final int origSize = getSize();
-    spec.rec_out = rec_out;
-    setSize(origSize);
-  }
-
-  public long getReduceOutputBytes() {
-    assert REDUCE_SPEC == getType();
-    return spec.bytes_out;
-  };
-  public void setReduceOutputBytes(long b_out) {
-    assert REDUCE_SPEC == getType();
-    final int origSize = getSize();
-    spec.bytes_out = b_out;
-    setSize(origSize);
-  }
-
-  /**
-   * Get the {@link ResourceUsageMetrics} stored in the key.
-   */
-  public ResourceUsageMetrics getReduceResourceUsageMetrics() {
-    assert REDUCE_SPEC == getType();
-    return spec.metrics;
-  }
-  
-  /**
-   * Store the {@link ResourceUsageMetrics} in the key.
-   */
-  public void setReduceResourceUsageMetrics(ResourceUsageMetrics metrics) {
-    assert REDUCE_SPEC == getType();
-    spec.setResourceUsageSpecification(metrics);
-  }
-  
-  public byte getType() {
-    return type;
-  }
-  public void setType(byte type) throws IOException {
-    final int origSize = getSize();
-    switch (type) {
-      case REDUCE_SPEC:
-      case DATA:
-        this.type = type;
-        break;
-      default:
-        throw new IOException("Invalid type: " + type);
-    }
-    setSize(origSize);
-  }
-
-  public void setSpec(Spec spec) {
-    assert REDUCE_SPEC == getType();
-    final int origSize = getSize();
-    this.spec.set(spec);
-    setSize(origSize);
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    super.readFields(in);
-    setType(in.readByte());
-    if (REDUCE_SPEC == getType()) {
-      spec.readFields(in);
-    }
-  }
-  @Override
-  public void write(DataOutput out) throws IOException {
-    super.write(out);
-    final byte t = getType();
-    out.writeByte(t);
-    if (REDUCE_SPEC == t) {
-      spec.write(out);
-    }
-  }
-  int fixedBytes() {
-    return super.fixedBytes() +
-      (REDUCE_SPEC == getType() ? spec.getSize() : 0) + META_BYTES;
-  }
-  @Override
-  public int compareTo(GridmixRecord other) {
-    final GridmixKey o = (GridmixKey) other;
-    final byte t1 = getType();
-    final byte t2 = o.getType();
-    if (t1 != t2) {
-      return t1 - t2;
-    }
-    return super.compareTo(other);
-  }
-
-  /**
-   * Note that while the spec is not explicitly included, changing the spec
-   * may change its size, which will affect equality.
-   */
-  @Override
-  public boolean equals(Object other) {
-    if (this == other) {
-      return true;
-    }
-    if (other != null && other.getClass() == getClass()) {
-      final GridmixKey o = ((GridmixKey)other);
-      return getType() == o.getType() && super.equals(o);
-    }
-    return false;
-  }
-
-  @Override
-  public int hashCode() {
-    return super.hashCode() ^ getType();
-  }
-
-  public static class Spec implements Writable {
-    long rec_in;
-    long rec_out;
-    long bytes_out;
-    private ResourceUsageMetrics metrics = null;
-    private int sizeOfResourceUsageMetrics = 0;
-    public Spec() { }
-
-    public void set(Spec other) {
-      rec_in = other.rec_in;
-      bytes_out = other.bytes_out;
-      rec_out = other.rec_out;
-      setResourceUsageSpecification(other.metrics);
-    }
-
-    /**
-     * Sets the {@link ResourceUsageMetrics} for this {@link Spec}.
-     */
-    public void setResourceUsageSpecification(ResourceUsageMetrics metrics) {
-      this.metrics = metrics;
-      if (metrics != null) {
-        this.sizeOfResourceUsageMetrics = metrics.size();
-      } else {
-        this.sizeOfResourceUsageMetrics = 0;
-      }
-    }
-    
-    public int getSize() {
-      return WritableUtils.getVIntSize(rec_in) +
-             WritableUtils.getVIntSize(rec_out) +
-             WritableUtils.getVIntSize(bytes_out) +
-             WritableUtils.getVIntSize(sizeOfResourceUsageMetrics) +
-             sizeOfResourceUsageMetrics;
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      rec_in = WritableUtils.readVLong(in);
-      rec_out = WritableUtils.readVLong(in);
-      bytes_out = WritableUtils.readVLong(in);
-      sizeOfResourceUsageMetrics =  WritableUtils.readVInt(in);
-      if (sizeOfResourceUsageMetrics > 0) {
-        metrics = new ResourceUsageMetrics();
-        metrics.readFields(in);
-      }
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      WritableUtils.writeVLong(out, rec_in);
-      WritableUtils.writeVLong(out, rec_out);
-      WritableUtils.writeVLong(out, bytes_out);
-      WritableUtils.writeVInt(out, sizeOfResourceUsageMetrics);
-      if (sizeOfResourceUsageMetrics > 0) {
-        metrics.write(out);
-      }
-    }
-  }
-
-  public static class Comparator extends GridmixRecord.Comparator {
-
-    private final DataInputBuffer di = new DataInputBuffer();
-    private final byte[] reset = di.getData();
-
-    public Comparator() {
-      super(GridmixKey.class);
-    }
-
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      try {
-        di.reset(b1, s1, l1);
-        final int x1 = WritableUtils.readVInt(di);
-        di.reset(b2, s2, l2);
-        final int x2 = WritableUtils.readVInt(di);
-        final int ret = (b1[s1 + x1] != b2[s2 + x2])
-          ? b1[s1 + x1] - b2[s2 + x2]
-          : super.compare(b1, s1, x1, b2, s2, x2);
-        di.reset(reset, 0, 0);
-        return ret;
-      } catch (IOException e) {
-        throw new RuntimeException(e);
-      }
-    }
-
-    static {
-      WritableComparator.define(GridmixKey.class, new Comparator());
-    }
-  }
-}
-
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
deleted file mode 100644
index afb95cab87c..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixRecord.java
+++ /dev/null
@@ -1,272 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.EOFException;
-import java.io.IOException;
-import java.nio.charset.StandardCharsets;
-import java.util.Arrays;
-
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.WritableComparable;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-
-class GridmixRecord implements WritableComparable<GridmixRecord> {
-
-  private static final int FIXED_BYTES = 1;
-  private int size = -1;
-  private long seed;
-  private final DataInputBuffer dib =
-    new DataInputBuffer();
-  private final DataOutputBuffer dob =
-    new DataOutputBuffer(Long.SIZE / Byte.SIZE);
-  private byte[] literal = dob.getData();
-  private boolean compressible = false;
-  private float compressionRatio = 
-    CompressionEmulationUtil.DEFAULT_COMPRESSION_RATIO;
-  private RandomTextDataGenerator rtg = null;
-
-  GridmixRecord() {
-    this(1, 0L);
-  }
-
-  GridmixRecord(int size, long seed) {
-    this.seed = seed;
-    setSizeInternal(size);
-  }
-
-  public int getSize() {
-    return size;
-  }
-
-  public void setSize(int size) {
-    setSizeInternal(size);
-  }
-
-  void setCompressibility(boolean compressible, float ratio) {
-    this.compressible = compressible;
-    this.compressionRatio = ratio;
-    // Initialize the RandomTextDataGenerator once for every GridMix record
-    // Note that RandomTextDataGenerator is needed only when the GridMix record
-    // is configured to generate compressible text data.
-    if (compressible) {
-      rtg = 
-        CompressionEmulationUtil.getRandomTextDataGenerator(ratio, 
-                                   RandomTextDataGenerator.DEFAULT_SEED);
-    }
-  }
-  
-  private void setSizeInternal(int size) {
-    this.size = Math.max(1, size);
-    try {
-      seed = maskSeed(seed, this.size);
-      dob.reset();
-      dob.writeLong(seed);
-    } catch (IOException e) {
-      throw new RuntimeException(e);
-    }
-  }
-
-  public final void setSeed(long seed) {
-    this.seed = seed;
-  }
-
-  /** Marsaglia, 2003. */
-  long nextRand(long x) {
-    x ^= (x << 13);
-    x ^= (x >>> 7);
-    return (x ^= (x << 17));
-  }
-
-  /**
-   * Generate random text data that can be compressed. If the record is marked
-   * compressible (via {@link FileOutputFormat#COMPRESS}), only then the 
-   * random data will be text data else 
-   * {@link GridmixRecord#writeRandom(DataOutput, int)} will be invoked.
-   */
-  private void writeRandomText(DataOutput out, final int size) 
-  throws IOException {
-    long tmp = seed;
-    out.writeLong(tmp);
-    int i = size - (Long.SIZE / Byte.SIZE);
-    //TODO Should we use long for size. What if the data is more than 4G?
-    
-    String randomWord = rtg.getRandomWord();
-    byte[] bytes = randomWord.getBytes(StandardCharsets.UTF_8);
-    long randomWordSize = bytes.length;
-    while (i >= randomWordSize) {
-      out.write(bytes);
-      i -= randomWordSize;
-      
-      // get the next random word
-      randomWord = rtg.getRandomWord();
-      bytes = randomWord.getBytes(StandardCharsets.UTF_8);
-      // determine the random word size
-      randomWordSize = bytes.length;
-    }
-    
-    // pad the remaining bytes
-    if (i > 0) {
-      out.write(bytes, 0, i);
-    }
-  }
-  
-  public void writeRandom(DataOutput out, final int size) throws IOException {
-    long tmp = seed;
-    out.writeLong(tmp);
-    int i = size - (Long.SIZE / Byte.SIZE);
-    while (i > Long.SIZE / Byte.SIZE - 1) {
-      tmp = nextRand(tmp);
-      out.writeLong(tmp);
-      i -= Long.SIZE / Byte.SIZE;
-    }
-    for (tmp = nextRand(tmp); i > 0; --i) {
-      out.writeByte((int)(tmp & 0xFF));
-      tmp >>>= Byte.SIZE;
-    }
-  }
-
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    size = WritableUtils.readVInt(in);
-    int payload = size - WritableUtils.getVIntSize(size);
-    if (payload > Long.SIZE / Byte.SIZE) {
-      seed = in.readLong();
-      payload -= Long.SIZE / Byte.SIZE;
-    } else {
-      Arrays.fill(literal, (byte)0);
-      in.readFully(literal, 0, payload);
-      dib.reset(literal, 0, literal.length);
-      seed = dib.readLong();
-      payload = 0;
-    }
-    final int vBytes = in.skipBytes(payload);
-    if (vBytes != payload) {
-      throw new EOFException("Expected " + payload + ", read " + vBytes);
-    }
-  }
-
-  @Override
-  public void write(DataOutput out) throws IOException {
-    // data bytes including vint encoding
-    WritableUtils.writeVInt(out, size);
-    final int payload = size - WritableUtils.getVIntSize(size);
-    if (payload > Long.SIZE / Byte.SIZE) {
-      if (compressible) {
-        writeRandomText(out, payload);
-      } else {
-        writeRandom(out, payload);
-      }
-    } else if (payload > 0) {
-      //TODO What is compressible is turned on? LOG is a bad idea!
-      out.write(literal, 0, payload);
-    }
-  }
-
-  @Override
-  public int compareTo(GridmixRecord other) {
-    return compareSeed(other.seed,
-        Math.max(0, other.getSize() - other.fixedBytes()));
-  }
-
-  int fixedBytes() {
-    // min vint size
-    return FIXED_BYTES;
-  }
-
-  private static long maskSeed(long sd, int sz) {
-    // Don't use fixedBytes here; subclasses will set intended random len
-    if (sz <= FIXED_BYTES) {
-      sd = 0L;
-    } else if (sz < Long.SIZE / Byte.SIZE + FIXED_BYTES) {
-      final int tmp = sz - FIXED_BYTES;
-      final long mask = (1L << (Byte.SIZE * tmp)) - 1;
-      sd &= mask << (Byte.SIZE * (Long.SIZE / Byte.SIZE - tmp));
-    }
-    return sd;
-  }
-
-  int compareSeed(long jSeed, int jSize) {
-    final int iSize = Math.max(0, getSize() - fixedBytes());
-    final int seedLen = Math.min(iSize, jSize) + FIXED_BYTES;
-    jSeed = maskSeed(jSeed, seedLen);
-    long iSeed = maskSeed(seed, seedLen);
-    final int cmplen = Math.min(iSize, jSize);
-    for (int i = 0; i < cmplen; i += Byte.SIZE) {
-      final int k = cmplen - i;
-      for (long j = Long.SIZE - Byte.SIZE;
-          j >= Math.max(0, Long.SIZE / Byte.SIZE - k) * Byte.SIZE;
-          j -= Byte.SIZE) {
-        final int xi = (int)((iSeed >>> j) & 0xFFL);
-        final int xj = (int)((jSeed >>> j) & 0xFFL);
-        if (xi != xj) {
-          return xi - xj;
-        }
-      }
-      iSeed = nextRand(iSeed);
-      jSeed = nextRand(jSeed);
-    }
-    return iSize - jSize;
-  }
-
-  @Override
-  public boolean equals(Object other) {
-    if (this == other) {
-      return true;
-    }
-    if (other != null && other.getClass() == getClass()) {
-      final GridmixRecord o = ((GridmixRecord)other);
-      return getSize() == o.getSize() && seed == o.seed;
-    }
-    return false;
-  }
-
-  @Override
-  public int hashCode() {
-    return (int)(seed * getSize());
-  }
-
-  public static class Comparator extends WritableComparator {
-
-    public Comparator() {
-      super(GridmixRecord.class);
-    }
-
-    public Comparator(Class<? extends WritableComparable<?>> sub) {
-      super(sub);
-    }
-
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      int n1 = WritableUtils.decodeVIntSize(b1[s1]);
-      int n2 = WritableUtils.decodeVIntSize(b2[s2]);
-      n1 -= WritableUtils.getVIntSize(n1);
-      n2 -= WritableUtils.getVIntSize(n2);
-      return compareBytes(b1, s1+n1, l1-n1, b2, s2+n2, l2-n2);
-    }
-
-    static {
-      WritableComparator.define(GridmixRecord.class, new Comparator());
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
deleted file mode 100644
index b611c9dfd00..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/GridmixSplit.java
+++ /dev/null
@@ -1,148 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-class GridmixSplit extends CombineFileSplit {
-  private int id;
-  private int nSpec;
-  private int maps;
-  private int reduces;
-  private long inputRecords;
-  private long outputBytes;
-  private long outputRecords;
-  private long maxMemory;
-  private double[] reduceBytes = new double[0];
-  private double[] reduceRecords = new double[0];
-
-  // Spec for reduces id mod this
-  private long[] reduceOutputBytes = new long[0];
-  private long[] reduceOutputRecords = new long[0];
-
-  GridmixSplit() {
-    super();
-  }
-
-  public GridmixSplit(CombineFileSplit cfsplit, int maps, int id,
-      long inputBytes, long inputRecords, long outputBytes,
-      long outputRecords, double[] reduceBytes, double[] reduceRecords,
-      long[] reduceOutputBytes, long[] reduceOutputRecords)
-      throws IOException {
-    super(cfsplit);
-    this.id = id;
-    this.maps = maps;
-    reduces = reduceBytes.length;
-    this.inputRecords = inputRecords;
-    this.outputBytes = outputBytes;
-    this.outputRecords = outputRecords;
-    this.reduceBytes = reduceBytes;
-    this.reduceRecords = reduceRecords;
-    nSpec = reduceOutputBytes.length;
-    this.reduceOutputBytes = reduceOutputBytes;
-    this.reduceOutputRecords = reduceOutputRecords;
-  }
-  public int getId() {
-    return id;
-  }
-  public int getMapCount() {
-    return maps;
-  }
-  public long getInputRecords() {
-    return inputRecords;
-  }
-  public long[] getOutputBytes() {
-    if (0 == reduces) {
-      return new long[] { outputBytes };
-    }
-    final long[] ret = new long[reduces];
-    for (int i = 0; i < reduces; ++i) {
-      ret[i] = Math.round(outputBytes * reduceBytes[i]);
-    }
-    return ret;
-  }
-  public long[] getOutputRecords() {
-    if (0 == reduces) {
-      return new long[] { outputRecords };
-    }
-    final long[] ret = new long[reduces];
-    for (int i = 0; i < reduces; ++i) {
-      ret[i] = Math.round(outputRecords * reduceRecords[i]);
-    }
-    return ret;
-  }
-  public long getReduceBytes(int i) {
-    return reduceOutputBytes[i];
-  }
-  public long getReduceRecords(int i) {
-    return reduceOutputRecords[i];
-  }
-  @Override
-  public void write(DataOutput out) throws IOException {
-    super.write(out);
-    WritableUtils.writeVInt(out, id);
-    WritableUtils.writeVInt(out, maps);
-    WritableUtils.writeVLong(out, inputRecords);
-    WritableUtils.writeVLong(out, outputBytes);
-    WritableUtils.writeVLong(out, outputRecords);
-    WritableUtils.writeVLong(out, maxMemory);
-    WritableUtils.writeVInt(out, reduces);
-    for (int i = 0; i < reduces; ++i) {
-      out.writeDouble(reduceBytes[i]);
-      out.writeDouble(reduceRecords[i]);
-    }
-    WritableUtils.writeVInt(out, nSpec);
-    for (int i = 0; i < nSpec; ++i) {
-      WritableUtils.writeVLong(out, reduceOutputBytes[i]);
-      WritableUtils.writeVLong(out, reduceOutputRecords[i]);
-    }
-  }
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    super.readFields(in);
-    id = WritableUtils.readVInt(in);
-    maps = WritableUtils.readVInt(in);
-    inputRecords = WritableUtils.readVLong(in);
-    outputBytes = WritableUtils.readVLong(in);
-    outputRecords = WritableUtils.readVLong(in);
-    maxMemory = WritableUtils.readVLong(in);
-    reduces = WritableUtils.readVInt(in);
-    if (reduceBytes.length < reduces) {
-      reduceBytes = new double[reduces];
-      reduceRecords = new double[reduces];
-    }
-    for (int i = 0; i < reduces; ++i) {
-      reduceBytes[i] = in.readDouble();
-      reduceRecords[i] = in.readDouble();
-    }
-    nSpec = WritableUtils.readVInt(in);
-    if (reduceOutputBytes.length < nSpec) {
-      reduceOutputBytes = new long[nSpec];
-      reduceOutputRecords = new long[nSpec];
-    }
-    for (int i = 0; i < nSpec; ++i) {
-      reduceOutputBytes[i] = WritableUtils.readVLong(in);
-      reduceOutputRecords[i] = WritableUtils.readVLong(in);
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
deleted file mode 100644
index 4867fa4b8c1..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/InputStriper.java
+++ /dev/null
@@ -1,137 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.Comparator;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map.Entry;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.BlockLocation;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.CompressionCodecFactory;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Given a {@link #FilePool}, obtain a set of files capable of satisfying
- * a full set of splits, then iterate over each source to fill the request.
- */
-class InputStriper {
-  public static final Logger LOG = LoggerFactory.getLogger(InputStriper.class);
-  int idx;
-  long currentStart;
-  FileStatus current;
-  final List<FileStatus> files = new ArrayList<FileStatus>();
-  final Configuration conf = new Configuration();
-
-  /**
-   * @param inputDir Pool from which files are requested.
-   * @param mapBytes Sum of all expected split requests.
-   */
-  InputStriper(FilePool inputDir, long mapBytes)
-      throws IOException {
-    final long inputBytes = inputDir.getInputFiles(mapBytes, files);
-    if (mapBytes > inputBytes) {
-      LOG.warn("Using " + inputBytes + "/" + mapBytes + " bytes");
-    }
-    if (files.isEmpty() && mapBytes > 0) {
-      throw new IOException("Failed to satisfy request for " + mapBytes);
-    }
-    current = files.isEmpty() ? null : files.get(0);
-  }
-
-  /**
-   * @param inputDir Pool used to resolve block locations.
-   * @param bytes Target byte count
-   * @param nLocs Number of block locations per split.
-   * @return A set of files satisfying the byte count, with locations weighted
-   *         to the dominating proportion of input bytes.
-   */
-  CombineFileSplit splitFor(FilePool inputDir, long bytes, int nLocs)
-      throws IOException {
-    final ArrayList<Path> paths = new ArrayList<Path>();
-    final ArrayList<Long> start = new ArrayList<Long>();
-    final ArrayList<Long> length = new ArrayList<Long>();
-    final HashMap<String,Double> sb = new HashMap<String,Double>();
-    do {
-      paths.add(current.getPath());
-      start.add(currentStart);
-      final long fromFile = Math.min(bytes, current.getLen() - currentStart);
-      length.add(fromFile);
-      for (BlockLocation loc :
-          inputDir.locationsFor(current, currentStart, fromFile)) {
-        final double tedium = loc.getLength() / (1.0 * bytes);
-        for (String l : loc.getHosts()) {
-          Double j = sb.get(l);
-          if (null == j) {
-            sb.put(l, tedium);
-          } else {
-            sb.put(l, j.doubleValue() + tedium);
-          }
-        }
-      }
-      currentStart += fromFile;
-      bytes -= fromFile;
-      // Switch to a new file if
-      //  - the current file is uncompressed and completely used
-      //  - the current file is compressed
-      
-      CompressionCodecFactory compressionCodecs = 
-        new CompressionCodecFactory(conf);
-      CompressionCodec codec = compressionCodecs.getCodec(current.getPath());
-      if (current.getLen() - currentStart == 0
-          || codec != null) {
-        current = files.get(++idx % files.size());
-        currentStart = 0;
-      }
-    } while (bytes > 0);
-    final ArrayList<Entry<String,Double>> sort =
-      new ArrayList<Entry<String,Double>>(sb.entrySet());
-    Collections.sort(sort, hostRank);
-    final String[] hosts = new String[Math.min(nLocs, sort.size())];
-    for (int i = 0; i < nLocs && i < sort.size(); ++i) {
-      hosts[i] = sort.get(i).getKey();
-    }
-    return new CombineFileSplit(paths.toArray(new Path[0]),
-        toLongArray(start), toLongArray(length), hosts);
-  }
-
-  private long[] toLongArray(final ArrayList<Long> sigh) {
-    final long[] ret = new long[sigh.size()];
-    for (int i = 0; i < ret.length; ++i) {
-      ret[i] = sigh.get(i);
-    }
-    return ret;
-  }
-
-  static final Comparator<Entry<String,Double>> hostRank =
-    new Comparator<Entry<String,Double>>() {
-      public int compare(Entry<String,Double> a, Entry<String,Double> b) {
-        return Double.compare(a.getValue(), b.getValue());
-      }
-    };
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
deleted file mode 100644
index a6fc6c69546..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/IntermediateRecordFactory.java
+++ /dev/null
@@ -1,110 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * Factory passing reduce specification as its last record.
- */
-class IntermediateRecordFactory extends RecordFactory {
-
-  private final GridmixKey.Spec spec;
-  private final RecordFactory factory;
-  private final int partition;
-  private final long targetRecords;
-  private boolean done = false;
-  private long accRecords = 0L;
-
-  /**
-   * @param targetBytes Expected byte count.
-   * @param targetRecords Expected record count; will emit spec records after
-   *                      this boundary is passed.
-   * @param partition Reduce to which records are emitted.
-   * @param spec Specification to emit.
-   * @param conf Unused.
-   */
-  public IntermediateRecordFactory(long targetBytes, long targetRecords,
-      int partition, GridmixKey.Spec spec, Configuration conf) {
-    this(new AvgRecordFactory(targetBytes, targetRecords, conf), partition,
-        targetRecords, spec, conf);
-  }
-
-  /**
-   * @param factory Factory from which byte/record counts are obtained.
-   * @param partition Reduce to which records are emitted.
-   * @param targetRecords Expected record count; will emit spec records after
-   *                      this boundary is passed.
-   * @param spec Specification to emit.
-   * @param conf Unused.
-   */
-  public IntermediateRecordFactory(RecordFactory factory, int partition,
-      long targetRecords, GridmixKey.Spec spec, Configuration conf) {
-    this.spec = spec;
-    this.factory = factory;
-    this.partition = partition;
-    this.targetRecords = targetRecords;
-  }
-
-  @Override
-  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
-    assert key != null;
-    final boolean rslt = factory.next(key, val);
-    ++accRecords;
-    if (rslt) {
-      if (accRecords < targetRecords) {
-        key.setType(GridmixKey.DATA);
-      } else {
-        final int orig = key.getSize();
-        key.setType(GridmixKey.REDUCE_SPEC);
-        spec.rec_in = accRecords;
-        key.setSpec(spec);
-        val.setSize(val.getSize() - (key.getSize() - orig));
-        // reset counters
-        accRecords = 0L;
-        spec.bytes_out = 0L;
-        spec.rec_out = 0L;
-        done = true;
-      }
-    } else if (!done) {
-      // ensure spec emitted
-      key.setType(GridmixKey.REDUCE_SPEC);
-      key.setPartition(partition);
-      key.setSize(0);
-      val.setSize(0);
-      spec.rec_in = 0L;
-      key.setSpec(spec);
-      done = true;
-      return true;
-    }
-    key.setPartition(partition);
-    return rslt;
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    return factory.getProgress();
-  }
-
-  @Override
-  public void close() throws IOException {
-    factory.close();
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobCreator.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
deleted file mode 100644
index c109e3fa3e2..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobCreator.java
+++ /dev/null
@@ -1,135 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.regex.Matcher;
-import java.util.regex.Pattern;
-
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-public enum JobCreator {
-
-  LOADJOB {
-    @Override
-    public GridmixJob createGridmixJob(
-      Configuration gridmixConf, long submissionMillis, JobStory jobdesc,
-      Path outRoot, UserGroupInformation ugi, int seq) throws IOException {
-
-      // Build configuration for this simulated job
-      Configuration conf = new Configuration(gridmixConf);
-      dce.configureDistCacheFiles(conf, jobdesc.getJobConf());
-      return new LoadJob(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
-    }
-
-    @Override
-    public boolean canEmulateDistCacheLoad() {
-      return true;
-    }
-  },
-
-  SLEEPJOB {
-    private String[] hosts;
-      
-    @Override
-    public GridmixJob createGridmixJob(
-      Configuration conf, long submissionMillis, JobStory jobdesc, Path outRoot,
-      UserGroupInformation ugi, int seq) throws IOException {
-      int numLocations = conf.getInt(SLEEPJOB_RANDOM_LOCATIONS, 0);
-      if (numLocations < 0) numLocations = 0;
-      if (hosts == null) {
-        final JobClient client = new JobClient(new JobConf(conf));
-        ClusterStatus stat = client.getClusterStatus(true);
-        final int nTrackers = stat.getTaskTrackers();
-        final ArrayList<String> hostList = new ArrayList<String>(nTrackers);
-        final Pattern trackerPattern = Pattern.compile("tracker_([^:]*):.*");
-        final Matcher m = trackerPattern.matcher("");
-        for (String tracker : stat.getActiveTrackerNames()) {
-          m.reset(tracker);
-          if (!m.find()) {
-            continue;
-          }
-          final String name = m.group(1);
-          hostList.add(name);
-        }
-        hosts = hostList.toArray(new String[hostList.size()]);
-      }
-      return new SleepJob(conf, submissionMillis, jobdesc, outRoot, ugi, seq,
-          numLocations, hosts);
-    }
-
-    @Override
-    public boolean canEmulateDistCacheLoad() {
-      return false;
-    }
-  };
-
-  public static final String GRIDMIX_JOB_TYPE = "gridmix.job.type";
-  public static final String SLEEPJOB_RANDOM_LOCATIONS = 
-    "gridmix.sleep.fake-locations";
-
-  /**
-   * Create Gridmix simulated job.
-   * @param conf configuration of simulated job
-   * @param submissionMillis At what time submission of this simulated job be
-   *                         done
-   * @param jobdesc JobStory obtained from trace
-   * @param outRoot gridmix output directory
-   * @param ugi UGI of job submitter of this simulated job
-   * @param seq job sequence number
-   * @return the created simulated job
-   * @throws IOException
-   */
-  public abstract GridmixJob createGridmixJob(
-    final Configuration conf, long submissionMillis, final JobStory jobdesc,
-    Path outRoot, UserGroupInformation ugi, final int seq) throws IOException;
-
-  public static JobCreator getPolicy(
-    Configuration conf, JobCreator defaultPolicy) {
-    return conf.getEnum(GRIDMIX_JOB_TYPE, defaultPolicy);
-  }
-
-  /**
-   * @return true if gridmix simulated jobs of this job type can emulate
-   *         distributed cache load
-   */
-  abstract boolean canEmulateDistCacheLoad();
-
-  DistributedCacheEmulator dce;
-  /**
-   * This method is to be called before calling any other method in JobCreator
-   * except canEmulateDistCacheLoad(), especially if canEmulateDistCacheLoad()
-   * returns true for that job type.
-   * @param e Distributed Cache Emulator
-   */
-  void setDistCacheEmulator(DistributedCacheEmulator e) {
-    this.dce = e;
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
deleted file mode 100644
index 4536e513361..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobFactory.java
+++ /dev/null
@@ -1,276 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.commons.lang3.StringUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants.Values;
-import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.TaskInfo;
-import org.apache.hadoop.tools.rumen.ZombieJobProducer;
-import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants;
-
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.List;
-import java.util.ArrayList;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.locks.ReentrantLock;
-import java.util.concurrent.atomic.AtomicInteger;
-
-
-/**
- * Component reading job traces generated by Rumen. Each job in the trace is
- * assigned a sequence number and given a submission time relative to the
- * job that preceded it. Jobs are enqueued in the JobSubmitter provided at
- * construction.
- * @see org.apache.hadoop.tools.rumen.HadoopLogsAnalyzer
- */
-abstract class JobFactory<T> implements Gridmix.Component<Void>,StatListener<T> {
-
-  public static final Logger LOG = LoggerFactory.getLogger(JobFactory.class);
-
-  protected final Path scratch;
-  protected final float rateFactor;
-  protected final Configuration conf;
-  protected final Thread rThread;
-  protected final AtomicInteger sequence;
-  protected final JobSubmitter submitter;
-  protected final CountDownLatch startFlag;
-  protected final UserResolver userResolver;
-  protected final JobCreator jobCreator;
-  protected volatile IOException error = null;
-  protected final JobStoryProducer jobProducer;
-  protected final ReentrantLock lock = new ReentrantLock(true);
-  protected int numJobsInTrace = 0;
-
-  /**
-   * Creating a new instance does not start the thread.
-   * @param submitter Component to which deserialized jobs are passed
-   * @param jobTrace Stream of job traces with which to construct a
-   *                 {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
-   * @param scratch Directory into which to write output from simulated jobs
-   * @param conf Config passed to all jobs to be submitted
-   * @param startFlag Latch released from main to start pipeline
-   * @throws java.io.IOException
-   */
-  public JobFactory(JobSubmitter submitter, InputStream jobTrace,
-      Path scratch, Configuration conf, CountDownLatch startFlag,
-      UserResolver userResolver) throws IOException {
-    this(submitter, new ZombieJobProducer(jobTrace, null), scratch, conf,
-        startFlag, userResolver);
-  }
-
-  /**
-   * Constructor permitting JobStoryProducer to be mocked.
-   * @param submitter Component to which deserialized jobs are passed
-   * @param jobProducer Producer generating JobStory objects.
-   * @param scratch Directory into which to write output from simulated jobs
-   * @param conf Config passed to all jobs to be submitted
-   * @param startFlag Latch released from main to start pipeline
-   */
-  protected JobFactory(JobSubmitter submitter, JobStoryProducer jobProducer,
-      Path scratch, Configuration conf, CountDownLatch startFlag,
-      UserResolver userResolver) {
-    sequence = new AtomicInteger(0);
-    this.scratch = scratch;
-    this.rateFactor = conf.getFloat(Gridmix.GRIDMIX_SUB_MUL, 1.0f);
-    this.jobProducer = jobProducer;
-    this.conf = new Configuration(conf);
-    this.submitter = submitter;
-    this.startFlag = startFlag;
-    this.rThread = createReaderThread();
-    if(LOG.isDebugEnabled()) {
-      LOG.debug(" The submission thread name is " + rThread.getName());
-    }
-    this.userResolver = userResolver;
-    this.jobCreator = JobCreator.getPolicy(conf, JobCreator.LOADJOB);
-  }
-
-  static class MinTaskInfo extends TaskInfo {
-    public MinTaskInfo(TaskInfo info) {
-      super(info.getInputBytes(), info.getInputRecords(),
-            info.getOutputBytes(), info.getOutputRecords(),
-            info.getTaskMemory(), info.getResourceUsageMetrics());
-    }
-    public long getInputBytes() {
-      return Math.max(0, super.getInputBytes());
-    }
-    public int getInputRecords() {
-      return Math.max(0, super.getInputRecords());
-    }
-    public long getOutputBytes() {
-      return Math.max(0, super.getOutputBytes());
-    }
-    public int getOutputRecords() {
-      return Math.max(0, super.getOutputRecords());
-    }
-    public long getTaskMemory() {
-      return Math.max(0, super.getTaskMemory());
-    }
-  }
-
-  protected static class FilterJobStory implements JobStory {
-
-    protected final JobStory job;
-
-    public FilterJobStory(JobStory job) {
-      this.job = job;
-    }
-    public JobConf getJobConf() { return job.getJobConf(); }
-    public String getName() { return job.getName(); }
-    public JobID getJobID() { return job.getJobID(); }
-    public String getUser() { return job.getUser(); }
-    public long getSubmissionTime() { return job.getSubmissionTime(); }
-    public InputSplit[] getInputSplits() { return job.getInputSplits(); }
-    public int getNumberMaps() { return job.getNumberMaps(); }
-    public int getNumberReduces() { return job.getNumberReduces(); }
-    public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
-      return job.getTaskInfo(taskType, taskNumber);
-    }
-    public TaskAttemptInfo getTaskAttemptInfo(TaskType taskType, int taskNumber,
-        int taskAttemptNumber) {
-      return job.getTaskAttemptInfo(taskType, taskNumber, taskAttemptNumber);
-    }
-    public TaskAttemptInfo getMapTaskAttemptInfoAdjusted(
-        int taskNumber, int taskAttemptNumber, int locality) {
-      return job.getMapTaskAttemptInfoAdjusted(
-          taskNumber, taskAttemptNumber, locality);
-    }
-    public Values getOutcome() {
-      return job.getOutcome();
-    }
-    public String getQueueName() {
-      return job.getQueueName();
-    }
-  }
-
-  protected abstract Thread createReaderThread() ; 
-
-  // gets the next job from the trace and does some bookkeeping for the same
-  private JobStory getNextJobFromTrace() throws IOException {
-    JobStory story = jobProducer.getNextJob();
-    if (story != null) {
-      ++numJobsInTrace;
-    }
-    return story;
-  }
-  
-  protected JobStory getNextJobFiltered() throws IOException {
-    JobStory job = getNextJobFromTrace();
-    // filter out the following jobs
-    //    - unsuccessful jobs
-    //    - jobs with missing submit-time
-    //    - reduce only jobs
-    // These jobs are not yet supported in Gridmix
-    while (job != null &&
-      (job.getOutcome() != Pre21JobHistoryConstants.Values.SUCCESS ||
-        job.getSubmissionTime() < 0 || job.getNumberMaps() == 0)) {
-      if (LOG.isDebugEnabled()) {
-        List<String> reason = new ArrayList<String>();
-        if (job.getOutcome() != Pre21JobHistoryConstants.Values.SUCCESS) {
-          reason.add("STATE (" + job.getOutcome().name() + ")");
-        }
-        if (job.getSubmissionTime() < 0) {
-          reason.add("SUBMISSION-TIME (" + job.getSubmissionTime() + ")");
-        }
-        if (job.getNumberMaps() == 0) {
-          reason.add("ZERO-MAPS-JOB");
-        }
-        
-        // TODO This should never happen. Probably we missed something!
-        if (reason.size() == 0) {
-          reason.add("N/A");
-        }
-        
-        LOG.debug("Ignoring job " + job.getJobID() + " from the input trace."
-                  + " Reason: " + StringUtils.join(reason, ","));
-      }
-      job = getNextJobFromTrace();
-    }
-    return null == job ? null : new FilterJobStory(job) {
-      @Override
-      public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
-        TaskInfo info = this.job.getTaskInfo(taskType, taskNumber);
-        if (info != null) {
-          info = new MinTaskInfo(info);
-        } else {
-          info = new MinTaskInfo(new TaskInfo(0, 0, 0, 0, 0));
-        }
-        return info;
-      }
-    };
-  }
-
-
-  /**
-   * Obtain the error that caused the thread to exit unexpectedly.
-   */
-  public IOException error() {
-    return error;
-  }
-
-  /**
-   * Add is disabled.
-   * @throws UnsupportedOperationException
-   */
-  public void add(Void ignored) {
-    throw new UnsupportedOperationException(getClass().getName() +
-        " is at the start of the pipeline and accepts no events");
-  }
-
-  /**
-   * Start the reader thread, wait for latch if necessary.
-   */
-  public void start() {
-    rThread.start();
-  }
-
-  /**
-   * Wait for the reader thread to exhaust the job trace.
-   */
-  public void join(long millis) throws InterruptedException {
-    rThread.join(millis);
-  }
-
-  /**
-   * Interrupt the reader thread.
-   */
-  public void shutdown() {
-    rThread.interrupt();
-  }
-
-  /**
-   * Interrupt the reader thread. This requires no special consideration, as
-   * the thread has no pending work queue.
-   */
-  public void abort() {
-    // Currently no special work
-    rThread.interrupt();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
deleted file mode 100644
index 0b06911be08..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobMonitor.java
+++ /dev/null
@@ -1,294 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.nio.channels.ClosedByInterruptException;
-import java.util.ArrayList;
-import java.util.LinkedList;
-import java.util.List;
-import java.util.Queue;
-import java.util.concurrent.BlockingQueue;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.TimeUnit;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobStatus;
-
-/**
- * Component accepting submitted, running {@link Statistics.JobStats} and 
- * responsible for monitoring jobs for success and failure. Once a job is 
- * submitted, it is polled for status until complete. If a job is complete, 
- * then the monitor thread returns immediately to the queue. If not, the monitor
- * will sleep for some duration.
- * 
- * {@link JobMonitor} can be configured to use multiple threads for polling
- * the job statuses. Use {@link Gridmix#GRIDMIX_JOBMONITOR_THREADS} to specify
- * the total number of monitoring threads. 
- * 
- * The duration for which a monitoring thread sleeps if the first job in the 
- * queue is running can also be configured. Use 
- * {@link Gridmix#GRIDMIX_JOBMONITOR_SLEEPTIME_MILLIS} to specify a custom 
- * value.
- */
-class JobMonitor implements Gridmix.Component<JobStats> {
-
-  public static final Logger LOG = LoggerFactory.getLogger(JobMonitor.class);
-
-  private final Queue<JobStats> mJobs;
-  private ExecutorService executor;
-  private int numPollingThreads;
-  private final BlockingQueue<JobStats> runningJobs;
-  private final long pollDelayMillis;
-  private Statistics statistics;
-  private boolean graceful = false;
-  private boolean shutdown = false;
-
-  /**
-   * Create a JobMonitor that sleeps for the specified duration after
-   * polling a still-running job.
-   * @param pollDelay Delay after polling a running job
-   * @param unit Time unit for pollDelaySec (rounded to milliseconds)
-   * @param statistics StatCollector , listener to job completion.
-   */
-  public JobMonitor(int pollDelay, TimeUnit unit, Statistics statistics, 
-                    int numPollingThreads) {
-    executor = Executors.newCachedThreadPool();
-    this.numPollingThreads = numPollingThreads;
-    runningJobs = new LinkedBlockingQueue<JobStats>();
-    mJobs = new LinkedList<JobStats>();
-    this.pollDelayMillis = TimeUnit.MILLISECONDS.convert(pollDelay, unit);
-    this.statistics = statistics;
-  }
-
-  /**
-   * Add a running job's status to the polling queue.
-   */
-  public void add(JobStats job) throws InterruptedException {
-      runningJobs.put(job);
-  }
-
-  /**
-   * Add a submission failed job's status, such that it can be communicated
-   * back to serial.
-   * TODO: Cleaner solution for this problem
-   * @param job
-   */
-  public void submissionFailed(JobStats job) {
-    String jobID = job.getJob().getConfiguration().get(Gridmix.ORIGINAL_JOB_ID);
-    LOG.info("Job submission failed notification for job " + jobID);
-    synchronized (statistics) {
-      this.statistics.add(job);
-    }
-  }
-
-  /**
-   * Temporary hook for recording job success.
-   */
-  protected void onSuccess(Job job) {
-    LOG.info(job.getJobName() + " (" + job.getJobID() + ")" + " success");
-  }
-
-  /**
-   * Temporary hook for recording job failure.
-   */
-  protected void onFailure(Job job) {
-    LOG.info(job.getJobName() + " (" + job.getJobID() + ")" + " failure");
-  }
-
-  /**
-   * If shutdown before all jobs have completed, any still-running jobs
-   * may be extracted from the component.
-   * @throws IllegalStateException If monitoring thread is still running.
-   * @return Any jobs submitted and not known to have completed.
-   */
-  List<JobStats> getRemainingJobs() {
-    synchronized (mJobs) {
-      return new ArrayList<JobStats>(mJobs);
-    }
-  }
-
-  /**
-   * Monitoring thread pulling running jobs from the component and into
-   * a queue to be polled for status.
-   */
-  private class MonitorThread extends Thread {
-
-    public MonitorThread(int i) {
-      super("GridmixJobMonitor-" + i);
-    }
-
-    @Override
-    public void run() {
-      boolean graceful;
-      boolean shutdown;
-      while (true) {
-        try {
-          synchronized (mJobs) {
-            graceful = JobMonitor.this.graceful;
-            shutdown = JobMonitor.this.shutdown;
-            runningJobs.drainTo(mJobs);
-          }
-
-          // shutdown conditions; either shutdown requested and all jobs
-          // have completed or abort requested and there are recently
-          // submitted jobs not in the monitored set
-          if (shutdown) {
-            if (!graceful) {
-              while (!runningJobs.isEmpty()) {
-                synchronized (mJobs) {
-                  runningJobs.drainTo(mJobs);
-                }
-              }
-              break;
-            }
-            
-            synchronized (mJobs) {
-              if (graceful && mJobs.isEmpty()) {
-                break;
-              }
-            }
-          }
-          JobStats jobStats = null;
-          synchronized (mJobs) {
-            jobStats = mJobs.poll();
-          }
-          while (jobStats != null) {
-            Job job = jobStats.getJob();
-            
-            try {
-              // get the job status
-              long start = System.currentTimeMillis();
-              JobStatus status = job.getStatus(); // cache the job status
-              long end = System.currentTimeMillis();
-              
-              if (LOG.isDebugEnabled()) {
-                LOG.debug("Status polling for job " + job.getJobID() + " took "
-                          + (end-start) + "ms.");
-              }
-              
-              // update the job progress
-              jobStats.updateJobStatus(status);
-              
-              // if the job is complete, let others know
-              if (status.isJobComplete()) {
-                if (status.getState() == JobStatus.State.SUCCEEDED) {
-                  onSuccess(job);
-                } else {
-                  onFailure(job);
-                }
-                synchronized (statistics) {
-                  statistics.add(jobStats);
-                }
-              } else {
-                // add the running job back and break
-                synchronized (mJobs) {
-                  if (!mJobs.offer(jobStats)) {
-                    LOG.error("Lost job " + (null == job.getJobName()
-                         ? "<unknown>" : job.getJobName())); // should never
-                                                             // happen
-                  }
-                }
-                break;
-              }
-            } catch (IOException e) {
-              if (e.getCause() instanceof ClosedByInterruptException) {
-                // Job doesn't throw InterruptedException, but RPC socket layer
-                // is blocking and may throw a wrapped Exception if this thread
-                // is interrupted. Since the lower level cleared the flag,
-                // reset it here
-                Thread.currentThread().interrupt();
-              } else {
-                LOG.warn("Lost job " + (null == job.getJobName()
-                     ? "<unknown>" : job.getJobName()), e);
-                synchronized (statistics) {
-                  statistics.add(jobStats);
-                }
-              }
-            }
-            
-            // get the next job
-            synchronized (mJobs) {
-              jobStats = mJobs.poll();
-            }
-          }
-          
-          // sleep for a while before checking again
-          try {
-            TimeUnit.MILLISECONDS.sleep(pollDelayMillis);
-          } catch (InterruptedException e) {
-            shutdown = true;
-            continue;
-          }
-        } catch (Throwable e) {
-          LOG.warn("Unexpected exception: ", e);
-        }
-      }
-    }
-  }
-
-  /**
-   * Start the internal, monitoring thread.
-   */
-  public void start() {
-    for (int i = 0; i < numPollingThreads; ++i) {
-      executor.execute(new MonitorThread(i));
-    }
-  }
-
-  /**
-   * Wait for the monitor to halt, assuming shutdown or abort have been
-   * called. Note that, since submission may be sporatic, this will hang
-   * if no form of shutdown has been requested.
-   */
-  public void join(long millis) throws InterruptedException {
-    executor.awaitTermination(millis, TimeUnit.MILLISECONDS);
-  }
-
-  /**
-   * Drain all submitted jobs to a queue and stop the monitoring thread.
-   * Upstream submitter is assumed dead.
-   */
-  public void abort() {
-    synchronized (mJobs) {
-      graceful = false;
-      shutdown = true;
-    }
-    executor.shutdown();
-  }
-
-  /**
-   * When all monitored jobs have completed, stop the monitoring thread.
-   * Upstream submitter is assumed dead.
-   */
-  public void shutdown() {
-    synchronized (mJobs) {
-      graceful = true;
-      shutdown = true;
-    }
-    executor.shutdown();
-  }
-}
-
-
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
deleted file mode 100644
index ac41256da9e..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/JobSubmitter.java
+++ /dev/null
@@ -1,225 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.nio.channels.ClosedByInterruptException;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.RejectedExecutionException;
-import java.util.concurrent.Semaphore;
-import java.util.concurrent.ThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-
-/**
- * Component accepting deserialized job traces, computing split data, and
- * submitting to the cluster on deadline. Each job added from an upstream
- * factory must be submitted to the cluster by the deadline recorded on it.
- * Once submitted, jobs must be added to a downstream component for
- * monitoring.
- */
-class JobSubmitter implements Gridmix.Component<GridmixJob> {
-
-  public static final Logger LOG = LoggerFactory.getLogger(JobSubmitter.class);
-
-  private final Semaphore sem;
-  private final Statistics statistics;
-  private final FilePool inputDir;
-  private final JobMonitor monitor;
-  private final ExecutorService sched;
-  private volatile boolean shutdown = false;
-  private final int queueDepth;
-
-  /**
-   * Initialize the submission component with downstream monitor and pool of
-   * files from which split data may be read.
-   * @param monitor Monitor component to which jobs should be passed
-   * @param threads Number of submission threads
-   *   See {@link Gridmix#GRIDMIX_SUB_THR}.
-   * @param queueDepth Max depth of pending work queue
-   *   See {@link Gridmix#GRIDMIX_QUE_DEP}.
-   * @param inputDir Set of files from which split data may be mined for
-   *   synthetic jobs.
-   * @param statistics
-   */
-  public JobSubmitter(JobMonitor monitor, int threads, int queueDepth,
-      FilePool inputDir, Statistics statistics) {
-    this.queueDepth = queueDepth;
-    sem = new Semaphore(queueDepth);
-    sched = new ThreadPoolExecutor(threads, threads, 0L,
-        TimeUnit.MILLISECONDS, new LinkedBlockingQueue<Runnable>());
-    this.inputDir = inputDir;
-    this.monitor = monitor;
-    this.statistics = statistics;
-  }
-
-  /**
-   * Runnable wrapping a job to be submitted to the cluster.
-   */
-  private class SubmitTask implements Runnable {
-
-    final GridmixJob job;
-    public SubmitTask(GridmixJob job) {
-      this.job = job;
-    }
-    public void run() {
-      JobStats stats = 
-        Statistics.generateJobStats(job.getJob(), job.getJobDesc());
-      try {
-        // pre-compute split information
-        try {
-          long start = System.currentTimeMillis();
-          job.buildSplits(inputDir);
-          long end = System.currentTimeMillis();
-          LOG.info("[JobSubmitter] Time taken to build splits for job " 
-                   + job.getJob().getJobID() + ": " + (end - start) + " ms.");
-        } catch (IOException e) {
-          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
-                   + job.getUgi(), e);
-          monitor.submissionFailed(stats);
-          return;
-        } catch (Exception e) {
-          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
-                   + job.getUgi(), e);
-          monitor.submissionFailed(stats);
-          return;
-        }
-        // Sleep until deadline
-        long nsDelay = job.getDelay(TimeUnit.NANOSECONDS);
-        while (nsDelay > 0) {
-          TimeUnit.NANOSECONDS.sleep(nsDelay);
-          nsDelay = job.getDelay(TimeUnit.NANOSECONDS);
-        }
-        try {
-          // submit job
-          long start = System.currentTimeMillis();
-          job.call();
-          long end = System.currentTimeMillis();
-          LOG.info("[JobSubmitter] Time taken to submit the job " 
-                   + job.getJob().getJobID() + ": " + (end - start) + " ms.");
-          
-          // mark it as submitted
-          job.setSubmitted();
-          
-          // add to the monitor
-          monitor.add(stats);
-          
-          // add to the statistics
-          statistics.addJobStats(stats);
-          if (LOG.isDebugEnabled()) {
-            String jobID = 
-              job.getJob().getConfiguration().get(Gridmix.ORIGINAL_JOB_ID);
-            LOG.debug("Original job '" + jobID + "' is being simulated as '" 
-                      + job.getJob().getJobID() + "'");
-            LOG.debug("SUBMIT " + job + "@" + System.currentTimeMillis() 
-                      + " (" + job.getJob().getJobID() + ")");
-          }
-        } catch (IOException e) {
-          LOG.warn("Failed to submit " + job.getJob().getJobName() + " as " 
-                   + job.getUgi(), e);
-          if (e.getCause() instanceof ClosedByInterruptException) {
-            throw new InterruptedException("Failed to submit " +
-                job.getJob().getJobName());
-          }
-          monitor.submissionFailed(stats);
-        } catch (ClassNotFoundException e) {
-          LOG.warn("Failed to submit " + job.getJob().getJobName(), e);
-          monitor.submissionFailed(stats);
-        }
-      } catch (InterruptedException e) {
-        // abort execution, remove splits if nesc
-        // TODO release ThdLoc
-        GridmixJob.pullDescription(job.id());
-        Thread.currentThread().interrupt();
-        monitor.submissionFailed(stats);
-      } catch(Exception e) {
-        //Due to some exception job wasnt submitted.
-        LOG.info(" Job " + job.getJob().getJobID() + " submission failed " , e);
-        monitor.submissionFailed(stats);
-      } finally {
-        sem.release();
-      }
-    }
-  }
-
-  /**
-   * Enqueue the job to be submitted per the deadline associated with it.
-   */
-  public void add(final GridmixJob job) throws InterruptedException {
-    final boolean addToQueue = !shutdown;
-    if (addToQueue) {
-      final SubmitTask task = new SubmitTask(job);
-      LOG.info("Total number of queued jobs: " 
-               + (queueDepth - sem.availablePermits()));
-      sem.acquire();
-      try {
-        sched.execute(task);
-      } catch (RejectedExecutionException e) {
-        sem.release();
-      }
-    }
-  }
-
-  /**
-   * (Re)scan the set of input files from which splits are derived.
-   * @throws java.io.IOException
-   */
-  public void refreshFilePool() throws IOException {
-    inputDir.refresh();
-  }
-
-  /**
-   * Does nothing, as the threadpool is already initialized and waiting for
-   * work from the upstream factory.
-   */
-  public void start() { }
-
-  /**
-   * Continue running until all queued jobs have been submitted to the
-   * cluster.
-   */
-  public void join(long millis) throws InterruptedException {
-    if (!shutdown) {
-      throw new IllegalStateException("Cannot wait for active submit thread");
-    }
-    sched.awaitTermination(millis, TimeUnit.MILLISECONDS);
-  }
-
-  /**
-   * Finish all jobs pending submission, but do not accept new work.
-   */
-  public void shutdown() {
-    // complete pending tasks, but accept no new tasks
-    shutdown = true;
-    sched.shutdown();
-  }
-
-  /**
-   * Discard pending work, including precomputed work waiting to be
-   * submitted.
-   */
-  public void abort() {
-    //pendingJobs.clear();
-    shutdown = true;
-    sched.shutdownNow();
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
deleted file mode 100644
index d1229ce2d8f..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadJob.java
+++ /dev/null
@@ -1,663 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskCounter;
-import org.apache.hadoop.mapreduce.TaskInputOutputContext;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.tools.rumen.TaskInfo;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin;
-
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-
-/**
- * Synthetic job generated from a trace description.
- */
-class LoadJob extends GridmixJob {
-
-  public static final Logger LOG = LoggerFactory.getLogger(LoadJob.class);
-
-  public LoadJob(final Configuration conf, long submissionMillis, 
-                 final JobStory jobdesc, Path outRoot, UserGroupInformation ugi,
-                 final int seq) throws IOException {
-    super(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
-  }
-
-  public Job call() throws IOException, InterruptedException,
-                           ClassNotFoundException {
-    ugi.doAs(
-      new PrivilegedExceptionAction<Job>() {
-        public Job run() throws IOException, ClassNotFoundException,
-                                InterruptedException {
-          job.setMapperClass(LoadMapper.class);
-          job.setReducerClass(LoadReducer.class);
-          job.setNumReduceTasks(jobdesc.getNumberReduces());
-          job.setMapOutputKeyClass(GridmixKey.class);
-          job.setMapOutputValueClass(GridmixRecord.class);
-          job.setSortComparatorClass(LoadSortComparator.class);
-          job.setGroupingComparatorClass(SpecGroupingComparator.class);
-          job.setInputFormatClass(LoadInputFormat.class);
-          job.setOutputFormatClass(RawBytesOutputFormat.class);
-          job.setPartitionerClass(DraftPartitioner.class);
-          job.setJarByClass(LoadJob.class);
-          job.getConfiguration().setBoolean(Job.USED_GENERIC_PARSER, true);
-          FileOutputFormat.setOutputPath(job, outdir);
-          job.submit();
-          return job;
-        }
-      });
-
-    return job;
-  }
-
-  @Override
-  protected boolean canEmulateCompression() {
-    return true;
-  }
-  
-  /**
-   * This is a load matching key comparator which will make sure that the
-   * resource usage load is matched even when the framework is in control.
-   */
-  public static class LoadSortComparator extends GridmixKey.Comparator {
-    private ResourceUsageMatcherRunner matcher = null;
-    private boolean isConfigured = false;
-    
-    public LoadSortComparator() {
-      super();
-    }
-    
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      configure();
-      int ret = super.compare(b1, s1, l1, b2, s2, l2);
-      if (matcher != null) {
-        try {
-          matcher.match(); // match the resource usage now
-        } catch (Exception e) {}
-      }
-      return ret;
-    }
-    
-    //TODO Note that the sorter will be instantiated 2 times as follows
-    //       1. During the sort/spill in the map phase
-    //       2. During the merge in the sort phase
-    // We need the handle to the matcher thread only in (2).
-    // This logic can be relaxed to run only in (2).
-    private void configure() {
-      if (!isConfigured) {
-        ThreadGroup group = Thread.currentThread().getThreadGroup();
-        Thread[] threads = new Thread[group.activeCount() * 2];
-        group.enumerate(threads, true);
-        for (Thread t : threads) {
-          if (t != null && (t instanceof ResourceUsageMatcherRunner)) {
-            this.matcher = (ResourceUsageMatcherRunner) t;
-            isConfigured = true;
-            break;
-          }
-        }
-      }
-    }
-  }
-  
-  /**
-   * This is a progress based resource usage matcher.
-   */
-  @SuppressWarnings("unchecked")
-  static class ResourceUsageMatcherRunner extends Thread 
-  implements Progressive {
-    private final ResourceUsageMatcher matcher;
-    private final BoostingProgress progress;
-    private final long sleepTime;
-    private static final String SLEEP_CONFIG = 
-      "gridmix.emulators.resource-usage.sleep-duration";
-    private static final long DEFAULT_SLEEP_TIME = 100; // 100ms
-    
-    /**
-     * This is a progress bar that can be boosted for weaker use-cases.
-     */
-    private static class BoostingProgress implements Progressive {
-      private float boostValue = 0f;
-      TaskInputOutputContext context;
-      
-      BoostingProgress(TaskInputOutputContext context) {
-        this.context = context;
-      }
-      
-      void setBoostValue(float boostValue) {
-        this.boostValue = boostValue;
-      }
-      
-      @Override
-      public float getProgress() {
-        return Math.min(1f, context.getProgress() + boostValue);
-      }
-    }
-    
-    ResourceUsageMatcherRunner(final TaskInputOutputContext context, 
-                               ResourceUsageMetrics metrics) {
-      Configuration conf = context.getConfiguration();
-      
-      // set the resource calculator plugin
-      Class<? extends ResourceCalculatorPlugin> clazz =
-        conf.getClass(TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN,
-                      null, ResourceCalculatorPlugin.class);
-      ResourceCalculatorPlugin plugin = 
-        ResourceCalculatorPlugin.getResourceCalculatorPlugin(clazz, conf);
-      
-      // set the other parameters
-      this.sleepTime = conf.getLong(SLEEP_CONFIG, DEFAULT_SLEEP_TIME);
-      progress = new BoostingProgress(context);
-      
-      // instantiate a resource-usage-matcher
-      matcher = new ResourceUsageMatcher();
-      matcher.configure(conf, plugin, metrics, progress);
-    }
-    
-    protected void match() throws IOException, InterruptedException {
-      // match the resource usage
-      matcher.matchResourceUsage();
-    }
-    
-    @Override
-    public void run() {
-      LOG.info("Resource usage matcher thread started.");
-      try {
-        while (progress.getProgress() < 1) {
-          // match
-          match();
-          
-          // sleep for some time
-          try {
-            Thread.sleep(sleepTime);
-          } catch (Exception e) {}
-        }
-        
-        // match for progress = 1
-        match();
-        LOG.info("Resource usage emulation complete! Matcher exiting");
-      } catch (Exception e) {
-        LOG.info("Exception while running the resource-usage-emulation matcher"
-                 + " thread! Exiting.", e);
-      }
-    }
-    
-    @Override
-    public float getProgress() {
-      return matcher.getProgress();
-    }
-    
-    // boost the progress bar as fasten up the emulation cycles.
-    void boost(float value) {
-      progress.setBoostValue(value);
-    }
-  }
-  
-  // Makes sure that the TaskTracker doesn't kill the map/reduce tasks while
-  // they are emulating
-  private static class StatusReporter extends Thread {
-    private final TaskAttemptContext context;
-    private final Progressive progress;
-    
-    StatusReporter(TaskAttemptContext context, Progressive progress) {
-      this.context = context;
-      this.progress = progress;
-    }
-    
-    @Override
-    public void run() {
-      LOG.info("Status reporter thread started.");
-      try {
-        while (!isInterrupted() && progress.getProgress() < 1) {
-          // report progress
-          context.progress();
-
-          // sleep for some time
-          try {
-            Thread.sleep(100); // sleep for 100ms
-          } catch (Exception e) {}
-        }
-        
-        LOG.info("Status reporter thread exiting");
-      } catch (Exception e) {
-        LOG.info("Exception while running the status reporter thread!", e);
-      }
-    }
-  }
-  
-  public static class LoadMapper
-  extends Mapper<NullWritable, GridmixRecord, GridmixKey, GridmixRecord> {
-
-    private double acc;
-    private double ratio;
-    private final ArrayList<RecordFactory> reduces =
-      new ArrayList<RecordFactory>();
-    private final Random r = new Random();
-
-    private final GridmixKey key = new GridmixKey();
-    private final GridmixRecord val = new GridmixRecord();
-
-    private ResourceUsageMatcherRunner matcher = null;
-    private StatusReporter reporter = null;
-    
-    @Override
-    protected void setup(Context ctxt) 
-    throws IOException, InterruptedException {
-      final Configuration conf = ctxt.getConfiguration();
-      final LoadSplit split = (LoadSplit) ctxt.getInputSplit();
-      final int maps = split.getMapCount();
-      final long[] reduceBytes = split.getOutputBytes();
-      final long[] reduceRecords = split.getOutputRecords();
-
-      long totalRecords = 0L;
-      final int nReduces = ctxt.getNumReduceTasks();
-      if (nReduces > 0) {
-        // enable gridmix map output record for compression
-        boolean emulateMapOutputCompression = 
-          CompressionEmulationUtil.isCompressionEmulationEnabled(conf)
-          && conf.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false);
-        float compressionRatio = 1.0f;
-        if (emulateMapOutputCompression) {
-          compressionRatio = 
-            CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf);
-          LOG.info("GridMix is configured to use a compression ratio of " 
-                   + compressionRatio + " for the map output data.");
-          key.setCompressibility(true, compressionRatio);
-          val.setCompressibility(true, compressionRatio);
-        }
-        
-        int idx = 0;
-        int id = split.getId();
-        for (int i = 0; i < nReduces; ++i) {
-          final GridmixKey.Spec spec = new GridmixKey.Spec();
-          if (i == id) {
-            spec.bytes_out = split.getReduceBytes(idx);
-            spec.rec_out = split.getReduceRecords(idx);
-            spec.setResourceUsageSpecification(
-                   split.getReduceResourceUsageMetrics(idx));
-            ++idx;
-            id += maps;
-          }
-          
-          // set the map output bytes such that the final reduce input bytes 
-          // match the expected value obtained from the original job
-          long mapOutputBytes = reduceBytes[i];
-          if (emulateMapOutputCompression) {
-            mapOutputBytes /= compressionRatio;
-          }
-          reduces.add(new IntermediateRecordFactory(
-              new AvgRecordFactory(mapOutputBytes, reduceRecords[i], conf, 
-                                   5*1024),
-              i, reduceRecords[i], spec, conf));
-          totalRecords += reduceRecords[i];
-        }
-      } else {
-        long mapOutputBytes = reduceBytes[0];
-        
-        // enable gridmix job output compression
-        boolean emulateJobOutputCompression = 
-          CompressionEmulationUtil.isCompressionEmulationEnabled(conf)
-          && conf.getBoolean(FileOutputFormat.COMPRESS, false);
-
-        if (emulateJobOutputCompression) {
-          float compressionRatio = 
-            CompressionEmulationUtil.getJobOutputCompressionEmulationRatio(conf);
-          LOG.info("GridMix is configured to use a compression ratio of " 
-                   + compressionRatio + " for the job output data.");
-          key.setCompressibility(true, compressionRatio);
-          val.setCompressibility(true, compressionRatio);
-
-          // set the output size accordingly
-          mapOutputBytes /= compressionRatio;
-        }
-        reduces.add(new AvgRecordFactory(mapOutputBytes, reduceRecords[0],
-                                         conf, 5*1024));
-        totalRecords = reduceRecords[0];
-      }
-      final long splitRecords = split.getInputRecords();
-      int missingRecSize = 
-        conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 64*1024);
-      final long inputRecords = 
-        (splitRecords <= 0 && split.getLength() >= 0)
-        ? Math.max(1, split.getLength() / missingRecSize)
-        : splitRecords;
-      ratio = totalRecords / (1.0 * inputRecords);
-      acc = 0.0;
-      
-      matcher = new ResourceUsageMatcherRunner(ctxt, 
-                      split.getMapResourceUsageMetrics());
-      matcher.setDaemon(true);
-      
-      // start the status reporter thread
-      reporter = new StatusReporter(ctxt, matcher);
-      reporter.setDaemon(true);
-      reporter.start();
-    }
-
-    @Override
-    public void map(NullWritable ignored, GridmixRecord rec,
-                    Context context) throws IOException, InterruptedException {
-      acc += ratio;
-      while (acc >= 1.0 && !reduces.isEmpty()) {
-        key.setSeed(r.nextLong());
-        val.setSeed(r.nextLong());
-        final int idx = r.nextInt(reduces.size());
-        final RecordFactory f = reduces.get(idx);
-        if (!f.next(key, val)) {
-          reduces.remove(idx);
-          continue;
-        }
-        context.write(key, val);
-        acc -= 1.0;
-        
-        // match inline
-        try {
-          matcher.match();
-        } catch (Exception e) {
-          LOG.debug("Error in resource usage emulation! Message: ", e);
-        }
-      }
-    }
-
-    @Override
-    public void cleanup(Context context) 
-    throws IOException, InterruptedException {
-      LOG.info("Starting the cleanup phase.");
-      for (RecordFactory factory : reduces) {
-        key.setSeed(r.nextLong());
-        while (factory.next(key, val)) {
-          // send the progress update (maybe make this a thread)
-          context.progress();
-          
-          context.write(key, val);
-          key.setSeed(r.nextLong());
-          
-          // match inline
-          try {
-            matcher.match();
-          } catch (Exception e) {
-            LOG.debug("Error in resource usage emulation! Message: ", e);
-          }
-        }
-      }
-      
-      // check if the thread will get a chance to run or not
-      //  check if there will be a sort&spill->merge phase or not
-      //  check if the final sort&spill->merge phase is gonna happen or not
-      if (context.getNumReduceTasks() > 0 
-          && context.getCounter(TaskCounter.SPILLED_RECORDS).getValue() == 0) {
-        LOG.info("Boosting the map phase progress.");
-        // add the sort phase progress to the map phase and emulate
-        matcher.boost(0.33f);
-        matcher.match();
-      }
-      
-      // start the matcher thread since the map phase ends here
-      matcher.start();
-    }
-  }
-
-  public static class LoadReducer
-  extends Reducer<GridmixKey,GridmixRecord,NullWritable,GridmixRecord> {
-
-    private final Random r = new Random();
-    private final GridmixRecord val = new GridmixRecord();
-
-    private double acc;
-    private double ratio;
-    private RecordFactory factory;
-
-    private ResourceUsageMatcherRunner matcher = null;
-    private StatusReporter reporter = null;
-    
-    @Override
-    protected void setup(Context context)
-    throws IOException, InterruptedException {
-      if (!context.nextKey() 
-          || context.getCurrentKey().getType() != GridmixKey.REDUCE_SPEC) {
-        throw new IOException("Missing reduce spec");
-      }
-      long outBytes = 0L;
-      long outRecords = 0L;
-      long inRecords = 0L;
-      ResourceUsageMetrics metrics = new ResourceUsageMetrics();
-      for (GridmixRecord ignored : context.getValues()) {
-        final GridmixKey spec = context.getCurrentKey();
-        inRecords += spec.getReduceInputRecords();
-        outBytes += spec.getReduceOutputBytes();
-        outRecords += spec.getReduceOutputRecords();
-        if (spec.getReduceResourceUsageMetrics() != null) {
-          metrics = spec.getReduceResourceUsageMetrics();
-        }
-      }
-      if (0 == outRecords && inRecords > 0) {
-        LOG.info("Spec output bytes w/o records. Using input record count");
-        outRecords = inRecords;
-      }
-      
-      // enable gridmix reduce output record for compression
-      Configuration conf = context.getConfiguration();
-      if (CompressionEmulationUtil.isCompressionEmulationEnabled(conf)
-          && FileOutputFormat.getCompressOutput(context)) {
-        float compressionRatio = 
-          CompressionEmulationUtil
-            .getJobOutputCompressionEmulationRatio(conf);
-        LOG.info("GridMix is configured to use a compression ratio of " 
-                 + compressionRatio + " for the reduce output data.");
-        val.setCompressibility(true, compressionRatio);
-        
-        // Set the actual output data size to make sure that the actual output 
-        // data size is same after compression
-        outBytes /= compressionRatio;
-      }
-      
-      factory =
-        new AvgRecordFactory(outBytes, outRecords, 
-                             context.getConfiguration(), 5*1024);
-      ratio = outRecords / (1.0 * inRecords);
-      acc = 0.0;
-      
-      matcher = new ResourceUsageMatcherRunner(context, metrics);
-      
-      // start the status reporter thread
-      reporter = new StatusReporter(context, matcher);
-      reporter.start();
-    }
-    @Override
-    protected void reduce(GridmixKey key, Iterable<GridmixRecord> values,
-                          Context context) 
-    throws IOException, InterruptedException {
-      for (GridmixRecord ignored : values) {
-        acc += ratio;
-        while (acc >= 1.0 && factory.next(null, val)) {
-          context.write(NullWritable.get(), val);
-          acc -= 1.0;
-          
-          // match inline
-          try {
-            matcher.match();
-          } catch (Exception e) {
-            LOG.debug("Error in resource usage emulation! Message: ", e);
-          }
-        }
-      }
-    }
-    @Override
-    protected void cleanup(Context context)
-    throws IOException, InterruptedException {
-      val.setSeed(r.nextLong());
-      while (factory.next(null, val)) {
-        context.write(NullWritable.get(), val);
-        val.setSeed(r.nextLong());
-        
-        // match inline
-        try {
-          matcher.match();
-        } catch (Exception e) {
-          LOG.debug("Error in resource usage emulation! Message: ", e);
-        }
-      }
-    }
-  }
-
-  static class LoadRecordReader
-  extends RecordReader<NullWritable,GridmixRecord> {
-
-    private RecordFactory factory;
-    private final Random r = new Random();
-    private final GridmixRecord val = new GridmixRecord();
-
-    public LoadRecordReader() { }
-
-    @Override
-    public void initialize(InputSplit genericSplit, TaskAttemptContext ctxt)
-    throws IOException, InterruptedException {
-      final LoadSplit split = (LoadSplit)genericSplit;
-      final Configuration conf = ctxt.getConfiguration();
-      factory = 
-        new ReadRecordFactory(split.getLength(), split.getInputRecords(), 
-                              new FileQueue(split, conf), conf);
-    }
-
-    @Override
-    public boolean nextKeyValue() throws IOException {
-      val.setSeed(r.nextLong());
-      return factory.next(null, val);
-    }
-    @Override
-    public float getProgress() throws IOException {
-      return factory.getProgress();
-    }
-    @Override
-    public NullWritable getCurrentKey() {
-      return NullWritable.get();
-    }
-    @Override
-    public GridmixRecord getCurrentValue() {
-      return val;
-    }
-    @Override
-    public void close() throws IOException {
-      factory.close();
-    }
-  }
-
-  static class LoadInputFormat
-  extends InputFormat<NullWritable,GridmixRecord> {
-
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      return pullDescription(jobCtxt);
-    }
-    @Override
-    public RecordReader<NullWritable,GridmixRecord> createRecordReader(
-        InputSplit split, final TaskAttemptContext taskContext)
-        throws IOException {
-      return new LoadRecordReader();
-    }
-  }
-
-  @Override
-  void buildSplits(FilePool inputDir) throws IOException {
-    long mapInputBytesTotal = 0L;
-    long mapOutputBytesTotal = 0L;
-    long mapOutputRecordsTotal = 0L;
-    final JobStory jobdesc = getJobDesc();
-    if (null == jobdesc) {
-      return;
-    }
-    final int maps = jobdesc.getNumberMaps();
-    final int reds = jobdesc.getNumberReduces();
-    for (int i = 0; i < maps; ++i) {
-      final TaskInfo info = jobdesc.getTaskInfo(TaskType.MAP, i);
-      mapInputBytesTotal += info.getInputBytes();
-      mapOutputBytesTotal += info.getOutputBytes();
-      mapOutputRecordsTotal += info.getOutputRecords();
-    }
-    final double[] reduceRecordRatio = new double[reds];
-    final double[] reduceByteRatio = new double[reds];
-    for (int i = 0; i < reds; ++i) {
-      final TaskInfo info = jobdesc.getTaskInfo(TaskType.REDUCE, i);
-      reduceByteRatio[i] = info.getInputBytes() / (1.0 * mapOutputBytesTotal);
-      reduceRecordRatio[i] =
-        info.getInputRecords() / (1.0 * mapOutputRecordsTotal);
-    }
-    final InputStriper striper = new InputStriper(inputDir, mapInputBytesTotal);
-    final List<InputSplit> splits = new ArrayList<InputSplit>();
-    for (int i = 0; i < maps; ++i) {
-      final int nSpec = reds / maps + ((reds % maps) > i ? 1 : 0);
-      final long[] specBytes = new long[nSpec];
-      final long[] specRecords = new long[nSpec];
-      final ResourceUsageMetrics[] metrics = new ResourceUsageMetrics[nSpec];
-      for (int j = 0; j < nSpec; ++j) {
-        final TaskInfo info =
-          jobdesc.getTaskInfo(TaskType.REDUCE, i + j * maps);
-        specBytes[j] = info.getOutputBytes();
-        specRecords[j] = info.getOutputRecords();
-        metrics[j] = info.getResourceUsageMetrics();
-        if (LOG.isDebugEnabled()) {
-          LOG.debug(String.format("SPEC(%d) %d -> %d %d %d %d %d %d %d", id(), i,
-                    i + j * maps, info.getOutputRecords(), 
-                    info.getOutputBytes(), 
-                    info.getResourceUsageMetrics().getCumulativeCpuUsage(),
-                    info.getResourceUsageMetrics().getPhysicalMemoryUsage(),
-                    info.getResourceUsageMetrics().getVirtualMemoryUsage(),
-                    info.getResourceUsageMetrics().getHeapUsage()));
-        }
-      }
-      final TaskInfo info = jobdesc.getTaskInfo(TaskType.MAP, i);
-      long possiblyCompressedInputBytes = info.getInputBytes();
-      Configuration conf = job.getConfiguration();
-      long uncompressedInputBytes =
-          CompressionEmulationUtil.getUncompressedInputBytes(
-          possiblyCompressedInputBytes, conf);
-      splits.add(
-        new LoadSplit(striper.splitFor(inputDir, uncompressedInputBytes, 3), 
-                      maps, i, uncompressedInputBytes, info.getInputRecords(),
-                      info.getOutputBytes(), info.getOutputRecords(),
-                      reduceByteRatio, reduceRecordRatio, specBytes, 
-                      specRecords, info.getResourceUsageMetrics(),
-                      metrics));
-    }
-    pushDescription(id(), splits);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
deleted file mode 100644
index 27e75473a70..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/LoadSplit.java
+++ /dev/null
@@ -1,180 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-
-class LoadSplit extends CombineFileSplit {
-  private int id;
-  private int nSpec;
-  private int maps;
-  private int reduces;
-  private long inputRecords;
-  private long outputBytes;
-  private long outputRecords;
-  private long maxMemory;
-  private double[] reduceBytes = new double[0];
-  private double[] reduceRecords = new double[0];
-
-  // Spec for reduces id mod this
-  private long[] reduceOutputBytes = new long[0];
-  private long[] reduceOutputRecords = new long[0];
-
-  private ResourceUsageMetrics mapMetrics;
-  private ResourceUsageMetrics[] reduceMetrics;
-
-  LoadSplit() {
-    super();
-  }
-
-  public LoadSplit(CombineFileSplit cfsplit, int maps, int id, long inputBytes, 
-                   long inputRecords, long outputBytes, long outputRecords, 
-                   double[] reduceBytes, double[] reduceRecords, 
-                   long[] reduceOutputBytes, long[] reduceOutputRecords,
-                   ResourceUsageMetrics metrics,
-                   ResourceUsageMetrics[] rMetrics)
-  throws IOException {
-    super(cfsplit);
-    this.id = id;
-    this.maps = maps;
-    reduces = reduceBytes.length;
-    this.inputRecords = inputRecords;
-    this.outputBytes = outputBytes;
-    this.outputRecords = outputRecords;
-    this.reduceBytes = reduceBytes;
-    this.reduceRecords = reduceRecords;
-    nSpec = reduceOutputBytes.length;
-    this.reduceOutputBytes = reduceOutputBytes;
-    this.reduceOutputRecords = reduceOutputRecords;
-    this.mapMetrics = metrics;
-    this.reduceMetrics = rMetrics;
-  }
-
-  public int getId() {
-    return id;
-  }
-  public int getMapCount() {
-    return maps;
-  }
-  public long getInputRecords() {
-    return inputRecords;
-  }
-  public long[] getOutputBytes() {
-    if (0 == reduces) {
-      return new long[] { outputBytes };
-    }
-    final long[] ret = new long[reduces];
-    for (int i = 0; i < reduces; ++i) {
-      ret[i] = Math.round(outputBytes * reduceBytes[i]);
-    }
-    return ret;
-  }
-  public long[] getOutputRecords() {
-    if (0 == reduces) {
-      return new long[] { outputRecords };
-    }
-    final long[] ret = new long[reduces];
-    for (int i = 0; i < reduces; ++i) {
-      ret[i] = Math.round(outputRecords * reduceRecords[i]);
-    }
-    return ret;
-  }
-  public long getReduceBytes(int i) {
-    return reduceOutputBytes[i];
-  }
-  public long getReduceRecords(int i) {
-    return reduceOutputRecords[i];
-  }
-  
-  public ResourceUsageMetrics getMapResourceUsageMetrics() {
-    return mapMetrics;
-  }
-  
-  public ResourceUsageMetrics getReduceResourceUsageMetrics(int i) {
-    return reduceMetrics[i];
-  }
-  
-  @Override
-  public void write(DataOutput out) throws IOException {
-    super.write(out);
-    WritableUtils.writeVInt(out, id);
-    WritableUtils.writeVInt(out, maps);
-    WritableUtils.writeVLong(out, inputRecords);
-    WritableUtils.writeVLong(out, outputBytes);
-    WritableUtils.writeVLong(out, outputRecords);
-    WritableUtils.writeVLong(out, maxMemory);
-    WritableUtils.writeVInt(out, reduces);
-    for (int i = 0; i < reduces; ++i) {
-      out.writeDouble(reduceBytes[i]);
-      out.writeDouble(reduceRecords[i]);
-    }
-    WritableUtils.writeVInt(out, nSpec);
-    for (int i = 0; i < nSpec; ++i) {
-      WritableUtils.writeVLong(out, reduceOutputBytes[i]);
-      WritableUtils.writeVLong(out, reduceOutputRecords[i]);
-    }
-    mapMetrics.write(out);
-    int numReduceMetrics = (reduceMetrics == null) ? 0 : reduceMetrics.length;
-    WritableUtils.writeVInt(out, numReduceMetrics);
-    for (int i = 0; i < numReduceMetrics; ++i) {
-      reduceMetrics[i].write(out);
-    }
-  }
-  @Override
-  public void readFields(DataInput in) throws IOException {
-    super.readFields(in);
-    id = WritableUtils.readVInt(in);
-    maps = WritableUtils.readVInt(in);
-    inputRecords = WritableUtils.readVLong(in);
-    outputBytes = WritableUtils.readVLong(in);
-    outputRecords = WritableUtils.readVLong(in);
-    maxMemory = WritableUtils.readVLong(in);
-    reduces = WritableUtils.readVInt(in);
-    if (reduceBytes.length < reduces) {
-      reduceBytes = new double[reduces];
-      reduceRecords = new double[reduces];
-    }
-    for (int i = 0; i < reduces; ++i) {
-      reduceBytes[i] = in.readDouble();
-      reduceRecords[i] = in.readDouble();
-    }
-    nSpec = WritableUtils.readVInt(in);
-    if (reduceOutputBytes.length < nSpec) {
-      reduceOutputBytes = new long[nSpec];
-      reduceOutputRecords = new long[nSpec];
-    }
-    for (int i = 0; i < nSpec; ++i) {
-      reduceOutputBytes[i] = WritableUtils.readVLong(in);
-      reduceOutputRecords[i] = WritableUtils.readVLong(in);
-    }
-    mapMetrics = new ResourceUsageMetrics();
-    mapMetrics.readFields(in);
-    int numReduceMetrics = WritableUtils.readVInt(in);
-    reduceMetrics = new ResourceUsageMetrics[numReduceMetrics];
-    for (int i = 0; i < numReduceMetrics; ++i) {
-      reduceMetrics[i] = new ResourceUsageMetrics();
-      reduceMetrics[i].readFields(in);
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java
deleted file mode 100644
index 4f1399e4fe4..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Progressive.java
+++ /dev/null
@@ -1,25 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-/**
- * Used to track progress of tasks.
- */
-public interface Progressive {
-  public float getProgress();
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
deleted file mode 100644
index 15fc68e2d15..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/PseudoLocalFs.java
+++ /dev/null
@@ -1,338 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.Random;
-import java.net.URI;
-
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PositionedReadable;
-import org.apache.hadoop.fs.Seekable;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.util.Progressable;
-
-/**
- * Pseudo local file system that generates random data for any file on the fly
- * instead of storing files on disk. So opening same file multiple times will
- * not give same file content. There are no directories in this file system
- * other than the root and all the files are under root i.e. "/". All file URIs
- * on pseudo local file system should be of the format <code>
- * pseudo:///&lt;name&gt;.&lt;fileSize&gt;</code> where name is a unique name
- * and &lt;fileSize&gt; is a number representing the size of the file in bytes.
- */
-class PseudoLocalFs extends FileSystem {
-  Path home;
-  /**
-   * The creation time and modification time of all files in
-   * {@link PseudoLocalFs} is same.
-   */
-  private static final long TIME = System.currentTimeMillis();
-  private static final String HOME_DIR = "/";
-  private static final long BLOCK_SIZE  = 4 * 1024 * 1024L; // 4 MB
-  private static final int DEFAULT_BUFFER_SIZE = 1024  * 1024; // 1MB
-
-  static final URI NAME = URI.create("pseudo:///");
-
-  PseudoLocalFs() {
-    this(new Path(HOME_DIR));
-  }
-
-  PseudoLocalFs(Path home) {
-    super();
-    this.home = home;
-  }
-
-  @Override
-  public URI getUri() {
-    return NAME;
-  }
-
-  @Override
-  public Path getHomeDirectory() {
-    return home;
-  }
-
-  @Override
-  public Path getWorkingDirectory() {
-    return getHomeDirectory();
-  }
-
-  /**
-   * Generates a valid pseudo local file path from the given <code>fileId</code>
-   * and <code>fileSize</code>.
-   * @param fileId unique file id string
-   * @param fileSize file size
-   * @return the generated relative path
-   */
-  static Path generateFilePath(String fileId, long fileSize) {
-    return new Path(fileId + "." + fileSize);
-  }
-
-  /**
-   * Creating a pseudo local file is nothing but validating the file path.
-   * Actual data of the file is generated on the fly when client tries to open
-   * the file for reading.
-   * @param path file path to be created
-   */
-  @Override
-  public FSDataOutputStream create(Path path) throws IOException {
-    try {
-      validateFileNameFormat(path);
-    } catch (FileNotFoundException e) {
-      throw new IOException("File creation failed for " + path);
-    }
-    return null;
-  }
-
-  /**
-   * Validate if the path provided is of expected format of Pseudo Local File
-   * System based files.
-   * @param path file path
-   * @return the file size
-   * @throws FileNotFoundException
-   */
-  long validateFileNameFormat(Path path) throws FileNotFoundException {
-    path = this.makeQualified(path);
-    boolean valid = true;
-    long fileSize = 0;
-    if (!path.toUri().getScheme().equals(getUri().getScheme())) {
-      valid = false;
-    } else {
-      String[] parts = path.toUri().getPath().split("\\.");
-      try {
-        fileSize = Long.parseLong(parts[parts.length - 1]);
-        valid = (fileSize >= 0);
-      } catch (NumberFormatException e) {
-        valid = false;
-      }
-    }
-    if (!valid) {
-      throw new FileNotFoundException("File " + path
-          + " does not exist in pseudo local file system");
-    }
-    return fileSize;
-  }
-
-  /**
-   * @See create(Path) for details
-   */
-  @Override
-  public FSDataInputStream open(Path path, int bufferSize) throws IOException {
-    long fileSize = validateFileNameFormat(path);
-    InputStream in = new RandomInputStream(fileSize, bufferSize);
-    return new FSDataInputStream(in);
-  }
-
-  /**
-   * @See create(Path) for details
-   */
-  @Override
-  public FSDataInputStream open(Path path) throws IOException {
-    return open(path, DEFAULT_BUFFER_SIZE);
-  }
-
-  @Override
-  public FileStatus getFileStatus(Path path) throws IOException {
-    long fileSize = validateFileNameFormat(path);
-    return new FileStatus(fileSize, false, 1, BLOCK_SIZE, TIME, path);
-  }
-
-  @Override
-  public boolean exists(Path path) {
-    try{
-      validateFileNameFormat(path);
-    } catch (FileNotFoundException e) {
-      return false;
-    }
-    return true;
-  }
-
-  @Override
-  public FSDataOutputStream create(Path path, FsPermission permission,
-      boolean overwrite, int bufferSize, short replication, long blockSize,
-      Progressable progress) throws IOException {
-    return create(path);
-  }
-
-  @Override
-  public FileStatus[] listStatus(Path path) throws FileNotFoundException,
-      IOException {
-    return new FileStatus[] {getFileStatus(path)};
-  }
-
-  /**
-   * Input Stream that generates specified number of random bytes.
-   */
-  static class RandomInputStream extends InputStream
-      implements Seekable, PositionedReadable {
-
-    private final Random r = new Random();
-    private BytesWritable val = null;
-    private int positionInVal = 0;// current position in the buffer 'val'
-
-    private long totalSize = 0;// total number of random bytes to be generated
-    private long curPos = 0;// current position in this stream
-
-    /**
-     * @param size total number of random bytes to be generated in this stream
-     * @param bufferSize the buffer size. An internal buffer array of length
-     * <code>bufferSize</code> is created. If <code>bufferSize</code> is not a
-     * positive number, then a default value of 1MB is used.
-     */
-    RandomInputStream(long size, int bufferSize) {
-      totalSize = size;
-      if (bufferSize <= 0) {
-        bufferSize = DEFAULT_BUFFER_SIZE;
-      }
-      val = new BytesWritable(new byte[bufferSize]);
-    }
-
-    @Override
-    public int read() throws IOException {
-      byte[] b = new byte[1];
-      if (curPos < totalSize) {
-        if (positionInVal < val.getLength()) {// use buffered byte
-          b[0] = val.getBytes()[positionInVal++];
-          ++curPos;
-        } else {// generate data
-          int num = read(b);
-          if (num < 0) {
-            return num;
-          }
-        }
-      } else {
-        return -1;
-      }
-      return b[0];
-    }
-
-    @Override
-    public int read(byte[] bytes) throws IOException {
-      return read(bytes, 0, bytes.length);
-    }
-
-    @Override
-    public int read(byte[] bytes, int off, int len) throws IOException {
-      if (curPos == totalSize) {
-        return -1;// EOF
-      }
-      int numBytes = len;
-      if (numBytes > (totalSize - curPos)) {// position in file is close to EOF
-        numBytes = (int)(totalSize - curPos);
-      }
-      if (numBytes > (val.getLength() - positionInVal)) {
-        // need to generate data into val
-        r.nextBytes(val.getBytes());
-        positionInVal = 0;
-      }
-
-      System.arraycopy(val.getBytes(), positionInVal, bytes, off, numBytes);
-      curPos += numBytes;
-      positionInVal += numBytes;
-      return numBytes;
-    }
-
-    @Override
-    public int available() {
-      return (int)(val.getLength() - positionInVal);
-    }
-
-    @Override
-    public int read(long position, byte[] buffer, int offset, int length)
-        throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void readFully(long position, byte[] buffer) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public void readFully(long position, byte[] buffer, int offset, int length)
-        throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    /**
-     * Get the current position in this stream/pseudo-file
-     * @return the position in this stream/pseudo-file
-     * @throws IOException
-     */
-    @Override
-    public long getPos() throws IOException {
-      return curPos;
-    }
-
-    @Override
-    public void seek(long pos) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public boolean seekToNewSource(long targetPos) throws IOException {
-      throw new UnsupportedOperationException();
-    }
-  }
-
-  @Override
-  public FSDataOutputStream append(Path path, int bufferSize,
-      Progressable progress) throws IOException {
-    throw new UnsupportedOperationException("Append is not supported"
-        + " in pseudo local file system.");
-  }
-
-  @Override
-  public boolean mkdirs(Path f, FsPermission permission) throws IOException {
-    throw new UnsupportedOperationException("Mkdirs is not supported"
-        + " in pseudo local file system.");
-  }
-
-  @Override
-  public boolean rename(Path src, Path dst) throws IOException {
-    throw new UnsupportedOperationException("Rename is not supported"
-        + " in pseudo local file system.");
-  }
-
-  @Override
-  public boolean delete(Path path, boolean recursive) {
-    throw new UnsupportedOperationException("File deletion is not supported "
-        + "in pseudo local file system.");
-  }
-
-  @Override
-  public void setWorkingDirectory(Path newDir) {
-    throw new UnsupportedOperationException("SetWorkingDirectory "
-        + "is not supported in pseudo local file system.");
-  }
-
-  @Override
-  public Path makeQualified(Path path) {
-    // skip FileSystem#checkPath() to validate some other Filesystems
-    return path.makeQualified(this.getUri(), this.getWorkingDirectory());
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
deleted file mode 100644
index 95e8f908d1b..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomAlgorithms.java
+++ /dev/null
@@ -1,209 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Random;
-
-/**
- * Random algorithms.
- */
-public class RandomAlgorithms {
-  
-  private interface IndexMapper {
-    int get(int pos);
-    void swap(int a, int b);
-    int getSize();
-    void reset();
-  }
-
-  /**
-   * A sparse index mapping table - useful when we want to
-   * non-destructively permute a small fraction of a large array.
-   */
-  private static class SparseIndexMapper implements IndexMapper {
-    Map<Integer, Integer> mapping = new HashMap<Integer, Integer>();
-    int size;
-    
-    SparseIndexMapper(int size) { 
-      this.size = size;
-    }
-    
-    public int get(int pos) {
-      Integer mapped = mapping.get(pos);
-      if (mapped == null) return pos;
-      return mapped;
-    }
-
-    public void swap(int a, int b) {
-      if (a == b) return;
-      int valA = get(a);
-      int valB = get(b);
-      if (b == valA) {
-        mapping.remove(b);
-      } else {
-        mapping.put(b, valA);
-      }
-      if (a == valB) {
-        mapping.remove(a);
-      } else {
-        mapping.put(a, valB);
-      }
-    }
-    
-    public int getSize() {
-      return size;
-    }
-    
-    public void reset() {
-      mapping.clear();
-    }
-  }
-
-  /**
-   * A dense index mapping table - useful when we want to
-   * non-destructively permute a large fraction of an array.
-   */
-  private static class DenseIndexMapper implements IndexMapper {
-    int[] mapping;
-
-    DenseIndexMapper(int size) {
-      mapping = new int[size];
-      for (int i=0; i<size; ++i) {
-        mapping[i] = i;
-      }
-    }
-
-    public int get(int pos) {
-      if ( (pos < 0) || (pos>=mapping.length) ) {
-        throw new IndexOutOfBoundsException();
-      }
-      return mapping[pos];
-    }
-
-    public void swap(int a, int b) {
-      if (a == b) return;
-      int valA = get(a);
-      int valB = get(b);
-      mapping[a]=valB;
-      mapping[b]=valA;
-    }
-    
-    public int getSize() {
-      return mapping.length;
-    }
-    
-    public void reset() {
-      return;
-    }
-  }
-
-  /**
-   * Iteratively pick random numbers from pool 0..n-1. Each number can only be
-   * picked once.
-   */
-  public static class Selector {
-    private IndexMapper mapping;
-    private int n;
-    private Random rand;
-
-    /**
-     * Constructor.
-     * 
-     * @param n
-     *          The pool of integers: 0..n-1.
-     * @param selPcnt
-     *          Percentage of selected numbers. This is just a hint for internal
-     *          memory optimization.
-     * @param rand
-     *          Random number generator.
-     */
-    public Selector(int n, double selPcnt, Random rand) {
-      if (n <= 0) {
-        throw new IllegalArgumentException("n should be positive");
-      }
-      
-      boolean sparse = (n > 200) && (selPcnt < 0.1);
-      
-      this.n = n;
-      mapping = (sparse) ? new SparseIndexMapper(n) : new DenseIndexMapper(n);
-      this.rand = rand;
-    }
-    
-    /**
-     * Select the next random number.
-     * @return Random number selected. Or -1 if the remaining pool is empty.
-     */
-    public int next() {
-      switch (n) {
-      case 0: return -1;
-      case 1: 
-      {
-        int index = mapping.get(0);
-        --n;
-        return index;
-      }
-      default:
-      {
-        int pos = rand.nextInt(n);
-        int index = mapping.get(pos);
-        mapping.swap(pos, --n);
-        return index;
-      }
-      }
-    }
-
-    /**
-     * Get the remaining random number pool size.
-     */
-    public int getPoolSize() {
-      return n;
-    }
-    
-    /**
-     * Reset the selector for reuse usage.
-     */
-    public void reset() {
-      mapping.reset();
-      n = mapping.getSize();
-    }
-  }
-  
-  
-  /**
-   * Selecting m random integers from 0..n-1.
-   * @return An array of selected integers.
-   */
-  public static int[] select(int m, int n, Random rand) {
-    if (m >= n) {
-      int[] ret = new int[n];
-      for (int i=0; i<n; ++i) {
-        ret[i] = i;
-      }
-      return ret;
-    }
-    
-    Selector selector = new Selector(n, (float)m/n, rand);
-    int[] selected = new int[m];
-    for (int i=0; i<m; ++i) {
-      selected[i] = selector.next();
-    }
-    return selected;
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
deleted file mode 100644
index d5b7ad8c4b9..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RandomTextDataGenerator.java
+++ /dev/null
@@ -1,147 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.util.Arrays;
-import java.util.List;
-import java.util.Random;
-
-import org.apache.commons.lang3.RandomStringUtils;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * A random text generator. The words are simply sequences of alphabets.
- */
-class RandomTextDataGenerator {
-  static final Logger LOG = LoggerFactory.getLogger(RandomTextDataGenerator.class);
-  
-  /**
-   * Configuration key for random text data generator's list size.
-   */
-  static final String GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE = 
-    "gridmix.datagenerator.randomtext.listsize";
-  
-  /**
-   * Configuration key for random text data generator's word size.
-   */
-  static final String GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE = 
-    "gridmix.datagenerator.randomtext.wordsize";
-  
-  /**
-   * Default random text data generator's list size.
-   */
-  static final int DEFAULT_LIST_SIZE = 200;
-  
-  /**
-   * Default random text data generator's word size.
-   */
-  static final int DEFAULT_WORD_SIZE = 10;
-  
-  /**
-   * Default random text data generator's seed.
-   */
-  static final long DEFAULT_SEED = 0L;
-  
-  /**
-   * A list of random words
-   */
-  private String[] words;
-  private Random random;
-  
-  /**
-   * Constructor for {@link RandomTextDataGenerator} with default seed.
-   * @param size the total number of words to consider.
-   * @param wordSize Size of each word
-   */
-  RandomTextDataGenerator(int size, int wordSize) {
-    this(size, DEFAULT_SEED , wordSize);
-  }
-  
-  /**
-   * Constructor for {@link RandomTextDataGenerator}.
-   * @param size the total number of words to consider.
-   * @param seed Random number generator seed for repeatability
-   * @param wordSize Size of each word
-   */
-  RandomTextDataGenerator(int size, Long seed, int wordSize) {
-    random = new Random(seed);
-    words = new String[size];
-    
-    //TODO change the default with the actual stats
-    //TODO do u need varied sized words?
-    for (int i = 0; i < size; ++i) {
-      words[i] = 
-        RandomStringUtils.random(wordSize, 0, 0, true, false, null, random);
-    }
-  }
-  
-  /**
-   * Get the configured random text data generator's list size.
-   */
-  static int getRandomTextDataGeneratorListSize(Configuration conf) {
-    return conf.getInt(GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, DEFAULT_LIST_SIZE);
-  }
-  
-  /**
-   * Set the random text data generator's list size.
-   */
-  static void setRandomTextDataGeneratorListSize(Configuration conf, 
-                                                 int listSize) {
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Random text data generator is configured to use a dictionary " 
-                + " with " + listSize + " words");
-    }
-    conf.setInt(GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, listSize);
-  }
-  
-  /**
-   * Get the configured random text data generator word size.
-   */
-  static int getRandomTextDataGeneratorWordSize(Configuration conf) {
-    return conf.getInt(GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, DEFAULT_WORD_SIZE);
-  }
-  
-  /**
-   * Set the random text data generator word size.
-   */
-  static void setRandomTextDataGeneratorWordSize(Configuration conf, 
-                                                 int wordSize) {
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Random text data generator is configured to use a dictionary " 
-                + " with words of length " + wordSize);
-    }
-    conf.setInt(GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, wordSize);
-  }
-  
-  /**
-   * Returns a randomly selected word from a list of random words.
-   */
-  String getRandomWord() {
-    int index = random.nextInt(words.length);
-    return words[index];
-  }
-  
-  /**
-   * This is mainly for testing.
-   */
-  List<String> getRandomWords() {
-    return Arrays.asList(words);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
deleted file mode 100644
index f95c4b36a5c..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReadRecordFactory.java
+++ /dev/null
@@ -1,85 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.InputStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.IOUtils;
-
-/**
- * For every record consumed, read key + val bytes from the stream provided.
- */
-class ReadRecordFactory extends RecordFactory {
-
-  /**
-   * Size of internal, scratch buffer to read from internal stream.
-   */
-  public static final String GRIDMIX_READ_BUF_SIZE = "gridmix.read.buffer.size";
-
-  private final byte[] buf;
-  private final InputStream src;
-  private final RecordFactory factory;
-
-  /**
-   * @param targetBytes Expected byte count.
-   * @param targetRecords Expected record count.
-   * @param src Stream to read bytes.
-   * @param conf Used to establish read buffer size. @see #GRIDMIX_READ_BUF_SIZE
-   */
-  public ReadRecordFactory(long targetBytes, long targetRecords,
-      InputStream src, Configuration conf) {
-    this(new AvgRecordFactory(targetBytes, targetRecords, conf), src, conf);
-  }
-
-  /**
-   * @param factory Factory to draw record sizes.
-   * @param src Stream to read bytes.
-   * @param conf Used to establish read buffer size. @see #GRIDMIX_READ_BUF_SIZE
-   */
-  public ReadRecordFactory(RecordFactory factory, InputStream src,
-      Configuration conf) {
-    this.src = src;
-    this.factory = factory;
-    buf = new byte[conf.getInt(GRIDMIX_READ_BUF_SIZE, 64 * 1024)];
-  }
-
-  @Override
-  public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
-    if (!factory.next(key, val)) {
-      return false;
-    }
-    for (int len = (null == key ? 0 : key.getSize()) + val.getSize();
-         len > 0; len -= buf.length) {
-      IOUtils.readFully(src, buf, 0, Math.min(buf.length, len));
-    }
-    return true;
-  }
-
-  @Override
-  public float getProgress() throws IOException {
-    return factory.getProgress();
-  }
-
-  @Override
-  public void close() throws IOException {
-    IOUtils.cleanupWithLogger(null, src);
-    factory.close();
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
deleted file mode 100644
index 7abcf783300..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RecordFactory.java
+++ /dev/null
@@ -1,40 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-/**
- * Interface for producing records as inputs and outputs to tasks.
- */
-abstract class RecordFactory implements Closeable {
-
-  /**
-   * Transform the given record or perform some operation.
-   * @return true if the record should be emitted.
-   */
-  public abstract boolean next(GridmixKey key, GridmixRecord val)
-    throws IOException;
-
-  /**
-   * Estimate of exhausted record capacity.
-   */
-  public abstract float getProgress() throws IOException;
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
deleted file mode 100644
index fe3b5d36d98..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/ReplayJobFactory.java
+++ /dev/null
@@ -1,128 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-
- class ReplayJobFactory extends JobFactory<Statistics.ClusterStats> {
-  public static final Logger LOG = LoggerFactory.getLogger(ReplayJobFactory.class);
-
-  /**
-   * Creating a new instance does not start the thread.
-   *
-   * @param submitter   Component to which deserialized jobs are passed
-   * @param jobProducer Job story producer
-   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
-   * @param scratch     Directory into which to write output from simulated jobs
-   * @param conf        Config passed to all jobs to be submitted
-   * @param startFlag   Latch released from main to start pipeline
-   * @param resolver
-   * @throws java.io.IOException
-   */
-  public ReplayJobFactory(
-    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
-    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
-    throws IOException {
-    super(submitter, jobProducer, scratch, conf, startFlag, resolver);
-  }
-
-   
-    @Override
-  public Thread createReaderThread() {
-    return new ReplayReaderThread("ReplayJobFactory");
-  }
-
-   /**
-    * @param item
-    */
-   public void update(Statistics.ClusterStats item) {
-   }
-
-   private class ReplayReaderThread extends Thread {
-
-    public ReplayReaderThread(String threadName) {
-      super(threadName);
-    }
-
-
-    public void run() {
-      try {
-        startFlag.await();
-        if (Thread.currentThread().isInterrupted()) {
-          return;
-        }
-        final long initTime = TimeUnit.MILLISECONDS.convert(
-          System.nanoTime(), TimeUnit.NANOSECONDS);
-        LOG.info("START REPLAY @ " + initTime);
-        long first = -1;
-        long last = -1;
-        while (!Thread.currentThread().isInterrupted()) {
-          try {
-            final JobStory job = getNextJobFiltered();
-            if (null == job) {
-              return;
-            }
-            if (first < 0) {
-              first = job.getSubmissionTime();
-            }
-            final long current = job.getSubmissionTime();
-            if (current < last) {
-              LOG.warn("Job " + job.getJobID() + " out of order");
-              continue;
-            }
-            last = current;
-            submitter.add(
-              jobCreator.createGridmixJob(
-                conf, initTime + Math.round(rateFactor * (current - first)),
-                job, scratch,
-                userResolver.getTargetUgi(
-                  UserGroupInformation.createRemoteUser(job.getUser())), 
-                sequence.getAndIncrement()));
-          } catch (IOException e) {
-            error = e;
-            return;
-          }
-        }
-      } catch (InterruptedException e) {
-        // exit thread; ignore any jobs remaining in the trace
-      } finally {
-        IOUtils.cleanupWithLogger(null, jobProducer);
-      }
-    }
-  }
-
-   /**
-    * Start the reader thread, wait for latch if necessary.
-    */
-   @Override
-   public void start() {
-     this.rThread.start();
-   }
-
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
deleted file mode 100644
index 28379ae6639..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/RoundRobinUserResolver.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.util.LineReader;
-
-import java.io.IOException;
-import java.net.URI;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-
-public class RoundRobinUserResolver implements UserResolver {
-  public static final Logger LOG = LoggerFactory.getLogger(RoundRobinUserResolver.class);
-
-  private int uidx = 0;
-  private List<UserGroupInformation> users = Collections.emptyList();
-
-  /**
-   *  Mapping between user names of original cluster and UGIs of proxy users of
-   *  simulated cluster
-   */
-  private final HashMap<String,UserGroupInformation> usercache =
-      new HashMap<String,UserGroupInformation>();
-  
-  /**
-   * Userlist assumes one user per line.
-   * Each line in users-list-file is of the form &lt;username&gt;[,group]* 
-   * <br> Group names are ignored(they are not parsed at all).
-   */
-  private List<UserGroupInformation> parseUserList(URI userUri, 
-                                                   Configuration conf) 
-  throws IOException {
-    if (null == userUri) {
-      return Collections.emptyList();
-    }
-    
-    final Path userloc = new Path(userUri.toString());
-    final Text rawUgi = new Text();
-    final FileSystem fs = userloc.getFileSystem(conf);
-    final ArrayList<UserGroupInformation> ugiList =
-        new ArrayList<UserGroupInformation>();
-
-    LineReader in = null;
-    try {
-      in = new LineReader(fs.open(userloc));
-      while (in.readLine(rawUgi) > 0) {//line is of the form username[,group]*
-        if(rawUgi.toString().trim().equals("")) {
-          continue; //Continue on empty line
-        }
-        // e is end position of user name in this line
-        int e = rawUgi.find(",");
-        if (e == 0) {
-          throw new IOException("Missing username: " + rawUgi);
-        }
-        if (e == -1) {
-          e = rawUgi.getLength();
-        }
-        final String username = Text.decode(rawUgi.getBytes(), 0, e).trim();
-        UserGroupInformation ugi = null;
-        try {
-          ugi = UserGroupInformation.createProxyUser(username,
-                    UserGroupInformation.getLoginUser());
-        } catch (IOException ioe) {
-          LOG.error("Error while creating a proxy user " ,ioe);
-        }
-        if (ugi != null) {
-          ugiList.add(ugi);
-        }
-        // No need to parse groups, even if they exist. Go to next line
-      }
-    } finally {
-      if (in != null) {
-        in.close();
-      }
-    }
-    return ugiList;
-  }
-
-  @Override
-  public synchronized boolean setTargetUsers(URI userloc, Configuration conf)
-  throws IOException {
-    uidx = 0;
-    users = parseUserList(userloc, conf);
-    if (users.size() == 0) {
-      throw new IOException(buildEmptyUsersErrorMsg(userloc));
-    }
-    usercache.clear();
-    return true;
-  }
-
-  static String buildEmptyUsersErrorMsg(URI userloc) {
-    return "Empty user list is not allowed for RoundRobinUserResolver. Provided"
-    + " user resource URI '" + userloc + "' resulted in an empty user list.";
-  }
-
-  @Override
-  public synchronized UserGroupInformation getTargetUgi(
-    UserGroupInformation ugi) {
-    // UGI of proxy user
-    UserGroupInformation targetUGI = usercache.get(ugi.getUserName());
-    if (targetUGI == null) {
-      targetUGI = users.get(uidx++ % users.size());
-      usercache.put(ugi.getUserName(), targetUGI);
-    }
-    return targetUGI;
-  }
-
-  /**
-   * {@inheritDoc}
-   * <p>
-   * {@link RoundRobinUserResolver} needs to map the users in the
-   * trace to the provided list of target users. So user list is needed.
-   */
-  public boolean needsTargetUsersList() {
-    return true;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
deleted file mode 100644
index cb05ab63f1c..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SerialJobFactory.java
+++ /dev/null
@@ -1,182 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.io.IOException;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.locks.Condition;
-
-public class SerialJobFactory extends JobFactory<JobStats> {
-
-  public static final Logger LOG = LoggerFactory.getLogger(SerialJobFactory.class);
-  private final Condition jobCompleted = lock.newCondition();
-
-  /**
-   * Creating a new instance does not start the thread.
-   *
-   * @param submitter   Component to which deserialized jobs are passed
-   * @param jobProducer Job story producer
-   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
-   * @param scratch     Directory into which to write output from simulated jobs
-   * @param conf        Config passed to all jobs to be submitted
-   * @param startFlag   Latch released from main to start pipeline
-   * @throws java.io.IOException
-   */
-  public SerialJobFactory(
-    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
-    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
-    throws IOException {
-    super(submitter, jobProducer, scratch, conf, startFlag, resolver);
-  }
-
-  @Override
-  public Thread createReaderThread() {
-    return new SerialReaderThread("SerialJobFactory");
-  }
-
-  private class SerialReaderThread extends Thread {
-
-    public SerialReaderThread(String threadName) {
-      super(threadName);
-    }
-
-    /**
-     * SERIAL : In this scenario .  method waits on notification ,
-     * that a submitted job is actually completed. Logic is simple.
-     * ===
-     * while(true) {
-     * wait till previousjob is completed.
-     * break;
-     * }
-     * submit newJob.
-     * previousJob = newJob;
-     * ==
-     */
-    @Override
-    public void run() {
-      try {
-        startFlag.await();
-        if (Thread.currentThread().isInterrupted()) {
-          return;
-        }
-        LOG.info("START SERIAL @ " + System.currentTimeMillis());
-        GridmixJob prevJob;
-        while (!Thread.currentThread().isInterrupted()) {
-          final JobStory job;
-          try {
-            job = getNextJobFiltered();
-            if (null == job) {
-              return;
-            }
-            if (LOG.isDebugEnabled()) {
-              LOG.debug(
-                "Serial mode submitting job " + job.getName());
-            }
-            prevJob = jobCreator.createGridmixJob(
-              conf, 0L, job, scratch, 
-              userResolver.getTargetUgi(
-                UserGroupInformation.createRemoteUser(job.getUser())),
-              sequence.getAndIncrement());
-
-            lock.lock();
-            try {
-              LOG.info(" Submitted the job " + prevJob);
-              submitter.add(prevJob);
-            } finally {
-              lock.unlock();
-            }
-          } catch (IOException e) {
-            error = e;
-            //If submission of current job fails , try to submit the next job.
-            return;
-          }
-
-          if (prevJob != null) {
-            //Wait till previous job submitted is completed.
-            lock.lock();
-            try {
-              while (true) {
-                try {
-                  jobCompleted.await();
-                } catch (InterruptedException ie) {
-                  LOG.error(
-                    " Error in SerialJobFactory while waiting for job completion ",
-                    ie);
-                  return;
-                }
-                if (LOG.isDebugEnabled()) {
-                  LOG.debug(" job " + job.getName() + " completed ");
-                }
-                break;
-              }
-            } finally {
-              lock.unlock();
-            }
-            prevJob = null;
-          }
-        }
-      } catch (InterruptedException e) {
-        return;
-      } finally {
-        IOUtils.cleanupWithLogger(null, jobProducer);
-      }
-    }
-
-  }
-
-  /**
-   * SERIAL. Once you get notification from StatsCollector about the job
-   * completion ,simply notify the waiting thread.
-   *
-   * @param item
-   */
-  @Override
-  public void update(Statistics.JobStats item) {
-    //simply notify in case of serial submissions. We are just bothered
-    //if submitted job is completed or not.
-    lock.lock();
-    try {
-      jobCompleted.signalAll();
-    } finally {
-      lock.unlock();
-    }
-  }
-
-  /**
-   * Start the reader thread, wait for latch if necessary.
-   */
-  @Override
-  public void start() {
-    LOG.info(" Starting Serial submission ");
-    this.rThread.start();
-  }
-  // it is need for test 
-  void setDistCacheEmulator(DistributedCacheEmulator e) {
-    jobCreator.setDistCacheEmulator(e);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SleepJob.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
deleted file mode 100644
index 50261139f94..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SleepJob.java
+++ /dev/null
@@ -1,412 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.DataInput;
-import java.io.DataOutput;
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Random;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.Text;
-import org.apache.hadoop.io.Writable;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.TaskStatus;
-import org.apache.hadoop.mapred.gridmix.RandomAlgorithms.Selector;
-import org.apache.hadoop.mapreduce.InputFormat;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.Mapper;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.Reducer;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.lib.output.NullOutputFormat;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.util.Time;
-
-public class SleepJob extends GridmixJob {
-  public static final Logger LOG = LoggerFactory.getLogger(SleepJob.class);
-  private static final ThreadLocal <Random> rand = 
-    new ThreadLocal <Random> () {
-        @Override protected Random initialValue() {
-            return new Random();
-    }
-  };
-  
-  public static final String SLEEPJOB_MAPTASK_ONLY="gridmix.sleep.maptask-only";
-  private final boolean mapTasksOnly;
-  private final int fakeLocations;
-  private final String[] hosts;
-  private final Selector selector;
-  
-  /**
-   * Interval at which to report progress, in seconds.
-   */
-  public static final String GRIDMIX_SLEEP_INTERVAL = "gridmix.sleep.interval";
-  public static final String GRIDMIX_SLEEP_MAX_MAP_TIME = 
-    "gridmix.sleep.max-map-time";
-  public static final String GRIDMIX_SLEEP_MAX_REDUCE_TIME = 
-    "gridmix.sleep.max-reduce-time";
-
-  private final long mapMaxSleepTime, reduceMaxSleepTime;
-
-  public SleepJob(Configuration conf, long submissionMillis, JobStory jobdesc,
-      Path outRoot, UserGroupInformation ugi, int seq, int numLocations,
-      String[] hosts) throws IOException {
-    super(conf, submissionMillis, jobdesc, outRoot, ugi, seq);
-    this.fakeLocations = numLocations;
-    this.hosts = hosts.clone();
-    this.selector = (fakeLocations > 0)? new Selector(hosts.length, (float) fakeLocations
-        / hosts.length, rand.get()) : null;
-    this.mapTasksOnly = conf.getBoolean(SLEEPJOB_MAPTASK_ONLY, false);
-    mapMaxSleepTime = conf.getLong(GRIDMIX_SLEEP_MAX_MAP_TIME, Long.MAX_VALUE);
-    reduceMaxSleepTime = conf.getLong(GRIDMIX_SLEEP_MAX_REDUCE_TIME,
-        Long.MAX_VALUE);
-  }
-
-  @Override
-  protected boolean canEmulateCompression() {
-    return false;
-  }
-  
-  @Override
-  public Job call()
-    throws IOException, InterruptedException, ClassNotFoundException {
-    ugi.doAs(
-      new PrivilegedExceptionAction<Job>() {
-        public Job run()
-          throws IOException, ClassNotFoundException, InterruptedException {
-          job.setMapperClass(SleepMapper.class);
-          job.setReducerClass(SleepReducer.class);
-          job.setNumReduceTasks((mapTasksOnly) ? 0 : jobdesc.getNumberReduces());
-          job.setMapOutputKeyClass(GridmixKey.class);
-          job.setMapOutputValueClass(NullWritable.class);
-          job.setSortComparatorClass(GridmixKey.Comparator.class);
-          job.setGroupingComparatorClass(SpecGroupingComparator.class);
-          job.setInputFormatClass(SleepInputFormat.class);
-          job.setOutputFormatClass(NullOutputFormat.class);
-          job.setPartitionerClass(DraftPartitioner.class);
-          job.setJarByClass(SleepJob.class);
-          job.getConfiguration().setBoolean(Job.USED_GENERIC_PARSER, true);
-          job.submit();
-          return job;
-
-        }
-      });
-
-    return job;
-  }
-
-  public static class SleepMapper
-  extends Mapper<LongWritable, LongWritable, GridmixKey, NullWritable> {
-
-    @Override
-    public void map(LongWritable key, LongWritable value, Context context)
-    throws IOException, InterruptedException {
-      context.setStatus("Sleeping... " + value.get() + " ms left");
-      long now = System.currentTimeMillis();
-      if (now < key.get()) {
-        TimeUnit.MILLISECONDS.sleep(key.get() - now);
-      }
-    }
-
-    @Override
-    public void cleanup(Context context)
-    throws IOException, InterruptedException {
-      final int nReds = context.getNumReduceTasks();
-      if (nReds > 0) {
-        final SleepSplit split = (SleepSplit) context.getInputSplit();
-        int id = split.getId();
-        final int nMaps = split.getNumMaps();
-        //This is a hack to pass the sleep duration via Gridmix key
-        //TODO: We need to come up with better solution for this.
-        
-        final GridmixKey key = new GridmixKey(GridmixKey.REDUCE_SPEC, 0, 0L);
-        for (int i = id, idx = 0; i < nReds; i += nMaps) {
-          key.setPartition(i);
-          key.setReduceOutputBytes(split.getReduceDurations(idx++));
-          id += nReds;
-          context.write(key, NullWritable.get());
-        }
-      }
-    }
-
-  }
-
-  public static class SleepReducer
-  extends Reducer<GridmixKey, NullWritable, NullWritable, NullWritable> {
-
-    private long duration = 0L;
-
-    @Override
-    protected void setup(Context context)
-    throws IOException, InterruptedException {
-      if (!context.nextKey() ||
-        context.getCurrentKey().getType() != GridmixKey.REDUCE_SPEC) {
-        throw new IOException("Missing reduce spec");
-      }
-      for (NullWritable ignored : context.getValues()) {
-        final GridmixKey spec = context.getCurrentKey();
-        duration += spec.getReduceOutputBytes();
-      }
-      long sleepInterval = 
-        context.getConfiguration().getLong(GRIDMIX_SLEEP_INTERVAL, 5);
-      final long RINTERVAL = 
-        TimeUnit.MILLISECONDS.convert(sleepInterval, TimeUnit.SECONDS);
-      //This is to stop accumulating deviation from expected sleep time
-      //over a period of time.
-      long start = Time.monotonicNow();
-      long slept = 0L;
-      long sleep = 0L;
-      while (slept < duration) {
-        final long rem = duration - slept;
-        sleep = Math.min(rem, RINTERVAL);
-        context.setStatus("Sleeping... " + rem + " ms left");
-        TimeUnit.MILLISECONDS.sleep(sleep);
-        slept = Time.monotonicNow() - start;
-      }
-    }
-
-    @Override
-    protected void cleanup(Context context)
-    throws IOException, InterruptedException {
-      final String msg = "Slept for " + duration;
-      LOG.info(msg);
-      context.setStatus(msg);
-    }
-  }
-
-  public static class SleepInputFormat
-  extends InputFormat<LongWritable, LongWritable> {
-
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      return pullDescription(jobCtxt);
-    }
-
-    @Override
-    public RecordReader<LongWritable, LongWritable> createRecordReader(
-      InputSplit split, final TaskAttemptContext context)
-      throws IOException, InterruptedException {
-      final long duration = split.getLength();
-      long sleepInterval = 
-    	  context.getConfiguration().getLong(GRIDMIX_SLEEP_INTERVAL, 5);
-      final long RINTERVAL = 
-        TimeUnit.MILLISECONDS.convert(sleepInterval, TimeUnit.SECONDS);
-      if (RINTERVAL <= 0) {
-        throw new IOException(
-          "Invalid " + GRIDMIX_SLEEP_INTERVAL + ": " + RINTERVAL);
-      }
-      return new RecordReader<LongWritable, LongWritable>() {
-        long start = -1;
-        long slept = 0L;
-        long sleep = 0L;
-        final LongWritable key = new LongWritable();
-        final LongWritable val = new LongWritable();
-
-        @Override
-        public boolean nextKeyValue() throws IOException {
-          if (start == -1) {
-            start = System.currentTimeMillis();
-          }
-          slept += sleep;
-          sleep = Math.min(duration - slept, RINTERVAL);
-          key.set(slept + sleep + start);
-          val.set(duration - slept);
-          return slept < duration;
-        }
-
-        @Override
-        public float getProgress() throws IOException {
-          return slept / ((float) duration);
-        }
-
-        @Override
-        public LongWritable getCurrentKey() {
-          return key;
-        }
-
-        @Override
-        public LongWritable getCurrentValue() {
-          return val;
-        }
-
-        @Override
-        public void close() throws IOException {
-          final String msg = "Slept for " + duration;
-          LOG.info(msg);
-        }
-
-        public void initialize(InputSplit split, TaskAttemptContext ctxt) {
-        }
-      };
-    }
-  }
-
-  public static class SleepSplit extends InputSplit implements Writable {
-    private int id;
-    private int nSpec;
-    private int nMaps;
-    private long sleepDuration;
-    private long[] reduceDurations = new long[0];
-    private String[] locations = new String[0];
-
-    public SleepSplit() {
-    }
-
-    public SleepSplit(
-      int id, long sleepDuration, long[] reduceDurations, int nMaps,
-      String[] locations) {
-      this.id = id;
-      this.sleepDuration = sleepDuration;
-      nSpec = reduceDurations.length;
-      this.reduceDurations = reduceDurations.clone();
-      this.nMaps = nMaps;
-      this.locations = locations.clone();
-    }
-
-    @Override
-    public void write(DataOutput out) throws IOException {
-      WritableUtils.writeVInt(out, id);
-      WritableUtils.writeVLong(out, sleepDuration);
-      WritableUtils.writeVInt(out, nMaps);
-      WritableUtils.writeVInt(out, nSpec);
-      for (int i = 0; i < nSpec; ++i) {
-        WritableUtils.writeVLong(out, reduceDurations[i]);
-      }
-      WritableUtils.writeVInt(out, locations.length);
-      for (int i = 0; i < locations.length; ++i) {
-        Text.writeString(out, locations[i]);
-      }
-    }
-
-    @Override
-    public void readFields(DataInput in) throws IOException {
-      id = WritableUtils.readVInt(in);
-      sleepDuration = WritableUtils.readVLong(in);
-      nMaps = WritableUtils.readVInt(in);
-      nSpec = WritableUtils.readVInt(in);
-      if (reduceDurations.length < nSpec) {
-        reduceDurations = new long[nSpec];
-      }
-      for (int i = 0; i < nSpec; ++i) {
-        reduceDurations[i] = WritableUtils.readVLong(in);
-      }
-      final int nLoc = WritableUtils.readVInt(in);
-      if (nLoc != locations.length) {
-        locations = new String[nLoc];
-      }
-      for (int i = 0; i < nLoc; ++i) {
-        locations[i] = Text.readString(in);
-      }
-    }
-
-    @Override
-    public long getLength() {
-      return sleepDuration;
-    }
-
-    public int getId() {
-      return id;
-    }
-
-    public int getNumMaps() {
-      return nMaps;
-    }
-
-    public long getReduceDurations(int i) {
-      return reduceDurations[i];
-    }
-
-    @Override
-    public String[] getLocations() {
-      return locations.clone();
-    }
-  }
-
-  private TaskAttemptInfo getSuccessfulAttemptInfo(TaskType type, int task) {
-    TaskAttemptInfo ret;
-    for (int i = 0; true; ++i) {
-      // Rumen should make up an attempt if it's missing. Or this won't work
-      // at all. It's hard to discern what is happening in there.
-      ret = jobdesc.getTaskAttemptInfo(type, task, i);
-      if (ret.getRunState() == TaskStatus.State.SUCCEEDED) {
-        break;
-      }
-    }
-    if(ret.getRunState() != TaskStatus.State.SUCCEEDED) {
-      LOG.warn("No sucessful attempts tasktype " + type +" task "+ task);
-    }
-
-    return ret;
-  }
-
-  @Override
-  void buildSplits(FilePool inputDir) throws IOException {
-    final List<InputSplit> splits = new ArrayList<InputSplit>();
-    final int reds = (mapTasksOnly) ? 0 : jobdesc.getNumberReduces();
-    final int maps = jobdesc.getNumberMaps();
-    for (int i = 0; i < maps; ++i) {
-      final int nSpec = reds / maps + ((reds % maps) > i ? 1 : 0);
-      final long[] redDurations = new long[nSpec];
-      for (int j = 0; j < nSpec; ++j) {
-        final ReduceTaskAttemptInfo info =
-          (ReduceTaskAttemptInfo) getSuccessfulAttemptInfo(TaskType.REDUCE, 
-                                                           i + j * maps);
-        // Include only merge/reduce time
-        redDurations[j] = Math.min(reduceMaxSleepTime, info.getMergeRuntime()
-            + info.getReduceRuntime());
-        if (LOG.isDebugEnabled()) {
-          LOG.debug(
-            String.format(
-              "SPEC(%d) %d -> %d %d/%d", id(), i, i + j * maps, redDurations[j],
-              info.getRuntime()));
-        }
-      }
-      final TaskAttemptInfo info = getSuccessfulAttemptInfo(TaskType.MAP, i);
-      ArrayList<String> locations = new ArrayList<String>(fakeLocations);
-      if (fakeLocations > 0) {
-        selector.reset();
-      }
-      for (int k=0; k<fakeLocations; ++k) {
-        int index = selector.next();
-        if (index < 0) break;
-        locations.add(hosts[index]);
-      }
-
-      splits.add(new SleepSplit(i,
-          Math.min(info.getRuntime(), mapMaxSleepTime), redDurations, maps,
-          locations.toArray(new String[locations.size()])));    }
-    pushDescription(id(), splits);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java
deleted file mode 100644
index 2a0f74fa0ff..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StatListener.java
+++ /dev/null
@@ -1,32 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-/**
- * Stat listener.
- * @param <T>
- */
-interface StatListener<T>{
-
-  /**
-   * 
-   * @param item
-   */
-  void update(T item);
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java
deleted file mode 100644
index bf73f2a1faa..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Statistics.java
+++ /dev/null
@@ -1,405 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.gridmix.Gridmix.Component;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobStatus;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-
-import java.io.IOException;
-import java.security.PrivilegedExceptionAction;
-import java.util.Collection;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.CopyOnWriteArrayList;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.locks.Condition;
-import java.util.concurrent.locks.ReentrantLock;
-
-/**
- * Component collecting the stats required by other components
- * to make decisions.
- * Single thread collector tries to collect the stats (currently cluster stats)
- * and caches it internally.
- * Components interested in these stats need to register themselves and will get
- * notified either on every job completion event or some fixed time interval.
- */
-public class Statistics implements Component<Statistics.JobStats> {
-  public static final Logger LOG = LoggerFactory.getLogger(Statistics.class);
-
-  private final StatCollector statistics = new StatCollector();
-  private JobClient cluster;
-
-  //List of cluster status listeners.
-  private final List<StatListener<ClusterStats>> clusterStatlisteners =
-    new CopyOnWriteArrayList<StatListener<ClusterStats>>();
-
-  //List of job status listeners.
-  private final List<StatListener<JobStats>> jobStatListeners =
-    new CopyOnWriteArrayList<StatListener<JobStats>>();
-
-  // A map of job-sequence-id to job-stats of submitted jobs
-  private static final Map<Integer, JobStats> submittedJobsMap =
-    new ConcurrentHashMap<Integer, JobStats>();
-  
-  // total number of map tasks submitted
-  private static volatile int numMapsSubmitted = 0;
-
-  // total number of reduce tasks submitted
-  private static volatile int numReducesSubmitted = 0;
-  
-  private int completedJobsInCurrentInterval = 0;
-  private final int jtPollingInterval;
-  private volatile boolean shutdown = false;
-  private final int maxJobCompletedInInterval;
-  private static final String MAX_JOBS_COMPLETED_IN_POLL_INTERVAL_KEY =
-    "gridmix.max-jobs-completed-in-poll-interval";
-  private final ReentrantLock lock = new ReentrantLock();
-  private final Condition jobCompleted = lock.newCondition();
-  private final CountDownLatch startFlag;
-
-  public Statistics(
-    final Configuration conf, int pollingInterval, CountDownLatch startFlag)
-    throws IOException, InterruptedException {
-      UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-      this.cluster = ugi.doAs(new PrivilegedExceptionAction<JobClient>() {
-        public JobClient run() throws IOException {
-          return new JobClient(new JobConf(conf));
-        }
-      });
-
-    this.jtPollingInterval = pollingInterval;
-    maxJobCompletedInInterval = conf.getInt(
-      MAX_JOBS_COMPLETED_IN_POLL_INTERVAL_KEY, 1);
-    this.startFlag = startFlag;
-  }
-
-  /**
-   * Generates a job stats.
-   */
-  public static JobStats generateJobStats(Job job, JobStory jobdesc) {
-    int seq = GridmixJob.getJobSeqId(job);
-    // bail out if job description is missing for a job to be simulated
-    if (seq >= 0 && jobdesc == null) {
-      throw new IllegalArgumentException("JobStory not available for job " 
-                                         + job.getJobID());
-    }
-    
-    int maps = -1;
-    int reds = -1;
-    if (jobdesc != null) {
-      // Note that the ZombieJob will return a >= 0 value
-      maps = jobdesc.getNumberMaps();
-      reds = jobdesc.getNumberReduces();
-    }
-    return new JobStats(maps, reds, job);
-  }
-
-  private static void addToNumMapsSubmitted(int numMaps) {
-    numMapsSubmitted += numMaps;
-  }
-
-  private static void addToNumReducesSubmitted(int numReduces) {
-    numReducesSubmitted += numReduces;
-  }
-
-  private static void subtractFromNumMapsSubmitted(int numMaps) {
-    numMapsSubmitted -= numMaps;
-  }
-
-  private static void subtractFromNumReducesSubmitted(int numReduces) {
-    numReducesSubmitted -= numReduces;
-  }
-
-  /**
-   * Add a submitted job for monitoring.
-   */
-  public void addJobStats(JobStats stats) {
-    int seq = GridmixJob.getJobSeqId(stats.getJob());
-    if (seq < 0) {
-      LOG.info("Not tracking job " + stats.getJob().getJobName()
-               + " as seq id is less than zero: " + seq);
-      return;
-    }
-    submittedJobsMap.put(seq, stats);
-    addToNumMapsSubmitted(stats.getNoOfMaps());
-    addToNumReducesSubmitted(stats.getNoOfReds());
-  }
-
-  /**
-   * Used by JobMonitor to add the completed job.
-   */
-  @Override
-  public void add(Statistics.JobStats job) {
-    //This thread will be notified initially by job-monitor incase of
-    //data generation. Ignore that as we are getting once the input is
-    //generated.
-    if (!statistics.isAlive()) {
-      return;
-    }
-    JobStats stat = submittedJobsMap.remove(GridmixJob.getJobSeqId(job.getJob()));
-    
-    // stat cannot be null
-    if (stat == null) {
-      LOG.error("[Statistics] Missing entry for job " 
-                + job.getJob().getJobID());
-      return;
-    }
-    
-    // update the total number of submitted map/reduce task count
-    subtractFromNumMapsSubmitted(stat.getNoOfMaps());
-    subtractFromNumReducesSubmitted(stat.getNoOfReds());
-    
-    completedJobsInCurrentInterval++;
-    //check if we have reached the maximum level of job completions.
-    if (completedJobsInCurrentInterval >= maxJobCompletedInInterval) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(
-          " Reached maximum limit of jobs in a polling interval " +
-            completedJobsInCurrentInterval);
-      }
-      completedJobsInCurrentInterval = 0;
-      lock.lock();
-      try {
-        //Job is completed notify all the listeners.
-        for (StatListener<JobStats> l : jobStatListeners) {
-          l.update(stat);
-        }
-        this.jobCompleted.signalAll();
-      } finally {
-        lock.unlock();
-      }
-    }
-  }
-
-  //TODO: We have just 2 types of listeners as of now . If no of listeners
-  //increase then we should move to map kind of model.
-
-  public void addClusterStatsObservers(StatListener<ClusterStats> listener) {
-    clusterStatlisteners.add(listener);
-  }
-
-  public void addJobStatsListeners(StatListener<JobStats> listener) {
-    this.jobStatListeners.add(listener);
-  }
-
-  /**
-   * Attempt to start the service.
-   */
-  @Override
-  public void start() {
-    statistics.start();
-  }
-
-  private class StatCollector extends Thread {
-
-    StatCollector() {
-      super("StatsCollectorThread");
-    }
-
-    public void run() {
-      try {
-        startFlag.await();
-        if (Thread.currentThread().isInterrupted()) {
-          return;
-        }
-      } catch (InterruptedException ie) {
-        LOG.error(
-          "Statistics Error while waiting for other threads to get ready ", ie);
-        return;
-      }
-      while (!shutdown) {
-        lock.lock();
-        try {
-          jobCompleted.await(jtPollingInterval, TimeUnit.MILLISECONDS);
-        } catch (InterruptedException ie) {
-          if (!shutdown) {
-            LOG.error("Statistics interrupt while waiting for completion of "
-                + "a job.", ie);
-          }
-          return;
-        } finally {
-          lock.unlock();
-        }
-
-        //Fetch cluster data only if required.i.e .
-        // only if there are clusterStats listener.
-        if (clusterStatlisteners.size() > 0) {
-          try {
-            ClusterStatus clusterStatus = cluster.getClusterStatus();
-            updateAndNotifyClusterStatsListeners(clusterStatus);
-          } catch (IOException e) {
-            LOG.error(
-              "Statistics io exception while polling JT ", e);
-            return;
-          }
-        }
-      }
-    }
-
-    private void updateAndNotifyClusterStatsListeners(
-      ClusterStatus clusterStatus) {
-      ClusterStats stats = ClusterStats.getClusterStats();
-      stats.setClusterMetric(clusterStatus);
-      for (StatListener<ClusterStats> listener : clusterStatlisteners) {
-        listener.update(stats);
-      }
-    }
-
-  }
-
-  /**
-   * Wait until the service completes. It is assumed that either a
-   * {@link #shutdown} or {@link #abort} has been requested.
-   */
-  @Override
-  public void join(long millis) throws InterruptedException {
-    statistics.join(millis);
-  }
-
-  @Override
-  public void shutdown() {
-    shutdown = true;
-    submittedJobsMap.clear();
-    clusterStatlisteners.clear();
-    jobStatListeners.clear();
-    statistics.interrupt();
-  }
-
-  @Override
-  public void abort() {
-    shutdown = true;
-    submittedJobsMap.clear();
-    clusterStatlisteners.clear();
-    jobStatListeners.clear();
-    statistics.interrupt();
-  }
-
-  /**
-   * Class to encapsulate the JobStats information.
-   * Current we just need information about completedJob.
-   * TODO: In future we need to extend this to send more information.
-   */
-  static class JobStats {
-    private final int noOfMaps;
-    private final int noOfReds;
-    private JobStatus currentStatus;
-    private final Job job;
-
-    public JobStats(int noOfMaps,int numOfReds, Job job){
-      this.job = job;
-      this.noOfMaps = noOfMaps;
-      this.noOfReds = numOfReds;
-    }
-    public int getNoOfMaps() {
-      return noOfMaps;
-    }
-    public int getNoOfReds() {
-      return noOfReds;
-    }
-
-    /**
-     * Returns the job ,
-     * We should not use job.getJobID it returns null in 20.1xx.
-     * Use (GridmixJob.getJobSeqId(job)) instead
-     * @return job
-     */
-    public Job getJob() {
-      return job;
-    }
-    
-    /**
-     * Update the job statistics.
-     */
-    public synchronized void updateJobStatus(JobStatus status) {
-      this.currentStatus = status;
-    }
-    
-    /**
-     * Get the current job status.
-     */
-    public synchronized JobStatus getJobStatus() {
-      return currentStatus;
-    }
-  }
-
-  static class ClusterStats {
-    private ClusterStatus status = null;
-    private static ClusterStats stats = new ClusterStats();
-
-    private ClusterStats() {
-
-    }
-
-    /**
-     * @return stats
-     */
-    static ClusterStats getClusterStats() {
-      return stats;
-    }
-
-    /**
-     * @param metrics
-     */
-    void setClusterMetric(ClusterStatus metrics) {
-      this.status = metrics;
-    }
-
-    /**
-     * @return metrics
-     */
-    public ClusterStatus getStatus() {
-      return status;
-    }
-
-    int getNumRunningJob() {
-      return submittedJobsMap.size();
-    }
-
-    /**
-     * @return runningWatitingJobs
-     */
-    static Collection<JobStats> getRunningJobStats() {
-      return submittedJobsMap.values();
-    }
-
-    /**
-     * Returns the total number of submitted map tasks
-     */
-    static int getSubmittedMapTasks() {
-      return numMapsSubmitted;
-    }
-    
-    /**
-     * Returns the total number of submitted reduce tasks
-     */
-    static int getSubmittedReduceTasks() {
-      return numReducesSubmitted;
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
deleted file mode 100644
index 4e7fc9c2bbd..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/StressJobFactory.java
+++ /dev/null
@@ -1,610 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.IOUtils;
-import org.apache.hadoop.mapred.ClusterStatus;
-import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.JobStatus;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-
-import java.io.IOException;
-import java.util.HashSet;
-import java.util.Set;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-public class StressJobFactory extends JobFactory<Statistics.ClusterStats> {
-  public static final Logger LOG = LoggerFactory.getLogger(StressJobFactory.class);
-
-  private final LoadStatus loadStatus = new LoadStatus();
-  /**
-   * The minimum ratio between pending+running map tasks (aka. incomplete map
-   * tasks) and cluster map slot capacity for us to consider the cluster is
-   * overloaded. For running maps, we only count them partially. Namely, a 40%
-   * completed map is counted as 0.6 map tasks in our calculation.
-   */
-  private static final float OVERLOAD_MAPTASK_MAPSLOT_RATIO = 2.0f;
-  public static final String CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO=
-      "gridmix.throttle.maps.task-to-slot-ratio";
-  final float overloadMapTaskMapSlotRatio;
-
-  /**
-   * The minimum ratio between pending+running reduce tasks (aka. incomplete
-   * reduce tasks) and cluster reduce slot capacity for us to consider the
-   * cluster is overloaded. For running reduces, we only count them partially.
-   * Namely, a 40% completed reduce is counted as 0.6 reduce tasks in our
-   * calculation.
-   */
-  private static final float OVERLOAD_REDUCETASK_REDUCESLOT_RATIO = 2.5f;
-  public static final String CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO=
-    "gridmix.throttle.reduces.task-to-slot-ratio";
-  final float overloadReduceTaskReduceSlotRatio;
-
-  /**
-   * The maximum share of the cluster's mapslot capacity that can be counted
-   * toward a job's incomplete map tasks in overload calculation.
-   */
-  private static final float MAX_MAPSLOT_SHARE_PER_JOB=0.1f;
-  public static final String CONF_MAX_MAPSLOT_SHARE_PER_JOB=
-    "gridmix.throttle.maps.max-slot-share-per-job";  
-  final float maxMapSlotSharePerJob;
-  
-  /**
-   * The maximum share of the cluster's reduceslot capacity that can be counted
-   * toward a job's incomplete reduce tasks in overload calculation.
-   */
-  private static final float MAX_REDUCESLOT_SHARE_PER_JOB=0.1f;
-  public static final String CONF_MAX_REDUCESLOT_SHARE_PER_JOB=
-    "gridmix.throttle.reducess.max-slot-share-per-job";  
-  final float maxReduceSlotSharePerJob;
-
-  /**
-   * The ratio of the maximum number of pending+running jobs over the number of
-   * task trackers.
-   */
-  private static final float MAX_JOB_TRACKER_RATIO=1.0f;
-  public static final String CONF_MAX_JOB_TRACKER_RATIO=
-    "gridmix.throttle.jobs-to-tracker-ratio";  
-  final float maxJobTrackerRatio;
-
-  /**
-   * Represents a list of blacklisted jobs. Jobs are blacklisted when either 
-   * they are complete or their status cannot be obtained. Stress mode will 
-   * ignore blacklisted jobs from its overload computation.
-   */
-  private Set<JobID> blacklistedJobs = new HashSet<JobID>();
-  
-  /**
-   * Creating a new instance does not start the thread.
-   *
-   * @param submitter   Component to which deserialized jobs are passed
-   * @param jobProducer Stream of job traces with which to construct a
-   *                    {@link org.apache.hadoop.tools.rumen.ZombieJobProducer}
-   * @param scratch     Directory into which to write output from simulated jobs
-   * @param conf        Config passed to all jobs to be submitted
-   * @param startFlag   Latch released from main to start pipeline
-   * @throws java.io.IOException
-   */
-  public StressJobFactory(
-    JobSubmitter submitter, JobStoryProducer jobProducer, Path scratch,
-    Configuration conf, CountDownLatch startFlag, UserResolver resolver)
-    throws IOException {
-    super(
-      submitter, jobProducer, scratch, conf, startFlag, resolver);
-    overloadMapTaskMapSlotRatio = conf.getFloat(
-        CONF_OVERLOAD_MAPTASK_MAPSLOT_RATIO, OVERLOAD_MAPTASK_MAPSLOT_RATIO);
-    overloadReduceTaskReduceSlotRatio = conf.getFloat(
-        CONF_OVERLOAD_REDUCETASK_REDUCESLOT_RATIO, 
-        OVERLOAD_REDUCETASK_REDUCESLOT_RATIO);
-    maxMapSlotSharePerJob = conf.getFloat(
-        CONF_MAX_MAPSLOT_SHARE_PER_JOB, MAX_MAPSLOT_SHARE_PER_JOB);
-    maxReduceSlotSharePerJob = conf.getFloat(
-        CONF_MAX_REDUCESLOT_SHARE_PER_JOB, MAX_REDUCESLOT_SHARE_PER_JOB);
-    maxJobTrackerRatio = conf.getFloat(
-        CONF_MAX_JOB_TRACKER_RATIO, MAX_JOB_TRACKER_RATIO);
-  }
-
-  public Thread createReaderThread() {
-    return new StressReaderThread("StressJobFactory");
-  }
-
-  /*
-  * Worker thread responsible for reading descriptions, assigning sequence
-  * numbers, and normalizing time.
-  */
-  private class StressReaderThread extends Thread {
-
-    public StressReaderThread(String name) {
-      super(name);
-    }
-
-    /**
-     * STRESS: Submits the job in STRESS mode.
-     * while(JT is overloaded) {
-     * wait();
-     * }
-     * If not overloaded , get number of slots available.
-     * Keep submitting the jobs till ,total jobs  is sufficient to
-     * load the JT.
-     * That is submit  (Sigma(no of maps/Job)) > (2 * no of slots available)
-     */
-    public void run() {
-      try {
-        startFlag.await();
-        if (Thread.currentThread().isInterrupted()) {
-          LOG.warn("[STRESS] Interrupted before start!. Exiting..");
-          return;
-        }
-        LOG.info("START STRESS @ " + System.currentTimeMillis());
-        while (!Thread.currentThread().isInterrupted()) {
-          try {
-            while (loadStatus.overloaded()) {
-              // update the overload status
-              if (LOG.isDebugEnabled()) {
-                LOG.debug("Updating the overload status.");
-              }
-              try {
-                checkLoadAndGetSlotsToBackfill();
-              } catch (IOException ioe) {
-                LOG.warn("[STRESS] Check failed!", ioe);
-                return;
-              }
-              
-              // if the cluster is still overloaded, then sleep
-              if (loadStatus.overloaded()) {
-                if (LOG.isDebugEnabled()) {
-                  LOG.debug("[STRESS] Cluster overloaded in run! Sleeping...");
-                }
-
-                // sleep 
-                try {
-                  Thread.sleep(1000);
-                } catch (InterruptedException ie) {
-                  LOG.warn("[STRESS] Interrupted while sleeping! Exiting.", ie);
-                  return;
-                }
-              }
-            }
-
-            while (!loadStatus.overloaded()) {
-              if (LOG.isDebugEnabled()) {
-                LOG.debug("[STRESS] Cluster underloaded in run! Stressing...");
-              }
-              try {
-                //TODO This in-line read can block submission for large jobs.
-                final JobStory job = getNextJobFiltered();
-                if (null == job) {
-                  LOG.warn("[STRESS] Finished consuming the input trace. " 
-                           + "Exiting..");
-                  return;
-                }
-                if (LOG.isDebugEnabled()) {
-                  LOG.debug("Job Selected: " + job.getJobID());
-                }
-                
-                UserGroupInformation ugi = 
-                  UserGroupInformation.createRemoteUser(job.getUser());
-                UserGroupInformation tgtUgi = userResolver.getTargetUgi(ugi);
-                GridmixJob tJob = 
-                  jobCreator.createGridmixJob(conf, 0L, job, scratch, 
-                               tgtUgi, sequence.getAndIncrement());
-                
-                // submit the job
-                submitter.add(tJob);
-                
-                // TODO: We need to take care of scenario when one map/reduce
-                // takes more than 1 slot.
-                
-                // Lock the loadjob as we are making updates
-                int incompleteMapTasks = (int) calcEffectiveIncompleteMapTasks(
-                                                 loadStatus.getMapCapacity(), 
-                                                 job.getNumberMaps(), 0.0f);
-                loadStatus.decrementMapLoad(incompleteMapTasks);
-                
-                int incompleteReduceTasks = 
-                  (int) calcEffectiveIncompleteReduceTasks(
-                          loadStatus.getReduceCapacity(), 
-                          job.getNumberReduces(), 0.0f);
-                loadStatus.decrementReduceLoad(incompleteReduceTasks);
-                  
-                loadStatus.decrementJobLoad(1);
-              } catch (IOException e) {
-                LOG.error("[STRESS] Error while submitting the job ", e);
-                error = e;
-                return;
-              }
-
-            }
-          } finally {
-            // do nothing
-          }
-        }
-      } catch (InterruptedException e) {
-        LOG.error("[STRESS] Interrupted in the main block!", e);
-        return;
-      } finally {
-        IOUtils.cleanupWithLogger(null, jobProducer);
-      }
-    }
-  }
-
-  /**
-   * STRESS Once you get the notification from StatsCollector.Collect the
-   * clustermetrics. Update current loadStatus with new load status of JT.
-   *
-   * @param item
-   */
-  @Override
-  public void update(Statistics.ClusterStats item) {
-    ClusterStatus clusterStatus = item.getStatus();
-    try {
-      // update the max cluster map/reduce task capacity
-      loadStatus.updateMapCapacity(clusterStatus.getMaxMapTasks());
-      
-      loadStatus.updateReduceCapacity(clusterStatus.getMaxReduceTasks());
-      
-      int numTrackers = clusterStatus.getTaskTrackers();
-      int jobLoad = 
-        (int) (maxJobTrackerRatio * numTrackers) - item.getNumRunningJob();
-      loadStatus.updateJobLoad(jobLoad);
-    } catch (Exception e) {
-      LOG.error("Couldn't get the new Status",e);
-    }
-  }
-
-  float calcEffectiveIncompleteMapTasks(int mapSlotCapacity,
-      int numMaps, float mapProgress) {
-    float maxEffIncompleteMapTasks = Math.max(1.0f, mapSlotCapacity
-        * maxMapSlotSharePerJob);
-    float mapProgressAdjusted = Math.max(Math.min(mapProgress, 1.0f), 0.0f);
-    return Math.min(maxEffIncompleteMapTasks, 
-                    numMaps * (1.0f - mapProgressAdjusted));
-  }
-
-  float calcEffectiveIncompleteReduceTasks(int reduceSlotCapacity,
-      int numReduces, float reduceProgress) {
-    float maxEffIncompleteReduceTasks = Math.max(1.0f, reduceSlotCapacity
-        * maxReduceSlotSharePerJob);
-    float reduceProgressAdjusted = 
-      Math.max(Math.min(reduceProgress, 1.0f), 0.0f);
-    return Math.min(maxEffIncompleteReduceTasks, 
-                    numReduces * (1.0f - reduceProgressAdjusted));
-  }
-
-  /**
-   * We try to use some light-weight mechanism to determine cluster load.
-   *
-   * @throws java.io.IOException
-   */
-  protected void checkLoadAndGetSlotsToBackfill() 
-  throws IOException, InterruptedException {
-    if (loadStatus.getJobLoad() <= 0) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(System.currentTimeMillis() + " [JobLoad] Overloaded is "
-                  + Boolean.TRUE.toString() + " NumJobsBackfill is "
-                  + loadStatus.getJobLoad());
-      }
-      return; // stop calculation because we know it is overloaded.
-    }
-
-    int mapCapacity = loadStatus.getMapCapacity();
-    int reduceCapacity = loadStatus.getReduceCapacity();
-    
-    // return if the cluster status is not set
-    if (mapCapacity < 0 || reduceCapacity < 0) {
-      // note that, by default, the overload status is true
-      // missing cluster status will result into blocking of job submission
-      return;
-    }
-    
-    // Determine the max permissible map & reduce task load
-    int maxMapLoad = (int) (overloadMapTaskMapSlotRatio * mapCapacity);
-    int maxReduceLoad = 
-      (int) (overloadReduceTaskReduceSlotRatio * reduceCapacity);
-    
-    // compute the total number of map & reduce tasks submitted
-    int totalMapTasks = ClusterStats.getSubmittedMapTasks();
-    int totalReduceTasks = ClusterStats.getSubmittedReduceTasks();
-    
-    if (LOG.isDebugEnabled()) {
-      LOG.debug("Total submitted map tasks: " + totalMapTasks);
-      LOG.debug("Total submitted reduce tasks: " + totalReduceTasks);
-      LOG.debug("Max map load: " + maxMapLoad);
-      LOG.debug("Max reduce load: " + maxReduceLoad);
-    }
-    
-    // generate a pessimistic bound on the max running+pending map tasks
-    // this check is to avoid the heavy-duty actual map load calculation
-    int mapSlotsBackFill = (int) (maxMapLoad - totalMapTasks);
-    
-    // generate a pessimistic bound on the max running+pending reduce tasks
-    // this check is to avoid the heavy-duty actual reduce load calculation
-    int reduceSlotsBackFill = (int) (maxReduceLoad - totalReduceTasks);
-    
-    // maintain a list of seen job ids
-    Set<JobID> seenJobIDs = new HashSet<JobID>();
-    
-    // check if the total number of submitted map/reduce tasks exceeds the 
-    // permissible limit
-    if (totalMapTasks > maxMapLoad || totalReduceTasks > maxReduceLoad) {
-      // if yes, calculate the real load
-      float incompleteMapTasks = 0; // include pending & running map tasks.
-      float incompleteReduceTasks = 0; // include pending & running reduce tasks
-      
-      for (JobStats job : ClusterStats.getRunningJobStats()) {
-        JobID id = job.getJob().getJobID();
-        seenJobIDs.add(id);
-        
-        // Note that this is a hack! Ideally, ClusterStats.getRunningJobStats()
-        // should be smart enough to take care of completed jobs.
-        if (blacklistedJobs.contains(id)) {
-          LOG.warn("Ignoring blacklisted job: " + id);
-          continue;
-        }
-        
-        int noOfMaps = job.getNoOfMaps();
-        int noOfReduces = job.getNoOfReds();
-        
-        // consider polling for jobs where maps>0 and reds>0
-        // TODO: What about setup/cleanup tasks for cases where m=0 and r=0
-        //       What otherwise?
-        if (noOfMaps > 0 || noOfReduces > 0) {
-          // get the job's status
-          JobStatus status = job.getJobStatus();
-          
-          // blacklist completed jobs and continue
-          if (status != null && status.isJobComplete()) {
-            LOG.warn("Blacklisting completed job: " + id);
-            blacklistedJobs.add(id);
-            continue;
-          }
-          
-          // get the map and reduce tasks' progress
-          float mapProgress = 0f;
-          float reduceProgress = 0f;
-          
-          // check if the status is missing (this can happen for unpolled jobs)
-          if (status != null) {
-            mapProgress = status.getMapProgress();
-            reduceProgress = status.getReduceProgress();
-          }
-          
-          incompleteMapTasks += 
-            calcEffectiveIncompleteMapTasks(mapCapacity, noOfMaps, mapProgress);
-
-          // bail out early
-          int currentMapSlotsBackFill = (int) (maxMapLoad - incompleteMapTasks);
-          if (currentMapSlotsBackFill <= 0) {
-            // reset the reduce task load since we are bailing out
-            incompleteReduceTasks = totalReduceTasks;
-            if (LOG.isDebugEnabled()) {
-              LOG.debug("Terminating overload check due to high map load.");
-            }
-            break;
-          }
-
-          // compute the real reduce load
-          if (noOfReduces > 0) {
-            incompleteReduceTasks += 
-              calcEffectiveIncompleteReduceTasks(reduceCapacity, noOfReduces, 
-                  reduceProgress);
-          }
-
-          // bail out early
-          int currentReduceSlotsBackFill = 
-            (int) (maxReduceLoad - incompleteReduceTasks);
-          if (currentReduceSlotsBackFill <= 0) {
-            // reset the map task load since we are bailing out
-            incompleteMapTasks = totalMapTasks;
-            if (LOG.isDebugEnabled()) {
-              LOG.debug("Terminating overload check due to high reduce load.");
-            }
-            break;
-          }
-        } else {
-          LOG.warn("Blacklisting empty job: " + id);
-          blacklistedJobs.add(id);
-        }
-      }
-
-      // calculate the real map load on the cluster
-      mapSlotsBackFill = (int) (maxMapLoad - incompleteMapTasks);
-      
-      // calculate the real reduce load on the cluster
-      reduceSlotsBackFill = (int)(maxReduceLoad - incompleteReduceTasks);
-      
-      // clean up the backlisted set to keep the memory footprint minimal
-      // retain only the jobs that are seen in this cycle
-      blacklistedJobs.retainAll(seenJobIDs);
-      if (LOG.isDebugEnabled() && blacklistedJobs.size() > 0) {
-        LOG.debug("Blacklisted jobs count: " + blacklistedJobs.size());
-      }
-    }
-    
-    // update
-    loadStatus.updateMapLoad(mapSlotsBackFill); 
-    loadStatus.updateReduceLoad(reduceSlotsBackFill);
-    
-    if (loadStatus.getMapLoad() <= 0) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(System.currentTimeMillis() + " [MAP-LOAD] Overloaded is "
-                  + Boolean.TRUE.toString() + " MapSlotsBackfill is "
-                  + loadStatus.getMapLoad());
-      }
-      return; // stop calculation because we know it is overloaded.
-    }
-    
-    if (loadStatus.getReduceLoad() <= 0) {
-      if (LOG.isDebugEnabled()) {
-        LOG.debug(System.currentTimeMillis() + " [REDUCE-LOAD] Overloaded is "
-                  + Boolean.TRUE.toString() + " ReduceSlotsBackfill is "
-                  + loadStatus.getReduceLoad());
-      }
-      return; // stop calculation because we know it is overloaded.
-    }
-
-    if (LOG.isDebugEnabled()) {
-      LOG.debug(System.currentTimeMillis() + " [OVERALL] Overloaded is "
-                + Boolean.FALSE.toString() + "Current load Status is " 
-                + loadStatus);
-    }
-  }
-
-  static class LoadStatus {
-    /**
-     * Additional number of map slots that can be requested before
-     * declaring (by Gridmix STRESS mode) the cluster as overloaded. 
-     */
-    private volatile int mapSlotsBackfill;
-    
-    /**
-     * Determines the total map slot capacity of the cluster.
-     */
-    private volatile int mapSlotCapacity;
-    
-    /**
-     * Additional number of reduce slots that can be requested before
-     * declaring (by Gridmix STRESS mode) the cluster as overloaded.
-     */
-    private volatile int reduceSlotsBackfill;
-    
-    /**
-     * Determines the total reduce slot capacity of the cluster.
-     */
-    private volatile int reduceSlotCapacity;
-
-    /**
-     * Determines the max count of running jobs in the cluster.
-     */
-    private volatile int numJobsBackfill;
-    
-    // set the default to true
-    private AtomicBoolean overloaded = new AtomicBoolean(true);
-
-    /**
-     * Construct the LoadStatus in an unknown state - assuming the cluster is
-     * overloaded by setting numSlotsBackfill=0.
-     */
-    LoadStatus() {
-      mapSlotsBackfill = 0;
-      reduceSlotsBackfill = 0;
-      numJobsBackfill = 0;
-      
-      mapSlotCapacity = -1;
-      reduceSlotCapacity = -1;
-    }
-    
-    public synchronized int getMapLoad() {
-      return mapSlotsBackfill;
-    }
-    
-    public synchronized int getMapCapacity() {
-      return mapSlotCapacity;
-    }
-    
-    public synchronized int getReduceLoad() {
-      return reduceSlotsBackfill;
-    }
-    
-    public synchronized int getReduceCapacity() {
-      return reduceSlotCapacity;
-    }
-    
-    public synchronized int getJobLoad() {
-      return numJobsBackfill;
-    }
-    
-    public synchronized void decrementMapLoad(int mapSlotsConsumed) {
-      this.mapSlotsBackfill -= mapSlotsConsumed;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void decrementReduceLoad(int reduceSlotsConsumed) {
-      this.reduceSlotsBackfill -= reduceSlotsConsumed;
-      updateOverloadStatus();
-    }
-
-    public synchronized void decrementJobLoad(int numJobsConsumed) {
-      this.numJobsBackfill -= numJobsConsumed;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateMapCapacity(int mapSlotsCapacity) {
-      this.mapSlotCapacity = mapSlotsCapacity;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateReduceCapacity(int reduceSlotsCapacity) {
-      this.reduceSlotCapacity = reduceSlotsCapacity;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateMapLoad(int mapSlotsBackfill) {
-      this.mapSlotsBackfill = mapSlotsBackfill;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateReduceLoad(int reduceSlotsBackfill) {
-      this.reduceSlotsBackfill = reduceSlotsBackfill;
-      updateOverloadStatus();
-    }
-    
-    public synchronized void updateJobLoad(int numJobsBackfill) {
-      this.numJobsBackfill = numJobsBackfill;
-      updateOverloadStatus();
-    }
-    
-    private synchronized void updateOverloadStatus() {
-      overloaded.set((mapSlotsBackfill <= 0) || (reduceSlotsBackfill <= 0)
-                     || (numJobsBackfill <= 0));
-    }
-    
-    public boolean overloaded() {
-      return overloaded.get();
-    }
-    
-    public synchronized String toString() {
-    // TODO Use StringBuilder instead
-      return " Overloaded = " + overloaded()
-             + ", MapSlotBackfill = " + mapSlotsBackfill 
-             + ", MapSlotCapacity = " + mapSlotCapacity
-             + ", ReduceSlotBackfill = " + reduceSlotsBackfill 
-             + ", ReduceSlotCapacity = " + reduceSlotCapacity
-             + ", NumJobsBackfill = " + numJobsBackfill;
-    }
-  }
-
-  /**
-   * Start the reader thread, wait for latch if necessary.
-   */
-  @Override
-  public void start() {
-    LOG.info(" Starting Stress submission ");
-    this.rThread.start();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
deleted file mode 100644
index 32ddad9913a..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/SubmitterUserResolver.java
+++ /dev/null
@@ -1,59 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.net.URI;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-/**
- * Resolves all UGIs to the submitting user.
- */
-public class SubmitterUserResolver implements UserResolver {
-  public static final Logger LOG = LoggerFactory.getLogger(SubmitterUserResolver.class);
-  
-  private UserGroupInformation ugi = null;
-
-  public SubmitterUserResolver() throws IOException {
-    LOG.info(" Current user resolver is SubmitterUserResolver ");
-    ugi = UserGroupInformation.getLoginUser();
-  }
-
-  public synchronized boolean setTargetUsers(URI userdesc, Configuration conf)
-      throws IOException {
-    return false;
-  }
-
-  public synchronized UserGroupInformation getTargetUgi(
-      UserGroupInformation ugi) {
-    return this.ugi;
-  }
-
-  /**
-   * {@inheritDoc}
-   * <p>
-   * Since {@link SubmitterUserResolver} returns the user name who is running
-   * gridmix, it doesn't need a target list of users.
-   */
-  public boolean needsTargetUsersList() {
-    return false;
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
deleted file mode 100644
index 16026f22d63..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/Summarizer.java
+++ /dev/null
@@ -1,75 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-
-/**
- * Summarizes various aspects of a {@link Gridmix} run.
- */
-class Summarizer {
-  private ExecutionSummarizer executionSummarizer;
-  private ClusterSummarizer clusterSummarizer;
-  protected static final String NA = "N/A";
-  
-  Summarizer() {
-    this(new String[]{NA});
-  }
-  
-  Summarizer(String[] args) {
-    executionSummarizer = new ExecutionSummarizer(args);
-    clusterSummarizer = new ClusterSummarizer();
-  }
-  
-  ExecutionSummarizer getExecutionSummarizer() {
-    return executionSummarizer;
-  }
-  
-  ClusterSummarizer getClusterSummarizer() {
-    return clusterSummarizer;
-  }
-  
-  void start(Configuration conf) {
-    executionSummarizer.start(conf);
-    clusterSummarizer.start(conf);
-  }
-  
-  /**
-   * This finalizes the summarizer.
-   */
-  @SuppressWarnings("unchecked")
-  void finalize(JobFactory factory, String path, long size, 
-                UserResolver resolver, DataStatistics stats, Configuration conf)
-  throws IOException {
-    executionSummarizer.finalize(factory, path, size, resolver, stats, conf);
-  }
-  
-  /**
-   * Summarizes the current {@link Gridmix} run and the cluster used. 
-   */
-  @Override
-  public String toString() {
-    StringBuilder builder = new StringBuilder();
-    builder.append(executionSummarizer.toString());
-    builder.append(clusterSummarizer.toString());
-    return builder.toString();
-  }
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
deleted file mode 100644
index ca8c98b98da..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/UserResolver.java
+++ /dev/null
@@ -1,65 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.net.URI;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.classification.InterfaceStability;
-
-/**
- * Maps users in the trace to a set of valid target users on the test cluster.
- */
-@InterfaceAudience.Private
-@InterfaceStability.Evolving
-public interface UserResolver {
-
-  /**
-   * Configure the user map given the URI and configuration. The resolver's
-   * contract will define how the resource will be interpreted, but the default
-   * will typically interpret the URI as a {@link org.apache.hadoop.fs.Path}
-   * listing target users.
-   * This method should be called only if {@link #needsTargetUsersList()}
-   * returns true.
-   * @param userdesc URI from which user information may be loaded per the
-   * subclass contract.
-   * @param conf The tool configuration.
-   * @return true if the resource provided was used in building the list of
-   * target users
-   */
-  public boolean setTargetUsers(URI userdesc, Configuration conf)
-    throws IOException;
-
-  /**
-   * Map the given UGI to another per the subclass contract.
-   * @param ugi User information from the trace.
-   */
-  public UserGroupInformation getTargetUgi(UserGroupInformation ugi);
-
-  /**
-   * Indicates whether this user resolver needs a list of target users to be
-   * provided.
-   *
-   * @return true if a list of target users is to be provided for this
-   * user resolver
-   */
-  public boolean needsTargetUsersList();
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
deleted file mode 100644
index 9f038073366..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/CumulativeCpuUsageEmulatorPlugin.java
+++ /dev/null
@@ -1,324 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
-
-import java.io.IOException;
-import java.util.Random;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.gridmix.Progressive;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin;
-
-/**
- * <p>A {@link ResourceUsageEmulatorPlugin} that emulates the cumulative CPU 
- * usage by performing certain CPU intensive operations. Performing such CPU 
- * intensive operations essentially uses up some CPU. Every 
- * {@link ResourceUsageEmulatorPlugin} is configured with a feedback module i.e 
- * a {@link ResourceCalculatorPlugin}, to monitor the resource usage.</p>
- * 
- * <p>{@link CumulativeCpuUsageEmulatorPlugin} emulates the CPU usage in steps. 
- * The frequency of emulation can be configured via 
- * {@link #CPU_EMULATION_PROGRESS_INTERVAL}.
- * CPU usage values are matched via emulation only on the interval boundaries.
- * </p>
- *  
- * {@link CumulativeCpuUsageEmulatorPlugin} is a wrapper program for managing 
- * the CPU usage emulation feature. It internally uses an emulation algorithm 
- * (called as core and described using {@link CpuUsageEmulatorCore}) for 
- * performing the actual emulation. Multiple calls to this core engine should 
- * use up some amount of CPU.<br>
- * 
- * <p>{@link CumulativeCpuUsageEmulatorPlugin} provides a calibration feature 
- * via {@link #initialize(Configuration, ResourceUsageMetrics, 
- *                        ResourceCalculatorPlugin, Progressive)} to calibrate 
- *  the plugin and its core for the underlying hardware. As a result of 
- *  calibration, every call to the emulation engine's core should roughly use up
- *  1% of the total usage value to be emulated. This makes sure that the 
- *  underlying hardware is profiled before use and that the plugin doesn't 
- *  accidently overuse the CPU. With 1% as the unit emulation target value for 
- *  the core engine, there will be roughly 100 calls to the engine resulting in 
- *  roughly 100 calls to the feedback (resource usage monitor) module. 
- *  Excessive usage of the feedback module is discouraged as 
- *  it might result into excess CPU usage resulting into no real CPU emulation.
- *  </p>
- */
-public class CumulativeCpuUsageEmulatorPlugin 
-implements ResourceUsageEmulatorPlugin {
-  protected CpuUsageEmulatorCore emulatorCore;
-  private ResourceCalculatorPlugin monitor;
-  private Progressive progress;
-  private boolean enabled = true;
-  private float emulationInterval; // emulation interval
-  private long targetCpuUsage = 0;
-  private float lastSeenProgress = 0;
-  private long lastSeenCpuUsage = 0;
-  
-  // Configuration parameters
-  public static final String CPU_EMULATION_PROGRESS_INTERVAL = 
-    "gridmix.emulators.resource-usage.cpu.emulation-interval";
-  private static final float DEFAULT_EMULATION_FREQUENCY = 0.1F; // 10 times
-
-  /**
-   * This is the core CPU usage emulation algorithm. This is the core engine
-   * which actually performs some CPU intensive operations to consume some
-   * amount of CPU. Multiple calls of {@link #compute()} should help the 
-   * plugin emulate the desired level of CPU usage. This core engine can be
-   * calibrated using the {@link #calibrate(ResourceCalculatorPlugin, long)}
-   * API to suit the underlying hardware better. It also can be used to optimize
-   * the emulation cycle.
-   */
-  public interface CpuUsageEmulatorCore {
-    /**
-     * Performs some computation to use up some CPU.
-     */
-    public void compute();
-    
-    /**
-     * Allows the core to calibrate itself.
-     */
-    public void calibrate(ResourceCalculatorPlugin monitor, 
-                          long totalCpuUsage);
-  }
-  
-  /**
-   * This is the core engine to emulate the CPU usage. The only responsibility 
-   * of this class is to perform certain math intensive operations to make sure 
-   * that some desired value of CPU is used.
-   */
-  public static class DefaultCpuUsageEmulator implements CpuUsageEmulatorCore {
-    // number of times to loop for performing the basic unit computation
-    private int numIterations;
-    private final Random random;
-    
-    /**
-     * This is to fool the JVM and make it think that we need the value 
-     * stored in the unit computation i.e {@link #compute()}. This will prevent
-     * the JVM from optimizing the code.
-     */
-    protected double returnValue;
-    
-    /**
-     * Initialized the {@link DefaultCpuUsageEmulator} with default values. 
-     * Note that the {@link DefaultCpuUsageEmulator} should be calibrated 
-     * (see {@link #calibrate(ResourceCalculatorPlugin, long)}) when initialized
-     * using this constructor.
-     */
-    public DefaultCpuUsageEmulator() {
-      this(-1);
-    }
-    
-    DefaultCpuUsageEmulator(int numIterations) {
-      this.numIterations = numIterations;
-      random = new Random();
-    }
-    
-    /**
-     * This will consume some desired level of CPU. This API will try to use up
-     * 'X' percent of the target cumulative CPU usage. Currently X is set to 
-     * 10%.
-     */
-    public void compute() {
-      for (int i = 0; i < numIterations; ++i) {
-        performUnitComputation();
-      }
-    }
-    
-    // Perform unit computation. The complete CPU emulation will be based on 
-    // multiple invocations to this unit computation module.
-    protected void performUnitComputation() {
-      //TODO can this be configurable too. Users/emulators should be able to 
-      // pick and choose what MATH operations to run.
-      // Example :
-      //           BASIC : ADD, SUB, MUL, DIV
-      //           ADV   : SQRT, SIN, COSIN..
-      //           COMPO : (BASIC/ADV)*
-      // Also define input generator. For now we can use the random number 
-      // generator. Later this can be changed to accept multiple sources.
-      
-      int randomData = random.nextInt();
-      int randomDataCube = randomData * randomData * randomData;
-      double randomDataCubeRoot = Math.cbrt(randomData);
-      returnValue = Math.log(Math.tan(randomDataCubeRoot 
-                                      * Math.exp(randomDataCube)) 
-                             * Math.sqrt(randomData));
-    }
-    
-    /**
-     * This will calibrate the algorithm such that a single invocation of
-     * {@link #compute()} emulates roughly 1% of the total desired resource 
-     * usage value.
-     */
-    public void calibrate(ResourceCalculatorPlugin monitor, 
-                          long totalCpuUsage) {
-      long initTime = monitor.getCumulativeCpuTime();
-      
-      long defaultLoopSize = 0;
-      long finalTime = initTime;
-      
-      //TODO Make this configurable
-      while (finalTime - initTime < 100) { // 100 ms
-        ++defaultLoopSize;
-        performUnitComputation(); //perform unit computation
-        finalTime = monitor.getCumulativeCpuTime();
-      }
-      
-      long referenceRuntime = finalTime - initTime;
-      
-      // time for one loop = (final-time - init-time) / total-loops
-      float timePerLoop = ((float)referenceRuntime) / defaultLoopSize;
-      
-      // compute the 1% of the total CPU usage desired
-      //TODO Make this configurable
-      long onePercent = totalCpuUsage / 100;
-      
-      // num-iterations for 1% = (total-desired-usage / 100) / time-for-one-loop
-      numIterations = Math.max(1, (int)((float)onePercent/timePerLoop));
-      
-      System.out.println("Calibration done. Basic computation runtime : " 
-          + timePerLoop + " milliseconds. Optimal number of iterations (1%): " 
-          + numIterations);
-    }
-  }
-  
-  public CumulativeCpuUsageEmulatorPlugin() {
-    this(new DefaultCpuUsageEmulator());
-  }
-  
-  /**
-   * For testing.
-   */
-  public CumulativeCpuUsageEmulatorPlugin(CpuUsageEmulatorCore core) {
-    emulatorCore = core;
-  }
-  
-  // Note that this weighing function uses only the current progress. In future,
-  // this might depend on progress, emulation-interval and expected target.
-  private float getWeightForProgressInterval(float progress) {
-    // we want some kind of exponential growth function that gives less weight
-    // on lower progress boundaries but high (exact emulation) near progress 
-    // value of 1.
-    // so here is how the current growth function looks like
-    //    progress    weight
-    //      0.1       0.0001
-    //      0.2       0.0016
-    //      0.3       0.0081
-    //      0.4       0.0256
-    //      0.5       0.0625
-    //      0.6       0.1296
-    //      0.7       0.2401
-    //      0.8       0.4096
-    //      0.9       0.6561
-    //      1.0       1.000
-    
-    return progress * progress * progress * progress;
-  }
-  
-  private synchronized long getCurrentCPUUsage() {
-    return monitor.getCumulativeCpuTime();
-  }
-  
-  @Override
-  public float getProgress() {
-    return enabled 
-           ? Math.min(1f, ((float)getCurrentCPUUsage())/targetCpuUsage)
-           : 1.0f;
-  }
-  
-  @Override
-  //TODO Multi-threading for speedup?
-  public void emulate() throws IOException, InterruptedException {
-    if (enabled) {
-      float currentProgress = progress.getProgress();
-      if (lastSeenProgress < currentProgress 
-          && ((currentProgress - lastSeenProgress) >= emulationInterval
-              || currentProgress == 1)) {
-        // Estimate the final cpu usage
-        //
-        //   Consider the following
-        //     Cl/Cc/Cp : Last/Current/Projected Cpu usage
-        //     Pl/Pc/Pp : Last/Current/Projected progress
-        //   Then
-        //     (Cp-Cc)/(Pp-Pc) = (Cc-Cl)/(Pc-Pl)
-        //   Solving this for Cp, we get
-        //     Cp = Cc + (1-Pc)*(Cc-Cl)/Pc-Pl)
-        //   Note that (Cc-Cl)/(Pc-Pl) is termed as 'rate' in the following 
-        //   section
-        
-        long currentCpuUsage = getCurrentCPUUsage();
-        // estimate the cpu usage rate
-        float rate = (currentCpuUsage - lastSeenCpuUsage)
-                     / (currentProgress - lastSeenProgress);
-        long projectedUsage = 
-          currentCpuUsage + (long)((1 - currentProgress) * rate);
-        
-        if (projectedUsage < targetCpuUsage) {
-          // determine the correction factor between the current usage and the
-          // expected usage and add some weight to the target
-          long currentWeighedTarget = 
-            (long)(targetCpuUsage 
-                   * getWeightForProgressInterval(currentProgress));
-          
-          while (getCurrentCPUUsage() < currentWeighedTarget) {
-            emulatorCore.compute();
-            // sleep for 100ms
-            try {
-              Thread.sleep(100);
-            } catch (InterruptedException ie) {
-              String message = 
-                "CumulativeCpuUsageEmulatorPlugin got interrupted. Exiting.";
-              throw new RuntimeException(message);
-            }
-          }
-        }
-        
-        // set the last seen progress
-        lastSeenProgress = progress.getProgress();
-        // set the last seen usage
-        lastSeenCpuUsage = getCurrentCPUUsage();
-      }
-    }
-  }
-
-  @Override
-  public void initialize(Configuration conf, ResourceUsageMetrics metrics,
-                         ResourceCalculatorPlugin monitor,
-                         Progressive progress) {
-    this.monitor = monitor;
-    this.progress = progress;
-    
-    // get the target CPU usage
-    targetCpuUsage = metrics.getCumulativeCpuUsage();
-    if (targetCpuUsage <= 0 ) {
-      enabled = false;
-      return;
-    } else {
-      enabled = true;
-    }
-    
-    emulationInterval =  conf.getFloat(CPU_EMULATION_PROGRESS_INTERVAL, 
-                                       DEFAULT_EMULATION_FREQUENCY);
-    
-    // calibrate the core cpu-usage utility
-    emulatorCore.calibrate(monitor, targetCpuUsage);
-    
-    // initialize the states
-    lastSeenProgress = 0;
-    lastSeenCpuUsage = 0;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
deleted file mode 100644
index 7a80e8df6c8..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageEmulatorPlugin.java
+++ /dev/null
@@ -1,63 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
-
-import java.io.IOException;
-
-import org.apache.hadoop.mapred.gridmix.Progressive;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.conf.Configuration;
-
-/**
- * <p>Each resource to be emulated should have a corresponding implementation 
- * class that implements {@link ResourceUsageEmulatorPlugin}.</p>
- * <br><br>
- * {@link ResourceUsageEmulatorPlugin} will be configured using the 
- * {@link #initialize(Configuration, ResourceUsageMetrics, 
- *                    ResourceCalculatorPlugin, Progressive)} call.
- * Every 
- * {@link ResourceUsageEmulatorPlugin} is also configured with a feedback module
- * i.e a {@link ResourceCalculatorPlugin}, to monitor the current resource 
- * usage. {@link ResourceUsageMetrics} decides the final resource usage value to
- * emulate. {@link Progressive} keeps track of the task's progress.
- * 
- * <br><br>
- * 
- * For configuring GridMix to load and and use a resource usage emulator, 
- * see {@link ResourceUsageMatcher}. 
- */
-public interface ResourceUsageEmulatorPlugin extends Progressive {
-  /**
-   * Initialize the plugin. This might involve
-   *   - initializing the variables
-   *   - calibrating the plugin
-   */
-  void initialize(Configuration conf, ResourceUsageMetrics metrics, 
-                  ResourceCalculatorPlugin monitor,
-                  Progressive progress);
-
-  /**
-   * Emulate the resource usage to match the usage target. The plugin can use
-   * the given {@link ResourceCalculatorPlugin} to query for the current 
-   * resource usage.
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  void emulate() throws IOException, InterruptedException;
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
deleted file mode 100644
index 38095800111..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/ResourceUsageMatcher.java
+++ /dev/null
@@ -1,111 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.gridmix.Progressive;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.util.ReflectionUtils;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin;
-
-/**
- * <p>This is the driver class for managing all the resource usage emulators.
- * {@link ResourceUsageMatcher} expects a comma separated list of 
- * {@link ResourceUsageEmulatorPlugin} implementations specified using 
- * {@link #RESOURCE_USAGE_EMULATION_PLUGINS} as the configuration parameter.</p>
- * 
- * <p>Note that the order in which the emulators are invoked is same as the 
- * order in which they are configured.
- */
-public class ResourceUsageMatcher implements Progressive {
-  /**
-   * Configuration key to set resource usage emulators.
-   */
-  public static final String RESOURCE_USAGE_EMULATION_PLUGINS =
-    "gridmix.emulators.resource-usage.plugins";
-  
-  private List<ResourceUsageEmulatorPlugin> emulationPlugins = 
-    new ArrayList<ResourceUsageEmulatorPlugin>();
-  
-  /**
-   * Configure the {@link ResourceUsageMatcher} to load the configured plugins
-   * and initialize them.
-   */
-  @SuppressWarnings("unchecked")
-  public void configure(Configuration conf, ResourceCalculatorPlugin monitor, 
-                        ResourceUsageMetrics metrics, Progressive progress) {
-    Class[] plugins = conf.getClasses(RESOURCE_USAGE_EMULATION_PLUGINS);
-    if (plugins == null) {
-      System.out.println("No resource usage emulator plugins configured.");
-    } else {
-      for (Class clazz : plugins) {
-        if (clazz != null) {
-          if (ResourceUsageEmulatorPlugin.class.isAssignableFrom(clazz)) {
-            ResourceUsageEmulatorPlugin plugin = 
-              (ResourceUsageEmulatorPlugin) ReflectionUtils.newInstance(clazz, 
-                                                                        conf);
-            emulationPlugins.add(plugin);
-          } else {
-            throw new RuntimeException("Misconfigured resource usage plugins. " 
-                + "Class " + clazz.getClass().getName() + " is not a resource "
-                + "usage plugin as it does not extend "
-                + ResourceUsageEmulatorPlugin.class.getName());
-          }
-        }
-      }
-    }
-
-    // initialize the emulators once all the configured emulator plugins are
-    // loaded
-    for (ResourceUsageEmulatorPlugin emulator : emulationPlugins) {
-      emulator.initialize(conf, metrics, monitor, progress);
-    }
-  }
-  
-  public void matchResourceUsage() throws IOException, InterruptedException {
-    for (ResourceUsageEmulatorPlugin emulator : emulationPlugins) {
-      // match the resource usage
-      emulator.emulate();
-    }
-  }
-  
-  /**
-   * Returns the average progress.
-   */
-  @Override
-  public float getProgress() {
-    if (emulationPlugins.size() > 0) {
-      // return the average progress
-      float progress = 0f;
-      for (ResourceUsageEmulatorPlugin emulator : emulationPlugins) {
-        // consider weighted progress of each emulator
-        progress += emulator.getProgress();
-      }
-
-      return progress / emulationPlugins.size();
-    }
-    
-    // if no emulators are configured then return 1
-    return 1f;
-    
-  }
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java b/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
deleted file mode 100644
index 16a26729640..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/java/org/apache/hadoop/mapred/gridmix/emulators/resourceusage/TotalHeapUsageEmulatorPlugin.java
+++ /dev/null
@@ -1,279 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix.emulators.resourceusage;
-
-import java.io.IOException;
-import java.util.ArrayList;
-
-import org.apache.hadoop.classification.VisibleForTesting;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.gridmix.Progressive;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin;
-
-/**
- * <p>A {@link ResourceUsageEmulatorPlugin} that emulates the total heap 
- * usage by loading the JVM heap memory. Adding smaller chunks of data to the 
- * heap will essentially use up some heap space thus forcing the JVM to expand 
- * its heap and thus resulting into increase in the heap usage.</p>
- * 
- * <p>{@link TotalHeapUsageEmulatorPlugin} emulates the heap usage in steps. 
- * The frequency of emulation can be configured via 
- * {@link #HEAP_EMULATION_PROGRESS_INTERVAL}.
- * Heap usage values are matched via emulation only at specific interval 
- * boundaries.
- * </p>
- *  
- * {@link TotalHeapUsageEmulatorPlugin} is a wrapper program for managing 
- * the heap usage emulation feature. It internally uses an emulation algorithm 
- * (called as core and described using {@link HeapUsageEmulatorCore}) for 
- * performing the actual emulation. Multiple calls to this core engine should 
- * use up some amount of heap.
- */
-public class TotalHeapUsageEmulatorPlugin 
-implements ResourceUsageEmulatorPlugin {
-  // Configuration parameters
-  //  the core engine to emulate heap usage
-  protected HeapUsageEmulatorCore emulatorCore;
-  //  the progress bar
-  private Progressive progress;
-  //  decides if this plugin can emulate heap usage or not
-  private boolean enabled = true;
-  //  the progress boundaries/interval where emulation should be done
-  private float emulationInterval;
-  //  target heap usage to emulate
-  private long targetHeapUsageInMB = 0;
-  
-  /**
-   * The frequency (based on task progress) with which memory-emulation code is
-   * run. If the value is set to 0.1 then the emulation will happen at 10% of 
-   * the task's progress. The default value of this parameter is 
-   * {@link #DEFAULT_EMULATION_PROGRESS_INTERVAL}.
-   */
-  public static final String HEAP_EMULATION_PROGRESS_INTERVAL = 
-    "gridmix.emulators.resource-usage.heap.emulation-interval";
-  
-  // Default value for emulation interval
-  private static final float DEFAULT_EMULATION_PROGRESS_INTERVAL = 0.1F; // 10 %
-
-  private float prevEmulationProgress = 0F;
-  
-  /**
-   * The minimum buffer reserved for other non-emulation activities.
-   */
-  public static final String MIN_HEAP_FREE_RATIO = 
-    "gridmix.emulators.resource-usage.heap.min-free-ratio";
-  
-  private float minFreeHeapRatio;
-  
-  private static final float DEFAULT_MIN_FREE_HEAP_RATIO = 0.3F;
-  
-  /**
-   * Determines the unit increase per call to the core engine's load API. This
-   * is expressed as a percentage of the difference between the expected total 
-   * heap usage and the current usage. 
-   */
-  public static final String HEAP_LOAD_RATIO = 
-    "gridmix.emulators.resource-usage.heap.load-ratio";
-  
-  private float heapLoadRatio;
-  
-  private static final float DEFAULT_HEAP_LOAD_RATIO = 0.1F;
-  
-  public static final int ONE_MB = 1024 * 1024;
-  
-  /**
-   * Defines the core heap usage emulation algorithm. This engine is expected
-   * to perform certain memory intensive operations to consume some
-   * amount of heap. {@link #load(long)} should load the current heap and 
-   * increase the heap usage by the specified value. This core engine can be 
-   * initialized using the {@link #initialize(ResourceCalculatorPlugin, long)} 
-   * API to suit the underlying hardware better.
-   */
-  public interface HeapUsageEmulatorCore {
-    /**
-     * Performs some memory intensive operations to use up some heap.
-     */
-    public void load(long sizeInMB);
-    
-    /**
-     * Initialize the core.
-     */
-    public void initialize(ResourceCalculatorPlugin monitor, 
-                           long totalHeapUsageInMB);
-    
-    /**
-     * Reset the resource usage
-     */
-    public void reset();
-  }
-  
-  /**
-   * This is the core engine to emulate the heap usage. The only responsibility 
-   * of this class is to perform certain memory intensive operations to make 
-   * sure that some desired value of heap is used.
-   */
-  public static class DefaultHeapUsageEmulator 
-  implements HeapUsageEmulatorCore {
-    // store the unit loads in a list
-    private static final ArrayList<Object> heapSpace =
-        new ArrayList<Object>();
-    
-    /**
-     * Increase heap usage by current process by the given amount.
-     * This is done by creating objects each of size 1MB.
-     */
-    public void load(long sizeInMB) {
-      for (long i = 0; i < sizeInMB; ++i) {
-        // Create another String object of size 1MB
-        heapSpace.add((Object)new byte[ONE_MB]);
-      }
-    }
-
-    /**
-     * Gets the total number of 1mb objects stored in the emulator.
-     *
-     * @return total number of 1mb objects.
-     */
-    @VisibleForTesting
-    public int getHeapSpaceSize() {
-      return heapSpace.size();
-    }
-
-    /**
-     * This will initialize the core and check if the core can emulate the 
-     * desired target on the underlying hardware.
-     */
-    public void initialize(ResourceCalculatorPlugin monitor, 
-                           long totalHeapUsageInMB) {
-      long maxPhysicalMemoryInMB = monitor.getPhysicalMemorySize() / ONE_MB ;
-      if(maxPhysicalMemoryInMB < totalHeapUsageInMB) {
-        throw new RuntimeException("Total heap the can be used is " 
-            + maxPhysicalMemoryInMB 
-            + " bytes while the emulator is configured to emulate a total of " 
-            + totalHeapUsageInMB + " bytes");
-      }
-    }
-    
-    /**
-     * Clear references to all the GridMix-allocated special objects so that 
-     * heap usage is reduced.
-     */
-    @Override
-    public void reset() {
-      heapSpace.clear();
-    }
-  }
-  
-  public TotalHeapUsageEmulatorPlugin() {
-    this(new DefaultHeapUsageEmulator());
-  }
-  
-  /**
-   * For testing.
-   */
-  public TotalHeapUsageEmulatorPlugin(HeapUsageEmulatorCore core) {
-    emulatorCore = core;
-  }
-  
-  protected long getTotalHeapUsageInMB() {
-    return Runtime.getRuntime().totalMemory() / ONE_MB;
-  }
-  
-  protected long getMaxHeapUsageInMB() {
-    return Runtime.getRuntime().maxMemory() / ONE_MB;
-  }
-  
-  @Override
-  public float getProgress() {
-    return enabled 
-           ? Math.min(1f, ((float)getTotalHeapUsageInMB())/targetHeapUsageInMB)
-           : 1.0f;
-  }
-  
-  @Override
-  public void emulate() throws IOException, InterruptedException {
-    if (enabled) {
-      float currentProgress = progress.getProgress();
-      if (prevEmulationProgress < currentProgress 
-          && ((currentProgress - prevEmulationProgress) >= emulationInterval
-              || currentProgress == 1)) {
-
-        long maxHeapSizeInMB = getMaxHeapUsageInMB();
-        long committedHeapSizeInMB = getTotalHeapUsageInMB();
-        
-        // Increase committed heap usage, if needed
-        // Using a linear weighing function for computing the expected usage
-        long expectedHeapUsageInMB = 
-          Math.min(maxHeapSizeInMB,
-                   (long) (targetHeapUsageInMB * currentProgress));
-        if (expectedHeapUsageInMB < maxHeapSizeInMB
-            && committedHeapSizeInMB < expectedHeapUsageInMB) {
-          long bufferInMB = (long)(minFreeHeapRatio * expectedHeapUsageInMB);
-          long currentDifferenceInMB = 
-            expectedHeapUsageInMB - committedHeapSizeInMB;
-          long currentIncrementLoadSizeInMB = 
-                (long)(currentDifferenceInMB * heapLoadRatio);
-          // Make sure that at least 1 MB is incremented.
-          currentIncrementLoadSizeInMB = 
-            Math.max(1, currentIncrementLoadSizeInMB);
-          while (committedHeapSizeInMB + bufferInMB < expectedHeapUsageInMB) {
-            // add blocks in order of X% of the difference, X = 10% by default
-            emulatorCore.load(currentIncrementLoadSizeInMB);
-            committedHeapSizeInMB = getTotalHeapUsageInMB();
-          }
-        }
-        
-        // store the emulation progress boundary
-        prevEmulationProgress = currentProgress;
-      }
-      
-      // reset the core so that the garbage is reclaimed
-      emulatorCore.reset();
-    }
-  }
-
-  @Override
-  public void initialize(Configuration conf, ResourceUsageMetrics metrics,
-                         ResourceCalculatorPlugin monitor,
-                         Progressive progress) {
-    this.progress = progress;
-    
-    // get the target heap usage
-    targetHeapUsageInMB = metrics.getHeapUsage() / ONE_MB;
-    if (targetHeapUsageInMB <= 0 ) {
-      enabled = false;
-      return;
-    } else {
-      // calibrate the core heap-usage utility
-      emulatorCore.initialize(monitor, targetHeapUsageInMB);
-      enabled = true;
-    }
-    
-    emulationInterval = 
-      conf.getFloat(HEAP_EMULATION_PROGRESS_INTERVAL, 
-                    DEFAULT_EMULATION_PROGRESS_INTERVAL);
-    
-    minFreeHeapRatio = conf.getFloat(MIN_HEAP_FREE_RATIO, 
-                                     DEFAULT_MIN_FREE_HEAP_RATIO);
-    
-    heapLoadRatio = conf.getFloat(HEAP_LOAD_RATIO, DEFAULT_HEAP_LOAD_RATIO);
-    
-    prevEmulationProgress = 0;
-  }
-}
\ No newline at end of file
diff --git a/hadoop-tools/hadoop-gridmix/src/main/shellprofile.d/hadoop-gridmix.sh b/hadoop-tools/hadoop-gridmix/src/main/shellprofile.d/hadoop-gridmix.sh
deleted file mode 100755
index 55997d04e3a..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/main/shellprofile.d/hadoop-gridmix.sh
+++ /dev/null
@@ -1,36 +0,0 @@
-#!/usr/bin/env bash
-
-# Licensed to the Apache Software Foundation (ASF) under one or more
-# contributor license agreements.  See the NOTICE file distributed with
-# this work for additional information regarding copyright ownership.
-# The ASF licenses this file to You under the Apache License, Version 2.0
-# (the "License"); you may not use this file except in compliance with
-# the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-
-if ! declare -f hadoop_subcommand_gridmix >/dev/null 2>/dev/null; then
-
-  if [[ "${HADOOP_SHELL_EXECNAME}" = hadoop ]]; then
-    hadoop_add_subcommand "gridmix" client "submit a mix of synthetic job, modeling a profiled from production load"
-  fi
-
-## @description  gridmix command for hadoop
-## @audience     public
-## @stability    stable
-## @replaceable  yes
-function hadoop_subcommand_gridmix
-{
-  # shellcheck disable=SC2034
-  HADOOP_CLASSNAME=org.apache.hadoop.mapred.gridmix.Gridmix
-  hadoop_add_to_classpath_tools hadoop-rumen
-  hadoop_add_to_classpath_tools hadoop-gridmix
-}
-
-fi
diff --git a/hadoop-tools/hadoop-gridmix/src/site/markdown/GridMix.md.vm b/hadoop-tools/hadoop-gridmix/src/site/markdown/GridMix.md.vm
deleted file mode 100644
index 0b18d4f8e9a..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/site/markdown/GridMix.md.vm
+++ /dev/null
@@ -1,807 +0,0 @@
-<!---
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-
-Gridmix
-=======
-
----
-
-- [Overview](#Overview)
-- [Usage](#Usage)
-- [General Configuration Parameters](#General_Configuration_Parameters)
-- [Job Types](#Job_Types)
-- [Job Submission Policies](#Job_Submission_Policies)
-- [Emulating Users and Queues](#Emulating_Users_and_Queues)
-- [Emulating Distributed Cache Load](#Emulating_Distributed_Cache_Load)
-- [Configuration of Simulated Jobs](#Configuration_of_Simulated_Jobs)
-- [Emulating Compression/Decompression](#Emulating_CompressionDecompression)
-- [Emulating High-Ram jobs](#Emulating_High-Ram_jobs)
-- [Emulating resource usages](#Emulating_resource_usages)
-- [Simplifying Assumptions](#Simplifying_Assumptions)
-- [Appendix](#Appendix)
-
----
-
-Overview
---------
-
-GridMix is a benchmark for Hadoop clusters. It submits a mix of
-synthetic jobs, modeling a profile mined from production loads.
-This version of the tool will attempt to model
-the resource profiles of production jobs to identify bottlenecks, guide
-development.
-
-To run GridMix, you need a MapReduce job trace describing the job mix
-for a given cluster. Such traces are typically generated by
-[Rumen](../hadoop-rumen/Rumen.html).
-GridMix also requires input data from which the
-synthetic jobs will be reading bytes. The input data need not be in any
-particular format, as the synthetic jobs are currently binary readers.
-If you are running on a new cluster, an optional step generating input
-data may precede the run.
-In order to emulate the load of production jobs from a given cluster
-on the same or another cluster, follow these steps:
-
-1.  Locate the job history files on the production cluster. This
-    location is specified by the
-    `mapreduce.jobhistory.done-dir` or 
-    `mapreduce.jobhistory.intermediate-done-dir`
-    configuration property of the cluster.
-    ([MapReduce historyserver](../hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapredCommands.html#historyserver)
-    moves job history files from `mapreduce.jobhistory.done-dir`
-    to `mapreduce.jobhistory.intermediate-done-dir`.)
-    
-2.  Run [Rumen](../hadoop-rumen/Rumen.html)
-    to build a job trace in JSON format for all or select jobs.
-
-3.  Use GridMix with the job trace on the benchmark cluster.
-
-Jobs submitted by GridMix have names of the form
-"`GRIDMIXnnnnnn`", where
-"`nnnnnn`" is a sequence number padded with leading zeroes.
-
-
-Usage
------
-
-Gridmix is provided as hadoop subcommand. Basic command-line usage without configuration parameters:
-
-```
-$ hadoop gridmix [-generate <size>] [-users <users-list>] <iopath> <trace>
-```
-
-Basic command-line usage with configuration parameters:
-
-```
-$ hadoop gridmix \
-  -Dgridmix.client.submit.threads=10 -Dgridmix.output.directory=foo \
-  [-generate <size>] [-users <users-list>] <iopath> <trace>
-```
-
-> Configuration parameters like
-> `-Dgridmix.client.submit.threads=10` and
-> `-Dgridmix.output.directory=foo` as given above should
-> be used  *before*  other GridMix parameters.
-
-The `<iopath>` parameter is the working directory for
-GridMix. Note that this can either be on the local file-system
-or on HDFS, but it is highly recommended that it be the same as that for
-the original job mix so that GridMix puts the same load on the local
-file-system and HDFS respectively.
-
-The `-generate` option is used to generate input data and
-Distributed Cache files for the synthetic jobs. It accepts standard units
-of size suffixes, e.g. `100g` will generate
-100 * 2<sup>30</sup> bytes as input data.
-The minimum size of input data in compressed format (128MB by default)
-is defined by `gridmix.min.file.size`.
-`<iopath>/input` is the destination directory for
-generated input data and/or the directory from which input data will be
-read. HDFS-based Distributed Cache files are generated under the
-distributed cache directory `<iopath>/distributedCache`.
-If some of the needed Distributed Cache files are already existing in the
-distributed cache directory, then only the remaining non-existing
-Distributed Cache files are generated when `-generate` option
-is specified.
-
-The `-users` option is used to point to a users-list
-file (see <a href="#usersqueues">Emulating Users and Queues</a>).
-
-The `<trace>` parameter is a path to a job trace
-generated by Rumen. This trace can be compressed (it must be readable
-using one of the compression codecs supported by the cluster) or
-uncompressed. Use "-" as the value of this parameter if you
-want to pass an  *uncompressed*  trace via the standard
-input-stream of GridMix.
-
-The supported configuration parameters are explained in the
-following sections.
-
-
-General Configuration Parameters
---------------------------------
-
-<table>
-  <tr>
-    <th>Parameter</th>
-    <th>Description</th>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.output.directory</code>
-    </td>
-    <td>The directory into which output will be written. If specified,
-    <code>iopath</code> will be relative to this parameter. The
-    submitting user must have read/write access to this directory. The
-    user should also be mindful of any quota issues that may arise
-    during a run. The default is "<code>gridmix</code>".</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.client.submit.threads</code>
-    </td>
-    <td>The number of threads submitting jobs to the cluster. This
-    also controls how many splits will be loaded into memory at a given
-    time, pending the submit time in the trace. Splits are pre-generated
-    to hit submission deadlines, so particularly dense traces may want
-    more submitting threads. However, storing splits in memory is
-    reasonably expensive, so you should raise this cautiously. The
-    default is 1 for the SERIAL job-submission policy (see
-    <a href="#policies">Job Submission Policies</a>) and one more than
-    the number of processors on the client machine for the other
-    policies.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.submit.multiplier</code>
-    </td>
-    <td>The multiplier to accelerate or decelerate the submission of
-    jobs. The time separating two jobs is multiplied by this factor.
-    The default value is 1.0. This is a crude mechanism to size
-    a job trace to a cluster.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.client.pending.queue.depth</code>
-    </td>
-    <td>The depth of the queue of job descriptions awaiting split
-    generation. The jobs read from the trace occupy a queue of this
-    depth before being processed by the submission threads. It is
-    unusual to configure this. The default is 5.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.gen.blocksize</code>
-    </td>
-    <td>The block-size of generated data. The default value is 256
-    MiB.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.gen.bytes.per.file</code>
-    </td>
-    <td>The maximum bytes written per file. The default value is 1
-    GiB.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.min.file.size</code>
-    </td>
-    <td>The minimum size of the input files. The default limit is 128
-    MiB. Tweak this parameter if you see an error-message like
-    "Found no satisfactory file" while testing GridMix with
-    a relatively-small input data-set.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.max.total.scan</code>
-    </td>
-    <td>The maximum size of the input files. The default limit is 100
-    TiB.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.task.jvm-options.enable</code>
-    </td>
-    <td>Enables Gridmix to configure the simulated task's max heap 
-        options using the values obtained from the original task (i.e via
-        trace).
-    </td>
-  </tr>
-</table>
-
-
-Job Types
----------
-
-GridMix takes as input a job trace, essentially a stream of
-JSON-encoded job descriptions. For each job description, the submission
-client obtains the original job submission time and for each task in
-that job, the byte and record counts read and written. Given this data,
-it constructs a synthetic job with the same byte and record patterns as
-recorded in the trace. It constructs jobs of two types:
-
-<table>
-  <tr>
-    <th>Job Type</th>
-    <th>Description</th>
-  </tr>
-  <tr>
-    <td>
-      <code>LOADJOB</code>
-    </td>
-    <td>A synthetic job that emulates the workload mentioned in Rumen
-    trace. In the current version we are supporting I/O. It reproduces
-    the I/O workload on the benchmark cluster. It does so by embedding
-    the detailed I/O information for every map and reduce task, such as
-    the number of bytes and records read and written, into each
-    job's input splits. The map tasks further relay the I/O patterns of
-    reduce tasks through the intermediate map output data.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>SLEEPJOB</code>
-    </td>
-    <td>A synthetic job where each task does  *nothing*  but sleep
-    for a certain duration as observed in the production trace. The
-    scalability of the ResourceManager is often limited by how many
-    heartbeats it can handle every second. (Heartbeats are periodic
-    messages sent from NodeManagers to update their status and grab new
-    tasks from the ResourceManager.) Since a benchmark cluster is typically
-    a fraction in size of a production cluster, the heartbeat traffic
-    generated by the slave nodes is well below the level of the
-    production cluster. One possible solution is to run multiple
-    NodeManagers on each slave node. This leads to the obvious problem that
-    the I/O workload generated by the synthetic jobs would thrash the
-    slave nodes. Hence the need for such a job.</td>
-  </tr>
-</table>
-
-The following configuration parameters affect the job type:
-
-<table>
-  <tr>
-    <th>Parameter</th>
-    <th>Description</th>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.job.type</code>
-    </td>
-    <td>The value for this key can be one of LOADJOB or SLEEPJOB. The
-    default value is LOADJOB.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.key.fraction</code>
-    </td>
-    <td>For a LOADJOB type of job, the fraction of a record used for
-    the data for the key. The default value is 0.1.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.sleep.maptask-only</code>
-    </td>
-    <td>For a SLEEPJOB type of job, whether to ignore the reduce
-    tasks for the job. The default is <code>false</code>.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.sleep.fake-locations</code>
-    </td>
-    <td>For a SLEEPJOB type of job, the number of fake locations
-    for map tasks for the job. The default is 0.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.sleep.max-map-time</code>
-    </td>
-    <td>For a SLEEPJOB type of job, the maximum runtime for map
-    tasks for the job in milliseconds. The default is unlimited.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.sleep.max-reduce-time</code>
-    </td>
-    <td>For a SLEEPJOB type of job, the maximum runtime for reduce
-    tasks for the job in milliseconds. The default is unlimited.</td>
-  </tr>
-</table>
-
-
-<a name="policies"></a>
-
-Job Submission Policies
------------------------
-
-GridMix controls the rate of job submission. This control can be
-based on the trace information or can be based on statistics it gathers
-from the ResourceManager. Based on the submission policies users define,
-GridMix uses the respective algorithm to control the job submission.
-There are currently three types of policies:
-
-<table>
-  <tr>
-    <th>Job Submission Policy</th>
-    <th>Description</th>
-  </tr>
-  <tr>
-    <td>
-      <code>STRESS</code>
-    </td>
-    <td>Keep submitting jobs so that the cluster remains under stress.
-    In this mode we control the rate of job submission by monitoring
-    the real-time load of the cluster so that we can maintain a stable
-    stress level of workload on the cluster. Based on the statistics we
-    gather we define if a cluster is  *underloaded*  or
-     *overloaded* . We consider a cluster  *underloaded*  if
-    and only if the following three conditions are true:
-    <ol>
-      <li>the number of pending and running jobs are under a threshold
-      TJ</li>
-      <li>the number of pending and running maps are under threshold
-      TM</li>
-      <li>the number of pending and running reduces are under threshold
-      TR</li>
-    </ol>
-    The thresholds TJ, TM and TR are proportional to the size of the
-    cluster and map, reduce slots capacities respectively. In case of a
-    cluster being  *overloaded* , we throttle the job submission.
-    In the actual calculation we also weigh each running task with its
-    remaining work - namely, a 90% complete task is only counted as 0.1
-    in calculation. Finally, to avoid a very large job blocking other
-    jobs, we limit the number of pending/waiting tasks each job can
-    contribute.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>REPLAY</code>
-    </td>
-    <td>In this mode we replay the job traces faithfully. This mode
-    exactly follows the time-intervals given in the actual job
-    trace.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>SERIAL</code>
-    </td>
-    <td>In this mode we submit the next job only once the job submitted
-    earlier is completed.</td>
-  </tr>
-</table>
-
-The following configuration parameters affect the job submission policy:
-
-<table>
-  <tr>
-    <th>Parameter</th>
-    <th>Description</th>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.job-submission.policy</code>
-    </td>
-    <td>The value for this key would be one of the three: STRESS, REPLAY
-    or SERIAL. In most of the cases the value of key would be STRESS or
-    REPLAY. The default value is STRESS.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.throttle.jobs-to-tracker-ratio</code>
-    </td>
-    <td>In STRESS mode, the minimum ratio of running jobs to
-    NodeManagers in a cluster for the cluster to be considered
-    *overloaded* . This is the threshold TJ referred to earlier.
-    The default is 1.0.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.throttle.maps.task-to-slot-ratio</code>
-    </td>
-    <td>In STRESS mode, the minimum ratio of pending and running map
-    tasks (i.e. incomplete map tasks) to the number of map slots for
-    a cluster for the cluster to be considered  *overloaded* .
-    This is the threshold TM referred to earlier. Running map tasks are
-    counted partially. For example, a 40% complete map task is counted
-    as 0.6 map tasks. The default is 2.0.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.throttle.reduces.task-to-slot-ratio</code>
-    </td>
-    <td>In STRESS mode, the minimum ratio of pending and running reduce
-    tasks (i.e. incomplete reduce tasks) to the number of reduce slots
-    for a cluster for the cluster to be considered  *overloaded* .
-    This is the threshold TR referred to earlier. Running reduce tasks
-    are counted partially. For example, a 30% complete reduce task is
-    counted as 0.7 reduce tasks. The default is 2.5.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.throttle.maps.max-slot-share-per-job</code>
-    </td>
-    <td>In STRESS mode, the maximum share of a cluster's map-slots
-    capacity that can be counted toward a job's incomplete map tasks in
-    overload calculation. The default is 0.1.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.throttle.reducess.max-slot-share-per-job</code>
-    </td>
-    <td>In STRESS mode, the maximum share of a cluster's reduce-slots
-    capacity that can be counted toward a job's incomplete reduce tasks
-    in overload calculation. The default is 0.1.</td>
-  </tr>
-</table>
-
-
-<a name="usersqueues"></a>
-
-Emulating Users and Queues
---------------------------
-
-Typical production clusters are often shared with different users and
-the cluster capacity is divided among different departments through job
-queues. Ensuring fairness among jobs from all users, honoring queue
-capacity allocation policies and avoiding an ill-behaving job from
-taking over the cluster adds significant complexity in Hadoop software.
-To be able to sufficiently test and discover bugs in these areas,
-GridMix must emulate the contentions of jobs from different users and/or
-submitted to different queues.
-
-Emulating multiple queues is easy - we simply set up the benchmark
-cluster with the same queue configuration as the production cluster and
-we configure synthetic jobs so that they get submitted to the same queue
-as recorded in the trace. However, not all users shown in the trace have
-accounts on the benchmark cluster. Instead, we set up a number of testing
-user accounts and associate each unique user in the trace to testing
-users in a round-robin fashion.
-
-The following configuration parameters affect the emulation of users
-and queues:
-
-<table>
-  <tr>
-    <th>Parameter</th>
-    <th>Description</th>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.job-submission.use-queue-in-trace</code>
-    </td>
-    <td>When set to <code>true</code> it uses exactly the same set of
-    queues as those mentioned in the trace. The default value is
-    <code>false</code>.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.job-submission.default-queue</code>
-    </td>
-    <td>Specifies the default queue to which all the jobs would be
-    submitted. If this parameter is not specified, GridMix uses the
-    default queue defined for the submitting user on the cluster.</td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.user.resolve.class</code>
-    </td>
-    <td>Specifies which <code>UserResolver</code> implementation to use.
-    We currently have three implementations:
-    <ol>
-      <li><code>org.apache.hadoop.mapred.gridmix.EchoUserResolver</code>
-      - submits a job as the user who submitted the original job. All
-      the users of the production cluster identified in the job trace
-      must also have accounts on the benchmark cluster in this case.</li>
-      <li><code>org.apache.hadoop.mapred.gridmix.SubmitterUserResolver</code>
-      - submits all the jobs as current GridMix user. In this case we
-      simply map all the users in the trace to the current GridMix user
-      and submit the job.</li>
-      <li><code>org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver</code>
-      - maps trace users to test users in a round-robin fashion. In
-      this case we set up a number of testing user accounts and
-      associate each unique user in the trace to testing users in a
-      round-robin fashion.</li>
-    </ol>
-    The default is
-    <code>org.apache.hadoop.mapred.gridmix.SubmitterUserResolver</code>.</td>
-  </tr>
-</table>
-
-If the parameter `gridmix.user.resolve.class` is set to
-`org.apache.hadoop.mapred.gridmix.RoundRobinUserResolver`,
-we need to define a users-list file with a list of test users.
-This is specified using the `-users` option to GridMix.
-
-<note>
-Specifying a users-list file using the `-users` option is
-mandatory when using the round-robin user-resolver. Other user-resolvers
-ignore this option.
-</note>
-
-A users-list file has one user per line, each line of the format:
-
-    <username>
-
-For example:
-
-    user1
-    user2
-    user3
-
-In the above example we have defined three users `user1`, `user2` and `user3`.
-Now we would associate each unique user in the trace to the above users
-defined in round-robin fashion. For example, if trace's users are
-`tuser1`, `tuser2`, `tuser3`, `tuser4` and `tuser5`, then the mappings would be:
-
-    tuser1 -> user1
-    tuser2 -> user2
-    tuser3 -> user3
-    tuser4 -> user1
-    tuser5 -> user2
-
-For backward compatibility reasons, each line of users-list file can
-contain username followed by groupnames in the form username[,group]*.
-The groupnames will be ignored by Gridmix.
-
-
-Emulating Distributed Cache Load
---------------------------------
-
-Gridmix emulates Distributed Cache load by default for LOADJOB type of
-jobs. This is done by precreating the needed Distributed Cache files for all
-the simulated jobs as part of a separate MapReduce job.
-
-Emulation of Distributed Cache load in gridmix simulated jobs can be
-disabled by configuring the property
-`gridmix.distributed-cache-emulation.enable` to
-`false`.
-But generation of Distributed Cache data by gridmix is driven by
-`-generate` option and is independent of this configuration
-property.
-
-Both generation of Distributed Cache files and emulation of
-Distributed Cache load are disabled if:
-
-* input trace comes from the standard input-stream instead of file, or
-* `<iopath>` specified is on local file-system, or
-* any of the ascendant directories of the distributed cache directory
-  i.e. `<iopath>/distributedCache` (including the distributed
-  cache directory) doesn't have execute permission for others.
-
-
-Configuration of Simulated Jobs
--------------------------------
-
-Gridmix3 sets some configuration properties in the simulated Jobs
-submitted by it so that they can be mapped back to the corresponding Job
-in the input Job trace. These configuration parameters include:
-
-<table>
-  <tr>
-    <th>Parameter</th>
-    <th>Description</th>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.job.original-job-id</code>
-    </td>
-    <td> The job id of the original cluster's job corresponding to this
-    simulated job.
-    </td>
-  </tr>
-  <tr>
-    <td>
-      <code>gridmix.job.original-job-name</code>
-    </td>
-    <td> The job name of the original cluster's job corresponding to this
-    simulated job.
-    </td>
-  </tr>
-</table>
-
-
-Emulating Compression/Decompression
------------------------------------
-
-MapReduce supports data compression and decompression. 
-Input to a MapReduce job can be compressed. Similarly, output of Map
-and Reduce tasks can also be compressed. Compression/Decompression 
-emulation in GridMix is important because emulating 
-compression/decompression will effect the CPU and Memory usage of the 
-task. A task emulating compression/decompression will affect other 
-tasks and daemons running on the same node.
-
-Compression emulation is enabled if 
-`gridmix.compression-emulation.enable` is set to
-`true`. By default compression emulation is enabled for 
-jobs of type  *LOADJOB* . With compression emulation enabled, 
-GridMix will now generate compressed text data with a constant 
-compression ratio. Hence a simulated GridMix job will now emulate 
-compression/decompression using compressible text data (having a 
-constant compression ratio), irrespective of the compression ratio 
-observed in the actual job.
-
-A typical MapReduce Job deals with data compression/decompression in 
-the following phases
-
-* `Job input data decompression: ` GridMix generates 
-  compressible input data when compression emulation is enabled. 
-  Based on the original job's configuration, a simulated GridMix job 
-  will use a decompressor to read the compressed input data. 
-  Currently, GridMix uses
-  `mapreduce.input.fileinputformat.inputdir` to determine 
-  if the original job used compressed input data or 
-  not. If the original job's input files are uncompressed then the 
-  simulated job will read the compressed input file without using a 
-  decompressor. 
-
-* `Intermediate data compression and decompression: `
-  If the original job has map output compression enabled then GridMix 
-  too will enable map output compression for the simulated job. 
-  Accordingly, the reducers will use a decompressor to read the map 
-  output data.
-
-* `Job output data compression: `
-  If the original job's output is compressed then GridMix 
-  too will enable job output compression for the simulated job. 
-       
-The following configuration parameters affect compression emulation
-
-<table>
-  <tr>
-    <th>Parameter</th>
-    <th>Description</th>
-  </tr>
-  <tr>
-    <td>gridmix.compression-emulation.enable</td>
-    <td>Enables compression emulation in simulated GridMix jobs. 
-        Default is true.</td>
-  </tr>
-</table>
-      
-With compression emulation turned on, GridMix will generate compressed
-input data. Hence the total size of the input 
-data will be lesser than the expected size. Set 
-`gridmix.min.file.size` to a smaller value (roughly 10% of
-`gridmix.gen.bytes.per.file`) for enabling GridMix to 
-correctly emulate compression.
-
-
-Emulating High-Ram jobs
------------------------
-
-MapReduce allows users to define a job as a High-Ram job. Tasks from a
-High-Ram job can occupy larger fraction of memory in task processes.
-Emulating this behavior is important because of the following reasons.
-
-* Impact on scheduler:  Scheduling of tasks from High-Ram jobs 
-  impacts the scheduling behavior as it might result into
-  resource reservation and utilization.
-
-* Impact on the node : Since High-Ram tasks occupy larger memory,
-  NodeManagers do some bookkeeping for allocating extra resources for 
-  these tasks. Thus this becomes a precursor for memory emulation
-  where tasks with high memory requirements needs to be considered
-  as a High-Ram task.
-
-High-Ram feature emulation can be disabled by setting  
-`gridmix.highram-emulation.enable` to `false`.
-    
-
-Emulating resource usages
--------------------------
-
-Usages of resources like CPU, physical memory, virtual memory, JVM heap
-etc are recorded by MapReduce using its task counters. This information
-is used by GridMix to emulate the resource usages in the simulated 
-tasks. Emulating resource usages will help GridMix exert similar load 
-on the test cluster as seen in the actual cluster.
-
-MapReduce tasks use up resources during its entire lifetime. GridMix
-also tries to mimic this behavior by spanning resource usage emulation
-across the entire lifetime of the simulated task. Each resource to be
-emulated should have an  *emulator*  associated with it.
-Each such  *emulator*  should implement the 
-`org.apache.hadoop.mapred.gridmix.emulators.resourceusage
-.ResourceUsageEmulatorPlugin` interface. Resource 
- *emulators*  in GridMix are  *plugins*  that can be 
-configured (plugged in or out) before every run. GridMix users can 
-configure multiple emulator  *plugins*  by passing a comma 
-separated list of  *emulators*  as a value for the 
-`gridmix.emulators.resource-usage.plugins` parameter. 
-
-List of  *emulators*  shipped with GridMix:
-
-* Cumulative CPU usage  *emulator* : 
-  GridMix uses the cumulative CPU usage value published by Rumen 
-  and makes sure that the total cumulative CPU usage of the simulated 
-  task is close to the value published by Rumen. GridMix can be 
-  configured to emulate cumulative CPU usage by adding 
-  `org.apache.hadoop.mapred.gridmix.emulators.resourceusage
-  .CumulativeCpuUsageEmulatorPlugin` to the list of emulator 
-   *plugins*  configured for the 
-  `gridmix.emulators.resource-usage.plugins` parameter.
-  CPU usage emulator is designed in such a way that
-  it only emulates at specific progress boundaries of the task. This 
-  interval can be configured using 
-  `gridmix.emulators.resource-usage.cpu.emulation-interval`.
-  The default value for this parameter is `0.1` i.e 
-  `10%`.
-
-* Total heap usage  *emulator* : 
-  GridMix uses the total heap usage value published by Rumen 
-  and makes sure that the total heap usage of the simulated 
-  task is close to the value published by Rumen. GridMix can be 
-  configured to emulate total heap usage by adding 
-  `org.apache.hadoop.mapred.gridmix.emulators.resourceusage
-  .TotalHeapUsageEmulatorPlugin` to the list of emulator 
-   *plugins*  configured for the 
-  `gridmix.emulators.resource-usage.plugins` parameter.
-  Heap usage emulator is designed in such a way that
-  it only emulates at specific progress boundaries of the task. This 
-  interval can be configured using 
-  `gridmix.emulators.resource-usage.heap.emulation-interval
-  `. The default value for this parameter is `0.1` 
-  i.e `10%` progress interval.
-
-Note that GridMix will emulate resource usages only for jobs of type *LOADJOB* .
-
-
-Simplifying Assumptions
------------------------
-
-GridMix will be developed in stages, incorporating feedback and
-patches from the community. Currently its intent is to evaluate
-MapReduce and HDFS performance and not the layers on top of them (i.e.
-the extensive lib and sub-project space). Given these two limitations,
-the following characteristics of job load are not currently captured in
-job traces and cannot be accurately reproduced in GridMix:
-
-* *Filesystem Properties*  - No attempt is made to match block
-  sizes, namespace hierarchies, or any property of input, intermediate
-  or output data other than the bytes/records consumed and emitted from
-  a given task. This implies that some of the most heavily-used parts of
-  the system - text processing, streaming, etc. - cannot be meaningfully tested 
-  with the current implementation.
-  
-* *I/O Rates*  - The rate at which records are
-  consumed/emitted is assumed to be limited only by the speed of the
-  reader/writer and constant throughout the task.
-  
-* *Memory Profile*  - No data on tasks' memory usage over time
-  is available, though the max heap-size is retained.
-  
-* *Skew*  - The records consumed and emitted to/from a given
-  task are assumed to follow observed averages, i.e. records will be
-  more regular than may be seen in the wild. Each map also generates
-  a proportional percentage of data for each reduce, so a job with
-  unbalanced input will be flattened.
-  
-* *Job Failure*  - User code is assumed to be correct.
-  
-* *Job Independence*  - The output or outcome of one job does
-  not affect when or whether a subsequent job will run.
-
-
-Appendix
---------
-
-There exist older versions of the GridMix tool.
-Issues tracking the original implementations of
-[GridMix1](https://issues.apache.org/jira/browse/HADOOP-2369),
-[GridMix2](https://issues.apache.org/jira/browse/HADOOP-3770),
-and [GridMix3](https://issues.apache.org/jira/browse/MAPREDUCE-776)
-can be found on the Apache Hadoop MapReduce JIRA. Other issues tracking
-the current development of GridMix can be found by searching
-[the Apache Hadoop MapReduce JIRA](https://issues.apache.org/jira/browse/MAPREDUCE/component/12313086).
diff --git a/hadoop-tools/hadoop-gridmix/src/site/resources/css/site.css b/hadoop-tools/hadoop-gridmix/src/site/resources/css/site.css
deleted file mode 100644
index f830baafa8c..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/site/resources/css/site.css
+++ /dev/null
@@ -1,30 +0,0 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one or more
-* contributor license agreements.  See the NOTICE file distributed with
-* this work for additional information regarding copyright ownership.
-* The ASF licenses this file to You under the Apache License, Version 2.0
-* (the "License"); you may not use this file except in compliance with
-* the License.  You may obtain a copy of the License at
-*
-*     http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing, software
-* distributed under the License is distributed on an "AS IS" BASIS,
-* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-* See the License for the specific language governing permissions and
-* limitations under the License.
-*/
-#banner {
-  height: 93px;
-  background: none;
-}
-
-#bannerLeft img {
-  margin-left: 30px;
-  margin-top: 10px;
-}
-
-#bannerRight img {
-  margin: 17px;
-}
-
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/CommonJobTest.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/CommonJobTest.java
deleted file mode 100644
index 73c03fd948a..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/CommonJobTest.java
+++ /dev/null
@@ -1,385 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
-
-import java.io.File;
-import java.io.IOException;
-import java.text.DecimalFormat;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.concurrent.BlockingQueue;
-import java.util.concurrent.CountDownLatch;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.TimeUnit;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.ContentSummary;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.LocatedFileStatus;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.RemoteIterator;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.mapred.Counters;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobID;
-import org.apache.hadoop.mapred.TaskReport;
-import org.apache.hadoop.mapreduce.Counter;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.TaskCounter;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.tools.rumen.TaskInfo;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.util.ToolRunner;
-
-public class CommonJobTest {
-  public static final Logger LOG = LoggerFactory.getLogger(Gridmix.class);
-
-  protected static int NJOBS = 2;
-  protected static final long GENDATA = 1; // in megabytes
-  protected static GridmixJobSubmissionPolicy policy = GridmixJobSubmissionPolicy.REPLAY;
-  private static File workspace = new File("target" + File.separator
-          + TestGridmixSubmission.class.getName() + "-test");
-
-  static class DebugGridmix extends Gridmix {
-
-    private JobFactory<?> factory;
-    private TestMonitor monitor;
-
-    @Override
-    protected JobMonitor createJobMonitor(Statistics stats, Configuration conf)
-            throws IOException {
-      monitor = new TestMonitor(3, stats);
-      return monitor;
-    }
-
-    @Override
-    protected JobFactory<?> createJobFactory(JobSubmitter submitter,
-                                             String traceIn, Path scratchDir, Configuration conf,
-                                             CountDownLatch startFlag, UserResolver userResolver) throws IOException {
-      factory = DebugJobFactory.getFactory(submitter, scratchDir, NJOBS, conf,
-              startFlag, userResolver);
-      return factory;
-    }
-
-    public void checkMonitor() throws Exception {
-      monitor.verify(((DebugJobFactory.Debuggable) factory).getSubmitted());
-    }
-  }
-
-  static class TestMonitor extends JobMonitor {
-    private final BlockingQueue<Job> retiredJobs;
-    private final int expected;
-    static final long SLOPBYTES = 1024;
-
-    public TestMonitor(int expected, Statistics stats) {
-      super(3, TimeUnit.SECONDS, stats, 1);
-      this.expected = expected;
-      retiredJobs = new LinkedBlockingQueue<Job>();
-    }
-
-    @Override
-    protected void onSuccess(Job job) {
-      LOG.info(" Job Success " + job);
-      retiredJobs.add(job);
-    }
-
-    @Override
-    protected void onFailure(Job job) {
-      fail("Job failure: " + job);
-    }
-
-    public void verify(ArrayList<JobStory> submitted) throws Exception {
-      assertEquals("Bad job count", expected, retiredJobs.size());
-
-      final ArrayList<Job> succeeded = new ArrayList<Job>();
-      assertEquals("Bad job count", expected, retiredJobs.drainTo(succeeded));
-      final HashMap<String, JobStory> sub = new HashMap<String, JobStory>();
-      for (JobStory spec : submitted) {
-        sub.put(spec.getJobID().toString(), spec);
-      }
-      for (Job job : succeeded) {
-        final String jobName = job.getJobName();
-        Configuration configuration = job.getConfiguration();
-        if (GenerateData.JOB_NAME.equals(jobName)) {
-          RemoteIterator<LocatedFileStatus> rit = GridmixTestUtils.dfs
-                  .listFiles(new Path("/"), true);
-          while (rit.hasNext()) {
-            System.out.println(rit.next().toString());
-          }
-          final Path in = new Path("foo").makeQualified(
-                  GridmixTestUtils.dfs.getUri(),
-                  GridmixTestUtils.dfs.getWorkingDirectory());
-          // data was compressed. All files = compressed test size+ logs= 1000000/2 + logs
-          final ContentSummary generated = GridmixTestUtils.dfs
-                  .getContentSummary(in);
-          assertEquals(550000, generated.getLength(), 10000);
-
-          Counter counter = job.getCounters()
-                  .getGroup("org.apache.hadoop.mapreduce.FileSystemCounter")
-                  .findCounter("HDFS_BYTES_WRITTEN");
-
-          assertEquals(generated.getLength(), counter.getValue());
-
-          continue;
-        } else if (GenerateDistCacheData.JOB_NAME.equals(jobName)) {
-          continue;
-        }
-
-        final String originalJobId = configuration.get(Gridmix.ORIGINAL_JOB_ID);
-        final JobStory spec = sub.get(originalJobId);
-        assertNotNull("No spec for " + jobName, spec);
-        assertNotNull("No counters for " + jobName, job.getCounters());
-        final String originalJobName = spec.getName();
-        System.out.println("originalJobName=" + originalJobName
-                + ";GridmixJobName=" + jobName + ";originalJobID=" + originalJobId);
-        assertTrue("Original job name is wrong.",
-                originalJobName.equals(configuration.get(Gridmix.ORIGINAL_JOB_NAME)));
-
-        // Gridmix job seqNum contains 6 digits
-        int seqNumLength = 6;
-        String jobSeqNum = new DecimalFormat("000000").format(configuration.getInt(
-                GridmixJob.GRIDMIX_JOB_SEQ, -1));
-        // Original job name is of the format MOCKJOB<6 digit sequence number>
-        // because MockJob jobNames are of this format.
-        assertTrue(originalJobName.substring(
-                originalJobName.length() - seqNumLength).equals(jobSeqNum));
-
-        assertTrue("Gridmix job name is not in the expected format.",
-                jobName.equals(GridmixJob.JOB_NAME_PREFIX + jobSeqNum));
-        final FileStatus stat = GridmixTestUtils.dfs.getFileStatus(new Path(
-                GridmixTestUtils.DEST, "" + Integer.parseInt(jobSeqNum)));
-        assertEquals("Wrong owner for " + jobName, spec.getUser(),
-                stat.getOwner());
-        final int nMaps = spec.getNumberMaps();
-        final int nReds = spec.getNumberReduces();
-
-        final JobClient client = new JobClient(
-                GridmixTestUtils.mrvl.getConfig());
-        final TaskReport[] mReports = client.getMapTaskReports(JobID
-                .downgrade(job.getJobID()));
-        assertEquals("Mismatched map count", nMaps, mReports.length);
-        check(TaskType.MAP, spec, mReports, 0, 0, SLOPBYTES, nReds);
-
-        final TaskReport[] rReports = client.getReduceTaskReports(JobID
-                .downgrade(job.getJobID()));
-        assertEquals("Mismatched reduce count", nReds, rReports.length);
-        check(TaskType.REDUCE, spec, rReports, nMaps * SLOPBYTES, 2 * nMaps, 0,
-                0);
-
-      }
-
-    }
-    // Verify if correct job queue is used
-    private void check(final TaskType type, JobStory spec,
-                       final TaskReport[] runTasks, long extraInputBytes,
-                       int extraInputRecords, long extraOutputBytes, int extraOutputRecords)
-            throws Exception {
-
-      long[] runInputRecords = new long[runTasks.length];
-      long[] runInputBytes = new long[runTasks.length];
-      long[] runOutputRecords = new long[runTasks.length];
-      long[] runOutputBytes = new long[runTasks.length];
-      long[] specInputRecords = new long[runTasks.length];
-      long[] specInputBytes = new long[runTasks.length];
-      long[] specOutputRecords = new long[runTasks.length];
-      long[] specOutputBytes = new long[runTasks.length];
-
-      for (int i = 0; i < runTasks.length; ++i) {
-        final TaskInfo specInfo;
-        final Counters counters = runTasks[i].getCounters();
-        switch (type) {
-          case MAP:
-            runInputBytes[i] = counters.findCounter("FileSystemCounters",
-                    "HDFS_BYTES_READ").getValue()
-                    - counters.findCounter(TaskCounter.SPLIT_RAW_BYTES).getValue();
-            runInputRecords[i] = (int) counters.findCounter(
-                    TaskCounter.MAP_INPUT_RECORDS).getValue();
-            runOutputBytes[i] = counters
-                    .findCounter(TaskCounter.MAP_OUTPUT_BYTES).getValue();
-            runOutputRecords[i] = (int) counters.findCounter(
-                    TaskCounter.MAP_OUTPUT_RECORDS).getValue();
-
-            specInfo = spec.getTaskInfo(TaskType.MAP, i);
-            specInputRecords[i] = specInfo.getInputRecords();
-            specInputBytes[i] = specInfo.getInputBytes();
-            specOutputRecords[i] = specInfo.getOutputRecords();
-            specOutputBytes[i] = specInfo.getOutputBytes();
-
-            LOG.info(String.format(type + " SPEC: %9d -> %9d :: %5d -> %5d\n",
-                    specInputBytes[i], specOutputBytes[i], specInputRecords[i],
-                    specOutputRecords[i]));
-            LOG.info(String.format(type + " RUN:  %9d -> %9d :: %5d -> %5d\n",
-                    runInputBytes[i], runOutputBytes[i], runInputRecords[i],
-                    runOutputRecords[i]));
-            break;
-          case REDUCE:
-            runInputBytes[i] = 0;
-            runInputRecords[i] = (int) counters.findCounter(
-                    TaskCounter.REDUCE_INPUT_RECORDS).getValue();
-            runOutputBytes[i] = counters.findCounter("FileSystemCounters",
-                    "HDFS_BYTES_WRITTEN").getValue();
-            runOutputRecords[i] = (int) counters.findCounter(
-                    TaskCounter.REDUCE_OUTPUT_RECORDS).getValue();
-
-            specInfo = spec.getTaskInfo(TaskType.REDUCE, i);
-            // There is no reliable counter for reduce input bytes. The
-            // variable-length encoding of intermediate records and other noise
-            // make this quantity difficult to estimate. The shuffle and spec
-            // input bytes are included in debug output for reference, but are
-            // not checked
-            specInputBytes[i] = 0;
-            specInputRecords[i] = specInfo.getInputRecords();
-            specOutputRecords[i] = specInfo.getOutputRecords();
-            specOutputBytes[i] = specInfo.getOutputBytes();
-            LOG.info(String.format(type + " SPEC: (%9d) -> %9d :: %5d -> %5d\n",
-                    specInfo.getInputBytes(), specOutputBytes[i],
-                    specInputRecords[i], specOutputRecords[i]));
-            LOG.info(String
-                    .format(type + " RUN:  (%9d) -> %9d :: %5d -> %5d\n", counters
-                            .findCounter(TaskCounter.REDUCE_SHUFFLE_BYTES).getValue(),
-                            runOutputBytes[i], runInputRecords[i], runOutputRecords[i]));
-            break;
-          default:
-            fail("Unexpected type: " + type);
-        }
-      }
-
-      // Check input bytes
-      Arrays.sort(specInputBytes);
-      Arrays.sort(runInputBytes);
-      for (int i = 0; i < runTasks.length; ++i) {
-        assertTrue("Mismatched " + type + " input bytes " + specInputBytes[i]
-                + "/" + runInputBytes[i],
-                eqPlusMinus(runInputBytes[i], specInputBytes[i], extraInputBytes));
-      }
-
-      // Check input records
-      Arrays.sort(specInputRecords);
-      Arrays.sort(runInputRecords);
-      for (int i = 0; i < runTasks.length; ++i) {
-        assertTrue(
-                "Mismatched " + type + " input records " + specInputRecords[i]
-                        + "/" + runInputRecords[i],
-                eqPlusMinus(runInputRecords[i], specInputRecords[i],
-                        extraInputRecords));
-      }
-
-      // Check output bytes
-      Arrays.sort(specOutputBytes);
-      Arrays.sort(runOutputBytes);
-      for (int i = 0; i < runTasks.length; ++i) {
-        assertTrue(
-                "Mismatched " + type + " output bytes " + specOutputBytes[i] + "/"
-                        + runOutputBytes[i],
-                eqPlusMinus(runOutputBytes[i], specOutputBytes[i], extraOutputBytes));
-      }
-
-      // Check output records
-      Arrays.sort(specOutputRecords);
-      Arrays.sort(runOutputRecords);
-      for (int i = 0; i < runTasks.length; ++i) {
-        assertTrue(
-                "Mismatched " + type + " output records " + specOutputRecords[i]
-                        + "/" + runOutputRecords[i],
-                eqPlusMinus(runOutputRecords[i], specOutputRecords[i],
-                        extraOutputRecords));
-      }
-
-    }
-
-    private static boolean eqPlusMinus(long a, long b, long x) {
-      final long diff = Math.abs(a - b);
-      return diff <= x;
-    }
-
-  }
-
-  protected void doSubmission(String jobCreatorName, boolean defaultOutputPath)
-          throws Exception {
-    final Path in = new Path("foo").makeQualified(
-            GridmixTestUtils.dfs.getUri(),
-            GridmixTestUtils.dfs.getWorkingDirectory());
-    final Path out = GridmixTestUtils.DEST.makeQualified(
-            GridmixTestUtils.dfs.getUri(),
-            GridmixTestUtils.dfs.getWorkingDirectory());
-    final Path root = new Path(workspace.getName()).makeQualified(
-        GridmixTestUtils.dfs.getUri(), GridmixTestUtils.dfs.getWorkingDirectory());
-    if (!workspace.exists()) {
-      assertTrue(workspace.mkdirs());
-    }
-    Configuration conf = null;
-
-    try {
-      ArrayList<String> argsList = new ArrayList<String>();
-
-      argsList.add("-D" + FilePool.GRIDMIX_MIN_FILE + "=0");
-      argsList.add("-D" + Gridmix.GRIDMIX_USR_RSV + "="
-              + EchoUserResolver.class.getName());
-      if (jobCreatorName != null) {
-        argsList.add("-D" + JobCreator.GRIDMIX_JOB_TYPE + "=" + jobCreatorName);
-      }
-
-      // Set the config property gridmix.output.directory only if
-      // defaultOutputPath is false. If defaultOutputPath is true, then
-      // let us allow gridmix to use the path foo/gridmix/ as output dir.
-      if (!defaultOutputPath) {
-        argsList.add("-D" + Gridmix.GRIDMIX_OUT_DIR + "=" + out);
-      }
-      argsList.add("-generate");
-      argsList.add(String.valueOf(GENDATA) + "m");
-      argsList.add(in.toString());
-      argsList.add("-"); // ignored by DebugGridmix
-
-      String[] argv = argsList.toArray(new String[argsList.size()]);
-
-      DebugGridmix client = new DebugGridmix();
-      conf = GridmixTestUtils.mrvl.getConfig();
-
-      CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-      conf.setEnum(GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, policy);
-
-      conf.setBoolean(GridmixJob.GRIDMIX_USE_QUEUE_IN_TRACE, true);
-      UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-      conf.set(MRJobConfig.USER_NAME, ugi.getUserName());
-
-      // allow synthetic users to create home directories
-      GridmixTestUtils.dfs.mkdirs(root, new FsPermission((short) 777));
-      GridmixTestUtils.dfs.setPermission(root, new FsPermission((short) 777));
-
-      int res = ToolRunner.run(conf, client, argv);
-      assertEquals("Client exited with nonzero status", 0, res);
-      client.checkMonitor();
-    } catch (Exception e) {
-      e.printStackTrace();
-    } finally {
-      in.getFileSystem(conf).delete(in, true);
-      out.getFileSystem(conf).delete(out, true);
-      root.getFileSystem(conf).delete(root, true);
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
deleted file mode 100644
index 99b4571b7e0..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobFactory.java
+++ /dev/null
@@ -1,106 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.tools.rumen.JobStory;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.concurrent.CountDownLatch;
-
-
-/**
- * Component generating random job traces for testing on a single node.
- */
-public class DebugJobFactory {
-
-  interface Debuggable {
-    ArrayList<JobStory> getSubmitted();
-  }
-
-  public static JobFactory<?> getFactory(
-    JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
-    CountDownLatch startFlag, UserResolver resolver) throws IOException {
-    GridmixJobSubmissionPolicy policy = GridmixJobSubmissionPolicy.getPolicy(
-      conf, GridmixJobSubmissionPolicy.STRESS);
-    if (policy == GridmixJobSubmissionPolicy.REPLAY) {
-      return new DebugReplayJobFactory(
-        submitter, scratch, numJobs, conf, startFlag, resolver);
-    } else if (policy == GridmixJobSubmissionPolicy.STRESS) {
-      return new DebugStressJobFactory(
-        submitter, scratch, numJobs, conf, startFlag, resolver);
-    } else if (policy == GridmixJobSubmissionPolicy.SERIAL) {
-      return new DebugSerialJobFactory(
-        submitter, scratch, numJobs, conf, startFlag, resolver);
-
-    }
-    return null;
-  }
-
-  static class DebugReplayJobFactory extends ReplayJobFactory
-    implements Debuggable {
-    public DebugReplayJobFactory(
-      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
-      CountDownLatch startFlag, UserResolver resolver) throws IOException {
-      super(
-        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
-        startFlag, resolver);
-    }
-
-    @Override
-    public ArrayList<JobStory> getSubmitted() {
-      return ((DebugJobProducer) jobProducer).submitted;
-    }
-
-  }
-
-  static class DebugSerialJobFactory extends SerialJobFactory
-    implements Debuggable {
-    public DebugSerialJobFactory(
-      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
-      CountDownLatch startFlag, UserResolver resolver) throws IOException {
-      super(
-        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
-        startFlag, resolver);
-    }
-
-    @Override
-    public ArrayList<JobStory> getSubmitted() {
-      return ((DebugJobProducer) jobProducer).submitted;
-    }
-  }
-
-  static class DebugStressJobFactory extends StressJobFactory
-    implements Debuggable {
-    public DebugStressJobFactory(
-      JobSubmitter submitter, Path scratch, int numJobs, Configuration conf,
-      CountDownLatch startFlag, UserResolver resolver) throws IOException {
-      super(
-        submitter, new DebugJobProducer(numJobs, conf), scratch, conf,
-        startFlag, resolver);
-    }
-
-    @Override
-    public ArrayList<JobStory> getSubmitted() {
-      return ((DebugJobProducer) jobProducer).submitted;
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
deleted file mode 100644
index 8c109fff254..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DebugJobProducer.java
+++ /dev/null
@@ -1,311 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.mapred.TaskStatus.State;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.MapTaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.ReduceTaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.TaskInfo;
-import org.apache.hadoop.tools.rumen.TaskAttemptInfo;
-import org.apache.hadoop.tools.rumen.Pre21JobHistoryConstants.Values;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.JobID;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.InputSplit;
-
-import java.util.ArrayList;
-import java.util.Random;
-import java.util.Arrays;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicLong;
-import java.util.concurrent.TimeUnit;
-import java.io.IOException;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-
-public class DebugJobProducer implements JobStoryProducer {
-  public static final Logger LOG = LoggerFactory.getLogger(DebugJobProducer.class);
-  final ArrayList<JobStory> submitted;
-  private final Configuration conf;
-  private final AtomicInteger numJobs;
-
-  public DebugJobProducer(int numJobs, Configuration conf) {
-    super();
-    MockJob.reset();
-    this.conf = conf;
-    this.numJobs = new AtomicInteger(numJobs);
-    this.submitted = new ArrayList<JobStory>();
-  }
-
-  @Override
-  public JobStory getNextJob() throws IOException {
-    if (numJobs.getAndDecrement() > 0) {
-      final MockJob ret = new MockJob(conf);
-      submitted.add(ret);
-      return ret;
-    }
-    return null;
-  }
-
-  @Override
-  public void close() {
-  }
-
-
-  static double[] getDistr(Random r, double mindist, int size) {
-    assert 0.0 <= mindist && mindist <= 1.0;
-    final double min = mindist / size;
-    final double rem = 1.0 - min * size;
-    final double[] tmp = new double[size];
-    for (int i = 0; i < tmp.length - 1; ++i) {
-      tmp[i] = r.nextDouble() * rem;
-    }
-    tmp[tmp.length - 1] = rem;
-    Arrays.sort(tmp);
-
-    final double[] ret = new double[size];
-    ret[0] = tmp[0] + min;
-    for (int i = 1; i < size; ++i) {
-      ret[i] = tmp[i] - tmp[i - 1] + min;
-    }
-    return ret;
-  }
-
-
-  /**
-   * Generate random task data for a synthetic job.
-   */
-  static class MockJob implements JobStory {
-
-    static final int MIN_REC = 1 << 14;
-    static final int MIN_BYTES = 1 << 20;
-    static final int VAR_REC = 1 << 14;
-    static final int VAR_BYTES = 4 << 20;
-    static final int MAX_MAP = 5;
-    static final int MAX_RED = 3;
-    final Configuration conf;
-
-    static void initDist(
-      Random r, double min, int[] recs, long[] bytes, long tot_recs,
-      long tot_bytes) {
-      final double[] recs_dist = getDistr(r, min, recs.length);
-      final double[] bytes_dist = getDistr(r, min, recs.length);
-      long totalbytes = 0L;
-      int totalrecs = 0;
-      for (int i = 0; i < recs.length; ++i) {
-        recs[i] = (int) Math.round(tot_recs * recs_dist[i]);
-        bytes[i] = Math.round(tot_bytes * bytes_dist[i]);
-        totalrecs += recs[i];
-        totalbytes += bytes[i];
-      }
-      // Add/remove excess
-      recs[0] += totalrecs - tot_recs;
-      bytes[0] += totalbytes - tot_bytes;
-      LOG.info(
-        "DIST: " + Arrays.toString(recs) + " " + tot_recs + "/" + totalrecs +
-          " " + Arrays.toString(bytes) + " " + tot_bytes + "/" + totalbytes);
-    }
-
-    private static final AtomicInteger seq = new AtomicInteger(0);
-    // set timestamp in the past
-    private static final AtomicLong timestamp = new AtomicLong(
-      System.currentTimeMillis() - TimeUnit.MILLISECONDS.convert(
-        60, TimeUnit.DAYS));
-
-    private final int id;
-    private final String name;
-    private final int[] m_recsIn, m_recsOut, r_recsIn, r_recsOut;
-    private final long[] m_bytesIn, m_bytesOut, r_bytesIn, r_bytesOut;
-    private final long submitTime;
-
-    public MockJob(Configuration conf) {
-      final Random r = new Random();
-      final long seed = r.nextLong();
-      r.setSeed(seed);
-      id = seq.getAndIncrement();
-      name = String.format("MOCKJOB%06d", id);
-
-      this.conf = conf;
-      LOG.info(name + " (" + seed + ")");
-      submitTime = timestamp.addAndGet(
-        TimeUnit.MILLISECONDS.convert(
-          r.nextInt(10), TimeUnit.SECONDS));
-
-      m_recsIn = new int[r.nextInt(MAX_MAP) + 1];
-      m_bytesIn = new long[m_recsIn.length];
-      m_recsOut = new int[m_recsIn.length];
-      m_bytesOut = new long[m_recsIn.length];
-
-      r_recsIn = new int[r.nextInt(MAX_RED) + 1];
-      r_bytesIn = new long[r_recsIn.length];
-      r_recsOut = new int[r_recsIn.length];
-      r_bytesOut = new long[r_recsIn.length];
-
-      // map input
-      final long map_recs = r.nextInt(VAR_REC) + MIN_REC;
-      final long map_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
-      initDist(r, 0.5, m_recsIn, m_bytesIn, map_recs, map_bytes);
-
-      // shuffle
-      final long shuffle_recs = r.nextInt(VAR_REC) + MIN_REC;
-      final long shuffle_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
-      initDist(r, 0.5, m_recsOut, m_bytesOut, shuffle_recs, shuffle_bytes);
-      initDist(r, 0.8, r_recsIn, r_bytesIn, shuffle_recs, shuffle_bytes);
-
-      // reduce output
-      final long red_recs = r.nextInt(VAR_REC) + MIN_REC;
-      final long red_bytes = r.nextInt(VAR_BYTES) + MIN_BYTES;
-      initDist(r, 0.5, r_recsOut, r_bytesOut, red_recs, red_bytes);
-
-      if (LOG.isDebugEnabled()) {
-        int iMapBTotal = 0, oMapBTotal = 0, iRedBTotal = 0, oRedBTotal = 0;
-        int iMapRTotal = 0, oMapRTotal = 0, iRedRTotal = 0, oRedRTotal = 0;
-        for (int i = 0; i < m_recsIn.length; ++i) {
-          iMapRTotal += m_recsIn[i];
-          iMapBTotal += m_bytesIn[i];
-          oMapRTotal += m_recsOut[i];
-          oMapBTotal += m_bytesOut[i];
-        }
-        for (int i = 0; i < r_recsIn.length; ++i) {
-          iRedRTotal += r_recsIn[i];
-          iRedBTotal += r_bytesIn[i];
-          oRedRTotal += r_recsOut[i];
-          oRedBTotal += r_bytesOut[i];
-        }
-        LOG.debug(
-          String.format(
-            "%s: M (%03d) %6d/%10d -> %6d/%10d" +
-              " R (%03d) %6d/%10d -> %6d/%10d @%d", name, m_bytesIn.length,
-            iMapRTotal, iMapBTotal, oMapRTotal, oMapBTotal, r_bytesIn.length,
-            iRedRTotal, iRedBTotal, oRedRTotal, oRedBTotal, submitTime));
-      }
-    }
-    @Override
-   public String getName() {
-     return name;
-    }
-
-   @Override
-   public String getUser() {
-     // Obtain user name from job configuration, if available.
-     // Otherwise use dummy user names.
-     String user = conf.get(MRJobConfig.USER_NAME);
-     if (user == null) {
-       user = String.format("foobar%d", id);
-     }
-     GridmixTestUtils.createHomeAndStagingDirectory(user, conf);
-     return user;
-   }
-
-   @Override
-   public JobID getJobID() {
-     return new JobID("job_mock_" + name, id);
-    }
-
-    @Override
-   public Values getOutcome() {
-     return Values.SUCCESS;
-    }
-
-   @Override
-   public long getSubmissionTime() {
-     return submitTime;
-   }
-
-   @Override
-   public int getNumberMaps() {
-     return m_bytesIn.length;
-   }
-
-   @Override
-   public int getNumberReduces() {
-     return r_bytesIn.length;
-   }
-    
-    @Override
-    public TaskInfo getTaskInfo(TaskType taskType, int taskNumber) {
-      switch (taskType) {
-        case MAP:
-          return new TaskInfo(m_bytesIn[taskNumber], m_recsIn[taskNumber],
-              m_bytesOut[taskNumber], m_recsOut[taskNumber], -1);
-        case REDUCE:
-          return new TaskInfo(r_bytesIn[taskNumber], r_recsIn[taskNumber],
-              r_bytesOut[taskNumber], r_recsOut[taskNumber], -1);
-        default:
-          throw new IllegalArgumentException("Not interested");
-      }
-    }
-
-    @Override
-    public InputSplit[] getInputSplits() {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public TaskAttemptInfo getTaskAttemptInfo(
-      TaskType taskType, int taskNumber, int taskAttemptNumber) {
-      switch (taskType) {
-        case MAP:
-          return new MapTaskAttemptInfo(
-            State.SUCCEEDED, 
-            new TaskInfo(
-              m_bytesIn[taskNumber], m_recsIn[taskNumber],
-              m_bytesOut[taskNumber], m_recsOut[taskNumber], -1),
-            100);
-
-        case REDUCE:
-          return new ReduceTaskAttemptInfo(
-            State.SUCCEEDED, 
-            new TaskInfo(
-              r_bytesIn[taskNumber], r_recsIn[taskNumber],
-              r_bytesOut[taskNumber], r_recsOut[taskNumber], -1),
-            100, 100, 100);
-      }
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public TaskAttemptInfo getMapTaskAttemptInfoAdjusted(
-      int taskNumber, int taskAttemptNumber, int locality) {
-      throw new UnsupportedOperationException();
-    }
-
-    @Override
-    public org.apache.hadoop.mapred.JobConf getJobConf() {
-      return new JobConf(conf);
-    }
-
-    @Override
-    public String getQueueName() {
-      String qName = "default";
-      return qName;
-    }
-    
-    public static void reset() {
-      seq.set(0);
-      timestamp.set(System.currentTimeMillis() - TimeUnit.MILLISECONDS.convert(
-        60, TimeUnit.DAYS));
-    }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DummyResourceCalculatorPlugin.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DummyResourceCalculatorPlugin.java
deleted file mode 100644
index 528202fd7f4..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/DummyResourceCalculatorPlugin.java
+++ /dev/null
@@ -1,154 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.classification.InterfaceAudience;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin;
-
-/**
- * Plugin class to test resource information reported by NM. Use configuration
- * items {@link #MAXVMEM_TESTING_PROPERTY} and {@link #MAXPMEM_TESTING_PROPERTY}
- * to tell NM the total vmem and the total pmem. Use configuration items
- * {@link #NUM_PROCESSORS}, {@link #CPU_FREQUENCY}, {@link #CUMULATIVE_CPU_TIME}
- * and {@link #CPU_USAGE} to tell TT the CPU information.
- */
-@InterfaceAudience.Private
-public class DummyResourceCalculatorPlugin extends ResourceCalculatorPlugin {
-
-  DummyResourceCalculatorPlugin() {
-    super(null);
-  }
-
-  /** max vmem on the TT */
-  public static final String MAXVMEM_TESTING_PROPERTY =
-      "mapred.tasktracker.maxvmem.testing";
-  /** max pmem on the TT */
-  public static final String MAXPMEM_TESTING_PROPERTY =
-      "mapred.tasktracker.maxpmem.testing";
-  /** number of processors for testing */
-  public static final String NUM_PROCESSORS =
-      "mapred.tasktracker.numprocessors.testing";
-  /** CPU frequency for testing */
-  public static final String CPU_FREQUENCY =
-      "mapred.tasktracker.cpufrequency.testing";
-  /** cumulative CPU usage time for testing */
-  public static final String CUMULATIVE_CPU_TIME =
-      "mapred.tasktracker.cumulativecputime.testing";
-  /** CPU usage percentage for testing */
-  public static final String CPU_USAGE = "mapred.tasktracker.cpuusage.testing";
-  /** cumulative number of bytes read over the network */
-  public static final String NETWORK_BYTES_READ =
-      "mapred.tasktracker.networkread.testing";
-  /** cumulative number of bytes written over the network */
-  public static final String NETWORK_BYTES_WRITTEN =
-      "mapred.tasktracker.networkwritten.testing";
-  /** cumulative number of bytes read from disks */
-  public static final String STORAGE_BYTES_READ =
-      "mapred.tasktracker.storageread.testing";
-  /** cumulative number of bytes written to disks */
-  public static final String STORAGE_BYTES_WRITTEN =
-      "mapred.tasktracker.storagewritten.testing";
-  /** process cumulative CPU usage time for testing */
-  public static final String PROC_CUMULATIVE_CPU_TIME =
-      "mapred.tasktracker.proccumulativecputime.testing";
-  /** process pmem for testing */
-  public static final String PROC_PMEM_TESTING_PROPERTY =
-      "mapred.tasktracker.procpmem.testing";
-  /** process vmem for testing */
-  public static final String PROC_VMEM_TESTING_PROPERTY =
-      "mapred.tasktracker.procvmem.testing";
-
-  /** {@inheritDoc} */
-  @Override
-  public long getVirtualMemorySize() {
-    return getConf().getLong(MAXVMEM_TESTING_PROPERTY, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getPhysicalMemorySize() {
-    return getConf().getLong(MAXPMEM_TESTING_PROPERTY, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getAvailableVirtualMemorySize() {
-    return getConf().getLong(MAXVMEM_TESTING_PROPERTY, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getAvailablePhysicalMemorySize() {
-    return getConf().getLong(MAXPMEM_TESTING_PROPERTY, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public int getNumProcessors() {
-    return getConf().getInt(NUM_PROCESSORS, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public int getNumCores() {
-    return getNumProcessors();
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getCpuFrequency() {
-    return getConf().getLong(CPU_FREQUENCY, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getCumulativeCpuTime() {
-    return getConf().getLong(CUMULATIVE_CPU_TIME, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public float getCpuUsagePercentage() {
-    return getConf().getFloat(CPU_USAGE, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getNetworkBytesRead() {
-    return getConf().getLong(NETWORK_BYTES_READ, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getNetworkBytesWritten() {
-    return getConf().getLong(NETWORK_BYTES_WRITTEN, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getStorageBytesRead() {
-    return getConf().getLong(STORAGE_BYTES_READ, -1);
-  }
-
-  /** {@inheritDoc} */
-  @Override
-  public long getStorageBytesWritten() {
-    return getConf().getLong(STORAGE_BYTES_WRITTEN, -1);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
deleted file mode 100644
index b3dbd545710..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/GridmixTestUtils.java
+++ /dev/null
@@ -1,120 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one or more
- * contributor license agreements. See the NOTICE file distributed with this
- * work for additional information regarding copyright ownership. The ASF
- * licenses this file to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance with the License.
- * You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
- * WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
- * License for the specific language governing permissions and limitations under
- * the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.hdfs.MiniDFSCluster;
-import org.apache.hadoop.mapred.MiniMRClientCluster;
-import org.apache.hadoop.mapred.MiniMRClientClusterFactory;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.conf.Configuration;
-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.PREFIX;
-
-import java.io.IOException;
-
-/**
- * This is a test class.
- */
-public class GridmixTestUtils {
-  private static final Logger LOG = LoggerFactory.getLogger(GridmixTestUtils.class);
-  static final Path DEST = new Path("/gridmix");
-  static FileSystem dfs = null;
-  static MiniDFSCluster dfsCluster = null;
-  static MiniMRClientCluster mrvl = null;
-  protected static final String GRIDMIX_USE_QUEUE_IN_TRACE = 
-      "gridmix.job-submission.use-queue-in-trace";
-  protected static final String GRIDMIX_DEFAULT_QUEUE = 
-      "gridmix.job-submission.default-queue";
-
-  public static void initCluster(Class<?> caller) throws IOException {
-    Configuration conf = new Configuration();
-//    conf.set("mapred.queue.names", "default,q1,q2");
-  conf.set("mapred.queue.names", "default");
-    conf.set(PREFIX + "root.queues", "default");
-    conf.set(PREFIX + "root.default.capacity", "100.0");
-    
-    
-    conf.setBoolean(GRIDMIX_USE_QUEUE_IN_TRACE, false);
-    conf.set(GRIDMIX_DEFAULT_QUEUE, "default");
-    
-
-    dfsCluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).format(true)
-        .build();// MiniDFSCluster(conf, 3, true, null);
-    dfs = dfsCluster.getFileSystem();
-    conf.set(JTConfig.JT_RETIREJOBS, "false");
-    mrvl = MiniMRClientClusterFactory.create(caller, 2, conf);
-    
-    conf = mrvl.getConfig();
-    String[] files = conf.getStrings(MRJobConfig.CACHE_FILES);
-    if (files != null) {
-      String[] timestamps = new String[files.length];
-      for (int i = 0; i < files.length; i++) {
-        timestamps[i] = Long.toString(System.currentTimeMillis());
-      }
-      conf.setStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS, timestamps);
-    }
-    
-  }
-
-  public static void shutdownCluster() throws IOException {
-    if (mrvl != null) {
-      mrvl.stop();
-    }
-    if (dfsCluster != null) {
-      dfsCluster.shutdown();
-    }
-  }
-
-  /**
-   * Methods to generate the home directory for dummy users.
-   * 
-   * @param conf
-   */
-  public static void createHomeAndStagingDirectory(String user,
-      Configuration conf) {
-    try {
-      FileSystem fs = dfsCluster.getFileSystem();
-      String path = "/user/" + user;
-      Path homeDirectory = new Path(path);
-      if (!fs.exists(homeDirectory)) {
-        LOG.info("Creating Home directory : " + homeDirectory);
-        fs.mkdirs(homeDirectory);
-        changePermission(user, homeDirectory, fs);
-
-      }    
-      changePermission(user, homeDirectory, fs);
-      Path stagingArea = new Path(
-          conf.get("mapreduce.jobtracker.staging.root.dir",
-              "/tmp/hadoop/mapred/staging"));
-      LOG.info("Creating Staging root directory : " + stagingArea);
-      fs.mkdirs(stagingArea);
-      fs.setPermission(stagingArea, new FsPermission((short) 0777));
-    } catch (IOException ioe) {
-      ioe.printStackTrace();
-    }
-  }
-
-  static void changePermission(String user, Path homeDirectory, FileSystem fs)
-      throws IOException {
-    fs.setOwner(homeDirectory, user, "");
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
deleted file mode 100644
index 152a8ee8c13..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestCompressionEmulationUtils.java
+++ /dev/null
@@ -1,586 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.BufferedReader;
-import java.io.BufferedWriter;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.InputStream;
-import java.io.InputStreamReader;
-import java.io.OutputStream;
-import java.io.OutputStreamWriter;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.io.compress.CompressionCodec;
-import org.apache.hadoop.io.compress.GzipCodec;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.Utils;
-import org.apache.hadoop.mapred.gridmix.CompressionEmulationUtil.RandomTextDataMapper;
-import org.apache.hadoop.mapred.gridmix.GenerateData.GenSplit;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-
-import static org.junit.Assert.*;
-import org.junit.Test;
-
-/**
- * Test {@link CompressionEmulationUtil}
- */
-public class TestCompressionEmulationUtils {
-  //TODO Remove this once LocalJobRunner can run Gridmix.
-  static class CustomInputFormat extends GenerateData.GenDataFormat {
-    @Override
-    public List<InputSplit> getSplits(JobContext jobCtxt) throws IOException {
-      // get the total data to be generated
-      long toGen =
-        jobCtxt.getConfiguration().getLong(GenerateData.GRIDMIX_GEN_BYTES, -1);
-      if (toGen < 0) {
-        throw new IOException("Invalid/missing generation bytes: " + toGen);
-      }
-      // get the total number of mappers configured
-      int totalMappersConfigured =
-        jobCtxt.getConfiguration().getInt(MRJobConfig.NUM_MAPS, -1);
-      if (totalMappersConfigured < 0) {
-        throw new IOException("Invalid/missing num mappers: " 
-                              + totalMappersConfigured);
-      }
-      
-      final long bytesPerTracker = toGen / totalMappersConfigured;
-      final ArrayList<InputSplit> splits = 
-        new ArrayList<InputSplit>(totalMappersConfigured);
-      for (int i = 0; i < totalMappersConfigured; ++i) {
-        splits.add(new GenSplit(bytesPerTracker, 
-                   new String[] { "tracker_local" }));
-      }
-      return splits;
-    }
-  }
-  
-  /**
-   * Test {@link RandomTextDataMapper} via {@link CompressionEmulationUtil}.
-   */
-  @Test
-  public void testRandomCompressedTextDataGenerator() throws Exception {
-    int wordSize = 10;
-    int listSize = 20;
-    long dataSize = 10*1024*1024;
-    
-    Configuration conf = new Configuration();
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    
-    // configure the RandomTextDataGenerator to generate desired sized data
-    conf.setInt(RandomTextDataGenerator.GRIDMIX_DATAGEN_RANDOMTEXT_LISTSIZE, 
-                listSize);
-    conf.setInt(RandomTextDataGenerator.GRIDMIX_DATAGEN_RANDOMTEXT_WORDSIZE, 
-                wordSize);
-    conf.setLong(GenerateData.GRIDMIX_GEN_BYTES, dataSize);
-    conf.set("mapreduce.job.hdfs-servers", "");
-    
-    FileSystem lfs = FileSystem.getLocal(conf);
-    
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir = new Path(rootTempDir, "TestRandomCompressedTextDataGenr");
-    lfs.delete(tempDir, true);
-    
-    runDataGenJob(conf, tempDir);
-    
-    // validate the output data
-    FileStatus[] files = 
-      lfs.listStatus(tempDir, new Utils.OutputFileUtils.OutputFilesFilter());
-    long size = 0;
-    long maxLineSize = 0;
-    
-    for (FileStatus status : files) {
-      InputStream in = 
-        CompressionEmulationUtil
-          .getPossiblyDecompressedInputStream(status.getPath(), conf, 0);
-      BufferedReader reader = new BufferedReader(new InputStreamReader(in));
-      String line = reader.readLine();
-      if (line != null) {
-        long lineSize = line.getBytes().length;
-        if (lineSize > maxLineSize) {
-          maxLineSize = lineSize;
-        }
-        while (line != null) {
-          for (String word : line.split("\\s")) {
-            size += word.getBytes().length;
-          }
-          line = reader.readLine();
-        }
-      }
-      reader.close();
-    }
-
-    assertTrue(size >= dataSize);
-    assertTrue(size <= dataSize + maxLineSize);
-  }
-  
-  /**
-   * Runs a GridMix data-generation job.
-   */
-  private static void runDataGenJob(Configuration conf, Path tempDir) 
-  throws IOException, ClassNotFoundException, InterruptedException {
-    JobClient client = new JobClient(conf);
-    
-    // get the local job runner
-    conf.setInt(MRJobConfig.NUM_MAPS, 1);
-    
-    Job job = Job.getInstance(conf);
-    
-    CompressionEmulationUtil.configure(job);
-    job.setInputFormatClass(CustomInputFormat.class);
-    
-    // set the output path
-    FileOutputFormat.setOutputPath(job, tempDir);
-    
-    // submit and wait for completion
-    job.submit();
-    int ret = job.waitForCompletion(true) ? 0 : 1;
-
-    assertEquals("Job Failed", 0, ret);
-  }
-  
-  /**
-   * Test if {@link RandomTextDataGenerator} can generate random text data 
-   * with the desired compression ratio. This involves
-   *   - using {@link CompressionEmulationUtil} to configure the MR job for 
-   *     generating the random text data with the desired compression ratio
-   *   - running the MR job
-   *   - test {@link RandomTextDataGenerator}'s output and match the output size
-   *     (compressed) with the expected compression ratio.
-   */
-  private void testCompressionRatioConfigure(float ratio)
-  throws Exception {
-    long dataSize = 10*1024*1024;
-    
-    Configuration conf = new Configuration();
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    
-    conf.setLong(GenerateData.GRIDMIX_GEN_BYTES, dataSize);
-    conf.set("mapreduce.job.hdfs-servers", "");
-    
-    float expectedRatio = CompressionEmulationUtil.DEFAULT_COMPRESSION_RATIO;
-    if (ratio > 0) {
-      // set the compression ratio in the conf
-      CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, ratio);
-      expectedRatio = 
-        CompressionEmulationUtil.standardizeCompressionRatio(ratio);
-    }
-    
-    // invoke the utility to map from ratio to word-size
-    CompressionEmulationUtil.setupDataGeneratorConfig(conf);
-    
-    FileSystem lfs = FileSystem.getLocal(conf);
-    
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir = 
-      new Path(rootTempDir, "TestCustomRandomCompressedTextDataGenr");
-    lfs.delete(tempDir, true);
-    
-    runDataGenJob(conf, tempDir);
-    
-    // validate the output data
-    FileStatus[] files = 
-      lfs.listStatus(tempDir, new Utils.OutputFileUtils.OutputFilesFilter());
-    long size = 0;
-    
-    for (FileStatus status : files) {
-      size += status.getLen();
-    }
-
-    float compressionRatio = ((float)size)/dataSize;
-    float stdRatio = 
-      CompressionEmulationUtil.standardizeCompressionRatio(compressionRatio);
-    
-    assertEquals(expectedRatio, stdRatio, 0.0D);
-  }
-  
-  /**
-   * Test compression ratio with multiple compression ratios.
-   */
-  @Test
-  public void testCompressionRatios() throws Exception {
-    // test default compression ratio i.e 0.5
-    testCompressionRatioConfigure(0F);
-    // test for a sample compression ratio of 0.2
-    testCompressionRatioConfigure(0.2F);
-    // test for a sample compression ratio of 0.4
-    testCompressionRatioConfigure(0.4F);
-    // test for a sample compression ratio of 0.65
-    testCompressionRatioConfigure(0.65F);
-    // test for a compression ratio of 0.682 which should be standardized
-    // to round(0.682) i.e 0.68
-    testCompressionRatioConfigure(0.682F);
-    // test for a compression ratio of 0.567 which should be standardized
-    // to round(0.567) i.e 0.57
-    testCompressionRatioConfigure(0.567F);
-    
-    // test with a compression ratio of 0.01 which less than the min supported
-    // value of 0.07
-    boolean failed = false;
-    try {
-      testCompressionRatioConfigure(0.01F);
-    } catch (RuntimeException re) {
-      failed = true;
-    }
-    assertTrue("Compression ratio min value (0.07) check failed!", failed);
-    
-    // test with a compression ratio of 0.01 which less than the max supported
-    // value of 0.68
-    failed = false;
-    try {
-      testCompressionRatioConfigure(0.7F);
-    } catch (RuntimeException re) {
-      failed = true;
-    }
-    assertTrue("Compression ratio max value (0.68) check failed!", failed);
-  }
-  
-  /**
-   * Test compression ratio standardization.
-   */
-  @Test
-  public void testCompressionRatioStandardization() throws Exception {
-    assertEquals(0.55F, 
-        CompressionEmulationUtil.standardizeCompressionRatio(0.55F), 0.0D);
-    assertEquals(0.65F, 
-        CompressionEmulationUtil.standardizeCompressionRatio(0.652F), 0.0D);
-    assertEquals(0.78F, 
-        CompressionEmulationUtil.standardizeCompressionRatio(0.777F), 0.0D);
-    assertEquals(0.86F, 
-        CompressionEmulationUtil.standardizeCompressionRatio(0.855F), 0.0D);
-  }
-  
-  /**
-   * Test map input compression ratio configuration utilities.
-   */
-  @Test
-  public void testInputCompressionRatioConfiguration() throws Exception {
-    Configuration conf = new Configuration();
-    float ratio = 0.567F;
-    CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf, ratio);
-    assertEquals(ratio, 
-        CompressionEmulationUtil.getMapInputCompressionEmulationRatio(conf), 
-        0.0D);
-  }
-  
-  /**
-   * Test map output compression ratio configuration utilities.
-   */
-  @Test
-  public void testIntermediateCompressionRatioConfiguration() 
-  throws Exception {
-    Configuration conf = new Configuration();
-    float ratio = 0.567F;
-    CompressionEmulationUtil.setMapOutputCompressionEmulationRatio(conf, ratio);
-    assertEquals(ratio, 
-        CompressionEmulationUtil.getMapOutputCompressionEmulationRatio(conf), 
-        0.0D);
-  }
-  
-  /**
-   * Test reduce output compression ratio configuration utilities.
-   */
-  @Test
-  public void testOutputCompressionRatioConfiguration() throws Exception {
-    Configuration conf = new Configuration();
-    float ratio = 0.567F;
-    CompressionEmulationUtil.setJobOutputCompressionEmulationRatio(conf, ratio);
-    assertEquals(ratio, 
-        CompressionEmulationUtil.getJobOutputCompressionEmulationRatio(conf),
-        0.0D);
-  }
-  
-  /**
-   * Test compressible {@link GridmixRecord}.
-   */
-  @Test
-  public void testCompressibleGridmixRecord() throws IOException {
-    JobConf conf = new JobConf();
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    
-    FileSystem lfs = FileSystem.getLocal(conf);
-    int dataSize = 1024 * 1024 * 10; // 10 MB
-    float ratio = 0.357F;
-    
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir = new Path(rootTempDir, 
-                            "TestPossiblyCompressibleGridmixRecord");
-    lfs.delete(tempDir, true);
-    
-    // define a compressible GridmixRecord
-    GridmixRecord record = new GridmixRecord(dataSize, 0);
-    record.setCompressibility(true, ratio); // enable compression
-    
-    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, 
-                  CompressionCodec.class);
-    org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(conf, true);
-    
-    // write the record to a file
-    Path recordFile = new Path(tempDir, "record");
-    OutputStream outStream = CompressionEmulationUtil
-                               .getPossiblyCompressedOutputStream(recordFile, 
-                                                                  conf);    
-    DataOutputStream out = new DataOutputStream(outStream);
-    record.write(out);
-    out.close();
-    outStream.close();
-    
-    // open the compressed stream for reading
-    Path actualRecordFile = recordFile.suffix(".gz");
-    InputStream in = 
-      CompressionEmulationUtil
-        .getPossiblyDecompressedInputStream(actualRecordFile, conf, 0);
-    
-    // get the compressed file size
-    long compressedFileSize = lfs.listStatus(actualRecordFile)[0].getLen();
-    
-    GridmixRecord recordRead = new GridmixRecord();
-    recordRead.readFields(new DataInputStream(in));
-    
-    assertEquals("Record size mismatch in a compressible GridmixRecord",
-                 dataSize, recordRead.getSize());
-    assertTrue("Failed to generate a compressible GridmixRecord",
-               recordRead.getSize() > compressedFileSize);
-    
-    // check if the record can generate data with the desired compression ratio
-    float seenRatio = ((float)compressedFileSize)/dataSize;
-    assertEquals(CompressionEmulationUtil.standardizeCompressionRatio(ratio), 
-        CompressionEmulationUtil.standardizeCompressionRatio(seenRatio), 1.0D);
-  }
-  
-  /**
-   * Test 
-   * {@link CompressionEmulationUtil#isCompressionEmulationEnabled(
-   *          org.apache.hadoop.conf.Configuration)}.
-   */
-  @Test
-  public void testIsCompressionEmulationEnabled() {
-    Configuration conf = new Configuration();
-    // Check default values
-    assertTrue(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
-    
-    // Check disabled
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
-    assertFalse(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
-    
-    // Check enabled
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    assertTrue(CompressionEmulationUtil.isCompressionEmulationEnabled(conf));
-  }
-  
-  /**
-   * Test 
-   * {@link CompressionEmulationUtil#getPossiblyDecompressedInputStream(Path, 
-   *                                   Configuration, long)}
-   *  and
-   *  {@link CompressionEmulationUtil#getPossiblyCompressedOutputStream(Path, 
-   *                                    Configuration)}.
-   */
-  @Test
-  public void testPossiblyCompressedDecompressedStreams() throws IOException {
-    JobConf conf = new JobConf();
-    FileSystem lfs = FileSystem.getLocal(conf);
-    String inputLine = "Hi Hello!";
-
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    conf.setBoolean(FileOutputFormat.COMPRESS, true);
-    conf.setClass(FileOutputFormat.COMPRESS_CODEC, GzipCodec.class, 
-                  CompressionCodec.class);
-
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir =
-      new Path(rootTempDir, "TestPossiblyCompressedDecompressedStreams");
-    lfs.delete(tempDir, true);
-
-    // create a compressed file
-    Path compressedFile = new Path(tempDir, "test");
-    OutputStream out = 
-      CompressionEmulationUtil.getPossiblyCompressedOutputStream(compressedFile, 
-                                                                 conf);
-    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out));
-    writer.write(inputLine);
-    writer.close();
-    
-    // now read back the data from the compressed stream
-    compressedFile = compressedFile.suffix(".gz");
-    InputStream in = 
-      CompressionEmulationUtil
-        .getPossiblyDecompressedInputStream(compressedFile, conf, 0);
-    BufferedReader reader = new BufferedReader(new InputStreamReader(in));
-    String readLine = reader.readLine();
-    assertEquals("Compression/Decompression error", inputLine, readLine);
-    reader.close();
-  }
-  
-  /**
-   * Test if 
-   * {@link CompressionEmulationUtil#configureCompressionEmulation(
-   *        org.apache.hadoop.mapred.JobConf, org.apache.hadoop.mapred.JobConf)}
-   *  can extract compression related configuration parameters.
-   */
-  @Test
-  public void testExtractCompressionConfigs() {
-    JobConf source = new JobConf();
-    JobConf target = new JobConf();
-    
-    // set the default values
-    source.setBoolean(FileOutputFormat.COMPRESS, false);
-    source.set(FileOutputFormat.COMPRESS_CODEC, "MyDefaultCodec");
-    source.set(FileOutputFormat.COMPRESS_TYPE, "MyDefaultType");
-    source.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false); 
-    source.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, "MyDefaultCodec2");
-    
-    CompressionEmulationUtil.configureCompressionEmulation(source, target);
-    
-    // check default values
-    assertFalse(target.getBoolean(FileOutputFormat.COMPRESS, true));
-    assertEquals("MyDefaultCodec", target.get(FileOutputFormat.COMPRESS_CODEC));
-    assertEquals("MyDefaultType", target.get(FileOutputFormat.COMPRESS_TYPE));
-    assertFalse(target.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true));
-    assertEquals("MyDefaultCodec2", 
-                 target.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC));
-    assertFalse(CompressionEmulationUtil
-                .isInputCompressionEmulationEnabled(target));
-    
-    // set new values
-    source.setBoolean(FileOutputFormat.COMPRESS, true);
-    source.set(FileOutputFormat.COMPRESS_CODEC, "MyCodec");
-    source.set(FileOutputFormat.COMPRESS_TYPE, "MyType");
-    source.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true); 
-    source.set(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC, "MyCodec2");
-    org.apache.hadoop.mapred.FileInputFormat.setInputPaths(source, "file.gz");
-    
-    target = new JobConf(); // reset
-    CompressionEmulationUtil.configureCompressionEmulation(source, target);
-    
-    // check new values
-    assertTrue(target.getBoolean(FileOutputFormat.COMPRESS, false));
-    assertEquals("MyCodec", target.get(FileOutputFormat.COMPRESS_CODEC));
-    assertEquals("MyType", target.get(FileOutputFormat.COMPRESS_TYPE));
-    assertTrue(target.getBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, false));
-    assertEquals("MyCodec2", 
-                 target.get(MRJobConfig.MAP_OUTPUT_COMPRESS_CODEC));
-    assertTrue(CompressionEmulationUtil
-               .isInputCompressionEmulationEnabled(target));
-  }
-  
-  /**
-   * Test of {@link FileQueue} can identify compressed file and provide
-   * readers to extract uncompressed data only if input-compression is enabled.
-   */
-  @Test
-  public void testFileQueueDecompression() throws IOException {
-    JobConf conf = new JobConf();
-    FileSystem lfs = FileSystem.getLocal(conf);
-    String inputLine = "Hi Hello!";
-    
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    org.apache.hadoop.mapred.FileOutputFormat.setCompressOutput(conf, true);
-    org.apache.hadoop.mapred.FileOutputFormat.setOutputCompressorClass(conf, 
-                                                GzipCodec.class);
-
-    // define the test's root temp directory
-    Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp")).makeQualified(
-            lfs.getUri(), lfs.getWorkingDirectory());
-
-    Path tempDir = new Path(rootTempDir, "TestFileQueueDecompression");
-    lfs.delete(tempDir, true);
-
-    // create a compressed file
-    Path compressedFile = new Path(tempDir, "test");
-    OutputStream out = 
-      CompressionEmulationUtil.getPossiblyCompressedOutputStream(compressedFile, 
-                                                                 conf);
-    BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(out));
-    writer.write(inputLine);
-    writer.close();
-    
-    compressedFile = compressedFile.suffix(".gz");
-    // now read back the data from the compressed stream using FileQueue
-    long fileSize = lfs.listStatus(compressedFile)[0].getLen();
-    CombineFileSplit split = 
-      new CombineFileSplit(new Path[] {compressedFile}, new long[] {fileSize});
-    FileQueue queue = new FileQueue(split, conf);
-    byte[] bytes = new byte[inputLine.getBytes().length];
-    queue.read(bytes);
-    queue.close();
-    String readLine = new String(bytes);
-    assertEquals("Compression/Decompression error", inputLine, readLine);
-  }
-
-  /**
-   * Tests the computation logic of uncompressed input bytes by
-   * {@link LoadJob#getUncompressedInputBytes(long, Configuration)}
-   */
-  @Test
-  public void testComputeUncompressedInputBytes() {
-    long possiblyCompressedInputBytes = 100000;
-    float compressionRatio = 0.45F;
-    Configuration conf = new Configuration();
-    CompressionEmulationUtil.setMapInputCompressionEmulationRatio(conf,
-        compressionRatio);
-
-    // By default, input compression emulation is diabled. Verify the
-    // computation of uncompressed input bytes.
-    long result = CompressionEmulationUtil.getUncompressedInputBytes(
-        possiblyCompressedInputBytes, conf);
-    assertEquals(possiblyCompressedInputBytes, result);
-
-    // Enable input compression emulation and verify uncompressed
-    // input bytes computation logic
-    CompressionEmulationUtil.setInputCompressionEmulationEnabled(conf, true);
-    result = CompressionEmulationUtil.getUncompressedInputBytes(
-        possiblyCompressedInputBytes, conf);
-    assertEquals((long)(possiblyCompressedInputBytes/compressionRatio), result);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestDistCacheEmulation.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestDistCacheEmulation.java
deleted file mode 100644
index caf05647b4e..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestDistCacheEmulation.java
+++ /dev/null
@@ -1,430 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsAction;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.JobContext;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.MapContext;
-import org.apache.hadoop.mapreduce.MapReduceTestUtil;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.lib.input.FileSplit;
-import org.apache.hadoop.mapreduce.task.MapContextImpl;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-/**
- * Validate emulation of distributed cache load in gridmix simulated jobs.
- * 
- */
-public class TestDistCacheEmulation {
-
-  private DistributedCacheEmulator dce = null;
-
-  @BeforeClass
-  public static void init() throws IOException {
-    GridmixTestUtils.initCluster(TestDistCacheEmulation.class);
-    File target=new File("target"+File.separator+TestDistCacheEmulation.class.getName());
-    if(!target.exists()){
-      assertTrue(target.mkdirs());
-    }
-    
-  }
-
-  @AfterClass
-  public static void shutDown() throws IOException {
-    GridmixTestUtils.shutdownCluster();
-  }
-
-  /**
-   * Validate the dist cache files generated by GenerateDistCacheData job.
-   * 
-   * @param jobConf
-   *          configuration of GenerateDistCacheData job.
-   * @param sortedFileSizes
-   *          array of sorted distributed cache file sizes
-   * @throws IOException
-   * @throws FileNotFoundException
-   */
-  private void validateDistCacheData(Configuration jobConf,
-      long[] sortedFileSizes) throws FileNotFoundException, IOException {
-    Path distCachePath = dce.getDistributedCacheDir();
-    String filesListFile = jobConf
-        .get(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_LIST);
-    FileSystem fs = FileSystem.get(jobConf);
-
-    // Validate the existence of Distributed Cache files list file directly
-    // under distributed cache directory
-    Path listFile = new Path(filesListFile);
-    assertTrue("Path of Distributed Cache files list file is wrong.",
-        distCachePath.equals(listFile.getParent().makeQualified(fs.getUri(), fs.getWorkingDirectory())));
-
-    // Delete the dist cache files list file
-    assertTrue(
-        "Failed to delete distributed Cache files list file " + listFile,
-        fs.delete(listFile,true));
-
-    List<Long> fileSizes = new ArrayList<Long>();
-    for (long size : sortedFileSizes) {
-      fileSizes.add(size);
-    }
-    // validate dist cache files after deleting the 'files list file'
-    validateDistCacheFiles(fileSizes, distCachePath);
-  }
-
-  /**
-   * Validate private/public distributed cache files.
-   * 
-   * @param filesSizesExpected
-   *          list of sizes of expected dist cache files
-   * @param distCacheDir
-   *          the distributed cache dir to be validated
-   * @throws IOException
-   * @throws FileNotFoundException
-   */
-  private void validateDistCacheFiles(List<Long> filesSizesExpected, Path distCacheDir)
-      throws FileNotFoundException, IOException {
-    // RemoteIterator<LocatedFileStatus> iter =
-    FileStatus[] statuses = GridmixTestUtils.dfs.listStatus(distCacheDir);
-    int numFiles = filesSizesExpected.size();
-    assertEquals("Number of files under distributed cache dir is wrong.",
-        numFiles, statuses.length);
-    for (int i = 0; i < numFiles; i++) {
-      FileStatus stat = statuses[i];
-      assertTrue("File size of distributed cache file "
-          + stat.getPath().toUri().getPath() + " is wrong.",
-          filesSizesExpected.remove(stat.getLen()));
-
-      FsPermission perm = stat.getPermission();
-      assertEquals("Wrong permissions for distributed cache file "
-          + stat.getPath().toUri().getPath(), new FsPermission(
-          GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_PERM), perm);
-    }
-  }
-
-  /**
-   * Configures 5 HDFS-based dist cache files and 1 local-FS-based dist cache
-   * file in the given Configuration object <code>conf</code>.
-   * 
-   * @param conf
-   *          configuration where dist cache config properties are to be set
-   * @return array of sorted HDFS-based distributed cache file sizes
-   * @throws IOException
-   */
-  private long[] configureDummyDistCacheFiles(Configuration conf)
-      throws IOException {
-    String user = UserGroupInformation.getCurrentUser().getShortUserName();
-    conf.set("user.name", user);
-    
-    // Set some dummy dist cache files in gridmix configuration so that they go
-    // into the configuration of JobStory objects.
-    String[] distCacheFiles = { "hdfs:///tmp/file1.txt",
-        "/tmp/" + user + "/.staging/job_1/file2.txt",
-        "hdfs:///user/user1/file3.txt", "/home/user2/file4.txt",
-        "subdir1/file5.txt", "subdir2/file6.gz" };
-
-    String[] fileSizes = { "400", "2500", "700", "1200", "1500", "500" };
-
-    String[] visibilities = { "true", "false", "false", "true", "true", "false" };
-    String[] timeStamps = { "1234", "2345", "34567", "5434", "125", "134" };
-
-    // DistributedCache.setCacheFiles(fileCaches, conf);
-    conf.setStrings(MRJobConfig.CACHE_FILES, distCacheFiles);
-    conf.setStrings(MRJobConfig.CACHE_FILES_SIZES, fileSizes);
-    conf.setStrings(JobContext.CACHE_FILE_VISIBILITIES, visibilities);
-    conf.setStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS, timeStamps);
-
-    // local FS based dist cache file whose path contains <user>/.staging is
-    // not created on HDFS. So file size 2500 is not added to sortedFileSizes.
-    long[] sortedFileSizes = new long[] { 1500, 1200, 700, 500, 400 };
-    return sortedFileSizes;
-  }
-
-  /**
-   * Runs setupGenerateDistCacheData() on a new DistrbutedCacheEmulator and and
-   * returns the jobConf. Fills the array <code>sortedFileSizes</code> that can
-   * be used for validation. Validation of exit code from
-   * setupGenerateDistCacheData() is done.
-   * 
-   * @param generate
-   *          true if -generate option is specified
-   * @param sortedFileSizes
-   *          sorted HDFS-based distributed cache file sizes
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  private Configuration runSetupGenerateDistCacheData(boolean generate,
-      long[] sortedFileSizes) throws IOException, InterruptedException {
-    Configuration conf = new Configuration();
-    long[] fileSizes = configureDummyDistCacheFiles(conf);
-    System.arraycopy(fileSizes, 0, sortedFileSizes, 0, fileSizes.length);
-
-    // Job stories of all 3 jobs will have same dist cache files in their
-    // configurations
-    final int numJobs = 3;
-    DebugJobProducer jobProducer = new DebugJobProducer(numJobs, conf);
-
-    Configuration jobConf = GridmixTestUtils.mrvl.getConfig();
-    Path ioPath = new Path("testSetupGenerateDistCacheData")
-        .makeQualified(GridmixTestUtils.dfs.getUri(),GridmixTestUtils.dfs.getWorkingDirectory());
-    FileSystem fs = FileSystem.get(jobConf);
-    if (fs.exists(ioPath)) {
-      fs.delete(ioPath, true);
-    }
-    FileSystem.mkdirs(fs, ioPath, new FsPermission((short) 0777));
-
-    dce = createDistributedCacheEmulator(jobConf, ioPath, generate);
-    int exitCode = dce.setupGenerateDistCacheData(jobProducer);
-    int expectedExitCode = generate ? 0
-        : Gridmix.MISSING_DIST_CACHE_FILES_ERROR;
-    assertEquals("setupGenerateDistCacheData failed.", expectedExitCode,
-        exitCode);
-
-    // reset back
-    resetDistCacheConfigProperties(jobConf);
-    return jobConf;
-  }
-
-  /**
-   * Reset the config properties related to Distributed Cache in the given job
-   * configuration <code>jobConf</code>.
-   * 
-   * @param jobConf
-   *          job configuration
-   */
-  private void resetDistCacheConfigProperties(Configuration jobConf) {
-    // reset current/latest property names
-    jobConf.setStrings(MRJobConfig.CACHE_FILES, "");
-    jobConf.setStrings(MRJobConfig.CACHE_FILES_SIZES, "");
-    jobConf.setStrings(MRJobConfig.CACHE_FILE_TIMESTAMPS, "");
-    jobConf.setStrings(JobContext.CACHE_FILE_VISIBILITIES, "");
-    // reset old property names
-    jobConf.setStrings("mapred.cache.files", "");
-    jobConf.setStrings("mapred.cache.files.filesizes", "");
-    jobConf.setStrings("mapred.cache.files.visibilities", "");
-    jobConf.setStrings("mapred.cache.files.timestamps", "");
-  }
-
-  /**
-   * Validate GenerateDistCacheData job if it creates dist cache files properly.
-   * 
-   * @throws Exception
-   */
-  @Test (timeout=200000)
-  public void testGenerateDistCacheData() throws Exception {
-    long[] sortedFileSizes = new long[5];
-    Configuration jobConf = runSetupGenerateDistCacheData(true, sortedFileSizes);
-    GridmixJob gridmixJob = new GenerateDistCacheData(jobConf);
-    Job job = gridmixJob.call();
-    assertEquals("Number of reduce tasks in GenerateDistCacheData is not 0.",
-        0, job.getNumReduceTasks());
-    assertTrue("GenerateDistCacheData job failed.",
-        job.waitForCompletion(false));
-    validateDistCacheData(jobConf, sortedFileSizes);
-  }
-
-  /**
-   * Validate setupGenerateDistCacheData by validating <li>permissions of the
-   * distributed cache directories and <li>content of the generated sequence
-   * file. This includes validation of dist cache file paths and their file
-   * sizes.
-   */
-  private void validateSetupGenDC(Configuration jobConf, long[] sortedFileSizes)
-      throws IOException, InterruptedException {
-    // build things needed for validation
-    long sumOfFileSizes = 0;
-    for (int i = 0; i < sortedFileSizes.length; i++) {
-      sumOfFileSizes += sortedFileSizes[i];
-    }
-
-    FileSystem fs = FileSystem.get(jobConf);
-    assertEquals("Number of distributed cache files to be generated is wrong.",
-        sortedFileSizes.length,
-        jobConf.getInt(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_COUNT, -1));
-    assertEquals("Total size of dist cache files to be generated is wrong.",
-        sumOfFileSizes,
-        jobConf.getLong(GenerateDistCacheData.GRIDMIX_DISTCACHE_BYTE_COUNT, -1));
-    Path filesListFile = new Path(
-        jobConf.get(GenerateDistCacheData.GRIDMIX_DISTCACHE_FILE_LIST));
-    FileStatus stat = fs.getFileStatus(filesListFile);
-    assertEquals("Wrong permissions of dist Cache files list file "
-        + filesListFile, new FsPermission((short) 0644), stat.getPermission());
-
-    InputSplit split = new FileSplit(filesListFile, 0, stat.getLen(),
-        (String[]) null);
-    TaskAttemptContext taskContext = MapReduceTestUtil
-        .createDummyMapTaskAttemptContext(jobConf);
-    RecordReader<LongWritable, BytesWritable> reader = new GenerateDistCacheData.GenDCDataFormat()
-        .createRecordReader(split, taskContext);
-    MapContext<LongWritable, BytesWritable, NullWritable, BytesWritable> mapContext = new MapContextImpl<LongWritable, BytesWritable, NullWritable, BytesWritable>(
-        jobConf, taskContext.getTaskAttemptID(), reader, null, null,
-        MapReduceTestUtil.createDummyReporter(), split);
-    reader.initialize(split, mapContext);
-
-    // start validating setupGenerateDistCacheData
-    doValidateSetupGenDC(reader, fs, sortedFileSizes);
-  }
-
-  /**
-   * Validate setupGenerateDistCacheData by validating <li>permissions of the
-   * distributed cache directory and <li>content of the generated sequence file.
-   * This includes validation of dist cache file paths and their file sizes.
-   */
-  private void doValidateSetupGenDC(
-      RecordReader<LongWritable, BytesWritable> reader, FileSystem fs,
-      long[] sortedFileSizes) throws IOException, InterruptedException {
-
-    // Validate permissions of dist cache directory
-    Path distCacheDir = dce.getDistributedCacheDir();
-    assertEquals(
-        "Wrong permissions for distributed cache dir " + distCacheDir,
-        fs.getFileStatus(distCacheDir).getPermission().getOtherAction()
-            .and(FsAction.EXECUTE), FsAction.EXECUTE);
-
-    // Validate the content of the sequence file generated by
-    // dce.setupGenerateDistCacheData().
-    LongWritable key = new LongWritable();
-    BytesWritable val = new BytesWritable();
-    for (int i = 0; i < sortedFileSizes.length; i++) {
-      assertTrue("Number of files written to the sequence file by "
-          + "setupGenerateDistCacheData is less than the expected.",
-          reader.nextKeyValue());
-      key = reader.getCurrentKey();
-      val = reader.getCurrentValue();
-      long fileSize = key.get();
-      String file = new String(val.getBytes(), 0, val.getLength());
-
-      // Dist Cache files should be sorted based on file size.
-      assertEquals("Dist cache file size is wrong.", sortedFileSizes[i],
-          fileSize);
-
-      // Validate dist cache file path.
-
-      // parent dir of dist cache file
-      Path parent = new Path(file).getParent().makeQualified(fs.getUri(),fs.getWorkingDirectory());
-      // should exist in dist cache dir
-      assertTrue("Public dist cache file path is wrong.",
-          distCacheDir.equals(parent));
-    }
-  }
-
-  /**
-   * Test if DistributedCacheEmulator's setup of GenerateDistCacheData is
-   * working as expected.
-   * 
-   * @throws IOException
-   * @throws InterruptedException
-   */
-  @Test  (timeout=20000)
-  public void testSetupGenerateDistCacheData() throws IOException,
-      InterruptedException {
-    long[] sortedFileSizes = new long[5];
-    Configuration jobConf = runSetupGenerateDistCacheData(true, sortedFileSizes);
-    validateSetupGenDC(jobConf, sortedFileSizes);
-
-    // Verify if correct exit code is seen when -generate option is missing and
-    // distributed cache files are missing in the expected path.
-    runSetupGenerateDistCacheData(false, sortedFileSizes);
-  }
-
-  /**
-   * Create DistributedCacheEmulator object and do the initialization by calling
-   * init() on it with dummy trace. Also configure the pseudo local FS.
-   */
-  private DistributedCacheEmulator createDistributedCacheEmulator(
-      Configuration conf, Path ioPath, boolean generate) throws IOException {
-    DistributedCacheEmulator dce = new DistributedCacheEmulator(conf, ioPath);
-    JobCreator jobCreator = JobCreator.getPolicy(conf, JobCreator.LOADJOB);
-    jobCreator.setDistCacheEmulator(dce);
-    dce.init("dummytrace", jobCreator, generate);
-    return dce;
-  }
-
-  /**
-   * Test the configuration property for disabling/enabling emulation of
-   * distributed cache load.
-   */
-  @Test  (timeout=2000)
-  public void testDistCacheEmulationConfigurability() throws IOException {
-    Configuration jobConf = GridmixTestUtils.mrvl.getConfig();
-    Path ioPath = new Path("testDistCacheEmulationConfigurability")
-        .makeQualified(GridmixTestUtils.dfs.getUri(),GridmixTestUtils.dfs.getWorkingDirectory());
-    FileSystem fs = FileSystem.get(jobConf);
-    FileSystem.mkdirs(fs, ioPath, new FsPermission((short) 0777));
-
-    // default config
-    dce = createDistributedCacheEmulator(jobConf, ioPath, false);
-    assertTrue("Default configuration of "
-        + DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE
-        + " is wrong.", dce.shouldEmulateDistCacheLoad());
-
-    // config property set to false
-    jobConf.setBoolean(
-        DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE, false);
-    dce = createDistributedCacheEmulator(jobConf, ioPath, false);
-    assertFalse("Disabling of emulation of distributed cache load by setting "
-        + DistributedCacheEmulator.GRIDMIX_EMULATE_DISTRIBUTEDCACHE
-        + " to false is not working.", dce.shouldEmulateDistCacheLoad());
-  }
-/** 
- * test method configureDistCacheFiles
- * 
- */
-  @Test  (timeout=2000)
-  public void testDistCacheEmulator() throws Exception {
-
-    Configuration conf = new Configuration();
-    configureDummyDistCacheFiles(conf);
-    File ws = new File("target" + File.separator + this.getClass().getName());
-    Path ioPath = new Path(ws.getAbsolutePath());
-
-    DistributedCacheEmulator dce = new DistributedCacheEmulator(conf, ioPath);
-    JobConf jobConf = new JobConf(conf);
-    jobConf.setUser(UserGroupInformation.getCurrentUser().getShortUserName());
-    File fin=new File("src"+File.separator+"test"+File.separator+"resources"+File.separator+"data"+File.separator+"wordcount.json");
-    dce.init(fin.getAbsolutePath(), JobCreator.LOADJOB, true);
-    dce.configureDistCacheFiles(conf, jobConf);
-    
-    String[] caches=conf.getStrings(MRJobConfig.CACHE_FILES);
-    String[] tmpfiles=conf.getStrings("tmpfiles");
-    // this method should fill caches AND tmpfiles  from MRJobConfig.CACHE_FILES property 
-    assertEquals(6, ((caches==null?0:caches.length)+(tmpfiles==null?0:tmpfiles.length)));
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFilePool.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFilePool.java
deleted file mode 100644
index f4a4ee60647..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFilePool.java
+++ /dev/null
@@ -1,189 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.io.OutputStream;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashSet;
-import java.util.Random;
-
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.BlockLocation;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-public class TestFilePool {
-
-  static final Logger LOG = LoggerFactory.getLogger(TestFileQueue.class);
-  static final int NFILES = 26;
-  static final Path base = getBaseDir();
-
-  static Path getBaseDir() {
-    try {
-      final Configuration conf = new Configuration();
-      final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-      return fs.makeQualified(new Path(
-          System.getProperty("test.build.data", "/tmp"), "testFilePool"));
-    } catch (IOException e) {
-      fail();
-    }
-    return null;
-  }
-
-  @BeforeClass
-  public static void setup() throws IOException {
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    fs.delete(base, true);
-    final Random r = new Random();
-    final long seed = r.nextLong();
-    r.setSeed(seed);
-    LOG.info("seed: " + seed);
-    fs.mkdirs(base);
-    for (int i = 0; i < NFILES; ++i) {
-      Path file = base;
-      for (double d = 0.6; d > 0.0; d *= 0.8) {
-        if (r.nextDouble() < d) {
-          file = new Path(base, Integer.toString(r.nextInt(3)));
-          continue;
-        }
-        break;
-      }
-      OutputStream out = null;
-      try {
-        out = fs.create(new Path(file, "" + (char)('A' + i)));
-        final byte[] b = new byte[1024];
-        Arrays.fill(b, (byte)('A' + i));
-        for (int len = ((i % 13) + 1) * 1024; len > 0; len -= 1024) {
-          out.write(b);
-        }
-      } finally {
-        if (out != null) {
-          out.close();
-        }
-      }
-    }
-  }
-
-  @AfterClass
-  public static void cleanup() throws IOException {
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    fs.delete(base, true);
-  }
-
-  @Test
-  public void testUnsuitable() throws Exception {
-    try {
-      final Configuration conf = new Configuration();
-      // all files 13k or less
-      conf.setLong(FilePool.GRIDMIX_MIN_FILE, 14 * 1024);
-      final FilePool pool = new FilePool(conf, base);
-      pool.refresh();
-    } catch (IOException e) {
-      return;
-    }
-    fail();
-  }
-
-  @Test
-  public void testPool() throws Exception {
-    final Random r = new Random();
-    final Configuration conf = new Configuration();
-    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);
-    final FilePool pool = new FilePool(conf, base);
-    pool.refresh();
-    final ArrayList<FileStatus> files = new ArrayList<FileStatus>();
-
-    // ensure 1k, 2k files excluded
-    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;
-    assertEquals(expectedPoolSize, pool.getInputFiles(Long.MAX_VALUE, files));
-    assertEquals(NFILES - 4, files.size());
-
-    // exact match
-    files.clear();
-    assertEquals(expectedPoolSize, pool.getInputFiles(expectedPoolSize, files));
-
-    // match random within 12k
-    files.clear();
-    final long rand = r.nextInt(expectedPoolSize);
-    assertTrue("Missed: " + rand,
-        (NFILES / 2) * 1024 > rand - pool.getInputFiles(rand, files));
-
-    // all files
-    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 0);
-    pool.refresh();
-    files.clear();
-    assertEquals((NFILES / 2 * (NFILES / 2 + 1)) * 1024,
-        pool.getInputFiles(Long.MAX_VALUE, files));
-  }
-
-  void checkSplitEq(FileSystem fs, CombineFileSplit split, long bytes)
-      throws Exception {
-    long splitBytes = 0L;
-    HashSet<Path> uniq = new HashSet<Path>();
-    for (int i = 0; i < split.getNumPaths(); ++i) {
-      splitBytes += split.getLength(i);
-      assertTrue(
-          split.getLength(i) <= fs.getFileStatus(split.getPath(i)).getLen());
-      assertFalse(uniq.contains(split.getPath(i)));
-      uniq.add(split.getPath(i));
-    }
-    assertEquals(bytes, splitBytes);
-  }
-
-  @Test
-  public void testStriper() throws Exception {
-    final Random r = new Random();
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    conf.setLong(FilePool.GRIDMIX_MIN_FILE, 3 * 1024);
-    final FilePool pool = new FilePool(conf, base) {
-      @Override
-      public BlockLocation[] locationsFor(FileStatus stat, long start, long len)
-          throws IOException {
-        return new BlockLocation[] { new BlockLocation() };
-      }
-    };
-    pool.refresh();
-
-    final int expectedPoolSize = (NFILES / 2 * (NFILES / 2 + 1) - 6) * 1024;
-    final InputStriper striper = new InputStriper(pool, expectedPoolSize);
-    int last = 0;
-    for (int i = 0; i < expectedPoolSize;
-        last = Math.min(expectedPoolSize - i, r.nextInt(expectedPoolSize))) {
-      checkSplitEq(fs, striper.splitFor(pool, last, 0), last);
-      i += last;
-    }
-    final InputStriper striper2 = new InputStriper(pool, expectedPoolSize);
-    checkSplitEq(fs, striper2.splitFor(pool, expectedPoolSize, 0),
-        expectedPoolSize);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFileQueue.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
deleted file mode 100644
index 41925655d13..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestFileQueue.java
+++ /dev/null
@@ -1,143 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.ByteArrayOutputStream;
-import java.io.IOException;
-import java.io.OutputStream;
-import java.util.Arrays;
-
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-
-public class TestFileQueue {
-
-  static final Logger LOG = LoggerFactory.getLogger(TestFileQueue.class);
-  static final int NFILES = 4;
-  static final int BLOCK = 256;
-  static final Path[] paths = new Path[NFILES];
-  static final String[] loc = new String[NFILES];
-  static final long[] start = new long[NFILES];
-  static final long[] len = new long[NFILES];
-
-  @BeforeClass
-  public static void setup() throws IOException {
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    final Path p = fs.makeQualified(new Path(
-        System.getProperty("test.build.data", "/tmp"), "testFileQueue"));
-    fs.delete(p, true);
-    final byte[] b = new byte[BLOCK];
-    for (int i = 0; i < NFILES; ++i) {
-      Arrays.fill(b, (byte)('A' + i));
-      paths[i] = new Path(p, "" + (char)('A' + i));
-      OutputStream f = null;
-      try {
-        f = fs.create(paths[i]);
-        f.write(b);
-      } finally {
-        if (f != null) {
-          f.close();
-        }
-      }
-    }
-  }
-
-  @AfterClass
-  public static void cleanup() throws IOException {
-    final Configuration conf = new Configuration();
-    final FileSystem fs = FileSystem.getLocal(conf).getRaw();
-    final Path p = fs.makeQualified(new Path(
-        System.getProperty("test.build.data", "/tmp"), "testFileQueue"));
-    fs.delete(p, true);
-  }
-
-  static ByteArrayOutputStream fillVerif() throws IOException {
-    final byte[] b = new byte[BLOCK];
-    final ByteArrayOutputStream out = new ByteArrayOutputStream();
-    for (int i = 0; i < NFILES; ++i) {
-      Arrays.fill(b, (byte)('A' + i));
-      out.write(b, 0, (int)len[i]);
-    }
-    return out;
-  }
-
-  @Test
-  public void testRepeat() throws Exception {
-    final Configuration conf = new Configuration();
-    Arrays.fill(loc, "");
-    Arrays.fill(start, 0L);
-    Arrays.fill(len, BLOCK);
-
-    final ByteArrayOutputStream out = fillVerif();
-    final FileQueue q =
-      new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);
-    final byte[] verif = out.toByteArray();
-    final byte[] check = new byte[2 * NFILES * BLOCK];
-    q.read(check, 0, NFILES * BLOCK);
-    assertArrayEquals(verif, Arrays.copyOf(check, NFILES * BLOCK));
-
-    final byte[] verif2 = new byte[2 * NFILES * BLOCK];
-    System.arraycopy(verif, 0, verif2, 0, verif.length);
-    System.arraycopy(verif, 0, verif2, verif.length, verif.length);
-    q.read(check, 0, 2 * NFILES * BLOCK);
-    assertArrayEquals(verif2, check);
-
-  }
-
-  @Test
-  public void testUneven() throws Exception {
-    final Configuration conf = new Configuration();
-    Arrays.fill(loc, "");
-    Arrays.fill(start, 0L);
-    Arrays.fill(len, BLOCK);
-
-    final int B2 = BLOCK / 2;
-    for (int i = 0; i < NFILES; i += 2) {
-      start[i] += B2;
-      len[i] -= B2;
-    }
-    final FileQueue q =
-      new FileQueue(new CombineFileSplit(paths, start, len, loc), conf);
-    final ByteArrayOutputStream out = fillVerif();
-    final byte[] verif = out.toByteArray();
-    final byte[] check = new byte[NFILES / 2 * BLOCK + NFILES / 2 * B2];
-    q.read(check, 0, verif.length);
-    assertArrayEquals(verif, Arrays.copyOf(check, verif.length));
-    q.read(check, 0, verif.length);
-    assertArrayEquals(verif, Arrays.copyOf(check, verif.length));
-  }
-
-  @Test
-  public void testEmpty() throws Exception {
-    final Configuration conf = new Configuration();
-    // verify OK if unused
-    final FileQueue q = new FileQueue(new CombineFileSplit(
-          new Path[0], new long[0], new long[0], new String[0]), conf);
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridMixClasses.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridMixClasses.java
deleted file mode 100644
index 372b2cab388..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridMixClasses.java
+++ /dev/null
@@ -1,989 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.InputStream;
-import java.util.ArrayList;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.CountDownLatch;
-
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-import org.apache.hadoop.CustomOutputCommitter;
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataInputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.PositionedReadable;
-import org.apache.hadoop.fs.Seekable;
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.LongWritable;
-import org.apache.hadoop.io.NullWritable;
-import org.apache.hadoop.io.RawComparator;
-import org.apache.hadoop.io.WritableUtils;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.JobContext;
-import org.apache.hadoop.mapred.RawKeyValueIterator;
-import org.apache.hadoop.mapred.gridmix.GridmixKey.Spec;
-import org.apache.hadoop.mapred.gridmix.SleepJob.SleepReducer;
-import org.apache.hadoop.mapred.gridmix.SleepJob.SleepSplit;
-import org.apache.hadoop.mapreduce.Counter;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.MapContext;
-import org.apache.hadoop.mapreduce.OutputCommitter;
-import org.apache.hadoop.mapreduce.RecordReader;
-import org.apache.hadoop.mapreduce.RecordWriter;
-import org.apache.hadoop.mapreduce.ReduceContext;
-import org.apache.hadoop.mapreduce.StatusReporter;
-import org.apache.hadoop.mapreduce.TaskAttemptContext;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.mapreduce.Mapper.Context;
-import org.apache.hadoop.mapreduce.counters.GenericCounter;
-import org.apache.hadoop.mapreduce.lib.input.CombineFileSplit;
-import org.apache.hadoop.mapreduce.lib.map.WrappedMapper;
-import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
-import org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer;
-import org.apache.hadoop.mapreduce.task.MapContextImpl;
-import org.apache.hadoop.mapreduce.task.ReduceContextImpl;
-import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl;
-import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl.DummyReporter;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.tools.rumen.ZombieJobProducer;
-import org.apache.hadoop.util.Progress;
-import org.junit.Assert;
-import org.junit.Test;
-
-import static org.mockito.Mockito.*;
-
-import static org.junit.Assert.*;
-
-public class TestGridMixClasses {
-  private static final Logger LOG = LoggerFactory.getLogger(TestGridMixClasses.class);
-
-  /*
-   * simple test LoadSplit (getters,copy, write, read...)
-   */
-  @Test (timeout=1000)
-  public void testLoadSplit() throws Exception {
-
-    LoadSplit test = getLoadSplit();
-
-    ByteArrayOutputStream data = new ByteArrayOutputStream();
-    DataOutputStream out = new DataOutputStream(data);
-    test.write(out);
-    LoadSplit copy = new LoadSplit();
-    copy.readFields(new DataInputStream(new ByteArrayInputStream(data
-            .toByteArray())));
-
-    // data should be the same
-    assertEquals(test.getId(), copy.getId());
-    assertEquals(test.getMapCount(), copy.getMapCount());
-    assertEquals(test.getInputRecords(), copy.getInputRecords());
-
-    assertEquals(test.getOutputBytes()[0], copy.getOutputBytes()[0]);
-    assertEquals(test.getOutputRecords()[0], copy.getOutputRecords()[0]);
-    assertEquals(test.getReduceBytes(0), copy.getReduceBytes(0));
-    assertEquals(test.getReduceRecords(0), copy.getReduceRecords(0));
-    assertEquals(test.getMapResourceUsageMetrics().getCumulativeCpuUsage(),
-            copy.getMapResourceUsageMetrics().getCumulativeCpuUsage());
-    assertEquals(test.getReduceResourceUsageMetrics(0).getCumulativeCpuUsage(),
-            copy.getReduceResourceUsageMetrics(0).getCumulativeCpuUsage());
-
-  }
-
-  /*
-   * simple test GridmixSplit (copy, getters, write, read..)
-   */
-  @Test (timeout=1000)
-  public void testGridmixSplit() throws Exception {
-    Path[] files = {new Path("one"), new Path("two")};
-    long[] start = {1, 2};
-    long[] lengths = {100, 200};
-    String[] locations = {"locOne", "loctwo"};
-
-    CombineFileSplit cfSplit = new CombineFileSplit(files, start, lengths,
-            locations);
-    ResourceUsageMetrics metrics = new ResourceUsageMetrics();
-    metrics.setCumulativeCpuUsage(200);
-
-    double[] reduceBytes = {8.1d, 8.2d};
-    double[] reduceRecords = {9.1d, 9.2d};
-    long[] reduceOutputBytes = {101L, 102L};
-    long[] reduceOutputRecords = {111L, 112L};
-
-    GridmixSplit test = new GridmixSplit(cfSplit, 2, 3, 4L, 5L, 6L, 7L,
-            reduceBytes, reduceRecords, reduceOutputBytes, reduceOutputRecords);
-
-    ByteArrayOutputStream data = new ByteArrayOutputStream();
-    DataOutputStream out = new DataOutputStream(data);
-    test.write(out);
-    GridmixSplit copy = new GridmixSplit();
-    copy.readFields(new DataInputStream(new ByteArrayInputStream(data
-            .toByteArray())));
-
-    // data should be the same
-    assertEquals(test.getId(), copy.getId());
-    assertEquals(test.getMapCount(), copy.getMapCount());
-    assertEquals(test.getInputRecords(), copy.getInputRecords());
-
-    assertEquals(test.getOutputBytes()[0], copy.getOutputBytes()[0]);
-    assertEquals(test.getOutputRecords()[0], copy.getOutputRecords()[0]);
-    assertEquals(test.getReduceBytes(0), copy.getReduceBytes(0));
-    assertEquals(test.getReduceRecords(0), copy.getReduceRecords(0));
-
-  }
-
-  /*
-   * test LoadMapper loadMapper should write to writer record for each reduce
-   */
-  @SuppressWarnings({"rawtypes", "unchecked"})
-  @Test (timeout=10000)
-  public void testLoadMapper() throws Exception {
-
-    Configuration conf = new Configuration();
-    conf.setInt(JobContext.NUM_REDUCES, 2);
-
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);
-
-    TaskAttemptID taskId = new TaskAttemptID();
-    RecordReader<NullWritable, GridmixRecord> reader = new FakeRecordReader();
-
-    LoadRecordGkGrWriter writer = new LoadRecordGkGrWriter();
-
-    OutputCommitter committer = new CustomOutputCommitter();
-    StatusReporter reporter = new TaskAttemptContextImpl.DummyReporter();
-    LoadSplit split = getLoadSplit();
-
-    MapContext<NullWritable, GridmixRecord, GridmixKey, GridmixRecord> mapContext = new MapContextImpl<NullWritable, GridmixRecord, GridmixKey, GridmixRecord>(
-            conf, taskId, reader, writer, committer, reporter, split);
-    // context
-    Context ctx = new WrappedMapper<NullWritable, GridmixRecord, GridmixKey, GridmixRecord>()
-            .getMapContext(mapContext);
-
-    reader.initialize(split, ctx);
-    ctx.getConfiguration().setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);
-    CompressionEmulationUtil.setCompressionEmulationEnabled(
-            ctx.getConfiguration(), true);
-
-    LoadJob.LoadMapper mapper = new LoadJob.LoadMapper();
-    // setup, map, clean
-    mapper.run(ctx);
-
-    Map<GridmixKey, GridmixRecord> data = writer.getData();
-    // check result
-    assertEquals(2, data.size());
-
-  }
-
-  private LoadSplit getLoadSplit() throws Exception {
-
-    Path[] files = {new Path("one"), new Path("two")};
-    long[] start = {1, 2};
-    long[] lengths = {100, 200};
-    String[] locations = {"locOne", "loctwo"};
-
-    CombineFileSplit cfSplit = new CombineFileSplit(files, start, lengths,
-            locations);
-    ResourceUsageMetrics metrics = new ResourceUsageMetrics();
-    metrics.setCumulativeCpuUsage(200);
-    ResourceUsageMetrics[] rMetrics = {metrics};
-
-    double[] reduceBytes = {8.1d, 8.2d};
-    double[] reduceRecords = {9.1d, 9.2d};
-    long[] reduceOutputBytes = {101L, 102L};
-    long[] reduceOutputRecords = {111L, 112L};
-
-    return new LoadSplit(cfSplit, 2, 1, 4L, 5L, 6L, 7L,
-            reduceBytes, reduceRecords, reduceOutputBytes, reduceOutputRecords,
-            metrics, rMetrics);
-  }
-
-  private class FakeRecordLLReader extends
-          RecordReader<LongWritable, LongWritable> {
-
-    int counter = 10;
-
-    @Override
-    public void initialize(InputSplit split, TaskAttemptContext context)
-            throws IOException, InterruptedException {
-
-    }
-
-    @Override
-    public boolean nextKeyValue() throws IOException, InterruptedException {
-      counter--;
-      return counter > 0;
-    }
-
-    @Override
-    public LongWritable getCurrentKey() throws IOException,
-            InterruptedException {
-
-      return new LongWritable(counter);
-    }
-
-    @Override
-    public LongWritable getCurrentValue() throws IOException,
-            InterruptedException {
-      return new LongWritable(counter * 10);
-    }
-
-    @Override
-    public float getProgress() throws IOException, InterruptedException {
-      return counter / 10.0f;
-    }
-
-    @Override
-    public void close() throws IOException {
-      // restore data
-      counter = 10;
-    }
-  }
-
-  private class FakeRecordReader extends
-          RecordReader<NullWritable, GridmixRecord> {
-
-    int counter = 10;
-
-    @Override
-    public void initialize(InputSplit split, TaskAttemptContext context)
-            throws IOException, InterruptedException {
-
-    }
-
-    @Override
-    public boolean nextKeyValue() throws IOException, InterruptedException {
-      counter--;
-      return counter > 0;
-    }
-
-    @Override
-    public NullWritable getCurrentKey() throws IOException,
-            InterruptedException {
-
-      return NullWritable.get();
-    }
-
-    @Override
-    public GridmixRecord getCurrentValue() throws IOException,
-            InterruptedException {
-      return new GridmixRecord(100, 100L);
-    }
-
-    @Override
-    public float getProgress() throws IOException, InterruptedException {
-      return counter / 10.0f;
-    }
-
-    @Override
-    public void close() throws IOException {
-      // restore data
-      counter = 10;
-    }
-  }
-
-  private class LoadRecordGkGrWriter extends
-          RecordWriter<GridmixKey, GridmixRecord> {
-    private Map<GridmixKey, GridmixRecord> data = new HashMap<GridmixKey, GridmixRecord>();
-
-    @Override
-    public void write(GridmixKey key, GridmixRecord value) throws IOException,
-            InterruptedException {
-      data.put(key, value);
-    }
-
-    @Override
-    public void close(TaskAttemptContext context) throws IOException,
-            InterruptedException {
-    }
-
-    public Map<GridmixKey, GridmixRecord> getData() {
-      return data;
-    }
-
-  }
-
-  private class LoadRecordGkNullWriter extends
-          RecordWriter<GridmixKey, NullWritable> {
-    private Map<GridmixKey, NullWritable> data = new HashMap<GridmixKey, NullWritable>();
-
-    @Override
-    public void write(GridmixKey key, NullWritable value) throws IOException,
-            InterruptedException {
-      data.put(key, value);
-    }
-
-    @Override
-    public void close(TaskAttemptContext context) throws IOException,
-            InterruptedException {
-    }
-
-    public Map<GridmixKey, NullWritable> getData() {
-      return data;
-    }
-
-  }
-
-  private class LoadRecordWriter extends
-          RecordWriter<NullWritable, GridmixRecord> {
-    private Map<NullWritable, GridmixRecord> data = new HashMap<NullWritable, GridmixRecord>();
-
-    @Override
-    public void write(NullWritable key, GridmixRecord value)
-            throws IOException, InterruptedException {
-      data.put(key, value);
-    }
-
-    @Override
-    public void close(TaskAttemptContext context) throws IOException,
-            InterruptedException {
-    }
-
-    public Map<NullWritable, GridmixRecord> getData() {
-      return data;
-    }
-
-  }
-
-  /*
-   * test LoadSortComparator
-   */
-  @Test (timeout=3000)
-  public void testLoadJobLoadSortComparator() throws Exception {
-    LoadJob.LoadSortComparator test = new LoadJob.LoadSortComparator();
-
-    ByteArrayOutputStream data = new ByteArrayOutputStream();
-    DataOutputStream dos = new DataOutputStream(data);
-    WritableUtils.writeVInt(dos, 2);
-    WritableUtils.writeVInt(dos, 1);
-    WritableUtils.writeVInt(dos, 4);
-    WritableUtils.writeVInt(dos, 7);
-    WritableUtils.writeVInt(dos, 4);
-
-    byte[] b1 = data.toByteArray();
-
-    byte[] b2 = data.toByteArray();
-
-    // the same data should be equals
-    assertEquals(0, test.compare(b1, 0, 1, b2, 0, 1));
-    b2[2] = 5;
-    // compare like GridMixKey first byte: shift count -1=4-5
-    assertEquals(-1, test.compare(b1, 0, 1, b2, 0, 1));
-    b2[2] = 2;
-    // compare like GridMixKey first byte: shift count 2=4-2
-    assertEquals(2, test.compare(b1, 0, 1, b2, 0, 1));
-    // compare arrays by first byte witch offset (2-1) because 4==4
-    b2[2] = 4;
-    assertEquals(1, test.compare(b1, 0, 1, b2, 1, 1));
-
-  }
-
-  /*
-   * test SpecGroupingComparator
-   */
-  @Test (timeout=3000)
-  public void testGridmixJobSpecGroupingComparator() throws Exception {
-    GridmixJob.SpecGroupingComparator test = new GridmixJob.SpecGroupingComparator();
-
-    ByteArrayOutputStream data = new ByteArrayOutputStream();
-    DataOutputStream dos = new DataOutputStream(data);
-    WritableUtils.writeVInt(dos, 2);
-    WritableUtils.writeVInt(dos, 1);
-    // 0: REDUCE SPEC
-    WritableUtils.writeVInt(dos, 0);
-    WritableUtils.writeVInt(dos, 7);
-    WritableUtils.writeVInt(dos, 4);
-
-    byte[] b1 = data.toByteArray();
-
-    byte[] b2 = data.toByteArray();
-
-    // the same object should be equals
-    assertEquals(0, test.compare(b1, 0, 1, b2, 0, 1));
-    b2[2] = 1;
-    // for Reduce
-    assertEquals(-1, test.compare(b1, 0, 1, b2, 0, 1));
-    // by Reduce spec
-    b2[2] = 1; // 1: DATA SPEC
-    assertEquals(-1, test.compare(b1, 0, 1, b2, 0, 1));
-    // compare GridmixKey the same objects should be equals
-    assertEquals(0, test.compare(new GridmixKey(GridmixKey.DATA, 100, 2),
-            new GridmixKey(GridmixKey.DATA, 100, 2)));
-    // REDUSE SPEC
-    assertEquals(-1, test.compare(
-            new GridmixKey(GridmixKey.REDUCE_SPEC, 100, 2), new GridmixKey(
-            GridmixKey.DATA, 100, 2)));
-    assertEquals(1, test.compare(new GridmixKey(GridmixKey.DATA, 100, 2),
-            new GridmixKey(GridmixKey.REDUCE_SPEC, 100, 2)));
-    // only DATA
-    assertEquals(2, test.compare(new GridmixKey(GridmixKey.DATA, 102, 2),
-            new GridmixKey(GridmixKey.DATA, 100, 2)));
-
-  }
-
-  /*
-   * test CompareGridmixJob only equals and compare
-   */
-  @Test (timeout=30000)
-  public void testCompareGridmixJob() throws Exception {
-    Configuration conf = new Configuration();
-    Path outRoot = new Path("target");
-    JobStory jobDesc = mock(JobStory.class);
-    when(jobDesc.getName()).thenReturn("JobName");
-    when(jobDesc.getJobConf()).thenReturn(new JobConf(conf));
-    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
-    GridmixJob j1 = new LoadJob(conf, 1000L, jobDesc, outRoot, ugi, 0);
-    GridmixJob j2 = new LoadJob(conf, 1000L, jobDesc, outRoot, ugi, 0);
-    GridmixJob j3 = new LoadJob(conf, 1000L, jobDesc, outRoot, ugi, 1);
-    GridmixJob j4 = new LoadJob(conf, 1000L, jobDesc, outRoot, ugi, 1);
-
-    assertTrue(j1.equals(j2));
-    assertEquals(0, j1.compareTo(j2));
-    // Only one parameter matters
-    assertFalse(j1.equals(j3));
-    // compare id and submissionMillis
-    assertEquals(-1, j1.compareTo(j3));
-    assertEquals(-1, j1.compareTo(j4));
-
-  }
-
-  /*
-   * test ReadRecordFactory. should read all data from inputstream
-   */
-  @Test (timeout=3000)
-  public void testReadRecordFactory() throws Exception {
-
-    // RecordFactory factory, InputStream src, Configuration conf
-    RecordFactory rf = new FakeRecordFactory();
-    FakeInputStream input = new FakeInputStream();
-    ReadRecordFactory test = new ReadRecordFactory(rf, input,
-            new Configuration());
-    GridmixKey key = new GridmixKey(GridmixKey.DATA, 100, 2);
-    GridmixRecord val = new GridmixRecord(200, 2);
-    while (test.next(key, val)) {
-
-    }
-    // should be read 10* (GridmixKey.size +GridmixRecord.value)
-    assertEquals(3000, input.getCounter());
-    // should be -1 because all data readed;
-    assertEquals(-1, rf.getProgress(), 0.01);
-
-    test.close();
-  }
-
-  private class FakeRecordFactory extends RecordFactory {
-
-    private int counter = 10;
-
-    @Override
-    public void close() throws IOException {
-
-    }
-
-    @Override
-    public boolean next(GridmixKey key, GridmixRecord val) throws IOException {
-      counter--;
-      return counter >= 0;
-    }
-
-    @Override
-    public float getProgress() throws IOException {
-      return counter;
-    }
-
-  }
-
-  private class FakeInputStream extends InputStream implements Seekable,
-          PositionedReadable {
-    private long counter;
-
-    @Override
-    public int read() throws IOException {
-      return 0;
-    }
-
-    @Override
-    public int read(byte[] b, int off, int len) throws IOException {
-      int realLen = len - off;
-      counter += realLen;
-      for (int i = 0; i < b.length; i++) {
-        b[i] = 0;
-      }
-      return realLen;
-    }
-
-    public long getCounter() {
-      return counter;
-    }
-
-    @Override
-    public void seek(long pos) throws IOException {
-
-    }
-
-    @Override
-    public long getPos() throws IOException {
-      return counter;
-    }
-
-    @Override
-    public boolean seekToNewSource(long targetPos) throws IOException {
-      return false;
-    }
-
-    @Override
-    public int read(long position, byte[] buffer, int offset, int length)
-            throws IOException {
-      return 0;
-    }
-
-    @Override
-    public void readFully(long position, byte[] buffer, int offset, int length)
-            throws IOException {
-
-    }
-
-    @Override
-    public void readFully(long position, byte[] buffer) throws IOException {
-
-    }
-  }
-
-  private class FakeFSDataInputStream extends FSDataInputStream {
-
-    public FakeFSDataInputStream(InputStream in) throws IOException {
-      super(in);
-
-    }
-
-  }
-
-  /*
-   * test LoadRecordReader. It class reads data from some files.
-   */
-  @Test (timeout=3000)
-  public void testLoadJobLoadRecordReader() throws Exception {
-    LoadJob.LoadRecordReader test = new LoadJob.LoadRecordReader();
-    Configuration conf = new Configuration();
-
-    FileSystem fs1 = mock(FileSystem.class);
-    when(fs1.open(any(Path.class))).thenReturn(
-            new FakeFSDataInputStream(new FakeInputStream()));
-    Path p1 = mock(Path.class);
-    when(p1.getFileSystem(any())).thenReturn(fs1);
-
-    FileSystem fs2 = mock(FileSystem.class);
-    when(fs2.open(any(Path.class))).thenReturn(
-            new FakeFSDataInputStream(new FakeInputStream()));
-    Path p2 = mock(Path.class);
-    when(p2.getFileSystem(any())).thenReturn(fs2);
-
-    Path[] paths = {p1, p2};
-
-    long[] start = {0, 0};
-    long[] lengths = {1000, 1000};
-    String[] locations = {"temp1", "temp2"};
-    CombineFileSplit cfsplit = new CombineFileSplit(paths, start, lengths,
-            locations);
-    double[] reduceBytes = {100, 100};
-    double[] reduceRecords = {2, 2};
-    long[] reduceOutputBytes = {500, 500};
-    long[] reduceOutputRecords = {2, 2};
-    ResourceUsageMetrics metrics = new ResourceUsageMetrics();
-    ResourceUsageMetrics[] rMetrics = {new ResourceUsageMetrics(),
-            new ResourceUsageMetrics()};
-    LoadSplit input = new LoadSplit(cfsplit, 2, 3, 1500L, 2L, 3000L, 2L,
-            reduceBytes, reduceRecords, reduceOutputBytes, reduceOutputRecords,
-            metrics, rMetrics);
-    TaskAttemptID taskId = new TaskAttemptID();
-    TaskAttemptContext ctx = new TaskAttemptContextImpl(conf, taskId);
-    test.initialize(input, ctx);
-    GridmixRecord gr = test.getCurrentValue();
-    int counter = 0;
-    while (test.nextKeyValue()) {
-      gr = test.getCurrentValue();
-      if (counter == 0) {
-        // read first file
-        assertEquals(0.5, test.getProgress(), 0.001);
-      } else if (counter == 1) {
-        // read second file
-        assertEquals(1.0, test.getProgress(), 0.001);
-      }
-      //
-      assertEquals(1000, gr.getSize());
-      counter++;
-    }
-    assertEquals(1000, gr.getSize());
-    // Two files have been read
-    assertEquals(2, counter);
-
-    test.close();
-  }
-
-  /*
-   * test LoadReducer
-   */
-
-  @Test (timeout=3000)
-  public void testLoadJobLoadReducer() throws Exception {
-    LoadJob.LoadReducer test = new LoadJob.LoadReducer();
-
-    Configuration conf = new Configuration();
-    conf.setInt(JobContext.NUM_REDUCES, 2);
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    conf.setBoolean(FileOutputFormat.COMPRESS, true);
-
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);
-    TaskAttemptID taskid = new TaskAttemptID();
-
-    RawKeyValueIterator input = new FakeRawKeyValueIterator();
-
-    Counter counter = new GenericCounter();
-    Counter inputValueCounter = new GenericCounter();
-    LoadRecordWriter output = new LoadRecordWriter();
-
-    OutputCommitter committer = new CustomOutputCommitter();
-
-    StatusReporter reporter = new DummyReporter();
-    RawComparator<GridmixKey> comparator = new FakeRawComparator();
-
-    ReduceContext<GridmixKey, GridmixRecord, NullWritable, GridmixRecord> reduceContext = new ReduceContextImpl<GridmixKey, GridmixRecord, NullWritable, GridmixRecord>(
-            conf, taskid, input, counter, inputValueCounter, output, committer,
-            reporter, comparator, GridmixKey.class, GridmixRecord.class);
-    // read for previous data
-    reduceContext.nextKeyValue();
-    org.apache.hadoop.mapreduce.Reducer<GridmixKey, GridmixRecord, NullWritable, GridmixRecord>.Context context = new WrappedReducer<GridmixKey, GridmixRecord, NullWritable, GridmixRecord>()
-            .getReducerContext(reduceContext);
-
-    // test.setup(context);
-    test.run(context);
-    // have been readed 9 records (-1 for previous)
-    assertEquals(9, counter.getValue());
-    assertEquals(10, inputValueCounter.getValue());
-    assertEquals(1, output.getData().size());
-    GridmixRecord record = output.getData().values().iterator()
-            .next();
-
-    assertEquals(1593, record.getSize());
-  }
-
-  protected class FakeRawKeyValueIterator implements RawKeyValueIterator {
-
-    int counter = 10;
-
-    @Override
-    public DataInputBuffer getKey() throws IOException {
-      ByteArrayOutputStream dt = new ByteArrayOutputStream();
-      GridmixKey key = new GridmixKey(GridmixKey.REDUCE_SPEC, 10 * counter, 1L);
-      Spec spec = new Spec();
-      spec.rec_in = counter;
-      spec.rec_out = counter;
-      spec.bytes_out = counter * 100;
-
-      key.setSpec(spec);
-      key.write(new DataOutputStream(dt));
-      DataInputBuffer result = new DataInputBuffer();
-      byte[] b = dt.toByteArray();
-      result.reset(b, 0, b.length);
-      return result;
-    }
-
-    @Override
-    public DataInputBuffer getValue() throws IOException {
-      ByteArrayOutputStream dt = new ByteArrayOutputStream();
-      GridmixRecord key = new GridmixRecord(100, 1);
-      key.write(new DataOutputStream(dt));
-      DataInputBuffer result = new DataInputBuffer();
-      byte[] b = dt.toByteArray();
-      result.reset(b, 0, b.length);
-      return result;
-    }
-
-    @Override
-    public boolean next() throws IOException {
-      counter--;
-      return counter >= 0;
-    }
-
-    @Override
-    public void close() throws IOException {
-
-    }
-
-    @Override
-    public Progress getProgress() {
-      return null;
-    }
-
-  }
-
-  private class FakeRawComparator implements RawComparator<GridmixKey> {
-
-    @Override
-    public int compare(GridmixKey o1, GridmixKey o2) {
-      return o1.compareTo(o2);
-    }
-
-    @Override
-    public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {
-      if ((l1 - s1) != (l2 - s2)) {
-        return (l1 - s1) - (l2 - s2);
-      }
-      int len = l1 - s1;
-      for (int i = 0; i < len; i++) {
-        if (b1[s1 + i] != b2[s2 + i]) {
-          return b1[s1 + i] - b2[s2 + i];
-        }
-      }
-      return 0;
-    }
-
-  }
-
-  /*
-   * test SerialJobFactory
-   */
-  @Test (timeout=120000)
-  public void testSerialReaderThread() throws Exception {
-
-    Configuration conf = new Configuration();
-    File fin = new File("src" + File.separator + "test" + File.separator
-            + "resources" + File.separator + "data" + File.separator
-            + "wordcount2.json");
-    // read couple jobs from wordcount2.json
-    JobStoryProducer jobProducer = new ZombieJobProducer(new Path(
-            fin.getAbsolutePath()), null, conf);
-    CountDownLatch startFlag = new CountDownLatch(1);
-    UserResolver resolver = new SubmitterUserResolver();
-    FakeJobSubmitter submitter = new FakeJobSubmitter();
-    File ws = new File("target" + File.separator + this.getClass().getName());
-    if (!ws.exists()) {
-      Assert.assertTrue(ws.mkdirs());
-    }
-
-    SerialJobFactory jobFactory = new SerialJobFactory(submitter, jobProducer,
-            new Path(ws.getAbsolutePath()), conf, startFlag, resolver);
-
-    Path ioPath = new Path(ws.getAbsolutePath());
-    jobFactory.setDistCacheEmulator(new DistributedCacheEmulator(conf, ioPath));
-    Thread test = jobFactory.createReaderThread();
-    test.start();
-    Thread.sleep(1000);
-    // SerialReaderThread waits startFlag
-    assertEquals(0, submitter.getJobs().size());
-    // start!
-    startFlag.countDown();
-    while (test.isAlive()) {
-      Thread.sleep(1000);
-      jobFactory.update(null);
-    }
-    // submitter was called twice
-    assertEquals(2, submitter.getJobs().size());
-  }
-
-  private class FakeJobSubmitter extends JobSubmitter {
-    // counter for submitted jobs
-    private List<GridmixJob> jobs = new ArrayList<GridmixJob>();
-
-    public FakeJobSubmitter() {
-      super(null, 1, 1, null, null);
-
-    }
-
-    @Override
-    public void add(GridmixJob job) throws InterruptedException {
-      jobs.add(job);
-    }
-
-    public List<GridmixJob> getJobs() {
-      return jobs;
-    }
-  }
-
-  /*
-   * test SleepMapper
-   */
-  @SuppressWarnings({"unchecked", "rawtypes"})
-  @Test (timeout=30000)
-  public void testSleepMapper() throws Exception {
-    SleepJob.SleepMapper test = new SleepJob.SleepMapper();
-
-    Configuration conf = new Configuration();
-    conf.setInt(JobContext.NUM_REDUCES, 2);
-
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);
-    TaskAttemptID taskId = new TaskAttemptID();
-    FakeRecordLLReader reader = new FakeRecordLLReader();
-    LoadRecordGkNullWriter writer = new LoadRecordGkNullWriter();
-    OutputCommitter committer = new CustomOutputCommitter();
-    StatusReporter reporter = new TaskAttemptContextImpl.DummyReporter();
-    SleepSplit split = getSleepSplit();
-    MapContext<LongWritable, LongWritable, GridmixKey, NullWritable> mapcontext = new MapContextImpl<LongWritable, LongWritable, GridmixKey, NullWritable>(
-            conf, taskId, reader, writer, committer, reporter, split);
-    Context context = new WrappedMapper<LongWritable, LongWritable, GridmixKey, NullWritable>()
-            .getMapContext(mapcontext);
-
-    long start = System.currentTimeMillis();
-    LOG.info("start:" + start);
-    LongWritable key = new LongWritable(start + 2000);
-    LongWritable value = new LongWritable(start + 2000);
-    // should slip 2 sec
-    test.map(key, value, context);
-    LOG.info("finish:" + System.currentTimeMillis());
-    assertTrue(System.currentTimeMillis() >= (start + 2000));
-
-    test.cleanup(context);
-    assertEquals(1, writer.getData().size());
-  }
-
-  private SleepSplit getSleepSplit() throws Exception {
-
-    String[] locations = {"locOne", "loctwo"};
-
-    long[] reduceDurations = {101L, 102L};
-
-    return new SleepSplit(0, 2000L, reduceDurations, 2, locations);
-  }
-
-  /*
-   * test SleepReducer
-   */
-  @Test (timeout=3000)
-  public void testSleepReducer() throws Exception {
-    Configuration conf = new Configuration();
-    conf.setInt(JobContext.NUM_REDUCES, 2);
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    conf.setBoolean(FileOutputFormat.COMPRESS, true);
-
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    conf.setBoolean(MRJobConfig.MAP_OUTPUT_COMPRESS, true);
-    TaskAttemptID taskId = new TaskAttemptID();
-
-    RawKeyValueIterator input = new FakeRawKeyValueReducerIterator();
-
-    Counter counter = new GenericCounter();
-    Counter inputValueCounter = new GenericCounter();
-    RecordWriter<NullWritable, NullWritable> output = new LoadRecordReduceWriter();
-
-    OutputCommitter committer = new CustomOutputCommitter();
-
-    StatusReporter reporter = new DummyReporter();
-    RawComparator<GridmixKey> comparator = new FakeRawComparator();
-
-    ReduceContext<GridmixKey, NullWritable, NullWritable, NullWritable> reducecontext = new ReduceContextImpl<GridmixKey, NullWritable, NullWritable, NullWritable>(
-            conf, taskId, input, counter, inputValueCounter, output, committer,
-            reporter, comparator, GridmixKey.class, NullWritable.class);
-    org.apache.hadoop.mapreduce.Reducer<GridmixKey, NullWritable, NullWritable, NullWritable>.Context context = new WrappedReducer<GridmixKey, NullWritable, NullWritable, NullWritable>()
-            .getReducerContext(reducecontext);
-
-    SleepReducer test = new SleepReducer();
-    long start = System.currentTimeMillis();
-    test.setup(context);
-    long sleeper = context.getCurrentKey().getReduceOutputBytes();
-    // status has been changed
-    assertEquals("Sleeping... " + sleeper + " ms left", context.getStatus());
-    // should sleep 0.9 sec
-
-    assertTrue(System.currentTimeMillis() >= (start + sleeper));
-    test.cleanup(context);
-    // status has been changed again
-
-    assertEquals("Slept for " + sleeper, context.getStatus());
-
-  }
-
-  private class LoadRecordReduceWriter extends
-          RecordWriter<NullWritable, NullWritable> {
-
-    @Override
-    public void write(NullWritable key, NullWritable value) throws IOException,
-            InterruptedException {
-    }
-
-    @Override
-    public void close(TaskAttemptContext context) throws IOException,
-            InterruptedException {
-    }
-
-  }
-
-  protected class FakeRawKeyValueReducerIterator implements RawKeyValueIterator {
-
-    int counter = 10;
-
-    @Override
-    public DataInputBuffer getKey() throws IOException {
-      ByteArrayOutputStream dt = new ByteArrayOutputStream();
-      GridmixKey key = new GridmixKey(GridmixKey.REDUCE_SPEC, 10 * counter, 1L);
-      Spec spec = new Spec();
-      spec.rec_in = counter;
-      spec.rec_out = counter;
-      spec.bytes_out = counter * 100;
-
-      key.setSpec(spec);
-      key.write(new DataOutputStream(dt));
-      DataInputBuffer result = new DataInputBuffer();
-      byte[] b = dt.toByteArray();
-      result.reset(b, 0, b.length);
-      return result;
-    }
-
-    @Override
-    public DataInputBuffer getValue() throws IOException {
-      ByteArrayOutputStream dt = new ByteArrayOutputStream();
-      NullWritable key = NullWritable.get();
-      key.write(new DataOutputStream(dt));
-      DataInputBuffer result = new DataInputBuffer();
-      byte[] b = dt.toByteArray();
-      result.reset(b, 0, b.length);
-      return result;
-    }
-
-    @Override
-    public boolean next() throws IOException {
-      counter--;
-      return counter >= 0;
-    }
-
-    @Override
-    public void close() throws IOException {
-
-    }
-
-    @Override
-    public Progress getProgress() {
-      return null;
-    }
-
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
deleted file mode 100644
index d79c0101041..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixMemoryEmulation.java
+++ /dev/null
@@ -1,457 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.gridmix.DebugJobProducer.MockJob;
-import org.apache.hadoop.mapred.gridmix.TestHighRamJob.DummyGridmixJob;
-import org.apache.hadoop.mapred.gridmix.TestResourceUsageEmulators.FakeProgressive;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.TotalHeapUsageEmulatorPlugin.DefaultHeapUsageEmulator;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin;
-
-/**
- * Test Gridmix memory emulation.
- */
-public class TestGridmixMemoryEmulation {
-  /**
-   * This is a dummy class that fakes heap usage.
-   */
-  private static class FakeHeapUsageEmulatorCore 
-  extends DefaultHeapUsageEmulator {
-    private int numCalls = 0;
-    
-    @Override
-    public void load(long sizeInMB) {
-      ++numCalls;
-      super.load(sizeInMB);
-    }
-    
-    // Get the total number of times load() was invoked
-    int getNumCalls() {
-      return numCalls;
-    }
-    
-    // Get the total number of 1mb objects stored within
-    long getHeapUsageInMB() {
-      return getHeapSpaceSize();
-    }
-    
-    @Override
-    public void reset() {
-      // no op to stop emulate() from resetting
-    }
-    
-    /**
-     * For re-testing purpose.
-     */
-    void resetFake() {
-      numCalls = 0;
-      super.reset();
-    }
-  }
-
-  /**
-   * This is a dummy class that fakes the heap usage emulator plugin.
-   */
-  private static class FakeHeapUsageEmulatorPlugin 
-  extends TotalHeapUsageEmulatorPlugin {
-    private FakeHeapUsageEmulatorCore core;
-    
-    public FakeHeapUsageEmulatorPlugin(FakeHeapUsageEmulatorCore core) {
-      super(core);
-      this.core = core;
-    }
-    
-    @Override
-    protected long getMaxHeapUsageInMB() {
-      return Long.MAX_VALUE / ONE_MB;
-    }
-    
-    @Override
-    protected long getTotalHeapUsageInMB() {
-      return core.getHeapUsageInMB();
-    }
-  }
-  
-  /**
-   * Test {@link TotalHeapUsageEmulatorPlugin}'s core heap usage emulation 
-   * engine.
-   */
-  @Test
-  public void testHeapUsageEmulator() throws IOException {
-    FakeHeapUsageEmulatorCore heapEmulator = new FakeHeapUsageEmulatorCore();
-    
-    long testSizeInMB = 10; // 10 mb
-    long previousHeap = heapEmulator.getHeapUsageInMB();
-    heapEmulator.load(testSizeInMB);
-    long currentHeap = heapEmulator.getHeapUsageInMB();
-    
-    // check if the heap has increased by expected value
-    assertEquals("Default heap emulator failed to load 10mb", 
-                 previousHeap + testSizeInMB, currentHeap);
-    
-    // test reset
-    heapEmulator.resetFake();
-    assertEquals("Default heap emulator failed to reset", 
-                 0, heapEmulator.getHeapUsageInMB());
-  }
-
-  /**
-   * Test {@link TotalHeapUsageEmulatorPlugin}.
-   */
-  @Test
-  public void testTotalHeapUsageEmulatorPlugin() throws Exception {
-    Configuration conf = new Configuration();
-    // set the dummy resource calculator for testing
-    ResourceCalculatorPlugin monitor = new DummyResourceCalculatorPlugin();
-    long maxHeapUsage = 1024 * TotalHeapUsageEmulatorPlugin.ONE_MB; // 1GB
-    conf.setLong(DummyResourceCalculatorPlugin.MAXPMEM_TESTING_PROPERTY, 
-                 maxHeapUsage);
-    monitor.setConf(conf);
-    
-    // no buffer to be reserved
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
-    // only 1 call to be made per cycle
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
-    long targetHeapUsageInMB = 200; // 200mb
-    
-    // fake progress indicator
-    FakeProgressive fakeProgress = new FakeProgressive();
-    
-    // fake heap usage generator
-    FakeHeapUsageEmulatorCore fakeCore = new FakeHeapUsageEmulatorCore();
-    
-    // a heap usage emulator with fake core
-    FakeHeapUsageEmulatorPlugin heapPlugin = 
-      new FakeHeapUsageEmulatorPlugin(fakeCore);
-    
-    // test with invalid or missing resource usage value
-    ResourceUsageMetrics invalidUsage = 
-      TestResourceUsageEmulators.createMetrics(0);
-    heapPlugin.initialize(conf, invalidUsage, null, null);
-    
-    // test if disabled heap emulation plugin's emulate() call is a no-operation
-    // this will test if the emulation plugin is disabled or not
-    int numCallsPre = fakeCore.getNumCalls();
-    long heapUsagePre = fakeCore.getHeapUsageInMB();
-    heapPlugin.emulate();
-    int numCallsPost = fakeCore.getNumCalls();
-    long heapUsagePost = fakeCore.getHeapUsageInMB();
-    
-    //  test if no calls are made heap usage emulator core
-    assertEquals("Disabled heap usage emulation plugin works!", 
-                 numCallsPre, numCallsPost);
-    //  test if no calls are made heap usage emulator core
-    assertEquals("Disabled heap usage emulation plugin works!", 
-                 heapUsagePre, heapUsagePost);
-    
-    // test with get progress
-    float progress = heapPlugin.getProgress();
-    assertEquals("Invalid progress of disabled cumulative heap usage emulation "
-                 + "plugin!", 1.0f, progress, 0f);
-    
-    // test with wrong/invalid configuration
-    Boolean failed = null;
-    invalidUsage = 
-      TestResourceUsageEmulators.createMetrics(maxHeapUsage 
-                                   + TotalHeapUsageEmulatorPlugin.ONE_MB);
-    try {
-      heapPlugin.initialize(conf, invalidUsage, monitor, null);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull("Fail case failure!", failed);
-    assertTrue("Expected failure!", failed); 
-    
-    // test with valid resource usage value
-    ResourceUsageMetrics metrics = 
-      TestResourceUsageEmulators.createMetrics(targetHeapUsageInMB 
-                                   * TotalHeapUsageEmulatorPlugin.ONE_MB);
-    
-    // test with default emulation interval
-    // in every interval, the emulator will add 100% of the expected usage 
-    // (since gridmix.emulators.resource-usage.heap.load-ratio=1)
-    // so at 10%, emulator will add 10% (difference), at 20% it will add 10% ...
-    // So to emulate 200MB, it will add
-    //   20mb + 20mb + 20mb + 20mb + .. = 200mb 
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 
-                          10);
-    
-    // test with custom value for emulation interval of 20%
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_EMULATION_PROGRESS_INTERVAL,
-                  0.2F);
-    //  40mb + 40mb + 40mb + 40mb + 40mb = 200mb
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 5);
-    
-    // test with custom value of free heap ratio and load ratio = 1
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0.5F);
-    //  40mb + 0mb + 80mb + 0mb + 0mb = 120mb
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 120, 2);
-    
-    // test with custom value of heap load ratio and min free heap ratio = 0
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 0.5F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
-    // 20mb (call#1) + 20mb (call#1) + 20mb (call#2) + 20mb (call#2) +.. = 200mb
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 200, 
-                          10);
-    
-    // test with custom value of free heap ratio = 0.3 and load ratio = 0.5
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0.25F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 0.5F);
-    // 20mb (call#1) + 20mb (call#1) + 30mb (call#2) + 0mb (call#2) 
-    // + 30mb (call#3) + 0mb (call#3) + 35mb (call#4) + 0mb (call#4)
-    // + 37mb (call#5) + 0mb (call#5) = 162mb
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, heapPlugin, 162, 6);
-    
-    // test if emulation interval boundary is respected
-    fakeProgress = new FakeProgressive(); // initialize
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.MIN_HEAP_FREE_RATIO, 0F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_LOAD_RATIO, 1F);
-    conf.setFloat(TotalHeapUsageEmulatorPlugin.HEAP_EMULATION_PROGRESS_INTERVAL,
-                  0.25F);
-    heapPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    fakeCore.resetFake();
-    // take a snapshot after the initialization
-    long initHeapUsage = fakeCore.getHeapUsageInMB();
-    long initNumCallsUsage = fakeCore.getNumCalls();
-    // test with 0 progress
-    testEmulationBoundary(0F, fakeCore, fakeProgress, heapPlugin, initHeapUsage, 
-                          initNumCallsUsage, "[no-op, 0 progress]");
-    // test with 24% progress
-    testEmulationBoundary(0.24F, fakeCore, fakeProgress, heapPlugin, 
-                          initHeapUsage, initNumCallsUsage, 
-                          "[no-op, 24% progress]");
-    // test with 25% progress
-    testEmulationBoundary(0.25F, fakeCore, fakeProgress, heapPlugin, 
-        targetHeapUsageInMB / 4, 1, "[op, 25% progress]");
-    // test with 80% progress
-    testEmulationBoundary(0.80F, fakeCore, fakeProgress, heapPlugin, 
-        (targetHeapUsageInMB * 4) / 5, 2, "[op, 80% progress]");
-    
-    // now test if the final call with 100% progress ramps up the heap usage
-    testEmulationBoundary(1F, fakeCore, fakeProgress, heapPlugin, 
-        targetHeapUsageInMB, 3, "[op, 100% progress]");
-  }
-
-  // test whether the heap usage emulator achieves the desired target using
-  // desired calls to the underling core engine.
-  private static void testEmulationAccuracy(Configuration conf, 
-                        FakeHeapUsageEmulatorCore fakeCore,
-                        ResourceCalculatorPlugin monitor,
-                        ResourceUsageMetrics metrics,
-                        TotalHeapUsageEmulatorPlugin heapPlugin,
-                        long expectedTotalHeapUsageInMB,
-                        long expectedTotalNumCalls)
-  throws Exception {
-    FakeProgressive fakeProgress = new FakeProgressive();
-    fakeCore.resetFake();
-    heapPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    int numLoops = 0;
-    while (fakeProgress.getProgress() < 1) {
-      ++numLoops;
-      float progress = numLoops / 100.0F;
-      fakeProgress.setProgress(progress);
-      heapPlugin.emulate();
-    }
-    
-    // test if the resource plugin shows the expected usage
-    assertEquals("Cumulative heap usage emulator plugin failed (total usage)!", 
-                 expectedTotalHeapUsageInMB, fakeCore.getHeapUsageInMB(), 1L);
-    // test if the resource plugin shows the expected num calls
-    assertEquals("Cumulative heap usage emulator plugin failed (num calls)!", 
-                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
-  }
-
-  // tests if the heap usage emulation plugin emulates only at the expected
-  // progress gaps
-  private static void testEmulationBoundary(float progress, 
-      FakeHeapUsageEmulatorCore fakeCore, FakeProgressive fakeProgress, 
-      TotalHeapUsageEmulatorPlugin heapPlugin, long expectedTotalHeapUsageInMB, 
-      long expectedTotalNumCalls, String info) throws Exception {
-    fakeProgress.setProgress(progress);
-    heapPlugin.emulate();
-    // test heap usage
-    assertEquals("Emulation interval test for heap usage failed " + info + "!", 
-                 expectedTotalHeapUsageInMB, fakeCore.getHeapUsageInMB(), 0L);
-    // test num calls
-    assertEquals("Emulation interval test for heap usage failed " + info + "!", 
-                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
-  }
-  
-  /**
-   * Test the specified task java heap options.
-   */
-  @SuppressWarnings("deprecation")
-  private void testJavaHeapOptions(String mapOptions, 
-      String reduceOptions, String taskOptions, String defaultMapOptions, 
-      String defaultReduceOptions, String defaultTaskOptions, 
-      String expectedMapOptions, String expectedReduceOptions, 
-      String expectedTaskOptions) throws Exception {
-    Configuration simulatedConf = new Configuration();
-    // reset the configuration parameters
-    simulatedConf.unset(MRJobConfig.MAP_JAVA_OPTS);
-    simulatedConf.unset(MRJobConfig.REDUCE_JAVA_OPTS);
-    simulatedConf.unset(JobConf.MAPRED_TASK_JAVA_OPTS);
-    
-    // set the default map task options
-    if (defaultMapOptions != null) {
-      simulatedConf.set(MRJobConfig.MAP_JAVA_OPTS, defaultMapOptions);
-    }
-    // set the default reduce task options
-    if (defaultReduceOptions != null) {
-      simulatedConf.set(MRJobConfig.REDUCE_JAVA_OPTS, defaultReduceOptions);
-    }
-    // set the default task options
-    if (defaultTaskOptions != null) {
-      simulatedConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, defaultTaskOptions);
-    }
-    
-    Configuration originalConf = new Configuration();
-    // reset the configuration parameters
-    originalConf.unset(MRJobConfig.MAP_JAVA_OPTS);
-    originalConf.unset(MRJobConfig.REDUCE_JAVA_OPTS);
-    originalConf.unset(JobConf.MAPRED_TASK_JAVA_OPTS);
-    
-    // set the map task options
-    if (mapOptions != null) {
-      originalConf.set(MRJobConfig.MAP_JAVA_OPTS, mapOptions);
-    }
-    // set the reduce task options
-    if (reduceOptions != null) {
-      originalConf.set(MRJobConfig.REDUCE_JAVA_OPTS, reduceOptions);
-    }
-    // set the task options
-    if (taskOptions != null) {
-      originalConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, taskOptions);
-    }
-    
-    // configure the task jvm's heap options
-    GridmixJob.configureTaskJVMOptions(originalConf, simulatedConf);
-    
-    assertEquals("Map heap options mismatch!", expectedMapOptions, 
-                 simulatedConf.get(MRJobConfig.MAP_JAVA_OPTS));
-    assertEquals("Reduce heap options mismatch!", expectedReduceOptions, 
-                 simulatedConf.get(MRJobConfig.REDUCE_JAVA_OPTS));
-    assertEquals("Task heap options mismatch!", expectedTaskOptions, 
-                 simulatedConf.get(JobConf.MAPRED_TASK_JAVA_OPTS));
-  }
-  
-  /**
-   * Test task-level java heap options configuration in {@link GridmixJob}.
-   */
-  @Test
-  public void testJavaHeapOptions() throws Exception {
-    // test missing opts
-    testJavaHeapOptions(null, null, null, null, null, null, null, null, 
-                        null);
-    
-    // test original heap opts and missing default opts
-    testJavaHeapOptions("-Xms10m", "-Xms20m", "-Xms30m", null, null, null,
-                        null, null, null);
-    
-    // test missing opts with default opts
-    testJavaHeapOptions(null, null, null, "-Xms10m", "-Xms20m", "-Xms30m",
-                        "-Xms10m", "-Xms20m", "-Xms30m");
-    
-    // test empty option
-    testJavaHeapOptions("", "", "", null, null, null, null, null, null);
-    
-    // test empty default option and no original heap options
-    testJavaHeapOptions(null, null, null, "", "", "", "", "", "");
-    
-    // test empty opts and default opts
-    testJavaHeapOptions("", "", "", "-Xmx10m -Xms1m", "-Xmx50m -Xms2m", 
-                        "-Xms2m -Xmx100m", "-Xmx10m -Xms1m", "-Xmx50m -Xms2m", 
-                        "-Xms2m -Xmx100m");
-    
-    // test custom heap opts with no default opts
-    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx30m", null, null, null,
-                        "-Xmx10m", "-Xmx20m", "-Xmx30m");
-    
-    // test heap opts with default opts (multiple value)
-    testJavaHeapOptions("-Xms5m -Xmx200m", "-Xms15m -Xmx300m", 
-                        "-Xms25m -Xmx50m", "-XXabc", "-XXxyz", "-XXdef", 
-                        "-XXabc -Xmx200m", "-XXxyz -Xmx300m", "-XXdef -Xmx50m");
-    
-    // test heap opts with default opts (duplication of -Xmx)
-    testJavaHeapOptions("-Xms5m -Xmx200m", "-Xms15m -Xmx300m", 
-                        "-Xms25m -Xmx50m", "-XXabc -Xmx500m", "-XXxyz -Xmx600m",
-                        "-XXdef -Xmx700m", "-XXabc -Xmx200m", "-XXxyz -Xmx300m",
-                        "-XXdef -Xmx50m");
-    
-    // test heap opts with default opts (single value)
-    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx50m", "-Xms2m", 
-                        "-Xms3m", "-Xms5m", "-Xms2m -Xmx10m", "-Xms3m -Xmx20m",
-                        "-Xms5m -Xmx50m");
-    
-    // test heap opts with default opts (duplication of -Xmx)
-    testJavaHeapOptions("-Xmx10m", "-Xmx20m", "-Xmx50m", "-Xmx2m", 
-                        "-Xmx3m", "-Xmx5m", "-Xmx10m", "-Xmx20m", "-Xmx50m");
-  }
-  
-  /**
-   * Test disabled task heap options configuration in {@link GridmixJob}.
-   */
-  @Test
-  @SuppressWarnings("deprecation")
-  public void testJavaHeapOptionsDisabled() throws Exception {
-    Configuration gridmixConf = new Configuration();
-    gridmixConf.setBoolean(GridmixJob.GRIDMIX_TASK_JVM_OPTIONS_ENABLE, false);
-    
-    // set the default values of simulated job
-    gridmixConf.set(MRJobConfig.MAP_JAVA_OPTS, "-Xmx1m");
-    gridmixConf.set(MRJobConfig.REDUCE_JAVA_OPTS, "-Xmx2m");
-    gridmixConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, "-Xmx3m");
-    
-    // set the default map and reduce task options for original job
-    final JobConf originalConf = new JobConf();
-    originalConf.set(MRJobConfig.MAP_JAVA_OPTS, "-Xmx10m");
-    originalConf.set(MRJobConfig.REDUCE_JAVA_OPTS, "-Xmx20m");
-    originalConf.set(JobConf.MAPRED_TASK_JAVA_OPTS, "-Xmx30m");
-    
-    // define a mock job
-    MockJob story = new MockJob(originalConf) {
-      public JobConf getJobConf() {
-        return originalConf;
-      }
-    };
-    
-    GridmixJob job = new DummyGridmixJob(gridmixConf, story);
-    Job simulatedJob = job.getJob();
-    Configuration simulatedConf = simulatedJob.getConfiguration();
-    
-    assertEquals("Map heap options works when disabled!", "-Xmx1m", 
-                 simulatedConf.get(MRJobConfig.MAP_JAVA_OPTS));
-    assertEquals("Reduce heap options works when disabled!", "-Xmx2m", 
-                 simulatedConf.get(MRJobConfig.REDUCE_JAVA_OPTS));
-    assertEquals("Task heap options works when disabled!", "-Xmx3m", 
-                 simulatedConf.get(JobConf.MAPRED_TASK_JAVA_OPTS));
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
deleted file mode 100644
index ddd98df94cc..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixRecord.java
+++ /dev/null
@@ -1,278 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Random;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.io.DataInputBuffer;
-import org.apache.hadoop.io.DataOutputBuffer;
-import org.apache.hadoop.io.WritableComparator;
-import org.apache.hadoop.io.WritableUtils;
-
-public class TestGridmixRecord {
-  private static final Logger LOG = LoggerFactory.getLogger(TestGridmixRecord.class);
-
-  static void lengthTest(GridmixRecord x, GridmixRecord y, int min,
-      int max) throws Exception {
-    final Random r = new Random();
-    final long seed = r.nextLong();
-    r.setSeed(seed);
-    LOG.info("length: " + seed);
-    final DataInputBuffer in = new DataInputBuffer();
-    final DataOutputBuffer out1 = new DataOutputBuffer();
-    final DataOutputBuffer out2 = new DataOutputBuffer();
-    for (int i = min; i < max; ++i) {
-      setSerialize(x, r.nextLong(), i, out1);
-      // check write
-      assertEquals(i, out1.getLength());
-      // write to stream
-      x.write(out2);
-      // check read
-      in.reset(out1.getData(), 0, out1.getLength());
-      y.readFields(in);
-      assertEquals(i, x.getSize());
-      assertEquals(i, y.getSize());
-    }
-    // check stream read
-    in.reset(out2.getData(), 0, out2.getLength());
-    for (int i = min; i < max; ++i) {
-      y.readFields(in);
-      assertEquals(i, y.getSize());
-    }
-  }
-
-  static void randomReplayTest(GridmixRecord x, GridmixRecord y, int min,
-      int max) throws Exception {
-    final Random r = new Random();
-    final long seed = r.nextLong();
-    r.setSeed(seed);
-    LOG.info("randReplay: " + seed);
-    final DataOutputBuffer out1 = new DataOutputBuffer();
-    for (int i = min; i < max; ++i) {
-      final int s = out1.getLength();
-      x.setSeed(r.nextLong());
-      x.setSize(i);
-      x.write(out1);
-      assertEquals(i, out1.getLength() - s);
-    }
-    final DataInputBuffer in = new DataInputBuffer();
-    in.reset(out1.getData(), 0, out1.getLength());
-    final DataOutputBuffer out2 = new DataOutputBuffer();
-    // deserialize written records, write to separate buffer
-    for (int i = min; i < max; ++i) {
-      final int s = in.getPosition();
-      y.readFields(in);
-      assertEquals(i, in.getPosition() - s);
-      y.write(out2);
-    }
-    // verify written contents match
-    assertEquals(out1.getLength(), out2.getLength());
-    // assumes that writes will grow buffer deterministically
-    assertEquals("Bad test", out1.getData().length, out2.getData().length);
-    assertArrayEquals(out1.getData(), out2.getData());
-  }
-
-  static void eqSeedTest(GridmixRecord x, GridmixRecord y, int max)
-      throws Exception {
-    final Random r = new Random();
-    final long s = r.nextLong();
-    r.setSeed(s);
-    LOG.info("eqSeed: " + s);
-    assertEquals(x.fixedBytes(), y.fixedBytes());
-    final int min = x.fixedBytes() + 1;
-    final DataOutputBuffer out1 = new DataOutputBuffer();
-    final DataOutputBuffer out2 = new DataOutputBuffer();
-    for (int i = min; i < max; ++i) {
-      final long seed = r.nextLong();
-      setSerialize(x, seed, i, out1);
-      setSerialize(y, seed, i, out2);
-      assertEquals(x, y);
-      assertEquals(x.hashCode(), y.hashCode());
-
-      // verify written contents match
-      assertEquals(out1.getLength(), out2.getLength());
-      // assumes that writes will grow buffer deterministically
-      assertEquals("Bad test", out1.getData().length, out2.getData().length);
-      assertArrayEquals(out1.getData(), out2.getData());
-    }
-  }
-
-  static void binSortTest(GridmixRecord x, GridmixRecord y, int min,
-      int max, WritableComparator cmp) throws Exception {
-    final Random r = new Random();
-    final long s = r.nextLong();
-    r.setSeed(s);
-    LOG.info("sort: " + s);
-    final DataOutputBuffer out1 = new DataOutputBuffer();
-    final DataOutputBuffer out2 = new DataOutputBuffer();
-    for (int i = min; i < max; ++i) {
-      final long seed1 = r.nextLong();
-      setSerialize(x, seed1, i, out1);
-      assertEquals(0, x.compareSeed(seed1, Math.max(0, i - x.fixedBytes())));
-
-      final long seed2 = r.nextLong();
-      setSerialize(y, seed2, i, out2);
-      assertEquals(0, y.compareSeed(seed2, Math.max(0, i - x.fixedBytes())));
-
-      // for eq sized records, ensure byte cmp where req
-      final int chk = WritableComparator.compareBytes(
-          out1.getData(), 0, out1.getLength(),
-          out2.getData(), 0, out2.getLength());
-      assertEquals(Integer.signum(chk), Integer.signum(x.compareTo(y)));
-      assertEquals(Integer.signum(chk), Integer.signum(cmp.compare(
-            out1.getData(), 0, out1.getLength(),
-            out2.getData(), 0, out2.getLength())));
-      // write second copy, compare eq
-      final int s1 = out1.getLength();
-      x.write(out1);
-      assertEquals(0, cmp.compare(out1.getData(), 0, s1,
-            out1.getData(), s1, out1.getLength() - s1));
-      final int s2 = out2.getLength();
-      y.write(out2);
-      assertEquals(0, cmp.compare(out2.getData(), 0, s2,
-            out2.getData(), s2, out2.getLength() - s2));
-      assertEquals(Integer.signum(chk), Integer.signum(cmp.compare(out1.getData(), 0, s1,
-            out2.getData(), s2, out2.getLength() - s2)));
-    }
-  }
-
-  static void checkSpec(GridmixKey a, GridmixKey b) throws Exception {
-    final Random r = new Random();
-    final long s = r.nextLong();
-    r.setSeed(s);
-    LOG.info("spec: " + s);
-    final DataInputBuffer in = new DataInputBuffer();
-    final DataOutputBuffer out = new DataOutputBuffer();
-    a.setType(GridmixKey.REDUCE_SPEC);
-    b.setType(GridmixKey.REDUCE_SPEC);
-    for (int i = 0; i < 100; ++i) {
-      final int in_rec = r.nextInt(Integer.MAX_VALUE);
-      a.setReduceInputRecords(in_rec);
-      final int out_rec = r.nextInt(Integer.MAX_VALUE);
-      a.setReduceOutputRecords(out_rec);
-      final int out_bytes = r.nextInt(Integer.MAX_VALUE);
-      a.setReduceOutputBytes(out_bytes);
-      final int min = WritableUtils.getVIntSize(in_rec)
-                    + WritableUtils.getVIntSize(out_rec)
-                    + WritableUtils.getVIntSize(out_bytes)
-                    + WritableUtils.getVIntSize(0);
-      assertEquals(min + 2, a.fixedBytes()); // meta + vint min
-      final int size = r.nextInt(1024) + a.fixedBytes() + 1;
-      setSerialize(a, r.nextLong(), size, out);
-      assertEquals(size, out.getLength());
-      assertTrue(a.equals(a));
-      assertEquals(0, a.compareTo(a));
-
-      in.reset(out.getData(), 0, out.getLength());
-
-      b.readFields(in);
-      assertEquals(size, b.getSize());
-      assertEquals(in_rec, b.getReduceInputRecords());
-      assertEquals(out_rec, b.getReduceOutputRecords());
-      assertEquals(out_bytes, b.getReduceOutputBytes());
-      assertTrue(a.equals(b));
-      assertEquals(0, a.compareTo(b));
-      assertEquals(a.hashCode(), b.hashCode());
-    }
-  }
-
-  static void setSerialize(GridmixRecord x, long seed, int size,
-      DataOutputBuffer out) throws IOException {
-    x.setSeed(seed);
-    x.setSize(size);
-    out.reset();
-    x.write(out);
-  }
-
-  @Test
-  public void testKeySpec() throws Exception {
-    final int min = 6;
-    final int max = 300;
-    final GridmixKey a = new GridmixKey(GridmixKey.REDUCE_SPEC, 1, 0L);
-    final GridmixKey b = new GridmixKey(GridmixKey.REDUCE_SPEC, 1, 0L);
-    lengthTest(a, b, min, max);
-    randomReplayTest(a, b, min, max);
-    binSortTest(a, b, min, max, new GridmixKey.Comparator());
-    // 2 fixed GR bytes, 1 type, 3 spec
-    eqSeedTest(a, b, max);
-    checkSpec(a, b);
-  }
-
-  @Test
-  public void testKeyData() throws Exception {
-    final int min = 2;
-    final int max = 300;
-    final GridmixKey a = new GridmixKey(GridmixKey.DATA, 1, 0L);
-    final GridmixKey b = new GridmixKey(GridmixKey.DATA, 1, 0L);
-    lengthTest(a, b, min, max);
-    randomReplayTest(a, b, min, max);
-    binSortTest(a, b, min, max, new GridmixKey.Comparator());
-    // 2 fixed GR bytes, 1 type
-    eqSeedTest(a, b, 300);
-  }
-
-  @Test
-  public void testBaseRecord() throws Exception {
-    final int min = 1;
-    final int max = 300;
-    final GridmixRecord a = new GridmixRecord();
-    final GridmixRecord b = new GridmixRecord();
-    lengthTest(a, b, min, max);
-    randomReplayTest(a, b, min, max);
-    binSortTest(a, b, min, max, new GridmixRecord.Comparator());
-    // 2 fixed GR bytes
-    eqSeedTest(a, b, 300);
-  }
-
-  public static void main(String[] argv) throws Exception {
-    boolean fail = false;
-    final TestGridmixRecord test = new TestGridmixRecord();
-    try { test.testKeySpec(); } catch (Exception e) {
-      fail = true;
-      e.printStackTrace();
-    }
-    try {test.testKeyData(); } catch (Exception e) {
-      fail = true;
-      e.printStackTrace();
-    }
-    try {test.testBaseRecord(); } catch (Exception e) {
-      fail = true;
-      e.printStackTrace();
-    }
-    System.exit(fail ? -1 : 0);
-  }
-
-  static void printDebug(GridmixRecord a, GridmixRecord b) throws IOException {
-    DataOutputBuffer out = new DataOutputBuffer();
-    a.write(out);
-    System.out.println("A " +
-        Arrays.toString(Arrays.copyOf(out.getData(), out.getLength())));
-    out.reset();
-    b.write(out);
-    System.out.println("B " +
-        Arrays.toString(Arrays.copyOf(out.getData(), out.getLength())));
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSubmission.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSubmission.java
deleted file mode 100644
index f84984346ea..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSubmission.java
+++ /dev/null
@@ -1,203 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.test.GenericTestUtils;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.apache.hadoop.util.ExitUtil;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.slf4j.event.Level;
-
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.InputStream;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.util.zip.GZIPInputStream;
-
-import static org.apache.hadoop.test.GenericTestUtils.assertExceptionContains;
-import static org.junit.Assert.*;
-import static org.slf4j.LoggerFactory.getLogger;
-
-public class TestGridmixSubmission extends CommonJobTest {
-  private static File inSpace = new File("src" + File.separator + "test"
-          + File.separator + "resources" + File.separator + "data");
-
-
-  static {
-    GenericTestUtils.setLogLevel(
-        getLogger("org.apache.hadoop.mapred.gridmix"), Level.DEBUG);
-  }
-
-
-  @BeforeClass
-  public static void init() throws IOException {
-    GridmixTestUtils.initCluster(TestGridmixSubmission.class);
-
-    System.setProperty("src.test.data", inSpace.getAbsolutePath());
-  }
-
-  @AfterClass
-  public static void shutDown() throws IOException {
-    GridmixTestUtils.shutdownCluster();
-  }
-
-  /**
-   * Verifies that the given {@code JobStory} corresponds to the checked-in
-   * WordCount {@code JobStory}. The verification is effected via JUnit
-   * assertions.
-   *
-   * @param js the candidate JobStory.
-   */
-  private void verifyWordCountJobStory(JobStory js) {
-    assertNotNull("Null JobStory", js);
-    String expectedJobStory = "WordCount:johndoe:default:1285322645148:3:1";
-    String actualJobStory = js.getName() + ":" + js.getUser() + ":"
-            + js.getQueueName() + ":" + js.getSubmissionTime() + ":"
-            + js.getNumberMaps() + ":" + js.getNumberReduces();
-    assertEquals("Unexpected JobStory", expectedJobStory, actualJobStory);
-  }
-
-  /**
-   * Expands a file compressed using {@code gzip}.
-   *
-   * @param fs  the {@code FileSystem} corresponding to the given file.
-   * @param in  the path to the compressed file.
-   * @param out the path to the uncompressed output.
-   * @throws Exception if there was an error during the operation.
-   */
-  private void expandGzippedTrace(FileSystem fs, Path in, Path out)
-          throws Exception {
-    byte[] buff = new byte[4096];
-    GZIPInputStream gis = new GZIPInputStream(fs.open(in));
-    FSDataOutputStream fsdOs = fs.create(out);
-    int numRead;
-    while ((numRead = gis.read(buff, 0, buff.length)) != -1) {
-      fsdOs.write(buff, 0, numRead);
-    }
-    gis.close();
-    fsdOs.close();
-  }
-
-  /**
-   * Tests the reading of traces in GridMix3. These traces are generated by
-   * Rumen and are in the JSON format. The traces can optionally be compressed
-   * and uncompressed traces can also be passed to GridMix3 via its standard
-   * input stream. The testing is effected via JUnit assertions.
-   *
-   * @throws Exception if there was an error.
-   */
-  @Test (timeout=20000)
-  public void testTraceReader() throws Exception {
-    Configuration conf = new Configuration();
-    FileSystem lfs = FileSystem.getLocal(conf);
-    Path rootInputDir = new Path(System.getProperty("src.test.data"));
-    rootInputDir = rootInputDir.makeQualified(lfs.getUri(),
-            lfs.getWorkingDirectory());
-    Path rootTempDir = new Path(System.getProperty("test.build.data",
-            System.getProperty("java.io.tmpdir")), "testTraceReader");
-    rootTempDir = rootTempDir.makeQualified(lfs.getUri(),
-            lfs.getWorkingDirectory());
-    Path inputFile = new Path(rootInputDir, "wordcount.json.gz");
-    Path tempFile = new Path(rootTempDir, "gridmix3-wc.json");
-
-    InputStream origStdIn = System.in;
-    InputStream tmpIs = null;
-    try {
-      DebugGridmix dgm = new DebugGridmix();
-      JobStoryProducer jsp = dgm.createJobStoryProducer(inputFile.toString(),
-              conf);
-
-      LOG.info("Verifying JobStory from compressed trace...");
-      verifyWordCountJobStory(jsp.getNextJob());
-
-      expandGzippedTrace(lfs, inputFile, tempFile);
-      jsp = dgm.createJobStoryProducer(tempFile.toString(), conf);
-      LOG.info("Verifying JobStory from uncompressed trace...");
-      verifyWordCountJobStory(jsp.getNextJob());
-
-      tmpIs = lfs.open(tempFile);
-      System.setIn(tmpIs);
-      LOG.info("Verifying JobStory from trace in standard input...");
-      jsp = dgm.createJobStoryProducer("-", conf);
-      verifyWordCountJobStory(jsp.getNextJob());
-    } finally {
-      System.setIn(origStdIn);
-      if (tmpIs != null) {
-        tmpIs.close();
-      }
-      lfs.delete(rootTempDir, true);
-    }
-  }
-
-  @Test (timeout=500000)
-  public void testReplaySubmit() throws Exception {
-    policy = GridmixJobSubmissionPolicy.REPLAY;
-    LOG.info(" Replay started at " + System.currentTimeMillis());
-    doSubmission(null, false);
-    LOG.info(" Replay ended at " + System.currentTimeMillis());
-
-  }
-
-  @Test (timeout=500000)
-  public void testStressSubmit() throws Exception {
-    policy = GridmixJobSubmissionPolicy.STRESS;
-    LOG.info(" Stress started at " + System.currentTimeMillis());
-    doSubmission(null, false);
-    LOG.info(" Stress ended at " + System.currentTimeMillis());
-  }
-
-  // test empty request should be hint message
-  @Test (timeout=100000)
-  public void testMain() throws Exception {
-
-    SecurityManager securityManager = System.getSecurityManager();
-
-    final ByteArrayOutputStream bytes = new ByteArrayOutputStream();
-    final PrintStream out = new PrintStream(bytes);
-    final PrintStream oldOut = System.out;
-    System.setErr(out);
-    ExitUtil.disableSystemExit();
-    try {
-      String[] argv = new String[0];
-      DebugGridmix.main(argv);
-
-    } catch (ExitUtil.ExitException e) {
-      assertExceptionContains(ExitUtil.EXIT_EXCEPTION_MESSAGE, e);
-      ExitUtil.resetFirstExitException();
-    } finally {
-      System.setErr(oldOut);
-      System.setSecurityManager(securityManager);
-    }
-    String print = bytes.toString();
-    // should be printed tip in std error stream
-    assertTrue(print
-            .contains("Usage: gridmix [-generate <MiB>] [-users URI] [-Dname=value ...] <iopath> <trace>"));
-    assertTrue(print.contains("e.g. gridmix -generate 100m foo -"));
-  }
-
- 
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
deleted file mode 100644
index f8884625cbd..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestGridmixSummary.java
+++ /dev/null
@@ -1,391 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.CommonConfigurationKeys;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.fs.permission.FsPermission;
-import org.apache.hadoop.mapred.JobClient;
-import org.apache.hadoop.mapred.UtilsForTests;
-import org.apache.hadoop.mapred.gridmix.GenerateData.DataStatistics;
-import org.apache.hadoop.mapred.gridmix.Statistics.ClusterStats;
-import org.apache.hadoop.mapred.gridmix.Statistics.JobStats;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.apache.hadoop.tools.rumen.JobStoryProducer;
-import org.junit.Test;
-
-/**
- * Test {@link ExecutionSummarizer} and {@link ClusterSummarizer}.
- */
-public class TestGridmixSummary {
-  
-  /**
-   * Test {@link DataStatistics}.
-   */
-  @Test
-  public void testDataStatistics() throws Exception {
-    // test data-statistics getters with compression enabled
-    DataStatistics stats = new DataStatistics(10, 2, true);
-    assertEquals("Data size mismatch", 10, stats.getDataSize());
-    assertEquals("Num files mismatch", 2, stats.getNumFiles());
-    assertTrue("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test data-statistics getters with compression disabled
-    stats = new DataStatistics(100, 5, false);
-    assertEquals("Data size mismatch", 100, stats.getDataSize());
-    assertEquals("Num files mismatch", 5, stats.getNumFiles());
-    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test publish data stats
-    Configuration conf = new Configuration();
-    Path rootTempDir = new Path(System.getProperty("test.build.data", "/tmp"));
-    Path testDir = new Path(rootTempDir, "testDataStatistics");
-    FileSystem fs = testDir.getFileSystem(conf);
-    fs.delete(testDir, true);
-    Path testInputDir = new Path(testDir, "test");
-    fs.mkdirs(testInputDir);
-    
-    // test empty folder (compression = true)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    Boolean failed = null;
-    try {
-      GenerateData.publishDataStatistics(testInputDir, 1024L, conf);
-      failed = false;
-    } catch (RuntimeException e) {
-      failed = true;
-    }
-    assertNotNull("Expected failure!", failed);
-    assertTrue("Compression data publishing error", failed);
-    
-    // test with empty folder (compression = off)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
-    stats = GenerateData.publishDataStatistics(testInputDir, 1024L, conf);
-    assertEquals("Data size mismatch", 0, stats.getDataSize());
-    assertEquals("Num files mismatch", 0, stats.getNumFiles());
-    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test with some plain input data (compression = off)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
-    Path inputDataFile = new Path(testInputDir, "test");
-    long size = 
-      UtilsForTests.createTmpFileDFS(fs, inputDataFile, 
-          FsPermission.createImmutable((short)777), "hi hello bye").size();
-    stats = GenerateData.publishDataStatistics(testInputDir, -1, conf);
-    assertEquals("Data size mismatch", size, stats.getDataSize());
-    assertEquals("Num files mismatch", 1, stats.getNumFiles());
-    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test with some plain input data (compression = on)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    failed = null;
-    try {
-      GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
-      failed = false;
-    } catch (RuntimeException e) {
-      failed = true;
-    }
-    assertNotNull("Expected failure!", failed);
-    assertTrue("Compression data publishing error", failed);
-    
-    // test with some compressed input data (compression = off)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, false);
-    fs.delete(inputDataFile, false);
-    inputDataFile = new Path(testInputDir, "test.gz");
-    size = 
-      UtilsForTests.createTmpFileDFS(fs, inputDataFile, 
-          FsPermission.createImmutable((short)777), "hi hello").size();
-    stats =  GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
-    assertEquals("Data size mismatch", size, stats.getDataSize());
-    assertEquals("Num files mismatch", 1, stats.getNumFiles());
-    assertFalse("Compression configuration mismatch", stats.isDataCompressed());
-    
-    // test with some compressed input data (compression = on)
-    CompressionEmulationUtil.setCompressionEmulationEnabled(conf, true);
-    stats = GenerateData.publishDataStatistics(testInputDir, 1234L, conf);
-    assertEquals("Data size mismatch", size, stats.getDataSize());
-    assertEquals("Num files mismatch", 1, stats.getNumFiles());
-    assertTrue("Compression configuration mismatch", stats.isDataCompressed());
-  }
-  
-  /**
-   * A fake {@link JobFactory}.
-   */
-  @SuppressWarnings("rawtypes")
-  private static class FakeJobFactory extends JobFactory {
-    /**
-     * A fake {@link JobStoryProducer} for {@link FakeJobFactory}.
-     */
-    private static class FakeJobStoryProducer implements JobStoryProducer {
-      @Override
-      public void close() throws IOException {
-      }
-
-      @Override
-      public JobStory getNextJob() throws IOException {
-        return null;
-      }
-    }
-    
-    FakeJobFactory(Configuration conf) {
-      super(null, new FakeJobStoryProducer(), null, conf, null, null);
-    }
-    
-    @Override
-    public void update(Object item) {
-    }
-    
-    @Override
-    protected Thread createReaderThread() {
-      return new Thread();
-    }
-  }
-  
-  /**
-   * Test {@link ExecutionSummarizer}.
-   */
-  @Test
-  @SuppressWarnings({ "unchecked", "rawtypes" })
-  public void testExecutionSummarizer() throws IOException {
-    Configuration conf = new Configuration();
-    
-    ExecutionSummarizer es = new ExecutionSummarizer();
-    assertEquals("ExecutionSummarizer init failed", 
-                 Summarizer.NA, es.getCommandLineArgsString());
-    
-    long startTime = System.currentTimeMillis();
-    // test configuration parameters
-    String[] initArgs = new String[] {"-Xmx20m", "-Dtest.args='test'"};
-    es = new ExecutionSummarizer(initArgs);
-    
-    assertEquals("ExecutionSummarizer init failed", 
-                 "-Xmx20m -Dtest.args='test'", 
-                 es.getCommandLineArgsString());
-    
-    // test start time
-    assertTrue("Start time mismatch", es.getStartTime() >= startTime);
-    assertTrue("Start time mismatch", 
-               es.getStartTime() <= System.currentTimeMillis());
-    
-    // test start() of ExecutionSummarizer
-    es.update(null);
-    assertEquals("ExecutionSummarizer init failed", 0, 
-                 es.getSimulationStartTime());
-    testExecutionSummarizer(0, 0, 0, 0, 0, 0, 0, es);
-    
-    long simStartTime = System.currentTimeMillis();
-    es.start(null);
-    assertTrue("Simulation start time mismatch", 
-               es.getSimulationStartTime() >= simStartTime);
-    assertTrue("Simulation start time mismatch", 
-               es.getSimulationStartTime() <= System.currentTimeMillis());
-    
-    // test with job stats
-    JobStats stats = generateFakeJobStats(1, 10, true, false);
-    es.update(stats);
-    testExecutionSummarizer(1, 10, 0, 1, 1, 0, 0, es);
-    
-    // test with failed job 
-    stats = generateFakeJobStats(5, 1, false, false);
-    es.update(stats);
-    testExecutionSummarizer(6, 11, 0, 2, 1, 1, 0, es);
-    
-    // test with successful but lost job 
-    stats = generateFakeJobStats(1, 1, true, true);
-    es.update(stats);
-    testExecutionSummarizer(7, 12, 0, 3, 1, 1, 1, es);
-    
-    // test with failed but lost job 
-    stats = generateFakeJobStats(2, 2, false, true);
-    es.update(stats);
-    testExecutionSummarizer(9, 14, 0, 4, 1, 1, 2, es);
-    
-    // test finalize
-    //  define a fake job factory
-    JobFactory factory = new FakeJobFactory(conf);
-    
-    // fake the num jobs in trace
-    factory.numJobsInTrace = 3;
-    
-    Path rootTempDir = new Path(System.getProperty("test.build.data", "/tmp"));
-    Path testDir = new Path(rootTempDir, "testGridmixSummary");
-    Path testTraceFile = new Path(testDir, "test-trace.json");
-    FileSystem fs = FileSystem.getLocal(conf);
-    fs.create(testTraceFile).close();
-    
-    // finalize the summarizer
-    UserResolver resolver = new RoundRobinUserResolver();
-    DataStatistics dataStats = new DataStatistics(100, 2, true);
-    String policy = GridmixJobSubmissionPolicy.REPLAY.name();
-    conf.set(GridmixJobSubmissionPolicy.JOB_SUBMISSION_POLICY, policy);
-    es.finalize(factory, testTraceFile.toString(), 1024L, resolver, dataStats, 
-                conf);
-    
-    // test num jobs in trace
-    assertEquals("Mismtach in num jobs in trace", 3, es.getNumJobsInTrace());
-    
-    // test trace signature
-    String tid = 
-      ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
-    assertEquals("Mismatch in trace signature", 
-                 tid, es.getInputTraceSignature());
-    // test trace location
-    Path qPath = fs.makeQualified(testTraceFile);
-    assertEquals("Mismatch in trace filename", 
-                 qPath.toString(), es.getInputTraceLocation());
-    // test expected data size
-    assertEquals("Mismatch in expected data size", 
-                 "1 K", es.getExpectedDataSize());
-    // test input data statistics
-    assertEquals("Mismatch in input data statistics", 
-                 ExecutionSummarizer.stringifyDataStatistics(dataStats), 
-                 es.getInputDataStatistics());
-    // test user resolver
-    assertEquals("Mismatch in user resolver", 
-                 resolver.getClass().getName(), es.getUserResolver());
-    // test policy
-    assertEquals("Mismatch in policy", policy, es.getJobSubmissionPolicy());
-    
-    // test data stringification using large data
-    es.finalize(factory, testTraceFile.toString(), 1024*1024*1024*10L, resolver,
-                dataStats, conf);
-    assertEquals("Mismatch in expected data size", 
-                 "10 G", es.getExpectedDataSize());
-    
-    // test trace signature uniqueness
-    //  touch the trace file
-    fs.delete(testTraceFile, false);
-    //  sleep for 1 sec
-    try {
-      Thread.sleep(1000);
-    } catch (InterruptedException ie) {}
-    fs.create(testTraceFile).close();
-    es.finalize(factory, testTraceFile.toString(), 0L, resolver, dataStats, 
-                conf);
-    // test missing expected data size
-    assertEquals("Mismatch in trace data size", 
-                 Summarizer.NA, es.getExpectedDataSize());
-    assertFalse("Mismatch in trace signature", 
-                tid.equals(es.getInputTraceSignature()));
-    // get the new identifier
-    tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
-    assertEquals("Mismatch in trace signature", 
-                 tid, es.getInputTraceSignature());
-    
-    testTraceFile = new Path(testDir, "test-trace2.json");
-    fs.create(testTraceFile).close();
-    es.finalize(factory, testTraceFile.toString(), 0L, resolver, dataStats, 
-                conf);
-    assertFalse("Mismatch in trace signature", 
-                tid.equals(es.getInputTraceSignature()));
-    // get the new identifier
-    tid = ExecutionSummarizer.getTraceSignature(testTraceFile.toString());
-    assertEquals("Mismatch in trace signature", 
-                 tid, es.getInputTraceSignature());
-    
-    // finalize trace identifier '-' input
-    es.finalize(factory, "-", 0L, resolver, dataStats, conf);
-    assertEquals("Mismatch in trace signature",
-                 Summarizer.NA, es.getInputTraceSignature());
-    assertEquals("Mismatch in trace file location", 
-                 Summarizer.NA, es.getInputTraceLocation());
-  }
-  
-  // test the ExecutionSummarizer
-  private static void testExecutionSummarizer(int numMaps, int numReds,
-      int totalJobsInTrace, int totalJobSubmitted, int numSuccessfulJob, 
-      int numFailedJobs, int numLostJobs, ExecutionSummarizer es) {
-    assertEquals("ExecutionSummarizer test failed [num-maps]", 
-                 numMaps, es.getNumMapTasksLaunched());
-    assertEquals("ExecutionSummarizer test failed [num-reducers]", 
-                 numReds, es.getNumReduceTasksLaunched());
-    assertEquals("ExecutionSummarizer test failed [num-jobs-in-trace]", 
-                 totalJobsInTrace, es.getNumJobsInTrace());
-    assertEquals("ExecutionSummarizer test failed [num-submitted jobs]", 
-                 totalJobSubmitted, es.getNumSubmittedJobs());
-    assertEquals("ExecutionSummarizer test failed [num-successful-jobs]", 
-                 numSuccessfulJob, es.getNumSuccessfulJobs());
-    assertEquals("ExecutionSummarizer test failed [num-failed jobs]", 
-                 numFailedJobs, es.getNumFailedJobs());
-    assertEquals("ExecutionSummarizer test failed [num-lost jobs]", 
-                 numLostJobs, es.getNumLostJobs());
-  }
-  
-  // generate fake job stats
-  @SuppressWarnings("deprecation")
-  private static JobStats generateFakeJobStats(final int numMaps, 
-      final int numReds, final boolean isSuccessful, final boolean lost) 
-  throws IOException {
-    // A fake job 
-    Job fakeJob = new Job() {
-      @Override
-      public int getNumReduceTasks() {
-        return numReds;
-      };
-      
-      @Override
-      public boolean isSuccessful() throws IOException {
-        if (lost) {
-          throw new IOException("Test failure!");
-        }
-        return isSuccessful;
-      };
-    };
-    return new JobStats(numMaps, numReds, fakeJob);
-  }
-  
-  /**
-   * Test {@link ClusterSummarizer}.
-   */
-  @Test
-  public void testClusterSummarizer() throws IOException {
-    ClusterSummarizer cs = new ClusterSummarizer();
-    Configuration conf = new Configuration();
-    
-    String jt = "test-jt:1234";
-    String nn = "test-nn:5678";
-    conf.set(JTConfig.JT_IPC_ADDRESS, jt);
-    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, nn);
-    cs.start(conf);
-    
-    assertEquals("JT name mismatch", jt, cs.getJobTrackerInfo());
-    assertEquals("NN name mismatch", nn, cs.getNamenodeInfo());
-    
-    ClusterStats cStats = ClusterStats.getClusterStats();
-    conf.set(JTConfig.JT_IPC_ADDRESS, "local");
-    conf.set(CommonConfigurationKeys.FS_DEFAULT_NAME_KEY, "local");
-    JobClient jc = new JobClient(conf);
-    cStats.setClusterMetric(jc.getClusterStatus());
-    
-    cs.update(cStats);
-    
-    // test
-    assertEquals("Cluster summary test failed!", 1, cs.getMaxMapTasks());
-    assertEquals("Cluster summary test failed!", 1, cs.getMaxReduceTasks());
-    assertEquals("Cluster summary test failed!", 1, cs.getNumActiveTrackers());
-    assertEquals("Cluster summary test failed!", 0, 
-                 cs.getNumBlacklistedTrackers());
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
deleted file mode 100644
index 179c94112d3..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestHighRamJob.java
+++ /dev/null
@@ -1,194 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.io.IOException;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapred.JobConf;
-import org.apache.hadoop.mapred.gridmix.DebugJobProducer.MockJob;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.mapreduce.MRConfig;
-import org.apache.hadoop.mapreduce.MRJobConfig;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.junit.Test;
-
-/**
- * Test if Gridmix correctly configures the simulated job's configuration for
- * high ram job properties.
- */
-public class TestHighRamJob {
-  /**
-   * A dummy {@link GridmixJob} that opens up the simulated job for testing.
-   */
-  protected static class DummyGridmixJob extends GridmixJob {
-    public DummyGridmixJob(Configuration conf, JobStory desc) 
-    throws IOException {
-      super(conf, System.currentTimeMillis(), desc, new Path("test"), 
-            UserGroupInformation.getCurrentUser(), -1);
-    }
-    
-    /**
-     * Do nothing since this is a dummy gridmix job.
-     */
-    @Override
-    public Job call() throws Exception {
-      return null;
-    }
-    
-    @Override
-    protected boolean canEmulateCompression() {
-      // return false as we don't need compression
-      return false;
-    }
-    
-    protected Job getJob() {
-      // open the simulated job for testing
-      return job;
-    }
-  }
-  
-  private static void testHighRamConfig(long jobMapMB, long jobReduceMB, 
-      long clusterMapMB, long clusterReduceMB, long simulatedClusterMapMB, 
-      long simulatedClusterReduceMB, long expectedMapMB, long expectedReduceMB, 
-      Configuration gConf) 
-  throws IOException {
-    Configuration simulatedJobConf = new Configuration(gConf);
-    simulatedJobConf.setLong(MRConfig.MAPMEMORY_MB, simulatedClusterMapMB);
-    simulatedJobConf.setLong(MRConfig.REDUCEMEMORY_MB, 
-                             simulatedClusterReduceMB);
-    
-    // define a source conf
-    Configuration sourceConf = new Configuration();
-    
-    // configure the original job
-    sourceConf.setLong(MRJobConfig.MAP_MEMORY_MB, jobMapMB);
-    sourceConf.setLong(MRConfig.MAPMEMORY_MB, clusterMapMB);
-    sourceConf.setLong(MRJobConfig.REDUCE_MEMORY_MB, jobReduceMB);
-    sourceConf.setLong(MRConfig.REDUCEMEMORY_MB, clusterReduceMB);
-    
-    // define a mock job
-    MockJob story = new MockJob(sourceConf);
-    
-    GridmixJob job = new DummyGridmixJob(simulatedJobConf, story);
-    Job simulatedJob = job.getJob();
-    JobConf simulatedConf = (JobConf)simulatedJob.getConfiguration();
-    
-    // check if the high ram properties are not set
-    assertEquals(expectedMapMB, 
-                 simulatedConf.getMemoryRequired(TaskType.MAP));
-    assertEquals(expectedReduceMB, 
-                 simulatedConf.getMemoryRequired(TaskType.REDUCE));
-  }
-  
-  /**
-   * Tests high ram job properties configuration.
-   */
-  @SuppressWarnings("deprecation")
-  @Test
-  public void testHighRamFeatureEmulation() throws IOException {
-    // define the gridmix conf
-    Configuration gridmixConf = new Configuration();
-    
-    // test : check high ram emulation disabled
-    gridmixConf.setBoolean(GridmixJob.GRIDMIX_HIGHRAM_EMULATION_ENABLE, false);
-    testHighRamConfig(10, 20, 5, 10, MRJobConfig.DEFAULT_MAP_MEMORY_MB, 
-                      MRJobConfig.DEFAULT_REDUCE_MEMORY_MB, 
-                      MRJobConfig.DEFAULT_MAP_MEMORY_MB, 
-                      MRJobConfig.DEFAULT_REDUCE_MEMORY_MB, gridmixConf);
-    
-    // test : check with high ram enabled (default) and no scaling
-    gridmixConf = new Configuration();
-    // set the deprecated max memory limit
-    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
-                        20*1024*1024);
-    testHighRamConfig(10, 20, 5, 10, 5, 10, 10, 20, gridmixConf);
-    
-    // test : check with high ram enabled and scaling
-    gridmixConf = new Configuration();
-    // set the new max map/reduce memory limits
-    gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 100);
-    gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 300);
-    testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-    
-    // test : check with high ram enabled and map memory scaling mismatch 
-    //        (deprecated)
-    gridmixConf = new Configuration();
-    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
-                        70*1024*1024);
-    Boolean failed = null;
-    try {
-      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull(failed);
-    assertTrue("Exception expected for exceeding map memory limit "
-               + "(deprecation)!", failed);
-    
-    // test : check with high ram enabled and reduce memory scaling mismatch 
-    //        (deprecated)
-    gridmixConf = new Configuration();
-    gridmixConf.setLong(JobConf.UPPER_LIMIT_ON_TASK_VMEM_PROPERTY, 
-                        150*1024*1024);
-    failed = null;
-    try {
-      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull(failed);
-    assertTrue("Exception expected for exceeding reduce memory limit "
-               + "(deprecation)!", failed);
-    
-    // test : check with high ram enabled and scaling mismatch on map limits
-    gridmixConf = new Configuration();
-    gridmixConf.setLong(JTConfig.JT_MAX_MAPMEMORY_MB, 70);
-    failed = null;
-    try {
-      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull(failed);
-    assertTrue("Exception expected for exceeding map memory limit!", failed);
-    
-    // test : check with high ram enabled and scaling mismatch on reduce 
-    //        limits
-    gridmixConf = new Configuration();
-    gridmixConf.setLong(JTConfig.JT_MAX_REDUCEMEMORY_MB, 200);
-    failed = null;
-    try {
-      testHighRamConfig(10, 45, 5, 15, 50, 100, 100, 300, gridmixConf);
-      failed = false;
-    } catch (Exception e) {
-      failed = true;
-    }
-    assertNotNull(failed);
-    assertTrue("Exception expected for exceeding reduce memory limit!", failed);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestLoadJob.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestLoadJob.java
deleted file mode 100644
index 3ed3d4e85ad..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestLoadJob.java
+++ /dev/null
@@ -1,82 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.test.GenericTestUtils;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.event.Level;
-
-import java.io.IOException;
-
-import static org.slf4j.LoggerFactory.getLogger;
-
-/*
- Test LoadJob Gridmix sends data to job and after that
- */
-public class TestLoadJob extends CommonJobTest {
-
-  public static final Logger LOG = getLogger(Gridmix.class);
-
-  static {
-    GenericTestUtils.setLogLevel(
-        getLogger("org.apache.hadoop.mapred.gridmix"), Level.DEBUG);
-    GenericTestUtils.setLogLevel(
-        getLogger(StressJobFactory.class), Level.DEBUG);
-  }
-
-
-  @BeforeClass
-  public static void init() throws IOException {
-    GridmixTestUtils.initCluster(TestLoadJob.class);
-  }
-
-  @AfterClass
-  public static void shutDown() throws IOException {
-    GridmixTestUtils.shutdownCluster();
-  }
-
-
-  /*
-  * test serial policy  with LoadJob. Task should execute without exceptions
-  */
-  @Test (timeout=500000)
-  public void testSerialSubmit() throws Exception {
-    policy = GridmixJobSubmissionPolicy.SERIAL;
-    LOG.info("Serial started at " + System.currentTimeMillis());
-    doSubmission(JobCreator.LOADJOB.name(), false);
-
-    LOG.info("Serial ended at " + System.currentTimeMillis());
-  }
-
-  /*
-   * test reply policy with LoadJob
-   */
-  @Test  (timeout=500000)
-  public void testReplaySubmit() throws Exception {
-    policy = GridmixJobSubmissionPolicy.REPLAY;
-    LOG.info(" Replay started at " + System.currentTimeMillis());
-    doSubmission(JobCreator.LOADJOB.name(), false);
-
-    LOG.info(" Replay ended at " + System.currentTimeMillis());
-  }
-
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
deleted file mode 100644
index 7179c5d0dda..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestPseudoLocalFs.java
+++ /dev/null
@@ -1,233 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.InputStream;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileStatus;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.junit.Test;
-
-/**
- * Test the basic functionality of PseudoLocalFs
- */
-public class TestPseudoLocalFs {
-
-  /**
-   * Test if a file on PseudoLocalFs of a specific size can be opened and read.
-   * Validate the size of the data read.
-   * Test the read methods of {@link PseudoLocalFs.RandomInputStream}.
-   * @throws Exception
-   */
-  @Test
-  public void testPseudoLocalFsFileSize() throws Exception {
-    long fileSize = 10000;
-    Path path = PseudoLocalFs.generateFilePath("myPsedoFile", fileSize);
-    PseudoLocalFs pfs = new PseudoLocalFs();
-    pfs.create(path);
-
-    // Read 1 byte at a time and validate file size.
-    InputStream in = pfs.open(path, 0);
-    long totalSize = 0;
-
-    while (in.read() >= 0) {
-      ++totalSize;
-    }
-    in.close();
-    assertEquals("File size mismatch with read().", fileSize, totalSize);
-
-    // Read data from PseudoLocalFs-based file into buffer to
-    // validate read(byte[]) and file size.
-    in = pfs.open(path, 0);
-    totalSize = 0;
-    byte[] b = new byte[1024];
-    int bytesRead = in.read(b);
-    while (bytesRead >= 0) {
-      totalSize += bytesRead;
-      bytesRead = in.read(b);
-    }
-    assertEquals("File size mismatch with read(byte[]).", fileSize, totalSize);
-  }
-
-  /**
-   * Validate if file status is obtained for correctly formed file paths on
-   * PseudoLocalFs and also verify if appropriate exception is thrown for
-   * invalid file paths.
-   * @param pfs Pseudo Local File System
-   * @param path file path for which getFileStatus() is to be called
-   * @param shouldSucceed <code>true</code> if getFileStatus() should succeed
-   * @throws IOException
-   */
-  private void validateGetFileStatus(FileSystem pfs, Path path,
-      boolean shouldSucceed) throws IOException {
-    boolean expectedExceptionSeen = false;
-    FileStatus stat = null;
-    try {
-      stat = pfs.getFileStatus(path);
-    } catch(FileNotFoundException e) {
-      expectedExceptionSeen = true;
-    }
-    if (shouldSucceed) {
-      assertFalse("getFileStatus() has thrown Exception for valid file name "
-                  + path, expectedExceptionSeen);
-      assertNotNull("Missing file status for a valid file.", stat);
-
-      // validate fileSize
-      String[] parts = path.toUri().getPath().split("\\.");
-      long expectedFileSize = Long.parseLong(parts[parts.length - 1]);
-      assertEquals("Invalid file size.", expectedFileSize, stat.getLen());
-    } else {
-      assertTrue("getFileStatus() did not throw Exception for invalid file "
-                 + " name " + path, expectedExceptionSeen);
-    }
-  }
-
-  /**
-   * Validate if file creation succeeds for correctly formed file paths on
-   * PseudoLocalFs and also verify if appropriate exception is thrown for
-   * invalid file paths.
-   * @param pfs Pseudo Local File System
-   * @param path file path for which create() is to be called
-   * @param shouldSucceed <code>true</code> if create() should succeed
-   * @throws IOException
-   */
-  private void validateCreate(FileSystem pfs, Path path,
-      boolean shouldSucceed) throws IOException {
-    boolean expectedExceptionSeen = false;
-    try {
-      pfs.create(path);
-    } catch(IOException e) {
-      expectedExceptionSeen = true;
-    }
-    if (shouldSucceed) {
-      assertFalse("create() has thrown Exception for valid file name "
-                  + path, expectedExceptionSeen);
-    } else {
-      assertTrue("create() did not throw Exception for invalid file name "
-                 + path, expectedExceptionSeen);
-    }
-  }
-
-  /**
-   * Validate if opening of file succeeds for correctly formed file paths on
-   * PseudoLocalFs and also verify if appropriate exception is thrown for
-   * invalid file paths.
-   * @param pfs Pseudo Local File System
-   * @param path file path for which open() is to be called
-   * @param shouldSucceed <code>true</code> if open() should succeed
-   * @throws IOException
-   */
-  private void validateOpen(FileSystem pfs, Path path,
-      boolean shouldSucceed) throws IOException {
-    boolean expectedExceptionSeen = false;
-    try {
-      pfs.open(path);
-    } catch(IOException e) {
-      expectedExceptionSeen = true;
-    }
-    if (shouldSucceed) {
-      assertFalse("open() has thrown Exception for valid file name "
-                  + path, expectedExceptionSeen);
-    } else {
-      assertTrue("open() did not throw Exception for invalid file name "
-                 + path, expectedExceptionSeen);
-    }
-  }
-
-  /**
-   * Validate if exists() returns <code>true</code> for correctly formed file
-   * paths on PseudoLocalFs and returns <code>false</code> for improperly
-   * formed file paths.
-   * @param pfs Pseudo Local File System
-   * @param path file path for which exists() is to be called
-   * @param shouldSucceed expected return value of exists(&lt;path&gt;)
-   * @throws IOException
-   */
-  private void validateExists(FileSystem pfs, Path path,
-      boolean shouldSucceed) throws IOException {
-    boolean ret = pfs.exists(path);
-    if (shouldSucceed) {
-      assertTrue("exists() returned false for valid file name " + path, ret);
-    } else {
-      assertFalse("exists() returned true for invalid file name " + path, ret);
-    }
-  }
-
-  /**
-   *  Test Pseudo Local File System methods like getFileStatus(), create(),
-   *  open(), exists() for <li> valid file paths and <li> invalid file paths.
-   * @throws IOException
-   */
-  @Test
-  public void testPseudoLocalFsFileNames() throws IOException {
-    PseudoLocalFs pfs = new PseudoLocalFs();
-    Configuration conf = new Configuration();
-    conf.setClass("fs.pseudo.impl", PseudoLocalFs.class, FileSystem.class);
-
-    Path path = new Path("pseudo:///myPsedoFile.1234");
-    FileSystem testFs = path.getFileSystem(conf);
-    assertEquals("Failed to obtain a pseudo local file system object from path",
-                 pfs.getUri().getScheme(), testFs.getUri().getScheme());
-
-    // Validate PseudoLocalFS operations on URI of some other file system
-    path = new Path("file:///myPsedoFile.12345");
-    validateGetFileStatus(pfs, path, false);
-    validateCreate(pfs, path, false);
-    validateOpen(pfs, path, false);
-    validateExists(pfs, path, false);
-
-    path = new Path("pseudo:///myPsedoFile");//.<fileSize> missing
-    validateGetFileStatus(pfs, path, false);
-    validateCreate(pfs, path, false);
-    validateOpen(pfs, path, false);
-    validateExists(pfs, path, false);
-
-    // thing after final '.' is not a number
-    path = new Path("pseudo:///myPsedoFile.txt");
-    validateGetFileStatus(pfs, path, false);
-    validateCreate(pfs, path, false);
-    validateOpen(pfs, path, false);
-    validateExists(pfs, path, false);
-
-    // Generate valid file name(relative path) and validate operations on it
-    long fileSize = 231456;
-    path = PseudoLocalFs.generateFilePath("my.Psedo.File", fileSize);
-    // Validate the above generateFilePath()
-    assertEquals("generateFilePath() failed.", fileSize,
-                 pfs.validateFileNameFormat(path));
-
-    validateGetFileStatus(pfs, path, true);
-    validateCreate(pfs, path, true);
-    validateOpen(pfs, path, true);
-    validateExists(pfs, path, true);
-
-    // Validate operations on valid qualified path
-    path = new Path("myPsedoFile.1237");
-    path = pfs.makeQualified(path);
-    validateGetFileStatus(pfs, path, true);
-    validateCreate(pfs, path, true);
-    validateOpen(pfs, path, true);
-    validateExists(pfs, path, true);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
deleted file mode 100644
index 4e85ce28922..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomAlgorithm.java
+++ /dev/null
@@ -1,132 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import static org.junit.Assert.*;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Map;
-import java.util.Random;
-import java.util.Set;
-
-import org.junit.Test;
-
-public class TestRandomAlgorithm {
-  private static final int[][] parameters = new int[][] {
-    {5, 1, 1}, 
-    {10, 1, 2},
-    {10, 2, 2},
-    {20, 1, 3},
-    {20, 2, 3},
-    {20, 3, 3},
-    {100, 3, 10},
-    {100, 3, 100},
-    {100, 3, 1000},
-    {100, 3, 10000},
-    {100, 3, 100000},
-    {100, 3, 1000000}
-  };
-  
-  private List<Integer> convertIntArray(int[] from) {
-    List<Integer> ret = new ArrayList<Integer>(from.length);
-    for (int v : from) {
-      ret.add(v);
-    }
-    return ret;
-  }
-  
-  private void testRandomSelectSelector(int niter, int m, int n) {
-    RandomAlgorithms.Selector selector = new RandomAlgorithms.Selector(n,
-        (double) m / n, new Random());
-    Map<List<Integer>, Integer> results = new HashMap<List<Integer>, Integer>(
-        niter);
-    for (int i = 0; i < niter; ++i, selector.reset()) {
-      int[] result = new int[m];
-      for (int j = 0; j < m; ++j) {
-        int v = selector.next();
-        if (v < 0)
-          break;
-        result[j]=v;
-      }
-      Arrays.sort(result);
-      List<Integer> resultAsList = convertIntArray(result);
-      Integer count = results.get(resultAsList);
-      if (count == null) {
-        results.put(resultAsList, 1);
-      } else {
-        results.put(resultAsList, ++count);
-      }
-    }
-
-    verifyResults(results, m, n);
-  }
-
-  private void testRandomSelect(int niter, int m, int n) {
-    Random random = new Random();
-    Map<List<Integer>, Integer> results = new HashMap<List<Integer>, Integer>(
-        niter);
-    for (int i = 0; i < niter; ++i) {
-      int[] result = RandomAlgorithms.select(m, n, random);
-      Arrays.sort(result);
-      List<Integer> resultAsList = convertIntArray(result);
-      Integer count = results.get(resultAsList);
-      if (count == null) {
-        results.put(resultAsList, 1);
-      } else {
-        results.put(resultAsList, ++count);
-      }
-    }
-
-    verifyResults(results, m, n);
-  }
-
-  private void verifyResults(Map<List<Integer>, Integer> results, int m, int n) {
-    if (n>=10) {
-      assertTrue(results.size() >= Math.min(m, 2));
-    }
-    for (List<Integer> result : results.keySet()) {
-      assertEquals(m, result.size());
-      Set<Integer> seen = new HashSet<Integer>();
-      for (int v : result) {
-        System.out.printf("%d ", v);
-        assertTrue((v >= 0) && (v < n));
-        assertTrue(seen.add(v));
-      }
-      System.out.printf(" ==> %d\n", results.get(result));
-    }
-    System.out.println("====");
-  }
-  
-  @Test
-  public void testRandomSelect() {
-    for (int[] param : parameters) {
-    testRandomSelect(param[0], param[1], param[2]);
-    }
-  }
-  
-  @Test
-  public void testRandomSelectSelector() {
-    for (int[] param : parameters) {
-      testRandomSelectSelector(param[0], param[1], param[2]);
-      }
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
deleted file mode 100644
index e302db5b69f..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRandomTextDataGenerator.java
+++ /dev/null
@@ -1,84 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
-
-import org.apache.hadoop.mapred.gridmix.RandomTextDataGenerator;
-
-import static org.junit.Assert.*;
-import org.junit.Test;
-
-/**
- * Test {@link RandomTextDataGenerator}.
- */
-public class TestRandomTextDataGenerator {
-  /**
-   * Test if {@link RandomTextDataGenerator} can generate random words of 
-   * desired size.
-   */
-  @Test
-  public void testRandomTextDataGenerator() {
-    RandomTextDataGenerator rtdg = new RandomTextDataGenerator(10, 0L, 5);
-    List<String> words = rtdg.getRandomWords();
-
-    // check the size
-    assertEquals("List size mismatch", 10, words.size());
-
-    // check the words
-    Set<String> wordsSet = new HashSet<String>(words);
-    assertEquals("List size mismatch due to duplicates", 10, wordsSet.size());
-
-    // check the word lengths
-    for (String word : wordsSet) {
-      assertEquals("Word size mismatch", 5, word.length());
-    }
-  }
-  
-  /**
-   * Test if {@link RandomTextDataGenerator} can generate same words given the
-   * same list-size, word-length and seed.
-   */
-  @Test
-  public void testRandomTextDataGeneratorRepeatability() {
-    RandomTextDataGenerator rtdg1 = new RandomTextDataGenerator(10, 0L, 5);
-    List<String> words1 = rtdg1.getRandomWords();
-
-    RandomTextDataGenerator rtdg2 = new RandomTextDataGenerator(10, 0L, 5);
-    List<String> words2 = rtdg2.getRandomWords();
-    
-    assertTrue("List mismatch", words1.equals(words2));
-  }
-  
-  /**
-   * Test if {@link RandomTextDataGenerator} can generate different words given 
-   * different seeds.
-   */
-  @Test
-  public void testRandomTextDataGeneratorUniqueness() {
-    RandomTextDataGenerator rtdg1 = new RandomTextDataGenerator(10, 1L, 5);
-    Set<String> words1 = new HashSet(rtdg1.getRandomWords());
-
-    RandomTextDataGenerator rtdg2 = new RandomTextDataGenerator(10, 0L, 5);
-    Set<String> words2 = new HashSet(rtdg2.getRandomWords());
-    
-    assertFalse("List size mismatch across lists", words1.equals(words2));
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
deleted file mode 100644
index 5050e133c07..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestRecordFactory.java
+++ /dev/null
@@ -1,81 +0,0 @@
-
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.util.Random;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.DataOutputBuffer;
-
-public class TestRecordFactory {
-  private static final Logger LOG = LoggerFactory.getLogger(TestRecordFactory.class);
-
-  public static void testFactory(long targetBytes, long targetRecs)
-      throws Exception {
-    final Configuration conf = new Configuration();
-    final GridmixKey key = new GridmixKey();
-    final GridmixRecord val = new GridmixRecord();
-    LOG.info("Target bytes/records: " + targetBytes + "/" + targetRecs);
-    final RecordFactory f = new AvgRecordFactory(targetBytes, targetRecs, conf);
-    targetRecs = targetRecs <= 0 && targetBytes >= 0
-      ? Math.max(1,
-                 targetBytes 
-                 / conf.getInt(AvgRecordFactory.GRIDMIX_MISSING_REC_SIZE, 
-                               64 * 1024))
-      : targetRecs;
-
-    long records = 0L;
-    final DataOutputBuffer out = new DataOutputBuffer();
-    while (f.next(key, val)) {
-      ++records;
-      key.write(out);
-      val.write(out);
-    }
-    assertEquals(targetRecs, records);
-    assertEquals(targetBytes, out.getLength());
-  }
-
-  @Test
-  public void testRandom() throws Exception {
-    final Random r = new Random();
-    final long targetBytes = r.nextInt(1 << 20) + 3 * (1 << 14);
-    final long targetRecs = r.nextInt(1 << 14);
-    testFactory(targetBytes, targetRecs);
-  }
-
-  @Test
-  public void testAvg() throws Exception {
-    final Random r = new Random();
-    final long avgsize = r.nextInt(1 << 10) + 1;
-    final long targetRecs = r.nextInt(1 << 14);
-    testFactory(targetRecs * avgsize, targetRecs);
-  }
-
-  @Test
-  public void testZero() throws Exception {
-    final Random r = new Random();
-    final long targetBytes = r.nextInt(1 << 20);
-    testFactory(targetBytes, 0);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
deleted file mode 100644
index f14087bf3af..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestResourceUsageEmulators.java
+++ /dev/null
@@ -1,614 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.StatusReporter;
-import org.apache.hadoop.mapreduce.TaskAttemptID;
-import org.apache.hadoop.mapreduce.TaskInputOutputContext;
-import org.apache.hadoop.mapreduce.TaskType;
-import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig;
-import org.apache.hadoop.mapreduce.task.MapContextImpl;
-import org.apache.hadoop.tools.rumen.ResourceUsageMetrics;
-import org.apache.hadoop.mapred.gridmix.LoadJob.ResourceUsageMatcherRunner;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageEmulatorPlugin;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.ResourceUsageMatcher;
-import org.apache.hadoop.mapred.gridmix.emulators.resourceusage.CumulativeCpuUsageEmulatorPlugin.DefaultCpuUsageEmulator;
-import org.apache.hadoop.yarn.util.ResourceCalculatorPlugin;
-
-/**
- * Test Gridmix's resource emulator framework and supported plugins.
- */
-public class TestResourceUsageEmulators {
-  /**
-   * A {@link ResourceUsageEmulatorPlugin} implementation for testing purpose.
-   * It essentially creates a file named 'test' in the test directory.
-   */
-  static class TestResourceUsageEmulatorPlugin 
-  implements ResourceUsageEmulatorPlugin {
-    static final Path rootTempDir =
-        new Path(System.getProperty("test.build.data", "/tmp"));
-    static final Path tempDir = 
-      new Path(rootTempDir, "TestResourceUsageEmulatorPlugin");
-    static final String DEFAULT_IDENTIFIER = "test";
-    
-    private Path touchPath = null;
-    private FileSystem fs = null;
-    
-    @Override
-    public void emulate() throws IOException, InterruptedException {
-      // add some time between 2 calls to emulate()
-      try {
-        Thread.sleep(1000); // sleep for 1s
-      } catch (Exception e){}
-      
-      try {
-        fs.delete(touchPath, false); // delete the touch file
-        //TODO Search for a better touch utility
-        fs.create(touchPath).close(); // recreate it
-      } catch (Exception e) {
-        throw new RuntimeException(e);
-      }
-    }
-    
-    protected String getIdentifier() {
-      return DEFAULT_IDENTIFIER;
-    }
-    
-    private static Path getFilePath(String id) {
-      return new Path(tempDir, id);
-    }
-    
-    private static Path getInitFilePath(String id) {
-      return new Path(tempDir, id + ".init");
-    }
-    
-    @Override
-    public void initialize(Configuration conf, ResourceUsageMetrics metrics,
-        ResourceCalculatorPlugin monitor, Progressive progress) {
-      // add some time between 2 calls to initialize()
-      try {
-        Thread.sleep(1000); // sleep for 1s
-      } catch (Exception e){}
-      
-      try {
-        fs = FileSystem.getLocal(conf);
-        
-        Path initPath = getInitFilePath(getIdentifier());
-        fs.delete(initPath, false); // delete the old file
-        fs.create(initPath).close(); // create a new one
-        
-        touchPath = getFilePath(getIdentifier());
-        fs.delete(touchPath, false);
-      } catch (Exception e) {
-        
-      } finally {
-        if (fs != null) {
-          try {
-            fs.deleteOnExit(tempDir);
-          } catch (IOException ioe){}
-        }
-      }
-    }
-    
-    // test if the emulation framework successfully loaded this plugin
-    static long testInitialization(String id, Configuration conf) 
-    throws IOException {
-      Path testPath = getInitFilePath(id);
-      FileSystem fs = FileSystem.getLocal(conf);
-      return fs.exists(testPath) 
-             ? fs.getFileStatus(testPath).getModificationTime() 
-             : 0;
-    }
-    
-    // test if the emulation framework successfully loaded this plugin
-    static long testEmulation(String id, Configuration conf) 
-    throws IOException {
-      Path testPath = getFilePath(id);
-      FileSystem fs = FileSystem.getLocal(conf);
-      return fs.exists(testPath) 
-             ? fs.getFileStatus(testPath).getModificationTime() 
-             : 0;
-    }
-    
-    @Override
-    public float getProgress() {
-      try {
-        return fs.exists(touchPath) ? 1.0f : 0f;
-      } catch (IOException ioe) {}
-      return 0f;
-    }
-  }
-  
-  /**
-   * Test implementation of {@link ResourceUsageEmulatorPlugin} which creates
-   * a file named 'others' in the test directory.
-   */
-  static class TestOthers extends TestResourceUsageEmulatorPlugin {
-    static final String ID = "others";
-    
-    @Override
-    protected String getIdentifier() {
-      return ID;
-    }
-  }
-  
-  /**
-   * Test implementation of {@link ResourceUsageEmulatorPlugin} which creates
-   * a file named 'cpu' in the test directory.
-   */
-  static class TestCpu extends TestResourceUsageEmulatorPlugin {
-    static final String ID = "cpu";
-    
-    @Override
-    protected String getIdentifier() {
-      return ID;
-    }
-  }
-  
-  /**
-   * Test {@link ResourceUsageMatcher}.
-   */
-  @Test
-  public void testResourceUsageMatcher() throws Exception {
-    ResourceUsageMatcher matcher = new ResourceUsageMatcher();
-    Configuration conf = new Configuration();
-    conf.setClass(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
-                  TestResourceUsageEmulatorPlugin.class, 
-                  ResourceUsageEmulatorPlugin.class);
-    long currentTime = System.currentTimeMillis();
-    
-    matcher.configure(conf, null, null, null);
-    
-    matcher.matchResourceUsage();
-    
-    String id = TestResourceUsageEmulatorPlugin.DEFAULT_IDENTIFIER;
-    long result = 
-      TestResourceUsageEmulatorPlugin.testInitialization(id, conf);
-    assertTrue("Resource usage matcher failed to initialize the configured"
-               + " plugin", result > currentTime);
-    result = TestResourceUsageEmulatorPlugin.testEmulation(id, conf);
-    assertTrue("Resource usage matcher failed to load and emulate the"
-               + " configured plugin", result > currentTime);
-    
-    // test plugin order to first emulate cpu and then others
-    conf.setStrings(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
-                    TestCpu.class.getName() + "," + TestOthers.class.getName());
-    
-    matcher.configure(conf, null, null, null);
-
-    // test the initialization order
-    long time1 = 
-           TestResourceUsageEmulatorPlugin.testInitialization(TestCpu.ID, conf);
-    long time2 = 
-           TestResourceUsageEmulatorPlugin.testInitialization(TestOthers.ID, 
-                                                              conf);
-    assertTrue("Resource usage matcher failed to initialize the configured"
-               + " plugins in order", time1 < time2);
-    
-    matcher.matchResourceUsage();
-
-    // Note that the cpu usage emulator plugin is configured 1st and then the
-    // others plugin.
-    time1 = 
-      TestResourceUsageEmulatorPlugin.testInitialization(TestCpu.ID, conf);
-    time2 = 
-      TestResourceUsageEmulatorPlugin.testInitialization(TestOthers.ID, 
-                                                         conf);
-    assertTrue("Resource usage matcher failed to load the configured plugins", 
-               time1 < time2);
-  }
-  
-  /**
-   * Fakes the cumulative usage using {@link FakeCpuUsageEmulatorCore}.
-   */
-  static class FakeResourceUsageMonitor extends DummyResourceCalculatorPlugin {
-    private FakeCpuUsageEmulatorCore core;
-    
-    public FakeResourceUsageMonitor(FakeCpuUsageEmulatorCore core) {
-      this.core = core;
-    }
-    
-    /**
-     * A dummy CPU usage monitor. Every call to 
-     * {@link ResourceCalculatorPlugin#getCumulativeCpuTime()} will return the 
-     * value of {@link FakeCpuUsageEmulatorCore#getNumCalls()}.
-     */
-    @Override
-    public long getCumulativeCpuTime() {
-      return core.getCpuUsage();
-    }
-  }
-  
-  /**
-   * A dummy {@link Progressive} implementation that allows users to set the
-   * progress for testing. The {@link Progressive#getProgress()} call will 
-   * return the last progress value set using 
-   * {@link FakeProgressive#setProgress(float)}.
-   */
-  static class FakeProgressive implements Progressive {
-    private float progress = 0F;
-    @Override
-    public float getProgress() {
-      return progress;
-    }
-    
-    void setProgress(float progress) {
-      this.progress = progress;
-    }
-  }
-  
-  /**
-   * A dummy reporter for {@link LoadJob.ResourceUsageMatcherRunner}.
-   */
-  private static class DummyReporter extends StatusReporter {
-    private Progressive progress;
-    
-    DummyReporter(Progressive progress) {
-      this.progress = progress;
-    }
-    
-    @Override
-    public org.apache.hadoop.mapreduce.Counter getCounter(Enum<?> name) {
-      return null;
-    }
-    
-    @Override
-    public org.apache.hadoop.mapreduce.Counter getCounter(String group,
-                                                          String name) {
-      return null;
-    }
-    
-    @Override
-    public void progress() {
-    }
-    
-    @Override
-    public float getProgress() {
-      return progress.getProgress();
-    }
-    
-    @Override
-    public void setStatus(String status) {
-    }
-  }
-  
-  // Extends ResourceUsageMatcherRunner for testing.
-  @SuppressWarnings("unchecked")
-  private static class FakeResourceUsageMatcherRunner 
-  extends ResourceUsageMatcherRunner {
-    FakeResourceUsageMatcherRunner(TaskInputOutputContext context, 
-                                   ResourceUsageMetrics metrics) {
-      super(context, metrics);
-    }
-    
-    // test ResourceUsageMatcherRunner
-    void test() throws Exception {
-      super.match();
-    }
-  }
-  
-  /**
-   * Test {@link LoadJob.ResourceUsageMatcherRunner}.
-   */
-  @Test
-  @SuppressWarnings("unchecked")
-  public void testResourceUsageMatcherRunner() throws Exception {
-    Configuration conf = new Configuration();
-    FakeProgressive progress = new FakeProgressive();
-    
-    // set the resource calculator plugin
-    conf.setClass(TTConfig.TT_RESOURCE_CALCULATOR_PLUGIN,
-                  DummyResourceCalculatorPlugin.class, 
-                  ResourceCalculatorPlugin.class);
-    // set the resources
-    // set the resource implementation class
-    conf.setClass(ResourceUsageMatcher.RESOURCE_USAGE_EMULATION_PLUGINS, 
-                  TestResourceUsageEmulatorPlugin.class, 
-                  ResourceUsageEmulatorPlugin.class);
-    
-    long currentTime = System.currentTimeMillis();
-    
-    // initialize the matcher class
-    TaskAttemptID id = new TaskAttemptID("test", 1, TaskType.MAP, 1, 1);
-    StatusReporter reporter = new DummyReporter(progress);
-    TaskInputOutputContext context = 
-      new MapContextImpl(conf, id, null, null, null, reporter, null);
-    FakeResourceUsageMatcherRunner matcher = 
-      new FakeResourceUsageMatcherRunner(context, null);
-    
-    // check if the matcher initialized the plugin
-    String identifier = TestResourceUsageEmulatorPlugin.DEFAULT_IDENTIFIER;
-    long initTime = 
-      TestResourceUsageEmulatorPlugin.testInitialization(identifier, conf);
-    assertTrue("ResourceUsageMatcherRunner failed to initialize the"
-               + " configured plugin", initTime > currentTime);
-    
-    // check the progress
-    assertEquals("Progress mismatch in ResourceUsageMatcherRunner", 
-                 0, progress.getProgress(), 0D);
-    
-    // call match() and check progress
-    progress.setProgress(0.01f);
-    currentTime = System.currentTimeMillis();
-    matcher.test();
-    long emulateTime = 
-      TestResourceUsageEmulatorPlugin.testEmulation(identifier, conf);
-    assertTrue("ProgressBasedResourceUsageMatcher failed to load and emulate"
-               + " the configured plugin", emulateTime > currentTime);
-  }
-  
-  /**
-   * Test {@link CumulativeCpuUsageEmulatorPlugin}'s core CPU usage emulation 
-   * engine.
-   */
-  @Test
-  public void testCpuUsageEmulator() throws IOException {
-    // test CpuUsageEmulator calibration with fake resource calculator plugin
-    long target = 100000L; // 100 secs
-    int unitUsage = 50;
-    FakeCpuUsageEmulatorCore fakeCpuEmulator = new FakeCpuUsageEmulatorCore();
-    fakeCpuEmulator.setUnitUsage(unitUsage);
-    FakeResourceUsageMonitor fakeMonitor = 
-      new FakeResourceUsageMonitor(fakeCpuEmulator);
-    
-    // calibrate for 100ms
-    fakeCpuEmulator.calibrate(fakeMonitor, target);
-    
-    // by default, CpuUsageEmulator.calibrate() will consume 100ms of CPU usage
-    assertEquals("Fake calibration failed", 
-                 100, fakeMonitor.getCumulativeCpuTime());
-    assertEquals("Fake calibration failed", 
-                 100, fakeCpuEmulator.getCpuUsage());
-    // by default, CpuUsageEmulator.performUnitComputation() will be called 
-    // twice
-    assertEquals("Fake calibration failed", 
-                 2, fakeCpuEmulator.getNumCalls());
-  }
-  
-  /**
-   * This is a dummy class that fakes CPU usage.
-   */
-  private static class FakeCpuUsageEmulatorCore 
-  extends DefaultCpuUsageEmulator {
-    private int numCalls = 0;
-    private int unitUsage = 1;
-    private int cpuUsage = 0;
-    
-    @Override
-    protected void performUnitComputation() {
-      ++numCalls;
-      cpuUsage += unitUsage;
-    }
-    
-    int getNumCalls() {
-      return numCalls;
-    }
-    
-    int getCpuUsage() {
-      return cpuUsage;
-    }
-    
-    void reset() {
-      numCalls = 0;
-      cpuUsage = 0;
-    }
-    
-    void setUnitUsage(int unitUsage) {
-      this.unitUsage = unitUsage;
-    }
-  }
-  
-  // Creates a ResourceUsageMetrics object from the target usage
-  static ResourceUsageMetrics createMetrics(long target) {
-    ResourceUsageMetrics metrics = new ResourceUsageMetrics();
-    metrics.setCumulativeCpuUsage(target);
-    metrics.setVirtualMemoryUsage(target);
-    metrics.setPhysicalMemoryUsage(target);
-    metrics.setHeapUsage(target);
-    return metrics;
-  }
-  
-  /**
-   * Test {@link CumulativeCpuUsageEmulatorPlugin}.
-   */
-  @Test
-  public void testCumulativeCpuUsageEmulatorPlugin() throws Exception {
-    Configuration conf = new Configuration();
-    long targetCpuUsage = 1000L;
-    int unitCpuUsage = 50;
-    
-    // fake progress indicator
-    FakeProgressive fakeProgress = new FakeProgressive();
-    
-    // fake cpu usage generator
-    FakeCpuUsageEmulatorCore fakeCore = new FakeCpuUsageEmulatorCore();
-    fakeCore.setUnitUsage(unitCpuUsage);
-    
-    // a cumulative cpu usage emulator with fake core
-    CumulativeCpuUsageEmulatorPlugin cpuPlugin = 
-      new CumulativeCpuUsageEmulatorPlugin(fakeCore);
-    
-    // test with invalid or missing resource usage value
-    ResourceUsageMetrics invalidUsage = createMetrics(0);
-    cpuPlugin.initialize(conf, invalidUsage, null, null);
-    
-    // test if disabled cpu emulation plugin's emulate() call is a no-operation
-    // this will test if the emulation plugin is disabled or not
-    int numCallsPre = fakeCore.getNumCalls();
-    long cpuUsagePre = fakeCore.getCpuUsage();
-    cpuPlugin.emulate();
-    int numCallsPost = fakeCore.getNumCalls();
-    long cpuUsagePost = fakeCore.getCpuUsage();
-    
-    //  test if no calls are made cpu usage emulator core
-    assertEquals("Disabled cumulative CPU usage emulation plugin works!", 
-                 numCallsPre, numCallsPost);
-    
-    //  test if no calls are made cpu usage emulator core
-    assertEquals("Disabled cumulative CPU usage emulation plugin works!", 
-                 cpuUsagePre, cpuUsagePost);
-    
-    // test with get progress
-    float progress = cpuPlugin.getProgress();
-    assertEquals("Invalid progress of disabled cumulative CPU usage emulation " 
-                 + "plugin!", 1.0f, progress, 0f);
-    
-    // test with valid resource usage value
-    ResourceUsageMetrics metrics = createMetrics(targetCpuUsage);
-    
-    // fake monitor
-    ResourceCalculatorPlugin monitor = new FakeResourceUsageMonitor(fakeCore);
-    
-    // test with default emulation interval
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, cpuPlugin, 
-                          targetCpuUsage, targetCpuUsage / unitCpuUsage);
-    
-    // test with custom value for emulation interval of 20%
-    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
-                  0.2F);
-    testEmulationAccuracy(conf, fakeCore, monitor, metrics, cpuPlugin, 
-                          targetCpuUsage, targetCpuUsage / unitCpuUsage);
-    
-    // test if emulation interval boundary is respected (unit usage = 1)
-    //  test the case where the current progress is less than threshold
-    fakeProgress = new FakeProgressive(); // initialize
-    fakeCore.reset();
-    fakeCore.setUnitUsage(1);
-    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
-                  0.25F);
-    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    // take a snapshot after the initialization
-    long initCpuUsage = monitor.getCumulativeCpuTime();
-    long initNumCalls = fakeCore.getNumCalls();
-    // test with 0 progress
-    testEmulationBoundary(0F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, 
-                          initNumCalls, "[no-op, 0 progress]");
-    // test with 24% progress
-    testEmulationBoundary(0.24F, fakeCore, fakeProgress, cpuPlugin, 
-                          initCpuUsage, initNumCalls, "[no-op, 24% progress]");
-    // test with 25% progress
-    //  target = 1000ms, target emulation at 25% = 250ms, 
-    //  weighed target = 1000 * 0.25^4 (we are using progress^4 as the weight)
-    //                 ~ 4
-    //  but current usage = init-usage = 100, hence expected = 100
-    testEmulationBoundary(0.25F, fakeCore, fakeProgress, cpuPlugin, 
-                          initCpuUsage, initNumCalls, "[op, 25% progress]");
-    
-    // test with 80% progress
-    //  target = 1000ms, target emulation at 80% = 800ms, 
-    //  weighed target = 1000 * 0.25^4 (we are using progress^4 as the weight)
-    //                 ~ 410
-    //  current-usage = init-usage = 100, hence expected-usage = 410
-    testEmulationBoundary(0.80F, fakeCore, fakeProgress, cpuPlugin, 410, 410, 
-                          "[op, 80% progress]");
-    
-    // now test if the final call with 100% progress ramps up the CPU usage
-    testEmulationBoundary(1F, fakeCore, fakeProgress, cpuPlugin, targetCpuUsage,
-                          targetCpuUsage, "[op, 100% progress]");
-    
-    // test if emulation interval boundary is respected (unit usage = 50)
-    //  test the case where the current progress is less than threshold
-    fakeProgress = new FakeProgressive(); // initialize
-    fakeCore.reset();
-    fakeCore.setUnitUsage(unitCpuUsage);
-    conf.setFloat(CumulativeCpuUsageEmulatorPlugin.CPU_EMULATION_PROGRESS_INTERVAL,
-                  0.40F);
-    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    // take a snapshot after the initialization
-    initCpuUsage = monitor.getCumulativeCpuTime();
-    initNumCalls = fakeCore.getNumCalls();
-    // test with 0 progress
-    testEmulationBoundary(0F, fakeCore, fakeProgress, cpuPlugin, initCpuUsage, 
-                          initNumCalls, "[no-op, 0 progress]");
-    // test with 39% progress
-    testEmulationBoundary(0.39F, fakeCore, fakeProgress, cpuPlugin, 
-                          initCpuUsage, initNumCalls, "[no-op, 39% progress]");
-    // test with 40% progress
-    //  target = 1000ms, target emulation at 40% = 4000ms, 
-    //  weighed target = 1000 * 0.40^4 (we are using progress^4 as the weight)
-    //                 ~ 26
-    // current-usage = init-usage = 100, hence expected-usage = 100
-    testEmulationBoundary(0.40F, fakeCore, fakeProgress, cpuPlugin, 
-                          initCpuUsage, initNumCalls, "[op, 40% progress]");
-    
-    // test with 90% progress
-    //  target = 1000ms, target emulation at 90% = 900ms, 
-    //  weighed target = 1000 * 0.90^4 (we are using progress^4 as the weight)
-    //                 ~ 657
-    //  current-usage = init-usage = 100, hence expected-usage = 657 but 
-    //  the fake-core increases in steps of 50, hence final target = 700
-    testEmulationBoundary(0.90F, fakeCore, fakeProgress, cpuPlugin, 700, 
-                          700 / unitCpuUsage, "[op, 90% progress]");
-    
-    // now test if the final call with 100% progress ramps up the CPU usage
-    testEmulationBoundary(1F, fakeCore, fakeProgress, cpuPlugin, targetCpuUsage,
-                          targetCpuUsage / unitCpuUsage, "[op, 100% progress]");
-  }
-  
-  // test whether the CPU usage emulator achieves the desired target using
-  // desired calls to the underling core engine.
-  private static void testEmulationAccuracy(Configuration conf, 
-                        FakeCpuUsageEmulatorCore fakeCore,
-                        ResourceCalculatorPlugin monitor,
-                        ResourceUsageMetrics metrics,
-                        CumulativeCpuUsageEmulatorPlugin cpuPlugin,
-                        long expectedTotalCpuUsage, long expectedTotalNumCalls) 
-  throws Exception {
-    FakeProgressive fakeProgress = new FakeProgressive();
-    fakeCore.reset();
-    cpuPlugin.initialize(conf, metrics, monitor, fakeProgress);
-    int numLoops = 0;
-    while (fakeProgress.getProgress() < 1) {
-      ++numLoops;
-      float progress = (float)numLoops / 100;
-      fakeProgress.setProgress(progress);
-      cpuPlugin.emulate();
-    }
-    
-    // test if the resource plugin shows the expected invocations
-    assertEquals("Cumulative cpu usage emulator plugin failed (num calls)!", 
-                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
-    // test if the resource plugin shows the expected usage
-    assertEquals("Cumulative cpu usage emulator plugin failed (total usage)!", 
-                 expectedTotalCpuUsage, fakeCore.getCpuUsage(), 0L);
-  }
-  
-  // tests if the CPU usage emulation plugin emulates only at the expected
-  // progress gaps
-  private static void testEmulationBoundary(float progress, 
-      FakeCpuUsageEmulatorCore fakeCore, FakeProgressive fakeProgress, 
-      CumulativeCpuUsageEmulatorPlugin cpuPlugin, long expectedTotalCpuUsage, 
-      long expectedTotalNumCalls, String info) throws Exception {
-    fakeProgress.setProgress(progress);
-    cpuPlugin.emulate();
-    
-    assertEquals("Emulation interval test for cpu usage failed " + info + "!", 
-                 expectedTotalCpuUsage, fakeCore.getCpuUsage(), 0L);
-    assertEquals("Emulation interval test for num calls failed " + info + "!", 
-                 expectedTotalNumCalls, fakeCore.getNumCalls(), 0L);
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestSleepJob.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestSleepJob.java
deleted file mode 100644
index 5a648de5679..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestSleepJob.java
+++ /dev/null
@@ -1,142 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- * <p/>
- * http://www.apache.org/licenses/LICENSE-2.0
- * <p/>
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.mapreduce.InputSplit;
-import org.apache.hadoop.mapreduce.Job;
-import org.apache.hadoop.security.UserGroupInformation;
-import org.apache.hadoop.test.GenericTestUtils;
-import org.apache.hadoop.tools.rumen.JobStory;
-import org.junit.AfterClass;
-import org.junit.BeforeClass;
-import org.junit.Test;
-import org.slf4j.Logger;
-import org.slf4j.event.Level;
-
-import java.io.IOException;
-import java.util.List;
-
-import static org.junit.Assert.*;
-import static org.slf4j.LoggerFactory.getLogger;
-
-public class TestSleepJob extends CommonJobTest {
-
-  public static final Logger LOG = getLogger(Gridmix.class);
-
-  static {
-    GenericTestUtils.setLogLevel(
-        getLogger("org.apache.hadoop.mapred.gridmix"), Level.DEBUG);
-  }
-
-  static GridmixJobSubmissionPolicy policy = GridmixJobSubmissionPolicy.REPLAY;
-
-  @BeforeClass
-  public static void init() throws IOException {
-    GridmixTestUtils.initCluster(TestSleepJob.class);
-  }
-
-  @AfterClass
-  public static void shutDown() throws IOException {
-    GridmixTestUtils.shutdownCluster();
-  }
-
-
-  @Test
-  public void testMapTasksOnlySleepJobs() throws Exception {
-    Configuration configuration = GridmixTestUtils.mrvl.getConfig();
-
-    DebugJobProducer jobProducer = new DebugJobProducer(5, configuration);
-    configuration.setBoolean(SleepJob.SLEEPJOB_MAPTASK_ONLY, true);
-
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-    JobStory story;
-    int seq = 1;
-    while ((story = jobProducer.getNextJob()) != null) {
-      GridmixJob gridmixJob = JobCreator.SLEEPJOB.createGridmixJob(configuration, 0,
-              story, new Path("ignored"), ugi, seq++);
-      gridmixJob.buildSplits(null);
-      Job job = gridmixJob.call();
-      assertEquals(0, job.getNumReduceTasks());
-    }
-    jobProducer.close();
-    assertEquals(6, seq);
-  }
-
-  /*
-  * test RandomLocation
-  */
-  @Test
-  public void testRandomLocation() throws Exception {
-    UserGroupInformation ugi = UserGroupInformation.getLoginUser();
-
-    testRandomLocation(1, 10, ugi);
-    testRandomLocation(2, 10, ugi);
-  }
-
-  // test Serial submit
-  @Test
-  public void testSerialSubmit() throws Exception {
-    // set policy
-    policy = GridmixJobSubmissionPolicy.SERIAL;
-    LOG.info("Serial started at " + System.currentTimeMillis());
-    doSubmission(JobCreator.SLEEPJOB.name(), false);
-    LOG.info("Serial ended at " + System.currentTimeMillis());
-  }
-
-  @Test
-  public void testReplaySubmit() throws Exception {
-    policy = GridmixJobSubmissionPolicy.REPLAY;
-    LOG.info(" Replay started at " + System.currentTimeMillis());
-    doSubmission(JobCreator.SLEEPJOB.name(), false);
-    LOG.info(" Replay ended at " + System.currentTimeMillis());
-  }
-
-  @Test
-  public void testStressSubmit() throws Exception {
-    policy = GridmixJobSubmissionPolicy.STRESS;
-    LOG.info(" Replay started at " + System.currentTimeMillis());
-    doSubmission(JobCreator.SLEEPJOB.name(), false);
-    LOG.info(" Replay ended at " + System.currentTimeMillis());
-  }
-
-  private void testRandomLocation(int locations, int njobs,
-                                  UserGroupInformation ugi) throws Exception {
-    Configuration configuration = new Configuration();
-
-    DebugJobProducer jobProducer = new DebugJobProducer(njobs, configuration);
-    Configuration jconf = GridmixTestUtils.mrvl.getConfig();
-    jconf.setInt(JobCreator.SLEEPJOB_RANDOM_LOCATIONS, locations);
-
-    JobStory story;
-    int seq = 1;
-    while ((story = jobProducer.getNextJob()) != null) {
-      GridmixJob gridmixJob = JobCreator.SLEEPJOB.createGridmixJob(jconf, 0,
-              story, new Path("ignored"), ugi, seq++);
-      gridmixJob.buildSplits(null);
-      List<InputSplit> splits = new SleepJob.SleepInputFormat()
-              .getSplits(gridmixJob.getJob());
-      for (InputSplit split : splits) {
-        assertEquals(locations, split.getLocations().length);
-      }
-    }
-    jobProducer.close();
-  }
-
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestUserResolve.java b/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
deleted file mode 100644
index 44075157f5c..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/java/org/apache/hadoop/mapred/gridmix/TestUserResolve.java
+++ /dev/null
@@ -1,172 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.hadoop.mapred.gridmix;
-
-import java.io.IOException;
-import java.net.URI;
-
-import org.junit.BeforeClass;
-import org.junit.Test;
-import static org.junit.Assert.*;
-
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.fs.FSDataOutputStream;
-import org.apache.hadoop.fs.FileSystem;
-import org.apache.hadoop.fs.Path;
-import org.apache.hadoop.security.UserGroupInformation;
-
-public class TestUserResolve {
-
-  private static Path rootDir = null;
-  private static Configuration conf = null;
-  private static FileSystem fs = null;
-
-  @BeforeClass
-  public static void createRootDir() throws IOException {
-    conf = new Configuration();
-    fs = FileSystem.getLocal(conf);
-    rootDir = new Path(fs.makeQualified(new Path(
-        System.getProperty("test.build.data", "/tmp"))), "gridmixUserResolve");
-  }
-
-  /**
-   * Creates users file with the content as the String usersFileContent.
-   * @param usersFilePath    the path to the file that is to be created
-   * @param usersFileContent Content of users file
-   * @throws IOException
-   */
-  private static void writeUserList(Path usersFilePath, String usersFileContent)
-      throws IOException {
-
-    FSDataOutputStream out = null;
-    try {
-      out = fs.create(usersFilePath, true);
-      out.writeBytes(usersFileContent);
-    } finally {
-      if (out != null) {
-        out.close();
-      }
-    }
-  }
-
-  /**
-   * Validate RoundRobinUserResolver's behavior for bad user resource file.
-   * RoundRobinUserResolver.setTargetUsers() should throw proper Exception for
-   * the cases like
-   * <li> non existent user resource file and
-   * <li> empty user resource file
-   * 
-   * @param rslv              The RoundRobinUserResolver object
-   * @param userRsrc          users file
-   * @param expectedErrorMsg  expected error message
-   */
-  private void validateBadUsersFile(UserResolver rslv, URI userRsrc,
-      String expectedErrorMsg) {
-    boolean fail = false;
-    try {
-      rslv.setTargetUsers(userRsrc, conf);
-    } catch (IOException e) {
-      assertTrue("Exception message from RoundRobinUserResolver is wrong",
-          e.getMessage().equals(expectedErrorMsg));
-      fail = true;
-    }
-    assertTrue("User list required for RoundRobinUserResolver", fail);
-  }
-
-  /**
-   * Validate the behavior of {@link RoundRobinUserResolver} for different
-   * user resource files like
-   * <li> Empty user resource file
-   * <li> Non existent user resource file
-   * <li> User resource file with valid content
-   * @throws Exception
-   */
-  @Test
-  public void testRoundRobinResolver() throws Exception {
-
-    final UserResolver rslv = new RoundRobinUserResolver();
-    Path usersFilePath = new Path(rootDir, "users");
-    URI userRsrc = new URI(usersFilePath.toString());
-
-    // Check if the error message is as expected for non existent
-    // user resource file.
-    fs.delete(usersFilePath, false);
-    String expectedErrorMsg = "File " + userRsrc + " does not exist";
-    validateBadUsersFile(rslv, userRsrc, expectedErrorMsg);
-
-    // Check if the error message is as expected for empty user resource file
-    writeUserList(usersFilePath, "");// creates empty users file
-    expectedErrorMsg =
-        RoundRobinUserResolver.buildEmptyUsersErrorMsg(userRsrc);
-    validateBadUsersFile(rslv, userRsrc, expectedErrorMsg);
-
-    // Create user resource file with valid content like older users list file
-    // with usernames and groups
-    writeUserList(usersFilePath,
-        "user0,groupA,groupB,groupC\nuser1,groupA,groupC\n");
-    validateValidUsersFile(rslv, userRsrc);
-
-    // Create user resource file with valid content with
-    // usernames with groups and without groups
-    writeUserList(usersFilePath, "user0,groupA,groupB\nuser1,");
-    validateValidUsersFile(rslv, userRsrc);
-
-    // Create user resource file with valid content with
-    // usernames without groups
-    writeUserList(usersFilePath, "user0\nuser1");
-    validateValidUsersFile(rslv, userRsrc);
-  }
-
-  // Validate RoundRobinUserResolver for the case of
-  // user resource file with valid content.
-  private void validateValidUsersFile(UserResolver rslv, URI userRsrc)
-      throws IOException {
-    assertTrue(rslv.setTargetUsers(userRsrc, conf));
-    UserGroupInformation ugi1 = UserGroupInformation.createRemoteUser("hfre0");
-    assertEquals("user0", rslv.getTargetUgi(ugi1).getUserName());
-    assertEquals("user1", 
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre1"))
-            .getUserName());
-    assertEquals("user0",
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre2"))
-            .getUserName());
-    assertEquals("user0", rslv.getTargetUgi(ugi1).getUserName());
-    assertEquals("user1",
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre3"))
-            .getUserName());
-
-    // Verify if same user comes again, its mapped user name should be
-    // correct even though UGI is constructed again.
-    assertEquals("user0", rslv.getTargetUgi(
-        UserGroupInformation.createRemoteUser("hfre0")).getUserName());
-    assertEquals("user0",
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre5"))
-            .getUserName());
-    assertEquals("user0",
-        rslv.getTargetUgi(UserGroupInformation.createRemoteUser("hfre0"))
-            .getUserName());
-  }
-
-  @Test
-  public void testSubmitterResolver() throws Exception {
-    final UserResolver rslv = new SubmitterUserResolver();
-    assertFalse(rslv.needsTargetUsersList());
-    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();
-    assertEquals(ugi, rslv.getTargetUgi((UserGroupInformation)null));
-  }
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json b/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json
deleted file mode 100644
index 1b7ccf860b6..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json
+++ /dev/null
@@ -1,414 +0,0 @@
-{
-  "priority" : "NORMAL",
-  "jobID" : "job_201009241532_0001",
-  "user" : "johndoe",
-  "jobName" : "WordCount",
-  "mapTasks" : [ {
-    "startTime" : 1285322651360,
-    "taskID" : "task_201009241532_0001_m_000000",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322651366,
-      "finishTime" : 1285322658262,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000000_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 704270,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 48266,
-      "mapInputRecords" : 13427,
-      "mapOutputBytes" : 1182333,
-      "mapOutputRecords" : 126063,
-      "combineInputRecords" : 126063,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 6612,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322660778,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 704270,
-    "inputRecords" : 13427,
-    "outputBytes" : 48266,
-    "outputRecords" : 126063
-  }, {
-    "startTime" : 1285322651361,
-    "taskID" : "task_201009241532_0001_m_000001",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322651378,
-      "finishTime" : 1285322657906,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000001_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 577214,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 58143,
-      "mapInputRecords" : 13015,
-      "mapOutputBytes" : 985534,
-      "mapOutputRecords" : 108400,
-      "combineInputRecords" : 108400,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 8214,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322660781,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 577214,
-    "inputRecords" : 13015,
-    "outputBytes" : 58143,
-    "outputRecords" : 108400
-  }, {
-    "startTime" : 1285322660789,
-    "taskID" : "task_201009241532_0001_m_000002",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322660807,
-      "finishTime" : 1285322664865,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000002_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 163907,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 21510,
-      "mapInputRecords" : 3736,
-      "mapOutputBytes" : 275796,
-      "mapOutputRecords" : 30528,
-      "combineInputRecords" : 30528,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 3040,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322666805,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 163907,
-    "inputRecords" : 3736,
-    "outputBytes" : 21510,
-    "outputRecords" : 30528
-  } ],
-  "finishTime" : 1285322675837,
-  "reduceTasks" : [ {
-    "startTime" : 1285322660790,
-    "taskID" : "task_201009241532_0001_r_000000",
-    "taskType" : "REDUCE",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322660807,
-      "finishTime" : 1285322670759,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_r_000000_0",
-      "shuffleFinished" : 1285322667962,
-      "sortFinished" : 1285322668146,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : 122793,
-      "fileBytesRead" : 111026,
-      "fileBytesWritten" : 111026,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : 0,
-      "reduceInputGroups" : 11713,
-      "reduceInputRecords" : 17866,
-      "reduceShuffleBytes" : 127823,
-      "reduceOutputRecords" : 11713,
-      "spilledRecords" : 17866,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322672821,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 127823,
-    "inputRecords" : 17866,
-    "outputBytes" : 122793,
-    "outputRecords" : 11713
-  } ],
-  "submitTime" : 1285322645148,
-  "launchTime" : 1285322645614,
-  "totalMaps" : 3,
-  "totalReduces" : 1,
-  "otherTasks" : [ {
-    "startTime" : 1285322648294,
-    "taskID" : "task_201009241532_0001_m_000004",
-    "taskType" : "SETUP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322648482,
-      "finishTime" : 1285322649588,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000004_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : -1,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : -1,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 0,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322651351,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : -1,
-    "inputRecords" : -1,
-    "outputBytes" : -1,
-    "outputRecords" : -1
-  }, {
-    "startTime" : 1285322672829,
-    "taskID" : "task_201009241532_0001_m_000003",
-    "taskType" : "CLEANUP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322672838,
-      "finishTime" : 1285322673971,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000003_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : -1,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : -1,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 0,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322675835,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : -1,
-    "inputRecords" : -1,
-    "outputBytes" : -1,
-    "outputRecords" : -1
-  } ],
-  "computonsPerMapInputByte" : -1,
-  "computonsPerMapOutputByte" : -1,
-  "computonsPerReduceInputByte" : -1,
-  "computonsPerReduceOutputByte" : -1,
-  "heapMegabytes" : 1024,
-  "outcome" : "SUCCESS",
-  "jobtype" : "JAVA",
-  "directDependantJobs" : [ ],
-  "successfulMapAttemptCDFs" : [ {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 6896,
-    "minimum" : 4058,
-    "rankings" : [ {
-      "datum" : 4058,
-      "relativeRanking" : 0.05
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.1
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.15
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.2
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.25
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.3
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.35
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.4
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.45
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.5
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.55
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.6
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.65
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.7
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.75
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.8
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.85
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.9
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.95
-    } ],
-    "numberValues" : 3
-  } ],
-  "failedMapAttemptCDFs" : [ {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  } ],
-  "successfulReduceAttemptCDF" : {
-    "maximum" : 9952,
-    "minimum" : 9952,
-    "rankings" : [ {
-      "datum" : 9952,
-      "relativeRanking" : 0.05
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.1
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.15
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.2
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.25
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.3
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.35
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.4
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.45
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.5
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.55
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.6
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.65
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.7
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.75
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.8
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.85
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.9
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.95
-    } ],
-    "numberValues" : 1
-  },
-  "failedReduceAttemptCDF" : {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  },
-  "mapperTriesToSucceed" : [ 1.0 ],
-  "failedMapperFraction" : 0.0,
-  "relativeTime" : 0,
-  "queue" : "default",
-  "clusterMapMB" : -1,
-  "clusterReduceMB" : -1,
-  "jobMapMB" : 1024,
-  "jobReduceMB" : 1024
-}
diff --git a/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json.gz b/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount.json.gz
deleted file mode 100644
index c5cc40e7618a5ec775ccd0a827e1d46ea120e0b2..0000000000000000000000000000000000000000
GIT binary patch
literal 0
HcmV?d00001

literal 1452
zcmV;d1ylMTiwFq8l%-4p19xw7WMgl2Zgehcb8l_{?V3w(+c*$_@A(ym&rZsRL`i#d
zon+A>NekO)4@D8kHeG9@*Gkl)L6QGnN|Yr^l&vJJyDpGi;&5g-Lvr|WNax4NU@(jm
zFG@W2FdY0f7~XuCUXQPaqg?P$w0L)3wDM1P2tWWFWIz+6I{*L_N?A&iQfCvcB3hZ=
zxB)E$pQB_oiL#IhQQ*dNH@#1D*_Xi&-ozVI<|b_J2@8m9?gJ4MVgg1&fpL>EnEay}
z&0UcH;y**lV*U_|7_P@3x9u)tG>BQlKX3M<rOUi1%$vi^_eWd7CQ4Z~mNzT9b~B%y
zChqe7%{q$C=yx}WeR{Txf?;K|b2y|@S$m<EZWJ9G5mf0UH08bu$FtkX<YG3fU6jKp
z=cN2=&$QeDDZEX#Ui<X22!XEh9!_B`jS{v)v{|jww-1b_Q|cBn3mC`(s{AugC`QAU
z`P%bo!@Nr-nHqdH>KWsS^Dc}tHl<6RQN{W}%-zchA2P-@ihc??v=PQw)sXu@1dv*V
zuj9fCsp6@rmC#kT6iR<5Q5L7I(MWQs6GdSr7NUw*QBb0l*D3aVpROb!ArL9kC3Rki
z9tU5`ozR_v1Xz|`YQza$(}ag|RqhUD*SHTfDI16K-WZN}|E=vY_}U5}-=Z_-vMlY$
z@uAm<Ud}Wj>du2<)Jy>>hf0yMs$BRu>Ln3;E{WhVlW4odyGdj@fE<EE@GvAYEek>8
zP$V*KXyE-M0?^z;BF8pOZ0rXSunhouLDbYbP7vAhQfPq4hQ|jYIng>mRHZ>1L{bWM
zfT&p2eh}qz;yf2b=x9MifDNo35D~*B<{?0Y4g(@cup<rX7lQ~v6M}sp!WJfbID{+?
zcCwd47?{ZJ!%$P{I4OjI0rV+EY+xQAh2$(#C{*V_8$we0s1PdF^rVVt$Anm>jSI)+
znmq9`$I0b9{W6#AyUa~5&Tl6de+0ZNV42P@g;(;l`&<<y-|c88$0{v;Hj&*(>%yJH
zB#4mZ;N3`r5CTN@Vy&gp!8Iucm1<qWZrdF=>nW%pv>@)G8MJIddVmIz#V@3NP-}Wq
zxz_abQm|QwS7|+9dl8wWF{<@Z9eNv0au#(INco&dY)Z35;3-?uFriVLN}R-5xZJ36
zB1*R*i<s+QyJC59-83zhM7S(kBetQ*6B)<vnvU_@>dfe5-0Wh0`~TNxhRui6gE+%6
zZTk@QnQ@r<tOsTqwONxwzZYQL#nurOWBQet39)$yU~R)zU{wjC?!{6mRg9$P3YT;D
z*;hE+35m(o#rWn3Fu{j}``(i*>{##+n82@?c*R8Peq#O!m?$e5zEWJuL;FaRYiZ1N
z7j2a~DR$^g>!s_}>g;So-T0b*a~D-{29T(m_*i%!RGF^)8$Y(?();nJv1na+30<;t
z8q;v)hU|T`kXL|LaZ6rorRyv&#l~W%PtGr!rB>j6_kt`aUQrRk*g^mk+cYe`($y6>
z;JHvLovK>f?=uoNy!XOytt<$$U_p~luAhm)Hs9f1-IqNRd;4%C`!j&q-G6odtoZ(F
zsTOeX`HuXLPK8n7`3#t1i;rGp4YiE5Tq7-GE!SAfSj)AcWvt~|&sfWvXc_NuO`1=t
z9oCkXv6gFF%UH{`qh)Nis)jc2<X45Y%j>n(>sj>bzILyaDxz+xGbGpSh|n>S8j{r7
zcUsn}FF7syt)Z5&mTRPCtmPVO8Ed&Vw2ZY}>ltfV6D?yc*Or#CmTOzfSj)AeW!!gK
ziU+Rxtb8I2f8%LU{*P%)lX>FNbRNxea*GF_F9Uc6+Gjo2yiDBX_KA#hwfuHd%+L0H
zLA3sx(M()&%l9&I&t3Xi%8JYR^;`M6Et<t7H6;AHTq#$VYu+fU=ItksCqDrWrmn)J
GD*ynGGs_|X

diff --git a/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount2.json b/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount2.json
deleted file mode 100644
index 87fcfb92ee9..00000000000
--- a/hadoop-tools/hadoop-gridmix/src/test/resources/data/wordcount2.json
+++ /dev/null
@@ -1,828 +0,0 @@
-{
-  "priority" : "NORMAL",
-  "jobID" : "job_201009241532_0001",
-  "user" : "johndoe",
-  "jobName" : "WordCount",
-  "mapTasks" : [ {
-    "startTime" : 1285322651360,
-    "taskID" : "task_201009241532_0001_m_000000",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322651366,
-      "finishTime" : 1285322658262,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000000_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 704270,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 48266,
-      "mapInputRecords" : 13427,
-      "mapOutputBytes" : 1182333,
-      "mapOutputRecords" : 126063,
-      "combineInputRecords" : 126063,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 6612,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322660778,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 704270,
-    "inputRecords" : 13427,
-    "outputBytes" : 48266,
-    "outputRecords" : 126063
-  }, {
-    "startTime" : 1285322651361,
-    "taskID" : "task_201009241532_0001_m_000001",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322651378,
-      "finishTime" : 1285322657906,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000001_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 577214,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 58143,
-      "mapInputRecords" : 13015,
-      "mapOutputBytes" : 985534,
-      "mapOutputRecords" : 108400,
-      "combineInputRecords" : 108400,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 8214,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322660781,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 577214,
-    "inputRecords" : 13015,
-    "outputBytes" : 58143,
-    "outputRecords" : 108400
-  }, {
-    "startTime" : 1285322660789,
-    "taskID" : "task_201009241532_0001_m_000002",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322660807,
-      "finishTime" : 1285322664865,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000002_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 163907,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 21510,
-      "mapInputRecords" : 3736,
-      "mapOutputBytes" : 275796,
-      "mapOutputRecords" : 30528,
-      "combineInputRecords" : 30528,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 3040,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322666805,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 163907,
-    "inputRecords" : 3736,
-    "outputBytes" : 21510,
-    "outputRecords" : 30528
-  } ],
-  "finishTime" : 1285322675837,
-  "reduceTasks" : [ {
-    "startTime" : 1285322660790,
-    "taskID" : "task_201009241532_0001_r_000000",
-    "taskType" : "REDUCE",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322660807,
-      "finishTime" : 1285322670759,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_r_000000_0",
-      "shuffleFinished" : 1285322667962,
-      "sortFinished" : 1285322668146,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : 122793,
-      "fileBytesRead" : 111026,
-      "fileBytesWritten" : 111026,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : 0,
-      "reduceInputGroups" : 11713,
-      "reduceInputRecords" : 17866,
-      "reduceShuffleBytes" : 127823,
-      "reduceOutputRecords" : 11713,
-      "spilledRecords" : 17866,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322672821,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 127823,
-    "inputRecords" : 17866,
-    "outputBytes" : 122793,
-    "outputRecords" : 11713
-  } ],
-  "submitTime" : 1285322645148,
-  "launchTime" : 1285322645614,
-  "totalMaps" : 3,
-  "totalReduces" : 1,
-  "otherTasks" : [ {
-    "startTime" : 1285322648294,
-    "taskID" : "task_201009241532_0001_m_000004",
-    "taskType" : "SETUP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322648482,
-      "finishTime" : 1285322649588,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000004_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : -1,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : -1,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 0,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322651351,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : -1,
-    "inputRecords" : -1,
-    "outputBytes" : -1,
-    "outputRecords" : -1
-  }, {
-    "startTime" : 1285322672829,
-    "taskID" : "task_201009241532_0001_m_000003",
-    "taskType" : "CLEANUP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322672838,
-      "finishTime" : 1285322673971,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000003_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : -1,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : -1,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 0,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322675835,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : -1,
-    "inputRecords" : -1,
-    "outputBytes" : -1,
-    "outputRecords" : -1
-  } ],
-  "computonsPerMapInputByte" : -1,
-  "computonsPerMapOutputByte" : -1,
-  "computonsPerReduceInputByte" : -1,
-  "computonsPerReduceOutputByte" : -1,
-  "heapMegabytes" : 1024,
-  "outcome" : "SUCCESS",
-  "jobtype" : "JAVA",
-  "directDependantJobs" : [ ],
-  "successfulMapAttemptCDFs" : [ {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 6896,
-    "minimum" : 4058,
-    "rankings" : [ {
-      "datum" : 4058,
-      "relativeRanking" : 0.05
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.1
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.15
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.2
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.25
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.3
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.35
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.4
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.45
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.5
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.55
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.6
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.65
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.7
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.75
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.8
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.85
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.9
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.95
-    } ],
-    "numberValues" : 3
-  } ],
-  "failedMapAttemptCDFs" : [ {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  } ],
-  "successfulReduceAttemptCDF" : {
-    "maximum" : 9952,
-    "minimum" : 9952,
-    "rankings" : [ {
-      "datum" : 9952,
-      "relativeRanking" : 0.05
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.1
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.15
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.2
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.25
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.3
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.35
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.4
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.45
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.5
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.55
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.6
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.65
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.7
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.75
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.8
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.85
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.9
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.95
-    } ],
-    "numberValues" : 1
-  },
-  "failedReduceAttemptCDF" : {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  },
-  "mapperTriesToSucceed" : [ 1.0 ],
-  "failedMapperFraction" : 0.0,
-  "relativeTime" : 0,
-  "queue" : "default",
-  "clusterMapMB" : -1,
-  "clusterReduceMB" : -1,
-  "jobMapMB" : 1024,
-  "jobReduceMB" : 1024
-}
-{
-  "priority" : "NORMAL",
-  "jobID" : "job_201009241532_0001",
-  "user" : "johndoe",
-  "jobName" : "WordCount",
-  "mapTasks" : [ {
-    "startTime" : 1285322651360,
-    "taskID" : "task_201009241532_0001_m_000000",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322651366,
-      "finishTime" : 1285322658262,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000000_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 704270,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 48266,
-      "mapInputRecords" : 13427,
-      "mapOutputBytes" : 1182333,
-      "mapOutputRecords" : 126063,
-      "combineInputRecords" : 126063,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 6612,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322660778,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 704270,
-    "inputRecords" : 13427,
-    "outputBytes" : 48266,
-    "outputRecords" : 126063
-  }, {
-    "startTime" : 1285322651361,
-    "taskID" : "task_201009241532_0001_m_000001",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322651378,
-      "finishTime" : 1285322657906,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000001_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 577214,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 58143,
-      "mapInputRecords" : 13015,
-      "mapOutputBytes" : 985534,
-      "mapOutputRecords" : 108400,
-      "combineInputRecords" : 108400,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 8214,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322660781,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 577214,
-    "inputRecords" : 13015,
-    "outputBytes" : 58143,
-    "outputRecords" : 108400
-  }, {
-    "startTime" : 1285322660789,
-    "taskID" : "task_201009241532_0001_m_000002",
-    "taskType" : "MAP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322660807,
-      "finishTime" : 1285322664865,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000002_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : 163907,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : 21510,
-      "mapInputRecords" : 3736,
-      "mapOutputBytes" : 275796,
-      "mapOutputRecords" : 30528,
-      "combineInputRecords" : 30528,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 3040,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322666805,
-    "preferredLocations" : [ {
-      "layers" : [ "default-rack", "foo.example.com" ]
-    } ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 163907,
-    "inputRecords" : 3736,
-    "outputBytes" : 21510,
-    "outputRecords" : 30528
-  } ],
-  "finishTime" : 1285322675837,
-  "reduceTasks" : [ {
-    "startTime" : 1285322660790,
-    "taskID" : "task_201009241532_0001_r_000000",
-    "taskType" : "REDUCE",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322660807,
-      "finishTime" : 1285322670759,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_r_000000_0",
-      "shuffleFinished" : 1285322667962,
-      "sortFinished" : 1285322668146,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : 122793,
-      "fileBytesRead" : 111026,
-      "fileBytesWritten" : 111026,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : 0,
-      "reduceInputGroups" : 11713,
-      "reduceInputRecords" : 17866,
-      "reduceShuffleBytes" : 127823,
-      "reduceOutputRecords" : 11713,
-      "spilledRecords" : 17866,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322672821,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : 127823,
-    "inputRecords" : 17866,
-    "outputBytes" : 122793,
-    "outputRecords" : 11713
-  } ],
-  "submitTime" : 1285322645148,
-  "launchTime" : 1285322645614,
-  "totalMaps" : 3,
-  "totalReduces" : 1,
-  "otherTasks" : [ {
-    "startTime" : 1285322648294,
-    "taskID" : "task_201009241532_0001_m_000004",
-    "taskType" : "SETUP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322648482,
-      "finishTime" : 1285322649588,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000004_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : -1,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : -1,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 0,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322651351,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : -1,
-    "inputRecords" : -1,
-    "outputBytes" : -1,
-    "outputRecords" : -1
-  }, {
-    "startTime" : 1285322672829,
-    "taskID" : "task_201009241532_0001_m_000003",
-    "taskType" : "CLEANUP",
-    "attempts" : [ {
-      "location" : null,
-      "hostName" : "/default-rack/foo.example.com",
-      "startTime" : 1285322672838,
-      "finishTime" : 1285322673971,
-      "result" : "SUCCESS",
-      "attemptID" : "attempt_201009241532_0001_m_000003_0",
-      "shuffleFinished" : -1,
-      "sortFinished" : -1,
-      "hdfsBytesRead" : -1,
-      "hdfsBytesWritten" : -1,
-      "fileBytesRead" : -1,
-      "fileBytesWritten" : -1,
-      "mapInputRecords" : -1,
-      "mapOutputBytes" : -1,
-      "mapOutputRecords" : -1,
-      "combineInputRecords" : -1,
-      "reduceInputGroups" : -1,
-      "reduceInputRecords" : -1,
-      "reduceShuffleBytes" : -1,
-      "reduceOutputRecords" : -1,
-      "spilledRecords" : 0,
-      "mapInputBytes" : -1
-    } ],
-    "finishTime" : 1285322675835,
-    "preferredLocations" : [ ],
-    "taskStatus" : "SUCCESS",
-    "inputBytes" : -1,
-    "inputRecords" : -1,
-    "outputBytes" : -1,
-    "outputRecords" : -1
-  } ],
-  "computonsPerMapInputByte" : -1,
-  "computonsPerMapOutputByte" : -1,
-  "computonsPerReduceInputByte" : -1,
-  "computonsPerReduceOutputByte" : -1,
-  "heapMegabytes" : 1024,
-  "outcome" : "SUCCESS",
-  "jobtype" : "JAVA",
-  "directDependantJobs" : [ ],
-  "successfulMapAttemptCDFs" : [ {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 6896,
-    "minimum" : 4058,
-    "rankings" : [ {
-      "datum" : 4058,
-      "relativeRanking" : 0.05
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.1
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.15
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.2
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.25
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.3
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.35
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.4
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.45
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.5
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.55
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.6
-    }, {
-      "datum" : 4058,
-      "relativeRanking" : 0.65
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.7
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.75
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.8
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.85
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.9
-    }, {
-      "datum" : 6528,
-      "relativeRanking" : 0.95
-    } ],
-    "numberValues" : 3
-  } ],
-  "failedMapAttemptCDFs" : [ {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  }, {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  } ],
-  "successfulReduceAttemptCDF" : {
-    "maximum" : 9952,
-    "minimum" : 9952,
-    "rankings" : [ {
-      "datum" : 9952,
-      "relativeRanking" : 0.05
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.1
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.15
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.2
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.25
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.3
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.35
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.4
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.45
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.5
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.55
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.6
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.65
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.7
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.75
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.8
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.85
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.9
-    }, {
-      "datum" : 9952,
-      "relativeRanking" : 0.95
-    } ],
-    "numberValues" : 1
-  },
-  "failedReduceAttemptCDF" : {
-    "maximum" : 9223372036854775807,
-    "minimum" : -9223372036854775808,
-    "rankings" : [ ],
-    "numberValues" : 0
-  },
-  "mapperTriesToSucceed" : [ 1.0 ],
-  "failedMapperFraction" : 0.0,
-  "relativeTime" : 0,
-  "queue" : "default",
-  "clusterMapMB" : -1,
-  "clusterReduceMB" : -1,
-  "jobMapMB" : 1024,
-  "jobReduceMB" : 1024
-}
diff --git a/hadoop-tools/hadoop-tools-dist/pom.xml b/hadoop-tools/hadoop-tools-dist/pom.xml
index 1a78cc5cc0f..7c37aa1ff01 100644
--- a/hadoop-tools/hadoop-tools-dist/pom.xml
+++ b/hadoop-tools/hadoop-tools-dist/pom.xml
@@ -60,11 +60,6 @@
       <artifactId>hadoop-rumen</artifactId>
       <scope>compile</scope>
     </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-gridmix</artifactId>
-      <scope>compile</scope>
-    </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-pipes</artifactId>
diff --git a/hadoop-tools/pom.xml b/hadoop-tools/pom.xml
index eab1d560d97..7ffed40425b 100644
--- a/hadoop-tools/pom.xml
+++ b/hadoop-tools/pom.xml
@@ -34,7 +34,6 @@
     <module>hadoop-distcp</module>
     <module>hadoop-federation-balance</module>
     <module>hadoop-rumen</module>
-    <module>hadoop-gridmix</module>
     <module>hadoop-tools-dist</module>
     <module>hadoop-pipes</module>
     <module>hadoop-openstack</module>
