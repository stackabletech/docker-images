Entirely remove hadoop-pipes

From: Lars Francke <git@lars-francke.de>


---
 .../src/main/resources/assemblies/hadoop-tools.xml |   28 
 hadoop-tools/hadoop-pipes/pom.xml                  |   69 -
 hadoop-tools/hadoop-pipes/src/CMakeLists.txt       |   99 --
 .../src/main/native/examples/README.txt            |   16 
 .../src/main/native/examples/conf/word-part.xml    |   41 -
 .../src/main/native/examples/conf/word.xml         |   45 -
 .../src/main/native/examples/impl/sort.cc          |   96 --
 .../main/native/examples/impl/wordcount-nopipe.cc  |  148 --
 .../main/native/examples/impl/wordcount-part.cc    |   76 -
 .../main/native/examples/impl/wordcount-simple.cc  |   67 -
 .../src/main/native/pipes/api/hadoop/Pipes.hh      |  260 ----
 .../native/pipes/api/hadoop/TemplateFactory.hh     |   96 --
 .../pipes/debug/pipes-default-gdb-commands.txt     |   14 
 .../main/native/pipes/debug/pipes-default-script   |   15 
 .../src/main/native/pipes/impl/HadoopPipes.cc      | 1192 --------------------
 .../main/native/utils/api/hadoop/SerialUtils.hh    |  171 ---
 .../main/native/utils/api/hadoop/StringUtils.hh    |   81 -
 .../src/main/native/utils/impl/SerialUtils.cc      |  301 -----
 .../src/main/native/utils/impl/StringUtils.cc      |  180 ---
 hadoop-tools/hadoop-tools-dist/pom.xml             |    7 
 hadoop-tools/pom.xml                               |    1 
 21 files changed, 3003 deletions(-)
 delete mode 100644 hadoop-tools/hadoop-pipes/pom.xml
 delete mode 100644 hadoop-tools/hadoop-pipes/src/CMakeLists.txt
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/README.txt
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word-part.xml
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word.xml
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/impl/sort.cc
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-nopipe.cc
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-part.cc
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-simple.cc
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/Pipes.hh
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/TemplateFactory.hh
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-gdb-commands.txt
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-script
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/SerialUtils.hh
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/StringUtils.hh
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/utils/impl/SerialUtils.cc
 delete mode 100644 hadoop-tools/hadoop-pipes/src/main/native/utils/impl/StringUtils.cc

diff --git a/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml b/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
index f99ebc66e8a..a63fabd7429 100644
--- a/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
+++ b/hadoop-assemblies/src/main/resources/assemblies/hadoop-tools.xml
@@ -39,34 +39,6 @@
       <outputDirectory>/libexec/shellprofile.d</outputDirectory>
       <fileMode>0755</fileMode>
     </fileSet>
-    <fileSet>
-      <directory>../hadoop-pipes/src/main/native/pipes/api/hadoop</directory>
-      <includes>
-        <include>*.hh</include>
-      </includes>
-      <outputDirectory>/include</outputDirectory>
-    </fileSet>
-    <fileSet>
-      <directory>../hadoop-pipes/src/main/native/utils/api/hadoop</directory>
-      <includes>
-        <include>*.hh</include>
-      </includes>
-      <outputDirectory>/include</outputDirectory>
-    </fileSet>
-    <fileSet>
-      <directory>../hadoop-pipes/target/native</directory>
-      <includes>
-        <include>*.a</include>
-      </includes>
-      <outputDirectory>lib/native</outputDirectory>
-    </fileSet>
-    <fileSet>
-      <directory>../hadoop-pipes/target/native/examples</directory>
-      <includes>
-        <include>*</include>
-      </includes>
-      <outputDirectory>lib/native/examples</outputDirectory>
-    </fileSet>
     <fileSet>
       <directory>../hadoop-distcp/target</directory>
       <outputDirectory>/share/hadoop/${hadoop.component}/sources</outputDirectory>
diff --git a/hadoop-tools/hadoop-pipes/pom.xml b/hadoop-tools/hadoop-pipes/pom.xml
deleted file mode 100644
index 20143862dd7..00000000000
--- a/hadoop-tools/hadoop-pipes/pom.xml
+++ /dev/null
@@ -1,69 +0,0 @@
-<?xml version="1.0" encoding="UTF-8"?>
-<!--
-  Licensed under the Apache License, Version 2.0 (the "License");
-  you may not use this file except in compliance with the License.
-  You may obtain a copy of the License at
-
-    http://www.apache.org/licenses/LICENSE-2.0
-
-  Unless required by applicable law or agreed to in writing, software
-  distributed under the License is distributed on an "AS IS" BASIS,
-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-  See the License for the specific language governing permissions and
-  limitations under the License. See accompanying LICENSE file.
--->
-<project xmlns="http://maven.apache.org/POM/4.0.0"
-  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
-  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0
-                      https://maven.apache.org/xsd/maven-4.0.0.xsd">
-  <modelVersion>4.0.0</modelVersion>
-  <parent>
-    <groupId>org.apache.hadoop</groupId>
-    <artifactId>hadoop-project</artifactId>
-    <version>3.4.0</version>
-    <relativePath>../../hadoop-project</relativePath>
-  </parent>
-  <artifactId>hadoop-pipes</artifactId>
-  <version>3.4.0</version>
-  <description>Apache Hadoop Pipes</description>
-  <name>Apache Hadoop Pipes</name>
-  <packaging>pom</packaging>
-
-  <properties>
-    <hadoop.log.dir>${project.build.directory}/log</hadoop.log.dir>
-  </properties>
-
-  <profiles>
-    <profile>
-      <id>native</id>
-      <activation>
-        <activeByDefault>false</activeByDefault>
-      </activation>
-      <properties>
-        <openssl.prefix></openssl.prefix>
-      </properties>
-      <build>
-        <plugins>
-          <plugin>
-            <groupId>org.apache.hadoop</groupId>
-            <artifactId>hadoop-maven-plugins</artifactId>
-            <executions>
-              <execution>
-                <id>cmake-compile</id>
-                <phase>compile</phase>
-                <goals><goal>cmake-compile</goal></goals>
-                <configuration>
-                  <source>${basedir}/src</source>
-                  <vars>
-                    <JVM_ARCH_DATA_MODEL>${sun.arch.data.model}</JVM_ARCH_DATA_MODEL>
-                    <OPENSSL_ROOT_DIR>${openssl.prefix} </OPENSSL_ROOT_DIR>
-                  </vars>
-                </configuration>
-              </execution>
-            </executions>
-          </plugin>
-        </plugins>
-      </build>
-    </profile>
-  </profiles> 
-</project>
diff --git a/hadoop-tools/hadoop-pipes/src/CMakeLists.txt b/hadoop-tools/hadoop-pipes/src/CMakeLists.txt
deleted file mode 100644
index ce6ee317936..00000000000
--- a/hadoop-tools/hadoop-pipes/src/CMakeLists.txt
+++ /dev/null
@@ -1,99 +0,0 @@
-#
-# Licensed to the Apache Software Foundation (ASF) under one
-# or more contributor license agreements.  See the NOTICE file
-# distributed with this work for additional information
-# regarding copyright ownership.  The ASF licenses this file
-# to you under the Apache License, Version 2.0 (the
-# "License"); you may not use this file except in compliance
-# with the License.  You may obtain a copy of the License at
-#
-#     http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing, software
-# distributed under the License is distributed on an "AS IS" BASIS,
-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-# See the License for the specific language governing permissions and
-# limitations under the License.
-#
-
-cmake_minimum_required(VERSION 3.1 FATAL_ERROR)
-
-list(APPEND CMAKE_MODULE_PATH ${CMAKE_SOURCE_DIR}/../../../hadoop-common-project/hadoop-common)
-include(HadoopCommon)
-
-find_package(OpenSSL REQUIRED)
-find_package(PkgConfig QUIET)
-pkg_check_modules(LIBTIRPC libtirpc)
-
-find_path(RPC_INCLUDE_DIRS NAMES rpc/rpc.h)
-
-if (NOT RPC_INCLUDE_DIRS)
-    find_path(TIRPC_INCLUDE_DIRS
-        NAMES netconfig.h
-        PATH_SUFFIXES tirpc
-        HINTS ${LIBTIRPC_INCLUDE_DIRS}
-    )
-
-    find_library(TIRPC_LIBRARIES
-        NAMES tirpc
-        HINTS ${LIBTIRPC_LIBRARY_DIRS}
-    )
-
-    include_directories(${TIRPC_INCLUDE_DIRS})
-endif()
-
-include_directories(
-    main/native/utils/api
-    main/native/pipes/api
-    ${CMAKE_CURRENT_SOURCE_DIR}
-    ${OPENSSL_INCLUDE_DIR}
-)
-
-# Example programs
-add_executable(wordcount-simple main/native/examples/impl/wordcount-simple.cc)
-target_link_libraries(wordcount-simple hadooppipes hadooputils)
-hadoop_output_directory(wordcount-simple examples)
-
-add_executable(wordcount-part main/native/examples/impl/wordcount-part.cc)
-target_link_libraries(wordcount-part hadooppipes hadooputils)
-hadoop_output_directory(wordcount-part examples)
-
-add_executable(wordcount-nopipe main/native/examples/impl/wordcount-nopipe.cc)
-target_link_libraries(wordcount-nopipe hadooppipes hadooputils)
-hadoop_output_directory(wordcount-nopipe examples)
-
-add_executable(pipes-sort main/native/examples/impl/sort.cc)
-target_link_libraries(pipes-sort hadooppipes hadooputils)
-hadoop_output_directory(pipes-sort examples)
-
-add_library(hadooputils STATIC
-    main/native/utils/impl/StringUtils.cc
-    main/native/utils/impl/SerialUtils.cc
-)
-if (NOT RPC_INCLUDE_DIRS AND LIBTIRPC_FOUND)
-    target_link_libraries(hadooputils tirpc)
-endif()
-
-add_library(hadooppipes STATIC
-    main/native/pipes/impl/HadoopPipes.cc
-)
-
-include(CheckLibraryExists)
-check_library_exists(dl dlopen "" NEED_LINK_DL)
-
-if(NEED_LINK_DL)
-    set(LIB_DL "dl")
-endif()
-
-if(${CMAKE_SYSTEM_NAME} MATCHES "SunOS")
-    exec_program("uname" ARGS "-r" OUTPUT_VARIABLE OS_VERSION)
-    if(OS_VERSION VERSION_LESS "5.12")
-        set(LIB_NET "socket" "nsl")
-    endif()
-endif()
-
-target_link_libraries(hadooppipes
-    ${OPENSSL_LIBRARIES}
-    ${LIB_DL}
-    ${LIB_NET}
-)
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/README.txt b/hadoop-tools/hadoop-pipes/src/main/native/examples/README.txt
deleted file mode 100644
index 6ea8ca11a71..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/examples/README.txt
+++ /dev/null
@@ -1,16 +0,0 @@
-To run the examples, first compile them:
-
-% mvn install -Pdist,native
-
-and then copy the binaries to dfs:
-
-% hdfs dfs -put target/native/examples/wordcount-simple /examples/bin/
-
-create an input directory with text files:
-
-% hdfs dfs -put my-data in-dir
-
-and run the word count example:
-
-% mapred pipes -conf src/main/native/examples/conf/word.xml \
-                   -input in-dir -output out-dir
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word-part.xml b/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word-part.xml
deleted file mode 100644
index 5425de205bb..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word-part.xml
+++ /dev/null
@@ -1,41 +0,0 @@
-<?xml version="1.0"?>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<configuration>
-
-<property>
-  <name>mapreduce.job.reduces</name>
-  <value>2</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.executable</name>
-  <value>hdfs:/examples/bin/wordcount-part</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.isjavarecordreader</name>
-  <value>true</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.isjavarecordwriter</name>
-  <value>true</value>
-</property>
-
-</configuration>
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word.xml b/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word.xml
deleted file mode 100644
index 9d1cd572dc3..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/examples/conf/word.xml
+++ /dev/null
@@ -1,45 +0,0 @@
-<?xml version="1.0"?>
-<!--
-   Licensed to the Apache Software Foundation (ASF) under one or more
-   contributor license agreements.  See the NOTICE file distributed with
-   this work for additional information regarding copyright ownership.
-   The ASF licenses this file to You under the Apache License, Version 2.0
-   (the "License"); you may not use this file except in compliance with
-   the License.  You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
--->
-
-<configuration>
-
-<property>
-  <name>mapreduce.job.reduces</name>
-  <value>2</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.executable</name>
-  <value>/examples/bin/wordcount-simple#wordcount-simple</value>
-  <description> Executable path is given as "path#executable-name"
-                sothat the executable will have a symlink in working directory.
-                This can be used for gdb debugging etc.
-  </description>
-</property>
-
-<property>
-  <name>mapreduce.pipes.isjavarecordreader</name>
-  <value>true</value>
-</property>
-
-<property>
-  <name>mapreduce.pipes.isjavarecordwriter</name>
-  <value>true</value>
-</property>
-
-</configuration>
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/sort.cc b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/sort.cc
deleted file mode 100644
index 529d6fb4cdc..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/sort.cc
+++ /dev/null
@@ -1,96 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hadoop/Pipes.hh"
-#include "hadoop/TemplateFactory.hh"
-
-class SortMap: public HadoopPipes::Mapper {
-private:
-  /* the fraction 0.0 to 1.0 of records to keep */
-  float keepFraction;
-  /* the number of records kept so far */
-  long long keptRecords;
-  /* the total number of records */
-  long long totalRecords;
-  static const std::string MAP_KEEP_PERCENT;
-public:
-  /*
-   * Look in the config to find the fraction of records to keep.
-   */
-  SortMap(HadoopPipes::TaskContext& context){
-    const HadoopPipes::JobConf* conf = context.getJobConf();
-    if (conf->hasKey(MAP_KEEP_PERCENT)) {
-      keepFraction = conf->getFloat(MAP_KEEP_PERCENT) / 100.0;
-    } else {
-      keepFraction = 1.0;
-    }
-    keptRecords = 0;
-    totalRecords = 0;
-  }
-
-  void map(HadoopPipes::MapContext& context) {
-    totalRecords += 1;
-    while ((float) keptRecords / totalRecords < keepFraction) {
-      keptRecords += 1;
-      context.emit(context.getInputKey(), context.getInputValue());
-    }
-  }
-};
-
-const std::string SortMap::MAP_KEEP_PERCENT("mapreduce.loadgen.sort.map.preserve.percent");
-
-class SortReduce: public HadoopPipes::Reducer {
-private:
-  /* the fraction 0.0 to 1.0 of records to keep */
-  float keepFraction;
-  /* the number of records kept so far */
-  long long keptRecords;
-  /* the total number of records */
-  long long totalRecords;
-  static const std::string REDUCE_KEEP_PERCENT;
-public:
-  SortReduce(HadoopPipes::TaskContext& context){
-    const HadoopPipes::JobConf* conf = context.getJobConf();
-    if (conf->hasKey(REDUCE_KEEP_PERCENT)) {
-      keepFraction = conf->getFloat(REDUCE_KEEP_PERCENT) / 100.0;
-    } else {
-      keepFraction = 1.0;
-    }
-    keptRecords = 0;
-    totalRecords = 0;
-  }
-
-  void reduce(HadoopPipes::ReduceContext& context) {
-    while (context.nextValue()) {
-      totalRecords += 1;
-      while ((float) keptRecords / totalRecords < keepFraction) {
-        keptRecords += 1;
-        context.emit(context.getInputKey(), context.getInputValue());
-      }
-    }
-  }
-};
-
-const std::string 
-  SortReduce::REDUCE_KEEP_PERCENT("mapreduce.loadgen.sort.reduce.preserve.percent");
-
-int main(int argc, char *argv[]) {
-  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<SortMap,
-                                                           SortReduce>());
-}
-
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-nopipe.cc b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-nopipe.cc
deleted file mode 100644
index a5fe7311a0c..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-nopipe.cc
+++ /dev/null
@@ -1,148 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#include "hadoop/Pipes.hh"
-#include "hadoop/TemplateFactory.hh"
-#include "hadoop/StringUtils.hh"
-#include "hadoop/SerialUtils.hh"
-
-#include <stdio.h>
-#include <sys/types.h>
-#include <sys/stat.h>
-
-const std::string WORDCOUNT = "WORDCOUNT";
-const std::string INPUT_WORDS = "INPUT_WORDS";
-const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
-
-class WordCountMap: public HadoopPipes::Mapper {
-public:
-  HadoopPipes::TaskContext::Counter* inputWords;
-  
-  WordCountMap(HadoopPipes::TaskContext& context) {
-    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
-  }
-  
-  void map(HadoopPipes::MapContext& context) {
-    std::vector<std::string> words = 
-      HadoopUtils::splitString(context.getInputValue(), " ");
-    for(unsigned int i=0; i < words.size(); ++i) {
-      context.emit(words[i], "1");
-    }
-    context.incrementCounter(inputWords, words.size());
-  }
-};
-
-class WordCountReduce: public HadoopPipes::Reducer {
-public:
-  HadoopPipes::TaskContext::Counter* outputWords;
-
-  WordCountReduce(HadoopPipes::TaskContext& context) {
-    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
-  }
-
-  void reduce(HadoopPipes::ReduceContext& context) {
-    int sum = 0;
-    while (context.nextValue()) {
-      sum += HadoopUtils::toInt(context.getInputValue());
-    }
-    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
-    context.incrementCounter(outputWords, 1); 
-  }
-};
-
-class WordCountReader: public HadoopPipes::RecordReader {
-private:
-  int64_t bytesTotal;
-  int64_t bytesRead;
-  FILE* file;
-public:
-  WordCountReader(HadoopPipes::MapContext& context) {
-    std::string filename;
-    HadoopUtils::StringInStream stream(context.getInputSplit());
-    HadoopUtils::deserializeString(filename, stream);
-    struct stat statResult;
-    stat(filename.c_str(), &statResult);
-    bytesTotal = statResult.st_size;
-    bytesRead = 0;
-    file = fopen(filename.c_str(), "rt");
-    HADOOP_ASSERT(file != NULL, "failed to open " + filename);
-  }
-
-  ~WordCountReader() {
-    fclose(file);
-  }
-
-  virtual bool next(std::string& key, std::string& value) {
-    key = HadoopUtils::toString(ftell(file));
-    int ch = getc(file);
-    bytesRead += 1;
-    value.clear();
-    while (ch != -1 && ch != '\n') {
-      value += ch;
-      ch = getc(file);
-      bytesRead += 1;
-    }
-    return ch != -1;
-  }
-
-  /**
-   * The progress of the record reader through the split as a value between
-   * 0.0 and 1.0.
-   */
-  virtual float getProgress() {
-    if (bytesTotal > 0) {
-      return (float)bytesRead / bytesTotal;
-    } else {
-      return 1.0f;
-    }
-  }
-};
-
-class WordCountWriter: public HadoopPipes::RecordWriter {
-private:
-  FILE* file;
-public:
-  WordCountWriter(HadoopPipes::ReduceContext& context) {
-    const HadoopPipes::JobConf* job = context.getJobConf();
-    int part = job->getInt("mapreduce.task.partition");
-    std::string outDir = job->get("mapreduce.task.output.dir");
-    // remove the file: schema substring
-    std::string::size_type posn = outDir.find(":");
-    HADOOP_ASSERT(posn != std::string::npos, 
-                  "no schema found in output dir: " + outDir);
-    outDir.erase(0, posn+1);
-    mkdir(outDir.c_str(), 0777);
-    std::string outFile = outDir + "/part-" + HadoopUtils::toString(part);
-    file = fopen(outFile.c_str(), "wt");
-    HADOOP_ASSERT(file != NULL, "can't open file for writing: " + outFile);
-  }
-
-  ~WordCountWriter() {
-    fclose(file);
-  }
-
-  void emit(const std::string& key, const std::string& value) {
-    fprintf(file, "%s -> %s\n", key.c_str(), value.c_str());
-  }
-};
-
-int main(int argc, char *argv[]) {
-  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
-                              WordCountReduce, void, void, WordCountReader,
-                              WordCountWriter>());
-}
-
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-part.cc b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-part.cc
deleted file mode 100644
index 37dc1993c18..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-part.cc
+++ /dev/null
@@ -1,76 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hadoop/Pipes.hh"
-#include "hadoop/TemplateFactory.hh"
-#include "hadoop/StringUtils.hh"
-
-const std::string WORDCOUNT = "WORDCOUNT";
-const std::string INPUT_WORDS = "INPUT_WORDS";
-const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
-
-class WordCountMap: public HadoopPipes::Mapper {
-public:
-  HadoopPipes::TaskContext::Counter* inputWords;
-  
-  WordCountMap(HadoopPipes::TaskContext& context) {
-    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
-  }
-  
-  void map(HadoopPipes::MapContext& context) {
-    std::vector<std::string> words = 
-      HadoopUtils::splitString(context.getInputValue(), " ");
-    for(unsigned int i=0; i < words.size(); ++i) {
-      context.emit(words[i], "1");
-    }
-    context.incrementCounter(inputWords, words.size());
-  }
-};
-
-class WordCountReduce: public HadoopPipes::Reducer {
-public:
-  HadoopPipes::TaskContext::Counter* outputWords;
-
-  WordCountReduce(HadoopPipes::TaskContext& context) {
-    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
-  }
-
-  void reduce(HadoopPipes::ReduceContext& context) {
-    int sum = 0;
-    while (context.nextValue()) {
-      sum += HadoopUtils::toInt(context.getInputValue());
-    }
-    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
-    context.incrementCounter(outputWords, 1); 
-  }
-};
-
-class WordCountPartitioner: public HadoopPipes::Partitioner {
-public:
-  WordCountPartitioner(HadoopPipes::TaskContext& context){}
-  virtual int partition(const std::string& key, int numOfReduces) {
-    return 0;
-  }
-};
-
-int main(int argc, char *argv[]) {
-  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
-                              WordCountReduce,WordCountPartitioner,
-                              WordCountReduce>());
-}
-
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-simple.cc b/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-simple.cc
deleted file mode 100644
index 64dd80183fa..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/examples/impl/wordcount-simple.cc
+++ /dev/null
@@ -1,67 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hadoop/Pipes.hh"
-#include "hadoop/TemplateFactory.hh"
-#include "hadoop/StringUtils.hh"
-
-const std::string WORDCOUNT = "WORDCOUNT";
-const std::string INPUT_WORDS = "INPUT_WORDS";
-const std::string OUTPUT_WORDS = "OUTPUT_WORDS";
-
-class WordCountMap: public HadoopPipes::Mapper {
-public:
-  HadoopPipes::TaskContext::Counter* inputWords;
-  
-  WordCountMap(HadoopPipes::TaskContext& context) {
-    inputWords = context.getCounter(WORDCOUNT, INPUT_WORDS);
-  }
-  
-  void map(HadoopPipes::MapContext& context) {
-    std::vector<std::string> words = 
-      HadoopUtils::splitString(context.getInputValue(), " ");
-    for(unsigned int i=0; i < words.size(); ++i) {
-      context.emit(words[i], "1");
-    }
-    context.incrementCounter(inputWords, words.size());
-  }
-};
-
-class WordCountReduce: public HadoopPipes::Reducer {
-public:
-  HadoopPipes::TaskContext::Counter* outputWords;
-
-  WordCountReduce(HadoopPipes::TaskContext& context) {
-    outputWords = context.getCounter(WORDCOUNT, OUTPUT_WORDS);
-  }
-
-  void reduce(HadoopPipes::ReduceContext& context) {
-    int sum = 0;
-    while (context.nextValue()) {
-      sum += HadoopUtils::toInt(context.getInputValue());
-    }
-    context.emit(context.getInputKey(), HadoopUtils::toString(sum));
-    context.incrementCounter(outputWords, 1); 
-  }
-};
-
-int main(int argc, char *argv[]) {
-  return HadoopPipes::runTask(HadoopPipes::TemplateFactory<WordCountMap, 
-                              WordCountReduce>());
-}
-
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/Pipes.hh b/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/Pipes.hh
deleted file mode 100644
index b5d0ddd1726..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/Pipes.hh
+++ /dev/null
@@ -1,260 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#ifndef HADOOP_PIPES_HH
-#define HADOOP_PIPES_HH
-
-#ifdef SWIG
-%module (directors="1") HadoopPipes
-%include "std_string.i"
-%feature("director") Mapper;
-%feature("director") Reducer;
-%feature("director") Partitioner;
-%feature("director") RecordReader;
-%feature("director") RecordWriter;
-%feature("director") Factory;
-#else
-#include <string>
-#endif
-
-#include <stdint.h>
-
-namespace HadoopPipes {
-
-/**
- * This interface defines the interface between application code and the 
- * foreign code interface to Hadoop Map/Reduce.
- */
-
-/**
- * A JobConf defines the properties for a job.
- */
-class JobConf {
-public:
-  virtual bool hasKey(const std::string& key) const = 0;
-  virtual const std::string& get(const std::string& key) const = 0;
-  virtual int getInt(const std::string& key) const = 0;
-  virtual float getFloat(const std::string& key) const = 0;
-  virtual bool getBoolean(const std::string&key) const = 0;
-  virtual ~JobConf() {}
-};
-
-/**
- * Task context provides the information about the task and job.
- */
-class TaskContext {
-public:
-  /**
-   * Counter to keep track of a property and its value.
-   */
-  class Counter {
-  private:
-    int id;
-  public:
-    Counter(int counterId) : id(counterId) {}
-    Counter(const Counter& counter) : id(counter.id) {}
-
-    int getId() const { return id; }
-  };
-  
-  /**
-   * Get the JobConf for the current task.
-   */
-  virtual const JobConf* getJobConf() = 0;
-
-  /**
-   * Get the current key. 
-   * @return the current key
-   */
-  virtual const std::string& getInputKey() = 0;
-
-  /**
-   * Get the current value. 
-   * @return the current value
-   */
-  virtual const std::string& getInputValue() = 0;
-
-  /**
-   * Generate an output record
-   */
-  virtual void emit(const std::string& key, const std::string& value) = 0;
-
-  /**
-   * Mark your task as having made progress without changing the status 
-   * message.
-   */
-  virtual void progress() = 0;
-
-  /**
-   * Set the status message and call progress.
-   */
-  virtual void setStatus(const std::string& status) = 0;
-
-  /**
-   * Register a counter with the given group and name.
-   */
-  virtual Counter* 
-    getCounter(const std::string& group, const std::string& name) = 0;
-
-  /**
-   * Increment the value of the counter with the given amount.
-   */
-  virtual void incrementCounter(const Counter* counter, uint64_t amount) = 0;
-  
-  virtual ~TaskContext() {}
-};
-
-class MapContext: public TaskContext {
-public:
-
-  /**
-   * Access the InputSplit of the mapper.
-   */
-  virtual const std::string& getInputSplit() = 0;
-
-  /**
-   * Get the name of the key class of the input to this task.
-   */
-  virtual const std::string& getInputKeyClass() = 0;
-
-  /**
-   * Get the name of the value class of the input to this task.
-   */
-  virtual const std::string& getInputValueClass() = 0;
-
-};
-
-class ReduceContext: public TaskContext {
-public:
-  /**
-   * Advance to the next value.
-   */
-  virtual bool nextValue() = 0;
-};
-
-class Closable {
-public:
-  virtual void close() {}
-  virtual ~Closable() {}
-};
-
-/**
- * The application's mapper class to do map.
- */
-class Mapper: public Closable {
-public:
-  virtual void map(MapContext& context) = 0;
-};
-
-/**
- * The application's reducer class to do reduce.
- */
-class Reducer: public Closable {
-public:
-  virtual void reduce(ReduceContext& context) = 0;
-};
-
-/**
- * User code to decide where each key should be sent.
- */
-class Partitioner {
-public:
-  virtual int partition(const std::string& key, int numOfReduces) = 0;
-  virtual ~Partitioner() {}
-};
-
-/**
- * For applications that want to read the input directly for the map function
- * they can define RecordReaders in C++.
- */
-class RecordReader: public Closable {
-public:
-  virtual bool next(std::string& key, std::string& value) = 0;
-
-  /**
-   * The progress of the record reader through the split as a value between
-   * 0.0 and 1.0.
-   */
-  virtual float getProgress() = 0;
-};
-
-/**
- * An object to write key/value pairs as they are emited from the reduce.
- */
-class RecordWriter: public Closable {
-public:
-  virtual void emit(const std::string& key,
-                    const std::string& value) = 0;
-};
-
-/**
- * A factory to create the necessary application objects.
- */
-class Factory {
-public:
-  virtual Mapper* createMapper(MapContext& context) const = 0;
-  virtual Reducer* createReducer(ReduceContext& context) const = 0;
-
-  /**
-   * Create a combiner, if this application has one.
-   * @return the new combiner or NULL, if one is not needed
-   */
-  virtual Reducer* createCombiner(MapContext& context) const {
-    return NULL; 
-  }
-
-  /**
-   * Create an application partitioner object.
-   * @return the new partitioner or NULL, if the default partitioner should be 
-   *     used.
-   */
-  virtual Partitioner* createPartitioner(MapContext& context) const {
-    return NULL;
-  }
-
-  /**
-   * Create an application record reader.
-   * @return the new RecordReader or NULL, if the Java RecordReader should be
-   *    used.
-   */
-  virtual RecordReader* createRecordReader(MapContext& context) const {
-    return NULL; 
-  }
-
-  /**
-   * Create an application record writer.
-   * @return the new RecordWriter or NULL, if the Java RecordWriter should be
-   *    used.
-   */
-  virtual RecordWriter* createRecordWriter(ReduceContext& context) const {
-    return NULL;
-  }
-
-  virtual ~Factory() {}
-};
-
-/**
- * Run the assigned task in the framework.
- * The user's main function should set the various functions using the 
- * set* functions above and then call this.
- * @return true, if the task succeeded.
- */
-bool runTask(const Factory& factory);
-
-}
-
-#endif
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/TemplateFactory.hh b/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/TemplateFactory.hh
deleted file mode 100644
index 22e10ae56fb..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/pipes/api/hadoop/TemplateFactory.hh
+++ /dev/null
@@ -1,96 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#ifndef HADOOP_PIPES_TEMPLATE_FACTORY_HH
-#define HADOOP_PIPES_TEMPLATE_FACTORY_HH
-
-namespace HadoopPipes {
-
-  template <class mapper, class reducer>
-  class TemplateFactory2: public Factory {
-  public:
-    Mapper* createMapper(MapContext& context) const {
-      return new mapper(context);
-    }
-    Reducer* createReducer(ReduceContext& context) const {
-      return new reducer(context);
-    }
-  };
-
-  template <class mapper, class reducer, class partitioner>
-  class TemplateFactory3: public TemplateFactory2<mapper,reducer> {
-  public:
-    Partitioner* createPartitioner(MapContext& context) const {
-      return new partitioner(context);
-    }
-  };
-
-  template <class mapper, class reducer>
-  class TemplateFactory3<mapper, reducer, void>
-      : public TemplateFactory2<mapper,reducer> {
-  };
-
-  template <class mapper, class reducer, class partitioner, class combiner>
-  class TemplateFactory4
-   : public TemplateFactory3<mapper,reducer,partitioner>{
-  public:
-    Reducer* createCombiner(MapContext& context) const {
-      return new combiner(context);
-    }
-  };
-
-  template <class mapper, class reducer, class partitioner>
-  class TemplateFactory4<mapper,reducer,partitioner,void>
-   : public TemplateFactory3<mapper,reducer,partitioner>{
-  };
-
-  template <class mapper, class reducer, class partitioner, 
-            class combiner, class recordReader>
-  class TemplateFactory5
-   : public TemplateFactory4<mapper,reducer,partitioner,combiner>{
-  public:
-    RecordReader* createRecordReader(MapContext& context) const {
-      return new recordReader(context);
-    }
-  };
-
-  template <class mapper, class reducer, class partitioner,class combiner>
-  class TemplateFactory5<mapper,reducer,partitioner,combiner,void>
-   : public TemplateFactory4<mapper,reducer,partitioner,combiner>{
-  };
-
-  template <class mapper, class reducer, class partitioner=void, 
-            class combiner=void, class recordReader=void, 
-            class recordWriter=void> 
-  class TemplateFactory
-   : public TemplateFactory5<mapper,reducer,partitioner,combiner,recordReader>{
-  public:
-    RecordWriter* createRecordWriter(ReduceContext& context) const {
-      return new recordWriter(context);
-    }
-  };
-
-  template <class mapper, class reducer, class partitioner, 
-            class combiner, class recordReader>
-  class TemplateFactory<mapper, reducer, partitioner, combiner, recordReader, 
-                        void>
-   : public TemplateFactory5<mapper,reducer,partitioner,combiner,recordReader>{
-  };
-
-}
-
-#endif
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-gdb-commands.txt b/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-gdb-commands.txt
deleted file mode 100644
index 906522c73c3..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-gdb-commands.txt
+++ /dev/null
@@ -1,14 +0,0 @@
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-info threads
-backtrace
-quit
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-script b/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-script
deleted file mode 100644
index 7b74fb67a1f..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/pipes/debug/pipes-default-script
+++ /dev/null
@@ -1,15 +0,0 @@
-#!/usr/bin/env bash
-#   Licensed under the Apache License, Version 2.0 (the "License");
-#   you may not use this file except in compliance with the License.
-#   You may obtain a copy of the License at
-#
-#       http://www.apache.org/licenses/LICENSE-2.0
-#
-#   Unless required by applicable law or agreed to in writing, software
-#   distributed under the License is distributed on an "AS IS" BASIS,
-#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-#   See the License for the specific language governing permissions and
-#   limitations under the License.
-core=$(find . -name 'core*')
-#Only pipes programs have 5th argument as program name.
-gdb -quiet "${5}" -c "${core}" -x "${HADOOP_HOME}/src/c++/pipes/debug/pipes-default-gdb-commands.txt"
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc b/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc
deleted file mode 100644
index 45cb8c20240..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/pipes/impl/HadoopPipes.cc
+++ /dev/null
@@ -1,1192 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-#include "hadoop/Pipes.hh"
-#include "hadoop/SerialUtils.hh"
-#include "hadoop/StringUtils.hh"
-
-#include <map>
-#include <vector>
-
-#include <errno.h>
-#include <netinet/in.h>
-#include <stdint.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <strings.h>
-#include <unistd.h>
-#include <sys/socket.h>
-#include <pthread.h>
-#include <iostream>
-#include <fstream>
-
-#include <openssl/hmac.h>
-#include <openssl/buffer.h>
-
-using std::map;
-using std::string;
-using std::vector;
-
-using namespace HadoopUtils;
-
-namespace HadoopPipes {
-
-  class JobConfImpl: public JobConf {
-  private:
-    map<string, string> values;
-  public:
-    void set(const string& key, const string& value) {
-      values[key] = value;
-    }
-
-    virtual bool hasKey(const string& key) const {
-      return values.find(key) != values.end();
-    }
-
-    virtual const string& get(const string& key) const {
-      map<string,string>::const_iterator itr = values.find(key);
-      if (itr == values.end()) {
-        throw Error("Key " + key + " not found in JobConf");
-      }
-      return itr->second;
-    }
-
-    virtual int getInt(const string& key) const {
-      const string& val = get(key);
-      return toInt(val);
-    }
-
-    virtual float getFloat(const string& key) const {
-      const string& val = get(key);
-      return toFloat(val);
-    }
-
-    virtual bool getBoolean(const string&key) const {
-      const string& val = get(key);
-      return toBool(val);
-    }
-  };
-
-  class DownwardProtocol {
-  public:
-    virtual void start(int protocol) = 0;
-    virtual void setJobConf(vector<string> values) = 0;
-    virtual void setInputTypes(string keyType, string valueType) = 0;
-    virtual void runMap(string inputSplit, int numReduces, bool pipedInput)= 0;
-    virtual void mapItem(const string& key, const string& value) = 0;
-    virtual void runReduce(int reduce, bool pipedOutput) = 0;
-    virtual void reduceKey(const string& key) = 0;
-    virtual void reduceValue(const string& value) = 0;
-    virtual void close() = 0;
-    virtual void abort() = 0;
-    virtual ~DownwardProtocol() {}
-  };
-
-  class UpwardProtocol {
-  public:
-    virtual void output(const string& key, const string& value) = 0;
-    virtual void partitionedOutput(int reduce, const string& key,
-                                   const string& value) = 0;
-    virtual void status(const string& message) = 0;
-    virtual void progress(float progress) = 0;
-    virtual void done() = 0;
-    virtual void registerCounter(int id, const string& group, 
-                                 const string& name) = 0;
-    virtual void 
-      incrementCounter(const TaskContext::Counter* counter, uint64_t amount) = 0;
-    virtual ~UpwardProtocol() {}
-  };
-
-  class Protocol {
-  public:
-    virtual void nextEvent() = 0;
-    virtual UpwardProtocol* getUplink() = 0;
-    virtual ~Protocol() {}
-  };
-
-  class TextUpwardProtocol: public UpwardProtocol {
-  private:
-    FILE* stream;
-    static const char fieldSeparator = '\t';
-    static const char lineSeparator = '\n';
-
-    void writeBuffer(const string& buffer) {
-      fputs(quoteString(buffer, "\t\n").c_str(), stream);
-    }
-
-  public:
-    TextUpwardProtocol(FILE* _stream): stream(_stream) {}
-    
-    virtual void output(const string& key, const string& value) {
-      fprintf(stream, "output%c", fieldSeparator);
-      writeBuffer(key);
-      fprintf(stream, "%c", fieldSeparator);
-      writeBuffer(value);
-      fprintf(stream, "%c", lineSeparator);
-    }
-
-    virtual void partitionedOutput(int reduce, const string& key,
-                                   const string& value) {
-      fprintf(stream, "parititionedOutput%c%d%c", fieldSeparator, reduce, 
-              fieldSeparator);
-      writeBuffer(key);
-      fprintf(stream, "%c", fieldSeparator);
-      writeBuffer(value);
-      fprintf(stream, "%c", lineSeparator);
-    }
-
-    virtual void status(const string& message) {
-      fprintf(stream, "status%c%s%c", fieldSeparator, message.c_str(), 
-              lineSeparator);
-    }
-
-    virtual void progress(float progress) {
-      fprintf(stream, "progress%c%f%c", fieldSeparator, progress, 
-              lineSeparator);
-    }
-
-    virtual void registerCounter(int id, const string& group, 
-                                 const string& name) {
-      fprintf(stream, "registerCounter%c%d%c%s%c%s%c", fieldSeparator, id,
-              fieldSeparator, group.c_str(), fieldSeparator, name.c_str(), 
-              lineSeparator);
-    }
-
-    virtual void incrementCounter(const TaskContext::Counter* counter, 
-                                  uint64_t amount) {
-      fprintf(stream, "incrCounter%c%d%c%ld%c", fieldSeparator, counter->getId(), 
-              fieldSeparator, (long)amount, lineSeparator);
-    }
-    
-    virtual void done() {
-      fprintf(stream, "done%c", lineSeparator);
-    }
-  };
-
-  class TextProtocol: public Protocol {
-  private:
-    FILE* downStream;
-    DownwardProtocol* handler;
-    UpwardProtocol* uplink;
-    string key;
-    string value;
-
-    int readUpto(string& buffer, const char* limit) {
-      int ch;
-      buffer.clear();
-      while ((ch = getc(downStream)) != -1) {
-        if (strchr(limit, ch) != NULL) {
-          return ch;
-        }
-        buffer += ch;
-      }
-      return -1;
-    }
-
-    static const char* delim;
-  public:
-
-    TextProtocol(FILE* down, DownwardProtocol* _handler, FILE* up) {
-      downStream = down;
-      uplink = new TextUpwardProtocol(up);
-      handler = _handler;
-    }
-
-    UpwardProtocol* getUplink() {
-      return uplink;
-    }
-
-    virtual void nextEvent() {
-      string command;
-      string arg;
-      int sep;
-      sep = readUpto(command, delim);
-      if (command == "mapItem") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(key, delim);
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(value, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->mapItem(key, value);
-      } else if (command == "reduceValue") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(value, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->reduceValue(value);
-      } else if (command == "reduceKey") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(key, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->reduceKey(key);
-      } else if (command == "start") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(arg, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->start(toInt(arg));
-      } else if (command == "setJobConf") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(arg, delim);
-        int len = toInt(arg);
-        vector<string> values(len);
-        for(int i=0; i < len; ++i) {
-          HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-          sep = readUpto(arg, delim);
-          values.push_back(arg);
-        }
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->setJobConf(values);
-      } else if (command == "setInputTypes") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(key, delim);
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(value, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->setInputTypes(key, value);
-      } else if (command == "runMap") {
-        string split;
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(split, delim);
-        string reduces;
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(reduces, delim);
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(arg, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->runMap(split, toInt(reduces), toBool(arg));
-      } else if (command == "runReduce") {
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        sep = readUpto(arg, delim);
-        HADOOP_ASSERT(sep == '\t', "Short text protocol command " + command);
-        string piped;
-        sep = readUpto(piped, delim);
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->runReduce(toInt(arg), toBool(piped));
-      } else if (command == "abort") { 
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->abort();
-      } else if (command == "close") {
-        HADOOP_ASSERT(sep == '\n', "Long text protocol command " + command);
-        handler->close();
-      } else {
-        throw Error("Illegal text protocol command " + command);
-      }
-    }
-
-    ~TextProtocol() {
-      delete uplink;
-    }
-  };
-  const char* TextProtocol::delim = "\t\n";
-
-  enum MESSAGE_TYPE {START_MESSAGE, SET_JOB_CONF, SET_INPUT_TYPES, RUN_MAP, 
-                     MAP_ITEM, RUN_REDUCE, REDUCE_KEY, REDUCE_VALUE, 
-                     CLOSE, ABORT, AUTHENTICATION_REQ,
-                     OUTPUT=50, PARTITIONED_OUTPUT, STATUS, PROGRESS, DONE,
-                     REGISTER_COUNTER, INCREMENT_COUNTER, AUTHENTICATION_RESP};
-
-  class BinaryUpwardProtocol: public UpwardProtocol {
-  private:
-    FileOutStream* stream;
-  public:
-    BinaryUpwardProtocol(FILE* _stream) {
-      stream = new FileOutStream();
-      HADOOP_ASSERT(stream->open(_stream), "problem opening stream");
-    }
-
-    virtual void authenticate(const string &responseDigest) {
-      serializeInt(AUTHENTICATION_RESP, *stream);
-      serializeString(responseDigest, *stream);
-      stream->flush();
-    }
-
-    virtual void output(const string& key, const string& value) {
-      serializeInt(OUTPUT, *stream);
-      serializeString(key, *stream);
-      serializeString(value, *stream);
-    }
-
-    virtual void partitionedOutput(int reduce, const string& key,
-                                   const string& value) {
-      serializeInt(PARTITIONED_OUTPUT, *stream);
-      serializeInt(reduce, *stream);
-      serializeString(key, *stream);
-      serializeString(value, *stream);
-    }
-
-    virtual void status(const string& message) {
-      serializeInt(STATUS, *stream);
-      serializeString(message, *stream);
-    }
-
-    virtual void progress(float progress) {
-      serializeInt(PROGRESS, *stream);
-      serializeFloat(progress, *stream);
-      stream->flush();
-    }
-
-    virtual void done() {
-      serializeInt(DONE, *stream);
-    }
-
-    virtual void registerCounter(int id, const string& group, 
-                                 const string& name) {
-      serializeInt(REGISTER_COUNTER, *stream);
-      serializeInt(id, *stream);
-      serializeString(group, *stream);
-      serializeString(name, *stream);
-    }
-
-    virtual void incrementCounter(const TaskContext::Counter* counter, 
-                                  uint64_t amount) {
-      serializeInt(INCREMENT_COUNTER, *stream);
-      serializeInt(counter->getId(), *stream);
-      serializeLong(amount, *stream);
-    }
-    
-    ~BinaryUpwardProtocol() {
-      delete stream;
-    }
-  };
-
-  class BinaryProtocol: public Protocol {
-  private:
-    FileInStream* downStream;
-    DownwardProtocol* handler;
-    BinaryUpwardProtocol * uplink;
-    string key;
-    string value;
-    string password;
-    bool authDone;
-    void getPassword(string &password) {
-      const char *passwordFile = getenv("hadoop.pipes.shared.secret.location");
-      if (passwordFile == NULL) {
-        return;
-      }
-      std::ifstream fstr(passwordFile, std::fstream::binary);
-      if (fstr.fail()) {
-        std::cerr << "Could not open the password file" << std::endl;
-        return;
-      } 
-      unsigned char * passBuff = new unsigned char [512];
-      fstr.read((char *)passBuff, 512);
-      int passwordLength = fstr.gcount();
-      fstr.close();
-      passBuff[passwordLength] = 0;
-      password.replace(0, passwordLength, (const char *) passBuff, passwordLength);
-      delete [] passBuff;
-      return; 
-    }
-
-    void verifyDigestAndRespond(string& digest, string& challenge) {
-      if (password.empty()) {
-        //password can be empty if process is running in debug mode from
-        //command file.
-        authDone = true;
-        return;
-      }
-
-      if (!verifyDigest(password, digest, challenge)) {
-        std::cerr << "Server failed to authenticate. Exiting" << std::endl;
-        exit(-1);
-      }
-      authDone = true;
-      string responseDigest = createDigest(password, digest);
-      uplink->authenticate(responseDigest);
-    }
-
-    bool verifyDigest(string &password, string& digest, string& challenge) {
-      string expectedDigest = createDigest(password, challenge);
-      if (digest == expectedDigest) {
-        return true;
-      } else {
-        return false;
-      }
-    }
-
-    string createDigest(string &password, string& msg) {
-#if OPENSSL_VERSION_NUMBER < 0x10100000L
-      HMAC_CTX ctx;
-      unsigned char digest[EVP_MAX_MD_SIZE];
-      HMAC_Init(&ctx, (const unsigned char *)password.c_str(), 
-          password.length(), EVP_sha1());
-      HMAC_Update(&ctx, (const unsigned char *)msg.c_str(), msg.length());
-      unsigned int digestLen;
-      HMAC_Final(&ctx, digest, &digestLen);
-      HMAC_cleanup(&ctx);
-#else
-      HMAC_CTX *ctx = HMAC_CTX_new();
-      unsigned char digest[EVP_MAX_MD_SIZE];
-      HMAC_Init_ex(ctx, (const unsigned char *)password.c_str(),
-          password.length(), EVP_sha1(), NULL);
-      HMAC_Update(ctx, (const unsigned char *)msg.c_str(), msg.length());
-      unsigned int digestLen;
-      HMAC_Final(ctx, digest, &digestLen);
-      HMAC_CTX_free(ctx);
-#endif
-      //now apply base64 encoding
-      BIO *bmem, *b64;
-      BUF_MEM *bptr;
-
-      b64 = BIO_new(BIO_f_base64());
-      bmem = BIO_new(BIO_s_mem());
-      b64 = BIO_push(b64, bmem);
-      BIO_write(b64, digest, digestLen);
-      BIO_flush(b64);
-      BIO_get_mem_ptr(b64, &bptr);
-
-      char digestBuffer[bptr->length];
-      memcpy(digestBuffer, bptr->data, bptr->length-1);
-      digestBuffer[bptr->length-1] = 0;
-      BIO_free_all(b64);
-
-      return string(digestBuffer);
-    }
-
-  public:
-    BinaryProtocol(FILE* down, DownwardProtocol* _handler, FILE* up) {
-      downStream = new FileInStream();
-      downStream->open(down);
-      uplink = new BinaryUpwardProtocol(up);
-      handler = _handler;
-      authDone = false;
-      getPassword(password);
-    }
-
-    UpwardProtocol* getUplink() {
-      return uplink;
-    }
-
-    virtual void nextEvent() {
-      int32_t cmd;
-      cmd = deserializeInt(*downStream);
-      if (!authDone && cmd != AUTHENTICATION_REQ) {
-        //Authentication request must be the first message if
-        //authentication is not complete
-        std::cerr << "Command:" << cmd << "received before authentication. " 
-            << "Exiting.." << std::endl;
-        exit(-1);
-      }
-      switch (cmd) {
-      case AUTHENTICATION_REQ: {
-        string digest;
-        string challenge;
-        deserializeString(digest, *downStream);
-        deserializeString(challenge, *downStream);
-        verifyDigestAndRespond(digest, challenge);
-        break;
-      }
-      case START_MESSAGE: {
-        int32_t prot;
-        prot = deserializeInt(*downStream);
-        handler->start(prot);
-        break;
-      }
-      case SET_JOB_CONF: {
-        int32_t entries;
-        entries = deserializeInt(*downStream);
-        vector<string> result(entries);
-        for(int i=0; i < entries; ++i) {
-          string item;
-          deserializeString(item, *downStream);
-          result.push_back(item);
-        }
-        handler->setJobConf(result);
-        break;
-      }
-      case SET_INPUT_TYPES: {
-        string keyType;
-        string valueType;
-        deserializeString(keyType, *downStream);
-        deserializeString(valueType, *downStream);
-        handler->setInputTypes(keyType, valueType);
-        break;
-      }
-      case RUN_MAP: {
-        string split;
-        int32_t numReduces;
-        int32_t piped;
-        deserializeString(split, *downStream);
-        numReduces = deserializeInt(*downStream);
-        piped = deserializeInt(*downStream);
-        handler->runMap(split, numReduces, piped);
-        break;
-      }
-      case MAP_ITEM: {
-        deserializeString(key, *downStream);
-        deserializeString(value, *downStream);
-        handler->mapItem(key, value);
-        break;
-      }
-      case RUN_REDUCE: {
-        int32_t reduce;
-        int32_t piped;
-        reduce = deserializeInt(*downStream);
-        piped = deserializeInt(*downStream);
-        handler->runReduce(reduce, piped);
-        break;
-      }
-      case REDUCE_KEY: {
-        deserializeString(key, *downStream);
-        handler->reduceKey(key);
-        break;
-      }
-      case REDUCE_VALUE: {
-        deserializeString(value, *downStream);
-        handler->reduceValue(value);
-        break;
-      }
-      case CLOSE:
-        handler->close();
-        break;
-      case ABORT:
-        handler->abort();
-        break;
-      default:
-        HADOOP_ASSERT(false, "Unknown binary command " + toString(cmd));
-      }
-    }
-
-    virtual ~BinaryProtocol() {
-      delete downStream;
-      delete uplink;
-    }
-  };
-
-  /**
-   * Define a context object to give to combiners that will let them
-   * go through the values and emit their results correctly.
-   */
-  class CombineContext: public ReduceContext {
-  private:
-    ReduceContext* baseContext;
-    Partitioner* partitioner;
-    int numReduces;
-    UpwardProtocol* uplink;
-    bool firstKey;
-    bool firstValue;
-    map<string, vector<string> >::iterator keyItr;
-    map<string, vector<string> >::iterator endKeyItr;
-    vector<string>::iterator valueItr;
-    vector<string>::iterator endValueItr;
-
-  public:
-    CombineContext(ReduceContext* _baseContext,
-                   Partitioner* _partitioner,
-                   int _numReduces,
-                   UpwardProtocol* _uplink,
-                   map<string, vector<string> >& data) {
-      baseContext = _baseContext;
-      partitioner = _partitioner;
-      numReduces = _numReduces;
-      uplink = _uplink;
-      keyItr = data.begin();
-      endKeyItr = data.end();
-      firstKey = true;
-      firstValue = true;
-    }
-
-    virtual const JobConf* getJobConf() {
-      return baseContext->getJobConf();
-    }
-
-    virtual const std::string& getInputKey() {
-      return keyItr->first;
-    }
-
-    virtual const std::string& getInputValue() {
-      return *valueItr;
-    }
-
-    virtual void emit(const std::string& key, const std::string& value) {
-      if (partitioner != NULL) {
-        uplink->partitionedOutput(partitioner->partition(key, numReduces),
-                                  key, value);
-      } else {
-        uplink->output(key, value);
-      }
-    }
-
-    virtual void progress() {
-      baseContext->progress();
-    }
-
-    virtual void setStatus(const std::string& status) {
-      baseContext->setStatus(status);
-    }
-
-    bool nextKey() {
-      if (firstKey) {
-        firstKey = false;
-      } else {
-        ++keyItr;
-      }
-      if (keyItr != endKeyItr) {
-        valueItr = keyItr->second.begin();
-        endValueItr = keyItr->second.end();
-        firstValue = true;
-        return true;
-      }
-      return false;
-    }
-
-    virtual bool nextValue() {
-      if (firstValue) {
-        firstValue = false;
-      } else {
-        ++valueItr;
-      }
-      return valueItr != endValueItr;
-    }
-    
-    virtual Counter* getCounter(const std::string& group, 
-                               const std::string& name) {
-      return baseContext->getCounter(group, name);
-    }
-
-    virtual void incrementCounter(const Counter* counter, uint64_t amount) {
-      baseContext->incrementCounter(counter, amount);
-    }
-  };
-
-  /**
-   * A RecordWriter that will take the map outputs, buffer them up and then
-   * combine then when the buffer is full.
-   */
-  class CombineRunner: public RecordWriter {
-  private:
-    map<string, vector<string> > data;
-    int64_t spillSize;
-    int64_t numBytes;
-    ReduceContext* baseContext;
-    Partitioner* partitioner;
-    int numReduces;
-    UpwardProtocol* uplink;
-    Reducer* combiner;
-  public:
-    CombineRunner(int64_t _spillSize, ReduceContext* _baseContext, 
-                  Reducer* _combiner, UpwardProtocol* _uplink, 
-                  Partitioner* _partitioner, int _numReduces) {
-      numBytes = 0;
-      spillSize = _spillSize;
-      baseContext = _baseContext;
-      partitioner = _partitioner;
-      numReduces = _numReduces;
-      uplink = _uplink;
-      combiner = _combiner;
-    }
-
-    virtual void emit(const std::string& key,
-                      const std::string& value) {
-      numBytes += key.length() + value.length();
-      data[key].push_back(value);
-      if (numBytes >= spillSize) {
-        spillAll();
-      }
-    }
-
-    virtual void close() {
-      spillAll();
-    }
-
-  private:
-    void spillAll() {
-      CombineContext context(baseContext, partitioner, numReduces, 
-                             uplink, data);
-      while (context.nextKey()) {
-        combiner->reduce(context);
-      }
-      data.clear();
-      numBytes = 0;
-    }
-  };
-
-  class TaskContextImpl: public MapContext, public ReduceContext, 
-                         public DownwardProtocol {
-  private:
-    bool done;
-    JobConf* jobConf;
-    string key;
-    const string* newKey;
-    const string* value;
-    bool hasTask;
-    bool isNewKey;
-    bool isNewValue;
-    string* inputKeyClass;
-    string* inputValueClass;
-    string status;
-    float progressFloat;
-    uint64_t lastProgress;
-    bool statusSet;
-    Protocol* protocol;
-    UpwardProtocol *uplink;
-    string* inputSplit;
-    RecordReader* reader;
-    Mapper* mapper;
-    Reducer* reducer;
-    RecordWriter* writer;
-    Partitioner* partitioner;
-    int numReduces;
-    const Factory* factory;
-    pthread_mutex_t mutexDone;
-    std::vector<int> registeredCounterIds;
-
-  public:
-
-    TaskContextImpl(const Factory& _factory) {
-      statusSet = false;
-      done = false;
-      newKey = NULL;
-      factory = &_factory;
-      jobConf = NULL;
-      inputKeyClass = NULL;
-      inputValueClass = NULL;
-      inputSplit = NULL;
-      mapper = NULL;
-      reducer = NULL;
-      reader = NULL;
-      writer = NULL;
-      partitioner = NULL;
-      protocol = NULL;
-      isNewKey = false;
-      isNewValue = false;
-      lastProgress = 0;
-      progressFloat = 0.0f;
-      hasTask = false;
-      pthread_mutex_init(&mutexDone, NULL);
-    }
-
-    void setProtocol(Protocol* _protocol, UpwardProtocol* _uplink) {
-
-      protocol = _protocol;
-      uplink = _uplink;
-    }
-
-    virtual void start(int protocol) {
-      if (protocol != 0) {
-        throw Error("Protocol version " + toString(protocol) + 
-                    " not supported");
-      }
-    }
-
-    virtual void setJobConf(vector<string> values) {
-      int len = values.size();
-      JobConfImpl* result = new JobConfImpl();
-      HADOOP_ASSERT(len % 2 == 0, "Odd length of job conf values");
-      for(int i=0; i < len; i += 2) {
-        result->set(values[i], values[i+1]);
-      }
-      jobConf = result;
-    }
-
-    virtual void setInputTypes(string keyType, string valueType) {
-      inputKeyClass = new string(keyType);
-      inputValueClass = new string(valueType);
-    }
-
-    virtual void runMap(string _inputSplit, int _numReduces, bool pipedInput) {
-      inputSplit = new string(_inputSplit);
-      reader = factory->createRecordReader(*this);
-      HADOOP_ASSERT((reader == NULL) == pipedInput,
-                    pipedInput ? "RecordReader defined when not needed.":
-                    "RecordReader not defined");
-      if (reader != NULL) {
-        value = new string();
-      }
-      mapper = factory->createMapper(*this);
-      numReduces = _numReduces;
-      if (numReduces != 0) { 
-        reducer = factory->createCombiner(*this);
-        partitioner = factory->createPartitioner(*this);
-      }
-      if (reducer != NULL) {
-        int64_t spillSize = 100;
-        if (jobConf->hasKey("mapreduce.task.io.sort.mb")) {
-          spillSize = jobConf->getInt("mapreduce.task.io.sort.mb");
-        }
-        writer = new CombineRunner(spillSize * 1024 * 1024, this, reducer, 
-                                   uplink, partitioner, numReduces);
-      }
-      hasTask = true;
-    }
-
-    virtual void mapItem(const string& _key, const string& _value) {
-      newKey = &_key;
-      value = &_value;
-      isNewKey = true;
-    }
-
-    virtual void runReduce(int reduce, bool pipedOutput) {
-      reducer = factory->createReducer(*this);
-      writer = factory->createRecordWriter(*this);
-      HADOOP_ASSERT((writer == NULL) == pipedOutput,
-                    pipedOutput ? "RecordWriter defined when not needed.":
-                    "RecordWriter not defined");
-      hasTask = true;
-    }
-
-    virtual void reduceKey(const string& _key) {
-      isNewKey = true;
-      newKey = &_key;
-    }
-
-    virtual void reduceValue(const string& _value) {
-      isNewValue = true;
-      value = &_value;
-    }
-    
-    virtual bool isDone() {
-      pthread_mutex_lock(&mutexDone);
-      bool doneCopy = done;
-      pthread_mutex_unlock(&mutexDone);
-      return doneCopy;
-    }
-
-    virtual void close() {
-      pthread_mutex_lock(&mutexDone);
-      done = true;
-      pthread_mutex_unlock(&mutexDone);
-    }
-
-    virtual void abort() {
-      throw Error("Aborted by driver");
-    }
-
-    void waitForTask() {
-      while (!done && !hasTask) {
-        protocol->nextEvent();
-      }
-    }
-
-    bool nextKey() {
-      if (reader == NULL) {
-        while (!isNewKey) {
-          nextValue();
-          if (done) {
-            return false;
-          }
-        }
-        key = *newKey;
-      } else {
-        if (!reader->next(key, const_cast<string&>(*value))) {
-          pthread_mutex_lock(&mutexDone);
-          done = true;
-          pthread_mutex_unlock(&mutexDone);
-          return false;
-        }
-        progressFloat = reader->getProgress();
-      }
-      isNewKey = false;
-      if (mapper != NULL) {
-        mapper->map(*this);
-      } else {
-        reducer->reduce(*this);
-      }
-      return true;
-    }
-
-    /**
-     * Advance to the next value.
-     */
-    virtual bool nextValue() {
-      if (isNewKey || done) {
-        return false;
-      }
-      isNewValue = false;
-      progress();
-      protocol->nextEvent();
-      return isNewValue;
-    }
-
-    /**
-     * Get the JobConf for the current task.
-     */
-    virtual JobConf* getJobConf() {
-      return jobConf;
-    }
-
-    /**
-     * Get the current key. 
-     * @return the current key or NULL if called before the first map or reduce
-     */
-    virtual const string& getInputKey() {
-      return key;
-    }
-
-    /**
-     * Get the current value. 
-     * @return the current value or NULL if called before the first map or 
-     *    reduce
-     */
-    virtual const string& getInputValue() {
-      return *value;
-    }
-
-    /**
-     * Mark your task as having made progress without changing the status 
-     * message.
-     */
-    virtual void progress() {
-      if (uplink != 0) {
-        uint64_t now = getCurrentMillis();
-        if (now - lastProgress > 1000) {
-          lastProgress = now;
-          if (statusSet) {
-            uplink->status(status);
-            statusSet = false;
-          }
-          uplink->progress(progressFloat);
-        }
-      }
-    }
-
-    /**
-     * Set the status message and call progress.
-     */
-    virtual void setStatus(const string& status) {
-      this->status = status;
-      statusSet = true;
-      progress();
-    }
-
-    /**
-     * Get the name of the key class of the input to this task.
-     */
-    virtual const string& getInputKeyClass() {
-      return *inputKeyClass;
-    }
-
-    /**
-     * Get the name of the value class of the input to this task.
-     */
-    virtual const string& getInputValueClass() {
-      return *inputValueClass;
-    }
-
-    /**
-     * Access the InputSplit of the mapper.
-     */
-    virtual const std::string& getInputSplit() {
-      return *inputSplit;
-    }
-
-    virtual void emit(const string& key, const string& value) {
-      progress();
-      if (writer != NULL) {
-        writer->emit(key, value);
-      } else if (partitioner != NULL) {
-        int part = partitioner->partition(key, numReduces);
-        uplink->partitionedOutput(part, key, value);
-      } else {
-        uplink->output(key, value);
-      }
-    }
-
-    /**
-     * Register a counter with the given group and name.
-     */
-    virtual Counter* getCounter(const std::string& group, 
-                               const std::string& name) {
-      int id = registeredCounterIds.size();
-      registeredCounterIds.push_back(id);
-      uplink->registerCounter(id, group, name);
-      return new Counter(id);
-    }
-
-    /**
-     * Increment the value of the counter with the given amount.
-     */
-    virtual void incrementCounter(const Counter* counter, uint64_t amount) {
-      uplink->incrementCounter(counter, amount); 
-    }
-
-    void closeAll() {
-      if (reader) {
-        reader->close();
-      }
-      if (mapper) {
-        mapper->close();
-      }
-      if (reducer) {
-        reducer->close();
-      }
-      if (writer) {
-        writer->close();
-      }
-    }
-
-    virtual ~TaskContextImpl() {
-      delete jobConf;
-      delete inputKeyClass;
-      delete inputValueClass;
-      delete inputSplit;
-      if (reader) {
-        delete value;
-      }
-      delete reader;
-      delete mapper;
-      delete reducer;
-      delete writer;
-      delete partitioner;
-      pthread_mutex_destroy(&mutexDone);
-    }
-  };
-
-  /**
-   * Ping the parent every 5 seconds to know if it is alive 
-   */
-  void* ping(void* ptr) {
-    TaskContextImpl* context = (TaskContextImpl*) ptr;
-    char* portStr = getenv("mapreduce.pipes.command.port");
-    int MAX_RETRIES = 3;
-    int remaining_retries = MAX_RETRIES;
-    while (!context->isDone()) {
-      try{
-        sleep(5);
-        int sock = -1;
-        if (portStr) {
-          sock = socket(PF_INET, SOCK_STREAM, 0);
-          HADOOP_ASSERT(sock != - 1,
-                        string("problem creating socket: ") + strerror(errno));
-          sockaddr_in addr;
-          addr.sin_family = AF_INET;
-          addr.sin_port = htons(toInt(portStr));
-          addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);
-          HADOOP_ASSERT(connect(sock, (sockaddr*) &addr, sizeof(addr)) == 0,
-                        string("problem connecting command socket: ") +
-                        strerror(errno));
-
-        }
-        if (sock != -1) {
-          int result = shutdown(sock, SHUT_RDWR);
-          HADOOP_ASSERT(result == 0, "problem shutting socket");
-          result = close(sock);
-          HADOOP_ASSERT(result == 0, "problem closing socket");
-        }
-        remaining_retries = MAX_RETRIES;
-      } catch (Error& err) {
-        if (!context->isDone()) {
-          fprintf(stderr, "Hadoop Pipes Exception: in ping %s\n", 
-                err.getMessage().c_str());
-          remaining_retries -= 1;
-          if (remaining_retries == 0) {
-            exit(1);
-          }
-        } else {
-          return NULL;
-        }
-      }
-    }
-    return NULL;
-  }
-
-  /**
-   * Run the assigned task in the framework.
-   * The user's main function should set the various functions using the 
-   * set* functions above and then call this.
-   * @return true, if the task succeeded.
-   */
-  bool runTask(const Factory& factory) {
-    try {
-      TaskContextImpl* context = new TaskContextImpl(factory);
-      Protocol* connection;
-      char* portStr = getenv("mapreduce.pipes.command.port");
-      int sock = -1;
-      FILE* stream = NULL;
-      FILE* outStream = NULL;
-      char *bufin = NULL;
-      char *bufout = NULL;
-      if (portStr) {
-        sock = socket(PF_INET, SOCK_STREAM, 0);
-        HADOOP_ASSERT(sock != - 1,
-                      string("problem creating socket: ") + strerror(errno));
-        sockaddr_in addr;
-        addr.sin_family = AF_INET;
-        addr.sin_port = htons(toInt(portStr));
-        addr.sin_addr.s_addr = htonl(INADDR_LOOPBACK);
-        HADOOP_ASSERT(connect(sock, (sockaddr*) &addr, sizeof(addr)) == 0,
-                      string("problem connecting command socket: ") +
-                      strerror(errno));
-
-        stream = fdopen(sock, "r");
-        outStream = fdopen(sock, "w");
-
-        // increase buffer size
-        int bufsize = 128*1024;
-        int setbuf;
-        bufin = new char[bufsize];
-        bufout = new char[bufsize];
-        setbuf = setvbuf(stream, bufin, _IOFBF, bufsize);
-        HADOOP_ASSERT(setbuf == 0, string("problem with setvbuf for inStream: ")
-                                     + strerror(errno));
-        setbuf = setvbuf(outStream, bufout, _IOFBF, bufsize);
-        HADOOP_ASSERT(setbuf == 0, string("problem with setvbuf for outStream: ")
-                                     + strerror(errno));
-        connection = new BinaryProtocol(stream, context, outStream);
-      } else if (getenv("mapreduce.pipes.commandfile")) {
-        char* filename = getenv("mapreduce.pipes.commandfile");
-        string outFilename = filename;
-        outFilename += ".out";
-        stream = fopen(filename, "r");
-        outStream = fopen(outFilename.c_str(), "w");
-        connection = new BinaryProtocol(stream, context, outStream);
-      } else {
-        connection = new TextProtocol(stdin, context, stdout);
-      }
-      context->setProtocol(connection, connection->getUplink());
-      pthread_t pingThread;
-      pthread_create(&pingThread, NULL, ping, (void*)(context));
-      context->waitForTask();
-      while (!context->isDone()) {
-        context->nextKey();
-      }
-      context->closeAll();
-      connection->getUplink()->done();
-      pthread_join(pingThread,NULL);
-      delete context;
-      delete connection;
-      if (stream != NULL) {
-        fflush(stream);
-      }
-      if (outStream != NULL) {
-        fflush(outStream);
-      }
-      fflush(stdout);
-      if (sock != -1) {
-        int result = shutdown(sock, SHUT_RDWR);
-        HADOOP_ASSERT(result == 0, "problem shutting socket");
-        result = close(sock);
-        HADOOP_ASSERT(result == 0, "problem closing socket");
-      }
-      if (stream != NULL) {
-        //fclose(stream);
-      }
-      if (outStream != NULL) {
-        //fclose(outStream);
-      } 
-      delete[] bufin;
-      delete[] bufout;
-      return true;
-    } catch (Error& err) {
-      fprintf(stderr, "Hadoop Pipes Exception: %s\n", 
-              err.getMessage().c_str());
-      return false;
-    }
-  }
-}
-
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/SerialUtils.hh b/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/SerialUtils.hh
deleted file mode 100644
index ec1838b51d7..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/SerialUtils.hh
+++ /dev/null
@@ -1,171 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#ifndef HADOOP_SERIAL_UTILS_HH
-#define HADOOP_SERIAL_UTILS_HH
-
-#include <string>
-#include <stdint.h>
-
-namespace HadoopUtils {
-
-  /**
-   * A simple exception class that records a message for the user.
-   */
-  class Error {
-  private:
-    std::string error;
-  public:
-
-    /**
-     * Create an error object with the given message.
-     */
-    Error(const std::string& msg);
-
-    /**
-     * Construct an error object with the given message that was created on
-     * the given file, line, and functino.
-     */
-    Error(const std::string& msg, 
-          const std::string& file, int line, const std::string& function);
-
-    /**
-     * Get the error message.
-     */
-    const std::string& getMessage() const;
-  };
-
-  /**
-   * Check to make sure that the condition is true, and throw an exception
-   * if it is not. The exception will contain the message and a description
-   * of the source location.
-   */
-  #define HADOOP_ASSERT(CONDITION, MESSAGE) \
-    { \
-      if (!(CONDITION)) { \
-        throw HadoopUtils::Error((MESSAGE), __FILE__, __LINE__, \
-                                    __func__); \
-      } \
-    }
-
-  /**
-   * An interface for an input stream.
-   */
-  class InStream {
-  public:
-    /**
-     * Reads len bytes from the stream into the buffer.
-     * @param buf the buffer to read into
-     * @param buflen the length of the buffer
-     * @throws Error if there are problems reading
-     */
-    virtual void read(void *buf, size_t len) = 0;
-    virtual ~InStream() {}
-  };
-
-  /**
-   * An interface for an output stream.
-   */
-  class OutStream {
-  public:
-    /**
-     * Write the given buffer to the stream.
-     * @param buf the data to write
-     * @param len the number of bytes to write
-     * @throws Error if there are problems writing
-     */
-    virtual void write(const void *buf, size_t len) = 0;
-    /**
-     * Flush the data to the underlying store.
-     */
-    virtual void flush() = 0;
-    virtual ~OutStream() {}
-  };
-
-  /**
-   * A class to read a file as a stream.
-   */
-  class FileInStream : public InStream {
-  public:
-    FileInStream();
-    bool open(const std::string& name);
-    bool open(FILE* file);
-    void read(void *buf, size_t buflen);
-    bool skip(size_t nbytes);
-    bool close();
-    virtual ~FileInStream();
-  private:
-    /**
-     * The file to write to.
-     */
-    FILE *mFile;
-    /**
-     * Does is this class responsible for closing the FILE*?
-     */
-    bool isOwned;
-  };
-
-  /**
-   * A class to write a stream to a file.
-   */
-  class FileOutStream: public OutStream {
-  public:
-
-    /**
-     * Create a stream that isn't bound to anything.
-     */
-    FileOutStream();
-
-    /**
-     * Create the given file, potentially overwriting an existing file.
-     */
-    bool open(const std::string& name, bool overwrite);
-    bool open(FILE* file);
-    void write(const void* buf, size_t len);
-    bool advance(size_t nbytes);
-    void flush();
-    bool close();
-    virtual ~FileOutStream();
-  private:
-    FILE *mFile;
-    bool isOwned;
-  };
-
-  /**
-   * A stream that reads from a string.
-   */
-  class StringInStream: public InStream {
-  public:
-    StringInStream(const std::string& str);
-    virtual void read(void *buf, size_t buflen);
-  private:
-    const std::string& buffer;
-    std::string::const_iterator itr;
-  };
-
-  void serializeInt(int32_t t, OutStream& stream);
-  int32_t deserializeInt(InStream& stream);
-  void serializeLong(int64_t t, OutStream& stream);
-  int64_t deserializeLong(InStream& stream);
-  void serializeFloat(float t, OutStream& stream);
-  void deserializeFloat(float& t, InStream& stream);
-  float deserializeFloat(InStream& stream);
-  void serializeString(const std::string& t, OutStream& stream);
-  void deserializeString(std::string& t, InStream& stream);
-}
-
-#endif
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/StringUtils.hh b/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/StringUtils.hh
deleted file mode 100644
index 47201727250..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/utils/api/hadoop/StringUtils.hh
+++ /dev/null
@@ -1,81 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#ifndef HADOOP_STRING_UTILS_HH
-#define HADOOP_STRING_UTILS_HH
-
-#include <stdint.h>
-#include <string>
-#include <vector>
-
-namespace HadoopUtils {
-
-  /**
-   * Convert an integer to a string.
-   */
-  std::string toString(int32_t x);
-
-  /**
-   * Convert a string to an integer.
-   * @throws Error if the string is not a valid integer
-   */
-  int32_t toInt(const std::string& val);
-
-  /**
-   * Convert the string to a float.
-   * @throws Error if the string is not a valid float
-   */
-  float toFloat(const std::string& val);
-
-  /**
-   * Convert the string to a boolean.
-   * @throws Error if the string is not a valid boolean value
-   */
-  bool toBool(const std::string& val);
-
-  /**
-   * Get the current time in the number of milliseconds since 1970.
-   */
-  uint64_t getCurrentMillis();
-
-  /**
-   * Split a string into "words". Multiple deliminators are treated as a single
-   * word break, so no zero-length words are returned.
-   * @param str the string to split
-   * @param separator a list of characters that divide words
-   */
-  std::vector<std::string> splitString(const std::string& str,
-                                       const char* separator);
-
-  /**
-   * Quote a string to avoid "\", non-printable characters, and the 
-   * deliminators.
-   * @param str the string to quote
-   * @param deliminators the set of characters to always quote
-   */
-  std::string quoteString(const std::string& str,
-                          const char* deliminators);
-
-  /**
-   * Unquote the given string to return the original string.
-   * @param str the string to unquote
-   */
-  std::string unquoteString(const std::string& str);
-
-}
-
-#endif
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/SerialUtils.cc b/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/SerialUtils.cc
deleted file mode 100644
index 401dfee80c2..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/SerialUtils.cc
+++ /dev/null
@@ -1,301 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#include "hadoop/SerialUtils.hh"
-#include "hadoop/StringUtils.hh"
-
-#include <errno.h>
-#include <rpc/types.h>
-#include <rpc/xdr.h>
-#include <string>
-#include <string.h>
-
-using std::string;
-
-namespace HadoopUtils {
-
-  Error::Error(const std::string& msg): error(msg) {
-  }
-
-  Error::Error(const std::string& msg, 
-               const std::string& file, int line, 
-               const std::string& function) {
-    error = msg + " at " + file + ":" + toString(line) + 
-            " in " + function;
-  }
-
-  const std::string& Error::getMessage() const {
-    return error;
-  }
-
-  FileInStream::FileInStream()
-  {
-    mFile = NULL;
-    isOwned = false;
-  }
-
-  bool FileInStream::open(const std::string& name)
-  {
-    mFile = fopen(name.c_str(), "rb");
-    isOwned = true;
-    return (mFile != NULL);
-  }
-
-  bool FileInStream::open(FILE* file)
-  {
-    mFile = file;
-    isOwned = false;
-    return (mFile != NULL);
-  }
-
-  void FileInStream::read(void *buf, size_t len)
-  {
-    size_t result = fread(buf, len, 1, mFile);
-    if (result == 0) {
-      if (feof(mFile)) {
-        HADOOP_ASSERT(false, "end of file");
-      } else {
-        HADOOP_ASSERT(false, string("read error on file: ") + strerror(errno));
-      }
-    }
-  }
-
-  bool FileInStream::skip(size_t nbytes)
-  {
-    return (0==fseek(mFile, nbytes, SEEK_CUR));
-  }
-
-  bool FileInStream::close()
-  {
-    int ret = 0;
-    if (mFile != NULL && isOwned) {
-      ret = fclose(mFile);
-    }
-    mFile = NULL;
-    return (ret==0);
-  }
-
-  FileInStream::~FileInStream()
-  {
-    if (mFile != NULL) {
-      close();
-    }
-  }
-
-  FileOutStream::FileOutStream()
-  {
-    mFile = NULL;
-    isOwned = false;
-  }
-
-  bool FileOutStream::open(const std::string& name, bool overwrite)
-  {
-    if (!overwrite) {
-      mFile = fopen(name.c_str(), "rb");
-      if (mFile != NULL) {
-        fclose(mFile);
-        return false;
-      }
-    }
-    mFile = fopen(name.c_str(), "wb");
-    isOwned = true;
-    return (mFile != NULL);
-  }
-
-  bool FileOutStream::open(FILE* file)
-  {
-    mFile = file;
-    isOwned = false;
-    return (mFile != NULL);
-  }
-
-  void FileOutStream::write(const void* buf, size_t len)
-  {
-    size_t result = fwrite(buf, len, 1, mFile);
-    HADOOP_ASSERT(result == 1,
-                  string("write error to file: ") + strerror(errno));
-  }
-
-  bool FileOutStream::advance(size_t nbytes)
-  {
-    return (0==fseek(mFile, nbytes, SEEK_CUR));
-  }
-
-  bool FileOutStream::close()
-  {
-    int ret = 0;
-    if (mFile != NULL && isOwned) {
-      ret = fclose(mFile);
-    }
-    mFile = NULL;
-    return (ret == 0);
-  }
-
-  void FileOutStream::flush()
-  {
-    fflush(mFile);
-  }
-
-  FileOutStream::~FileOutStream()
-  {
-    if (mFile != NULL) {
-      close();
-    }
-  }
-
-  StringInStream::StringInStream(const std::string& str): buffer(str) {
-    itr = buffer.begin();
-  }
-
-  void StringInStream::read(void *buf, size_t buflen) {
-    size_t bytes = 0;
-    char* output = (char*) buf;
-    std::string::const_iterator end = buffer.end();
-    while (bytes < buflen) {
-      output[bytes++] = *itr;
-      ++itr;
-      if (itr == end) {
-        break;
-      }
-    }
-    HADOOP_ASSERT(bytes == buflen, "unexpected end of string reached");
-  }
-
-  void serializeInt(int32_t t, OutStream& stream) {
-    serializeLong(t,stream);
-  }
-
-  void serializeLong(int64_t t, OutStream& stream)
-  {
-    if (t >= -112 && t <= 127) {
-      int8_t b = t;
-      stream.write(&b, 1);
-      return;
-    }
-        
-    int8_t len = -112;
-    if (t < 0) {
-      t ^= -1ll; // reset the sign bit
-      len = -120;
-    }
-        
-    uint64_t tmp = t;
-    while (tmp != 0) {
-      tmp = tmp >> 8;
-      len--;
-    }
-  
-    stream.write(&len, 1);      
-    len = (len < -120) ? -(len + 120) : -(len + 112);
-        
-    for (uint32_t idx = len; idx != 0; idx--) {
-      uint32_t shiftbits = (idx - 1) * 8;
-      uint64_t mask = 0xFFll << shiftbits;
-      uint8_t b = (t & mask) >> shiftbits;
-      stream.write(&b, 1);
-    }
-  }
-
-  int32_t deserializeInt(InStream& stream) {
-    return deserializeLong(stream);
-  }
-
-  int64_t deserializeLong(InStream& stream)
-  {
-    int8_t b;
-    stream.read(&b, 1);
-    if (b >= -112) {
-      return b;
-    }
-    bool negative;
-    int len;
-    if (b < -120) {
-      negative = true;
-      len = -120 - b;
-    } else {
-      negative = false;
-      len = -112 - b;
-    }
-    uint8_t barr[len];
-    stream.read(barr, len);
-    int64_t t = 0;
-    for (int idx = 0; idx < len; idx++) {
-      t = t << 8;
-      t |= (barr[idx] & 0xFF);
-    }
-    if (negative) {
-      t ^= -1ll;
-    }
-    return t;
-  }
-
-  void serializeFloat(float t, OutStream& stream)
-  {
-    char buf[sizeof(float)];
-    XDR xdrs;
-    xdrmem_create(&xdrs, buf, sizeof(float), XDR_ENCODE);
-    xdr_float(&xdrs, &t);
-    stream.write(buf, sizeof(float));
-  }
-
-  float deserializeFloat(InStream& stream)
-  {
-    float f;
-    deserializeFloat(f, stream);
-    return f;
-  }
-
-  void deserializeFloat(float& t, InStream& stream)
-  {
-    char buf[sizeof(float)];
-    stream.read(buf, sizeof(float));
-    XDR xdrs;
-    xdrmem_create(&xdrs, buf, sizeof(float), XDR_DECODE);
-    xdr_float(&xdrs, &t);
-  }
-
-  void serializeString(const std::string& t, OutStream& stream)
-  {
-    serializeInt(t.length(), stream);
-    if (t.length() > 0) {
-      stream.write(t.data(), t.length());
-    }
-  }
-
-  void deserializeString(std::string& t, InStream& stream)
-  {
-    int32_t len = deserializeInt(stream);
-    if (len > 0) {
-      // resize the string to the right length
-      t.resize(len);
-      // read into the string in 64k chunks
-      const int bufSize = 65536;
-      int offset = 0;
-      char buf[bufSize];
-      while (len > 0) {
-        int chunkLength = len > bufSize ? bufSize : len;
-        stream.read(buf, chunkLength);
-        t.replace(offset, chunkLength, buf, chunkLength);
-        offset += chunkLength;
-        len -= chunkLength;
-      }
-    } else {
-      t.clear();
-    }
-  }
-
-}
diff --git a/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/StringUtils.cc b/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/StringUtils.cc
deleted file mode 100644
index 0d2a878ab30..00000000000
--- a/hadoop-tools/hadoop-pipes/src/main/native/utils/impl/StringUtils.cc
+++ /dev/null
@@ -1,180 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-#include "hadoop/StringUtils.hh"
-#include "hadoop/SerialUtils.hh"
-
-#include <errno.h>
-#include <stdint.h>
-#include <stdio.h>
-#include <stdlib.h>
-#include <string.h>
-#include <strings.h>
-#include <sys/time.h>
-
-using std::string;
-using std::vector;
-
-namespace HadoopUtils {
-
-  string toString(int32_t x) {
-    char str[100];
-    sprintf(str, "%d", x);
-    return str;
-  }
-
-  int toInt(const string& val) {
-    int result;
-    char trash;
-    int num = sscanf(val.c_str(), "%d%c", &result, &trash);
-    HADOOP_ASSERT(num == 1,
-                  "Problem converting " + val + " to integer.");
-    return result;
-  }
-
-  float toFloat(const string& val) {
-    float result;
-    char trash;
-    int num = sscanf(val.c_str(), "%f%c", &result, &trash);
-    HADOOP_ASSERT(num == 1,
-                  "Problem converting " + val + " to float.");
-    return result;
-  }
-
-  bool toBool(const string& val) {
-    if (val == "true") {
-      return true;
-    } else if (val == "false") {
-      return false;
-    } else {
-      HADOOP_ASSERT(false,
-                    "Problem converting " + val + " to boolean.");
-    }
-  }
-
-  /**
-   * Get the current time in the number of milliseconds since 1970.
-   */
-  uint64_t getCurrentMillis() {
-    struct timeval tv;
-    struct timezone tz;
-    int sys = gettimeofday(&tv, &tz);
-    HADOOP_ASSERT(sys != -1, strerror(errno));
-    return tv.tv_sec * 1000 + tv.tv_usec / 1000;
-  }
-
-  vector<string> splitString(const std::string& str,
-			     const char* separator) {
-    vector<string> result;
-    string::size_type prev_pos=0;
-    string::size_type pos=0;
-    while ((pos = str.find_first_of(separator, prev_pos)) != string::npos) {
-      if (prev_pos < pos) {
-	result.push_back(str.substr(prev_pos, pos-prev_pos));
-      }
-      prev_pos = pos + 1;
-    }
-    if (prev_pos < str.size()) {
-      result.push_back(str.substr(prev_pos));
-    }
-    return result;
-  }
-
-  string quoteString(const string& str,
-                     const char* deliminators) {
-    
-    string result(str);
-    for(int i=result.length() -1; i >= 0; --i) {
-      char ch = result[i];
-      if (!isprint(ch) ||
-          ch == '\\' || 
-          strchr(deliminators, ch)) {
-        switch (ch) {
-        case '\\':
-          result.replace(i, 1, "\\\\");
-          break;
-        case '\t':
-          result.replace(i, 1, "\\t");
-          break;
-        case '\n':
-          result.replace(i, 1, "\\n");
-          break;
-        case ' ':
-          result.replace(i, 1, "\\s");
-          break;
-        default:
-          char buff[4];
-          sprintf(buff, "\\%02x", static_cast<unsigned char>(result[i]));
-          result.replace(i, 1, buff);
-        }
-      }
-    }
-    return result;
-  }
-
-  string unquoteString(const string& str) {
-    string result(str);
-    string::size_type current = result.find('\\');
-    while (current != string::npos) {
-      if (current + 1 < result.size()) {
-        char new_ch;
-        int num_chars;
-        if (isxdigit(result[current+1])) {
-          num_chars = 2;
-          HADOOP_ASSERT(current + num_chars < result.size(),
-                     "escape pattern \\<hex><hex> is missing second digit in '"
-                     + str + "'");
-          char sub_str[3];
-          sub_str[0] = result[current+1];
-          sub_str[1] = result[current+2];
-          sub_str[2] = '\0';
-          char* end_ptr = NULL;
-          long int int_val = strtol(sub_str, &end_ptr, 16);
-          HADOOP_ASSERT(*end_ptr == '\0' && int_val >= 0,
-                     "escape pattern \\<hex><hex> is broken in '" + str + "'");
-          new_ch = static_cast<char>(int_val);
-        } else {
-          num_chars = 1;
-          switch(result[current+1]) {
-          case '\\':
-            new_ch = '\\';
-            break;
-          case 't':
-            new_ch = '\t';
-            break;
-          case 'n':
-            new_ch = '\n';
-            break;
-          case 's':
-            new_ch = ' ';
-            break;
-          default:
-            string msg("unknow n escape character '");
-            msg += result[current+1];
-            HADOOP_ASSERT(false, msg + "' found in '" + str + "'");
-          }
-        }
-        result.replace(current, 1 + num_chars, 1, new_ch);
-        current = result.find('\\', current+1);
-      } else {
-        HADOOP_ASSERT(false, "trailing \\ in '" + str + "'");
-      }
-    }
-    return result;
-  }
-
-}
diff --git a/hadoop-tools/hadoop-tools-dist/pom.xml b/hadoop-tools/hadoop-tools-dist/pom.xml
index a8b1f23deb9..fe2dcb4ea6f 100644
--- a/hadoop-tools/hadoop-tools-dist/pom.xml
+++ b/hadoop-tools/hadoop-tools-dist/pom.xml
@@ -60,13 +60,6 @@
       <artifactId>hadoop-rumen</artifactId>
       <scope>compile</scope>
     </dependency>
-    <dependency>
-      <groupId>org.apache.hadoop</groupId>
-      <artifactId>hadoop-pipes</artifactId>
-      <scope>compile</scope>
-      <type>pom</type>
-      <version>${project.version}</version>
-    </dependency>
     <dependency>
       <groupId>org.apache.hadoop</groupId>
       <artifactId>hadoop-aws</artifactId>
diff --git a/hadoop-tools/pom.xml b/hadoop-tools/pom.xml
index 400a80cb803..0508c62b89b 100644
--- a/hadoop-tools/pom.xml
+++ b/hadoop-tools/pom.xml
@@ -35,7 +35,6 @@
     <module>hadoop-federation-balance</module>
     <module>hadoop-rumen</module>
     <module>hadoop-tools-dist</module>
-    <module>hadoop-pipes</module>
     <module>hadoop-sls</module>
     <module>hadoop-resourceestimator</module>
     <module>hadoop-azure</module>
