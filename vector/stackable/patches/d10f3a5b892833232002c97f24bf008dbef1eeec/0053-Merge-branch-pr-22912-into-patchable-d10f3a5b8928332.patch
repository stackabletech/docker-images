From 2cd946b24db47bf80e2c5b6f81b0a53f0318fbea Mon Sep 17 00:00:00 2001
From: dervoeti <lukas.krug@stackable.tech>
Date: Fri, 13 Feb 2026 18:09:33 +0100
Subject: Merge branch 'pr-22912' into
 patchable/d10f3a5b892833232002c97f24bf008dbef1eeec

---
 .github/actions/spelling/allow.txt            |   1 +
 Cargo.lock                                    |  23 +-
 Cargo.toml                                    |   4 +-
 .../24355_vector_top_controls.feature.md      |   3 +
 ...emote_write_healthcheck_uri.enhancement.md |   3 +
 .../7538_filesystem_inode_metrics.feature.md  |   3 +
 .../component_latency-metrics.enhancement.md  |   5 +
 .../src/vrl_functions/parse_dnstap.rs         |  62 ++-
 lib/enrichment/Cargo.toml                     |   1 +
 .../src/find_enrichment_table_records.rs      |  86 ++--
 .../src/get_enrichment_table_record.rs        |  91 +++--
 lib/enrichment/src/vrl_util.rs                |  26 +-
 lib/vector-core/src/config/global_options.rs  |  22 +-
 lib/vector-core/src/event/array.rs            |  58 ++-
 lib/vector-core/src/event/metadata.rs         |  88 ++--
 lib/vector-core/src/event/proto.rs            |  28 +-
 lib/vector-core/src/latency.rs                |  59 +++
 lib/vector-core/src/lib.rs                    |   1 +
 lib/vector-core/src/transform/outputs.rs      |  20 +
 lib/vector-lib/src/lib.rs                     |   4 +-
 lib/vector-top/Cargo.toml                     |   1 +
 lib/vector-top/src/dashboard.rs               | 309 +++++++++++++-
 lib/vector-top/src/events.rs                  |   6 +-
 lib/vector-top/src/input.rs                   | 229 +++++++++++
 lib/vector-top/src/lib.rs                     |   1 +
 lib/vector-top/src/state.rs                   | 367 ++++++++++++++++-
 lib/vector-vrl-metrics/Cargo.toml             |   1 +
 .../src/aggregate_vector_metrics.rs           |  58 ++-
 .../src/find_vector_metrics.rs                |  47 ++-
 .../src/get_vector_metric.rs                  |  47 ++-
 lib/vector-vrl/category/Cargo.toml            |  10 +
 lib/vector-vrl/category/src/lib.rs            |  16 +
 lib/vector-vrl/functions/Cargo.toml           |   1 +
 lib/vector-vrl/functions/src/get_secret.rs    |  10 +
 lib/vector-vrl/functions/src/remove_secret.rs |  10 +
 lib/vector-vrl/functions/src/set_secret.rs    |  11 +
 .../functions/src/set_semantic_meaning.rs     |  11 +
 src/config/compiler.rs                        |   2 +-
 src/config/validation.rs                      |  44 +-
 src/sinks/prometheus/remote_write/config.rs   |   7 +-
 src/sources/host_metrics/filesystem.rs        |  61 ++-
 src/test_util/mock/sinks/completion.rs        |  95 +++++
 src/test_util/mock/sinks/mod.rs               |   3 +
 src/test_util/mock/transforms/noop.rs         |  50 ++-
 src/top/cmd.rs                                |  18 +-
 src/top/mod.rs                                |  19 +
 src/topology/builder.rs                       | 381 ++++++++++--------
 src/topology/test/compliance.rs               |  31 +-
 src/topology/test/latency_metrics.rs          | 147 +++++++
 src/topology/test/mod.rs                      |  85 ++--
 src/transforms/dedupe/config.rs               | 118 ++----
 src/transforms/filter.rs                      |  18 +-
 src/transforms/log_to_metric.rs               |  70 ++--
 .../sinks/prometheus_remote_write.cue         |   5 +-
 .../components/sources/internal_metrics.cue   |  22 +
 .../cue/reference/components/transforms.cue   |   6 +-
 .../cue/reference/generated/configuration.cue |  22 +-
 website/package.json                          |  10 +-
 website/scripts/typesense-index.ts            |   6 +-
 website/scripts/typesense-sync.ts             |  32 +-
 website/typesense.config.json                 |  40 +-
 website/yarn.lock                             | 340 ++++++++++------
 62 files changed, 2526 insertions(+), 829 deletions(-)
 create mode 100644 changelog.d/24355_vector_top_controls.feature.md
 create mode 100644 changelog.d/24603_prometheus_remote_write_healthcheck_uri.enhancement.md
 create mode 100644 changelog.d/7538_filesystem_inode_metrics.feature.md
 create mode 100644 changelog.d/component_latency-metrics.enhancement.md
 create mode 100644 lib/vector-core/src/latency.rs
 create mode 100644 lib/vector-top/src/input.rs
 create mode 100644 lib/vector-vrl/category/Cargo.toml
 create mode 100644 lib/vector-vrl/category/src/lib.rs
 create mode 100644 src/test_util/mock/sinks/completion.rs
 create mode 100644 src/topology/test/latency_metrics.rs

diff --git a/.github/actions/spelling/allow.txt b/.github/actions/spelling/allow.txt
index 9b14c6dbd..7581a21c6 100644
--- a/.github/actions/spelling/allow.txt
+++ b/.github/actions/spelling/allow.txt
@@ -276,6 +276,7 @@ KDL
 keepappkey
 keephq
 kenton
+keybinds
 Kingcom
 Kolkata
 konqueror
diff --git a/Cargo.lock b/Cargo.lock
index 87f319373..0fb14d94d 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -3504,14 +3504,14 @@ dependencies = [
 
 [[package]]
 name = "dns-lookup"
-version = "2.0.4"
+version = "3.0.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e5766087c2235fec47fafa4cfecc81e494ee679d0fd4a59887ea0919bfb0e4fc"
+checksum = "6e39034cee21a2f5bbb66ba0e3689819c4bb5d00382a282006e802a7ffa6c41d"
 dependencies = [
  "cfg-if",
  "libc",
- "socket2 0.5.10",
- "windows-sys 0.48.0",
+ "socket2 0.6.0",
+ "windows-sys 0.60.2",
 ]
 
 [[package]]
@@ -3755,6 +3755,7 @@ dependencies = [
  "const-str",
  "dyn-clone",
  "indoc",
+ "vector-vrl-category",
  "vrl",
 ]
 
@@ -13025,6 +13026,7 @@ dependencies = [
  "indoc",
  "num-format",
  "ratatui",
+ "regex",
  "tokio",
  "tokio-stream",
  "unit-prefix",
@@ -13033,6 +13035,13 @@ dependencies = [
  "vector-common",
 ]
 
+[[package]]
+name = "vector-vrl-category"
+version = "0.1.0"
+dependencies = [
+ "strum 0.27.2",
+]
+
 [[package]]
 name = "vector-vrl-cli"
 version = "0.1.0"
@@ -13049,6 +13058,7 @@ dependencies = [
  "dnstap-parser",
  "enrichment",
  "indoc",
+ "vector-vrl-category",
  "vector-vrl-metrics",
  "vrl",
 ]
@@ -13063,6 +13073,7 @@ dependencies = [
  "tokio-stream",
  "vector-common",
  "vector-core",
+ "vector-vrl-category",
  "vrl",
 ]
 
@@ -13114,7 +13125,7 @@ checksum = "6a02e4885ed3bc0f2de90ea6dd45ebcbb66dacffe03547fadbb0eeae2770887d"
 [[package]]
 name = "vrl"
 version = "0.30.0"
-source = "git+https://github.com/vectordotdev/vrl.git?branch=main#1f2c963ed13c30a8897c3555f82e85c772cd5643"
+source = "git+https://github.com/vectordotdev/vrl.git?branch=main#7830c051501a08a328c53033364af55fd898aa86"
 dependencies = [
  "aes",
  "aes-siv",
@@ -13206,6 +13217,8 @@ dependencies = [
  "snafu 0.8.9",
  "snap",
  "strip-ansi-escapes",
+ "strum 0.26.3",
+ "strum_macros 0.26.4",
  "syslog_loose 0.22.0",
  "termcolor",
  "thiserror 2.0.17",
diff --git a/Cargo.toml b/Cargo.toml
index 2ea5a610a..2eb93c104 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -128,6 +128,7 @@ members = [
   "lib/vector-stream",
   "lib/vector-tap",
   "lib/vector-top",
+  "lib/vector-vrl/category",
   "lib/vector-vrl/cli",
   "lib/vector-vrl/functions",
   "lib/vector-vrl/tests",
@@ -207,6 +208,7 @@ vector-config-common = { path = "lib/vector-config-common" }
 vector-config-macros = { path = "lib/vector-config-macros" }
 vector-common-macros = { path = "lib/vector-common-macros" }
 vector-lib = { path = "lib/vector-lib", default-features = false, features = ["vrl"] }
+vector-vrl-category = { path = "lib/vector-vrl/category" }
 vector-vrl-functions = { path = "lib/vector-vrl/functions" }
 vrl = { git = "https://github.com/vectordotdev/vrl.git", branch = "main", features = ["arbitrary", "cli", "test", "test_framework"] }
 mock_instant = { version = "0.6" }
@@ -449,7 +451,7 @@ byteorder = "1.5.0"
 windows-service = "0.8.0"
 
 [target.'cfg(unix)'.dependencies]
-nix = { version = "0.26.2", default-features = false, features = ["socket", "signal"] }
+nix = { version = "0.26.2", default-features = false, features = ["socket", "signal", "fs"] }
 
 [target.'cfg(target_os = "linux")'.dependencies]
 netlink-packet-utils = "0.5.2"
diff --git a/changelog.d/24355_vector_top_controls.feature.md b/changelog.d/24355_vector_top_controls.feature.md
new file mode 100644
index 000000000..386fde3ab
--- /dev/null
+++ b/changelog.d/24355_vector_top_controls.feature.md
@@ -0,0 +1,3 @@
+Added new keybinds to `vector top` for scrolling, sorting and filtering. You can now press `?` when using `vector top` to see all available keybinds.
+
+authors: esensar Quad9DNS
diff --git a/changelog.d/24603_prometheus_remote_write_healthcheck_uri.enhancement.md b/changelog.d/24603_prometheus_remote_write_healthcheck_uri.enhancement.md
new file mode 100644
index 000000000..d49a0c461
--- /dev/null
+++ b/changelog.d/24603_prometheus_remote_write_healthcheck_uri.enhancement.md
@@ -0,0 +1,3 @@
+The `prometheus_remote_write` sink now supports the `healthcheck.uri` field to customize the healthcheck endpoint.
+
+authors: simonhammes
diff --git a/changelog.d/7538_filesystem_inode_metrics.feature.md b/changelog.d/7538_filesystem_inode_metrics.feature.md
new file mode 100644
index 000000000..c43ae8d68
--- /dev/null
+++ b/changelog.d/7538_filesystem_inode_metrics.feature.md
@@ -0,0 +1,3 @@
+Added inode metrics to the `host_metrics` source filesystem collector on unix systems. The `filesystem_inodes_total`, `filesystem_inodes_free`, `filesystem_inodes_used`, and `filesystem_inodes_used_ratio` metrics are now available.
+
+authors: mushrowan
diff --git a/changelog.d/component_latency-metrics.enhancement.md b/changelog.d/component_latency-metrics.enhancement.md
new file mode 100644
index 000000000..844a8867a
--- /dev/null
+++ b/changelog.d/component_latency-metrics.enhancement.md
@@ -0,0 +1,5 @@
+Added the `component_latency_seconds` histogram and
+`component_latency_mean_seconds` gauge internal metrics, exposing the time an
+event spends in a single transform including the transform buffer.
+
+authors: bruceg
diff --git a/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs b/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
index 24c8e3f7a..0d92e5835 100644
--- a/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
+++ b/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
@@ -1,3 +1,5 @@
+use std::sync::LazyLock;
+
 use base64::prelude::{BASE64_STANDARD, Engine as _};
 use dnsmsg_parser::dns_message_parser::DnsParserOptions;
 use vector_core::event::LogEvent;
@@ -5,6 +7,27 @@ use vrl::prelude::*;
 
 use crate::{parser::DnstapParser, schema::DnstapEventSchema};
 
+static DEFAULT_LOWERCASE_HOSTNAMES: LazyLock<Value> = LazyLock::new(|| Value::Boolean(false));
+
+static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
+    vec![
+        Parameter {
+            keyword: "value",
+            kind: kind::BYTES,
+            required: true,
+            description: "The base64 encoded representation of the DNSTAP data to parse.",
+            default: None,
+        },
+        Parameter {
+            keyword: "lowercase_hostnames",
+            kind: kind::BOOLEAN,
+            required: false,
+            description: "Whether to turn all hostnames found in resulting data lowercase, for consistency.",
+            default: Some(&DEFAULT_LOWERCASE_HOSTNAMES),
+        },
+    ]
+});
+
 #[derive(Clone, Copy, Debug)]
 pub struct ParseDnstap;
 
@@ -17,23 +40,25 @@ impl Function for ParseDnstap {
         "Parses the `value` as base64 encoded DNSTAP data."
     }
 
-    fn parameters(&self) -> &'static [Parameter] {
+    fn internal_failure_reasons(&self) -> &'static [&'static str] {
         &[
-            Parameter {
-                keyword: "value",
-                kind: kind::BYTES,
-                required: true,
-                description: "The base64 encoded representation of the DNSTAP data to parse.",
-            },
-            Parameter {
-                keyword: "lowercase_hostnames",
-                kind: kind::BOOLEAN,
-                required: false,
-                description: "Whether to turn all hostnames found in resulting data lowercase, for consistency.",
-            },
+            "`value` is not a valid base64 encoded string.",
+            "dnstap parsing failed for `value`",
         ]
     }
 
+    fn category(&self) -> &'static str {
+        Category::Parse.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::OBJECT
+    }
+
+    fn parameters(&self) -> &'static [Parameter] {
+        &PARAMETERS
+    }
+
     fn examples(&self) -> &'static [Example] {
         &[example!(
             title: "Parse dnstap query message",
@@ -151,9 +176,7 @@ impl Function for ParseDnstap {
         arguments: ArgumentList,
     ) -> Compiled {
         let value = arguments.required("value");
-        let lowercase_hostnames = arguments
-            .optional("lowercase_hostnames")
-            .unwrap_or_else(|| expr!(false));
+        let lowercase_hostnames = arguments.optional("lowercase_hostnames");
         Ok(ParseDnstapFn {
             value,
             lowercase_hostnames,
@@ -165,7 +188,7 @@ impl Function for ParseDnstap {
 #[derive(Debug, Clone)]
 struct ParseDnstapFn {
     value: Box<dyn Expression>,
-    lowercase_hostnames: Box<dyn Expression>,
+    lowercase_hostnames: Option<Box<dyn Expression>>,
 }
 
 impl FunctionExpression for ParseDnstapFn {
@@ -182,7 +205,10 @@ impl FunctionExpression for ParseDnstapFn {
                 .map_err(|_| format!("{input} is not a valid base64 encoded string"))?
                 .into(),
             DnsParserOptions {
-                lowercase_hostnames: self.lowercase_hostnames.resolve(ctx)?.try_boolean()?,
+                lowercase_hostnames: self
+                    .lowercase_hostnames
+                    .map_resolve_with_default(ctx, || DEFAULT_LOWERCASE_HOSTNAMES.clone())?
+                    .try_boolean()?,
             },
         )
         .map_err(|e| format!("dnstap parsing failed for {input}: {e}"))?;
diff --git a/lib/enrichment/Cargo.toml b/lib/enrichment/Cargo.toml
index 1d2a67a2c..151d329d1 100644
--- a/lib/enrichment/Cargo.toml
+++ b/lib/enrichment/Cargo.toml
@@ -12,3 +12,4 @@ const-str.workspace = true
 dyn-clone = { version = "1.0.20", default-features = false }
 indoc.workspace = true
 vrl.workspace = true
+vector-vrl-category.workspace = true
diff --git a/lib/enrichment/src/find_enrichment_table_records.rs b/lib/enrichment/src/find_enrichment_table_records.rs
index a604024a7..32a3cf949 100644
--- a/lib/enrichment/src/find_enrichment_table_records.rs
+++ b/lib/enrichment/src/find_enrichment_table_records.rs
@@ -1,12 +1,53 @@
-use std::collections::BTreeMap;
+use std::{collections::BTreeMap, sync::LazyLock};
 
+use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 use crate::{
     Case, Condition, IndexHandle, TableRegistry, TableSearch,
-    vrl_util::{self, add_index, evaluate_condition, is_case_sensitive},
+    vrl_util::{self, DEFAULT_CASE_SENSITIVE, add_index, evaluate_condition, is_case_sensitive},
 };
 
+static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
+    vec![
+        Parameter {
+            keyword: "table",
+            kind: kind::BYTES,
+            required: true,
+            description: "The [enrichment table](/docs/reference/glossary/#enrichment-tables) to search.",
+            default: None,
+        },
+        Parameter {
+            keyword: "condition",
+            kind: kind::OBJECT,
+            required: true,
+            description: "The condition to search on. Since the condition is used at boot time to create indices into the data, these conditions must be statically defined.",
+            default: None,
+        },
+        Parameter {
+            keyword: "select",
+            kind: kind::ARRAY,
+            required: false,
+            description: "A subset of fields from the enrichment table to return. If not specified, all fields are returned.",
+            default: None,
+        },
+        Parameter {
+            keyword: "case_sensitive",
+            kind: kind::BOOLEAN,
+            required: false,
+            description: "Whether text fields need to match cases exactly.",
+            default: Some(&DEFAULT_CASE_SENSITIVE),
+        },
+        Parameter {
+            keyword: "wildcard",
+            kind: kind::BYTES,
+            required: false,
+            description: "Value to use for wildcard matching in the search.",
+            default: None,
+        },
+    ]
+});
+
 fn find_enrichment_table_records(
     select: Option<Value>,
     enrichment_tables: &TableSearch,
@@ -58,39 +99,16 @@ impl Function for FindEnrichmentTableRecords {
         )
     }
 
+    fn category(&self) -> &'static str {
+        Category::Enrichment.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::ARRAY
+    }
+
     fn parameters(&self) -> &'static [Parameter] {
-        &[
-            Parameter {
-                keyword: "table",
-                kind: kind::BYTES,
-                required: true,
-                description: "The [enrichment table](/docs/reference/glossary/#enrichment-tables) to search.",
-            },
-            Parameter {
-                keyword: "condition",
-                kind: kind::OBJECT,
-                required: true,
-                description: "The condition to search on. Since the condition is used at boot time to create indices into the data, these conditions must be statically defined.",
-            },
-            Parameter {
-                keyword: "select",
-                kind: kind::ARRAY,
-                required: false,
-                description: "A subset of fields from the enrichment table to return. If not specified, all fields are returned.",
-            },
-            Parameter {
-                keyword: "case_sensitive",
-                kind: kind::BOOLEAN,
-                required: false,
-                description: "Whether text fields need to match cases exactly.",
-            },
-            Parameter {
-                keyword: "wildcard",
-                kind: kind::BYTES,
-                required: false,
-                description: "Value to use for wildcard matching in the search.",
-            },
-        ]
+        &PARAMETERS
     }
 
     fn examples(&self) -> &'static [Example] {
diff --git a/lib/enrichment/src/get_enrichment_table_record.rs b/lib/enrichment/src/get_enrichment_table_record.rs
index d347162e5..c8e92c3e7 100644
--- a/lib/enrichment/src/get_enrichment_table_record.rs
+++ b/lib/enrichment/src/get_enrichment_table_record.rs
@@ -1,12 +1,53 @@
-use std::collections::BTreeMap;
+use std::{collections::BTreeMap, sync::LazyLock};
 
+use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 use crate::{
     Case, Condition, IndexHandle, TableRegistry, TableSearch,
-    vrl_util::{self, add_index, evaluate_condition, is_case_sensitive},
+    vrl_util::{self, DEFAULT_CASE_SENSITIVE, add_index, evaluate_condition, is_case_sensitive},
 };
 
+static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
+    vec![
+        Parameter {
+            keyword: "table",
+            kind: kind::BYTES,
+            required: true,
+            description: "The [enrichment table](/docs/reference/glossary/#enrichment-tables) to search.",
+            default: None,
+        },
+        Parameter {
+            keyword: "condition",
+            kind: kind::OBJECT,
+            required: true,
+            description: "The condition to search on. Since the condition is used at boot time to create indices into the data, these conditions must be statically defined.",
+            default: None,
+        },
+        Parameter {
+            keyword: "select",
+            kind: kind::ARRAY,
+            required: false,
+            description: "A subset of fields from the enrichment table to return. If not specified, all fields are returned.",
+            default: None,
+        },
+        Parameter {
+            keyword: "case_sensitive",
+            kind: kind::BOOLEAN,
+            required: false,
+            description: "Whether the text fields match the case exactly.",
+            default: Some(&DEFAULT_CASE_SENSITIVE),
+        },
+        Parameter {
+            keyword: "wildcard",
+            kind: kind::BYTES,
+            required: false,
+            description: "Value to use for wildcard matching in the search.",
+            default: None,
+        },
+    ]
+});
+
 fn get_enrichment_table_record(
     select: Option<Value>,
     enrichment_tables: &TableSearch,
@@ -56,41 +97,25 @@ impl Function for GetEnrichmentTableRecord {
         USAGE
     }
 
-    fn parameters(&self) -> &'static [Parameter] {
+    fn internal_failure_reasons(&self) -> &'static [&'static str] {
         &[
-            Parameter {
-                keyword: "table",
-                kind: kind::BYTES,
-                required: true,
-                description: "The [enrichment table](/docs/reference/glossary/#enrichment-tables) to search.",
-            },
-            Parameter {
-                keyword: "condition",
-                kind: kind::OBJECT,
-                required: true,
-                description: "The condition to search on. Since the condition is used at boot time to create indices into the data, these conditions must be statically defined.",
-            },
-            Parameter {
-                keyword: "select",
-                kind: kind::ARRAY,
-                required: false,
-                description: "A subset of fields from the enrichment table to return. If not specified, all fields are returned.",
-            },
-            Parameter {
-                keyword: "case_sensitive",
-                kind: kind::BOOLEAN,
-                required: false,
-                description: "Whether the text fields match the case exactly.",
-            },
-            Parameter {
-                keyword: "wildcard",
-                kind: kind::BYTES,
-                required: false,
-                description: "Value to use for wildcard matching in the search.",
-            },
+            "The row is not found.",
+            "Multiple rows are found that match the condition.",
         ]
     }
 
+    fn category(&self) -> &'static str {
+        Category::Enrichment.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::OBJECT
+    }
+
+    fn parameters(&self) -> &'static [Parameter] {
+        &PARAMETERS
+    }
+
     fn examples(&self) -> &'static [Example] {
         &[example!(
             title: "find records",
diff --git a/lib/enrichment/src/vrl_util.rs b/lib/enrichment/src/vrl_util.rs
index 255cbeb3a..58909e5da 100644
--- a/lib/enrichment/src/vrl_util.rs
+++ b/lib/enrichment/src/vrl_util.rs
@@ -1,5 +1,5 @@
 //! Utilities shared between both VRL functions.
-use std::collections::BTreeMap;
+use std::{collections::BTreeMap, sync::LazyLock};
 
 use vrl::{
     diagnostic::{Label, Span},
@@ -103,26 +103,24 @@ pub(crate) fn add_index(
 
     Ok(index)
 }
+pub(crate) static DEFAULT_CASE_SENSITIVE: LazyLock<Value> = LazyLock::new(|| Value::Boolean(true));
 
 #[allow(clippy::result_large_err)]
 pub(crate) fn is_case_sensitive(
     arguments: &ArgumentList,
     state: &TypeState,
 ) -> Result<Case, function::Error> {
-    Ok(arguments
+    let case_sensitive = arguments
         .optional_literal("case_sensitive", state)?
-        .map(|value| {
-            let case_sensitive = value
-                .as_boolean()
-                .expect("case_sensitive should be boolean"); // This will have been caught by the type checker.
-
-            if case_sensitive {
-                Case::Sensitive
-            } else {
-                Case::Insensitive
-            }
-        })
-        .unwrap_or(Case::Sensitive))
+        .unwrap_or_else(|| DEFAULT_CASE_SENSITIVE.clone())
+        .as_boolean()
+        .expect("case_sensitive should be boolean"); // This will have been caught by the type checker.
+
+    Ok(if case_sensitive {
+        Case::Sensitive
+    } else {
+        Case::Insensitive
+    })
 }
 
 #[cfg(test)]
diff --git a/lib/vector-core/src/config/global_options.rs b/lib/vector-core/src/config/global_options.rs
index 93ad1d80f..864505519 100644
--- a/lib/vector-core/src/config/global_options.rs
+++ b/lib/vector-core/src/config/global_options.rs
@@ -143,16 +143,29 @@ pub struct GlobalOptions {
     /// The alpha value for the exponential weighted moving average (EWMA) of source and transform
     /// buffer utilization metrics.
     ///
-    /// This value specifies how much of the existing value is retained when each update is made.
-    /// Values closer to 1.0 result in the value adjusting slower to changes. The default value of
-    /// 0.9 is equivalent to a "half life" of 6-7 measurements.
+    /// This controls how quickly the `*_buffer_utilization_mean` gauges respond to new
+    /// observations. Values closer to 1.0 retain more of the previous value, leading to slower
+    /// adjustments. The default value of 0.9 is equivalent to a "half life" of 6-7 measurements.
     ///
-    /// Must be between 0 and 1 exclusive (0 < alpha < 1).
+    /// Must be between 0 and 1 exclusively (0 < alpha < 1).
     #[serde(default, skip_serializing_if = "crate::serde::is_default")]
     #[configurable(validation(range(min = 0.0, max = 1.0)))]
     #[configurable(metadata(docs::advanced))]
     pub buffer_utilization_ewma_alpha: Option<f64>,
 
+    /// The alpha value for the exponential weighted moving average (EWMA) of transform latency
+    /// metrics.
+    ///
+    /// This controls how quickly the `component_latency_mean_seconds` gauge responds to new
+    /// observations. Values closer to 1.0 retain more of the previous value, leading to slower
+    /// adjustments. The default value of 0.9 is equivalent to a "half life" of 6-7 measurements.
+    ///
+    /// Must be between 0 and 1 exclusively (0 < alpha < 1).
+    #[serde(default, skip_serializing_if = "crate::serde::is_default")]
+    #[configurable(validation(range(min = 0.0, max = 1.0)))]
+    #[configurable(metadata(docs::advanced))]
+    pub latency_ewma_alpha: Option<f64>,
+
     /// The interval, in seconds, at which the internal metrics cache for VRL is refreshed.
     /// This must be set to be able to access metrics in VRL functions.
     ///
@@ -311,6 +324,7 @@ impl GlobalOptions {
                 buffer_utilization_ewma_alpha: self
                     .buffer_utilization_ewma_alpha
                     .or(with.buffer_utilization_ewma_alpha),
+                latency_ewma_alpha: self.latency_ewma_alpha.or(with.latency_ewma_alpha),
                 metrics_storage_refresh_period: self
                     .metrics_storage_refresh_period
                     .or(with.metrics_storage_refresh_period),
diff --git a/lib/vector-core/src/event/array.rs b/lib/vector-core/src/event/array.rs
index 00c634991..2711b5fcf 100644
--- a/lib/vector-core/src/event/array.rs
+++ b/lib/vector-core/src/event/array.rs
@@ -2,7 +2,7 @@
 //! This module contains the definitions and wrapper types for handling
 //! arrays of type `Event`, in the various forms they may appear.
 
-use std::{iter, slice, sync::Arc, vec};
+use std::{iter, slice, vec};
 
 use futures::{Stream, stream};
 #[cfg(test)]
@@ -10,14 +10,13 @@ use quickcheck::{Arbitrary, Gen};
 use vector_buffers::EventCount;
 use vector_common::{
     byte_size_of::ByteSizeOf,
-    config::ComponentKey,
     finalization::{AddBatchNotifier, BatchNotifier, EventFinalizers, Finalizable},
     json_size::JsonSize,
 };
 
 use super::{
-    EstimatedJsonEncodedSizeOf, Event, EventDataEq, EventFinalizer, EventMutRef, EventRef,
-    LogEvent, Metric, TraceEvent,
+    EstimatedJsonEncodedSizeOf, Event, EventDataEq, EventFinalizer, EventMetadata, EventMutRef,
+    EventRef, LogEvent, Metric, TraceEvent,
 };
 
 /// The type alias for an array of `LogEvent` elements.
@@ -142,36 +141,6 @@ pub enum EventArray {
 }
 
 impl EventArray {
-    /// Sets the `OutputId` in the metadata for all the events in this array.
-    pub fn set_output_id(&mut self, output_id: &Arc<ComponentKey>) {
-        match self {
-            EventArray::Logs(logs) => {
-                for log in logs {
-                    log.metadata_mut().set_source_id(Arc::clone(output_id));
-                }
-            }
-            EventArray::Metrics(metrics) => {
-                for metric in metrics {
-                    metric.metadata_mut().set_source_id(Arc::clone(output_id));
-                }
-            }
-            EventArray::Traces(traces) => {
-                for trace in traces {
-                    trace.metadata_mut().set_source_id(Arc::clone(output_id));
-                }
-            }
-        }
-    }
-
-    /// Sets the `source_type` in the metadata for all metric events in this array.
-    pub fn set_source_type(&mut self, source_type: &'static str) {
-        if let EventArray::Metrics(metrics) = self {
-            for metric in metrics {
-                metric.metadata_mut().set_source_type(source_type);
-            }
-        }
-    }
-
     /// Iterate over references to this array's events.
     pub fn iter_events(&self) -> impl Iterator<Item = EventRef<'_>> {
         match self {
@@ -197,6 +166,27 @@ impl EventArray {
             _ => TypedArrayIterMut(None),
         }
     }
+
+    /// Applies a closure to each event's metadata in this array.
+    pub fn for_each_metadata_mut(&mut self, mut f: impl FnMut(&mut EventMetadata)) {
+        match self {
+            Self::Logs(logs) => {
+                for log in logs {
+                    f(log.metadata_mut());
+                }
+            }
+            Self::Metrics(metrics) => {
+                for metric in metrics {
+                    f(metric.metadata_mut());
+                }
+            }
+            Self::Traces(traces) => {
+                for trace in traces {
+                    f(trace.metadata_mut());
+                }
+            }
+        }
+    }
 }
 
 impl From<Event> for EventArray {
diff --git a/lib/vector-core/src/event/metadata.rs b/lib/vector-core/src/event/metadata.rs
index f860b03bb..09eaf60c2 100644
--- a/lib/vector-core/src/event/metadata.rs
+++ b/lib/vector-core/src/event/metadata.rs
@@ -1,6 +1,6 @@
 #![deny(missing_docs)]
 
-use std::{borrow::Cow, collections::BTreeMap, fmt, sync::Arc};
+use std::{borrow::Cow, collections::BTreeMap, fmt, sync::Arc, time::Instant};
 
 use derivative::Derivative;
 use lookup::OwnedTargetPath;
@@ -23,8 +23,17 @@ const SPLUNK_HEC_TOKEN: &str = "splunk_hec_token";
 
 /// The event metadata structure is a `Arc` wrapper around the actual metadata to avoid cloning the
 /// underlying data until it becomes necessary to provide a `mut` copy.
-#[derive(Clone, Debug, Deserialize, PartialEq, Serialize)]
-pub struct EventMetadata(pub(super) Arc<Inner>);
+#[derive(Clone, Debug, Derivative, Deserialize, Serialize)]
+#[derivative(PartialEq)]
+pub struct EventMetadata {
+    #[serde(flatten)]
+    pub(super) inner: Arc<Inner>,
+
+    /// The timestamp when the event last entered a transform buffer.
+    #[derivative(PartialEq = "ignore")]
+    #[serde(default, skip)]
+    pub(crate) last_transform_timestamp: Option<Instant>,
+}
 
 /// The actual metadata structure contained by both `struct Metric`
 /// and `struct LogEvent` types.
@@ -129,23 +138,26 @@ fn default_metadata_value() -> Value {
 impl EventMetadata {
     /// Creates `EventMetadata` with the given `Value`, and the rest of the fields with default values
     pub fn default_with_value(value: Value) -> Self {
-        Self(Arc::new(Inner {
-            value,
-            ..Default::default()
-        }))
+        Self {
+            inner: Arc::new(Inner {
+                value,
+                ..Default::default()
+            }),
+            last_transform_timestamp: None,
+        }
     }
 
     fn get_mut(&mut self) -> &mut Inner {
-        Arc::make_mut(&mut self.0)
+        Arc::make_mut(&mut self.inner)
     }
 
     pub(super) fn into_owned(self) -> Inner {
-        Arc::unwrap_or_clone(self.0)
+        Arc::unwrap_or_clone(self.inner)
     }
 
     /// Returns a reference to the metadata value
     pub fn value(&self) -> &Value {
-        &self.0.value
+        &self.inner.value
     }
 
     /// Returns a mutable reference to the metadata value
@@ -155,7 +167,7 @@ impl EventMetadata {
 
     /// Returns a reference to the secrets
     pub fn secrets(&self) -> &Secrets {
-        &self.0.secrets
+        &self.inner.secrets
     }
 
     /// Returns a mutable reference to the secrets
@@ -166,20 +178,20 @@ impl EventMetadata {
     /// Returns a reference to the metadata source id.
     #[must_use]
     pub fn source_id(&self) -> Option<&Arc<ComponentKey>> {
-        self.0.source_id.as_ref()
+        self.inner.source_id.as_ref()
     }
 
     /// Returns a reference to the metadata source type.
     #[must_use]
     pub fn source_type(&self) -> Option<&str> {
-        self.0.source_type.as_deref()
+        self.inner.source_type.as_deref()
     }
 
     /// Returns a reference to the metadata parent id. This is the `OutputId`
     /// of the previous component the event was sent through (if any).
     #[must_use]
     pub fn upstream_id(&self) -> Option<&OutputId> {
-        self.0.upstream_id.as_deref()
+        self.inner.upstream_id.as_deref()
     }
 
     /// Sets the `source_id` in the metadata to the provided value.
@@ -199,7 +211,7 @@ impl EventMetadata {
 
     /// Return the datadog API key, if it exists
     pub fn datadog_api_key(&self) -> Option<Arc<str>> {
-        self.0.secrets.get(DATADOG_API_KEY).cloned()
+        self.inner.secrets.get(DATADOG_API_KEY).cloned()
     }
 
     /// Set the datadog API key to passed value
@@ -209,7 +221,7 @@ impl EventMetadata {
 
     /// Return the splunk hec token, if it exists
     pub fn splunk_hec_token(&self) -> Option<Arc<str>> {
-        self.0.secrets.get(SPLUNK_HEC_TOKEN).cloned()
+        self.inner.secrets.get(SPLUNK_HEC_TOKEN).cloned()
     }
 
     /// Set the splunk hec token to passed value
@@ -227,17 +239,28 @@ impl EventMetadata {
 
     /// Fetches the dropped field by meaning.
     pub fn dropped_field(&self, meaning: impl AsRef<str>) -> Option<&Value> {
-        self.0.dropped_fields.get(meaning.as_ref())
+        self.inner.dropped_fields.get(meaning.as_ref())
     }
 
     /// Returns a reference to the `DatadogMetricOriginMetadata`.
     pub fn datadog_origin_metadata(&self) -> Option<&DatadogMetricOriginMetadata> {
-        self.0.datadog_origin_metadata.as_ref()
+        self.inner.datadog_origin_metadata.as_ref()
     }
 
     /// Returns a reference to the event id.
     pub fn source_event_id(&self) -> Option<Uuid> {
-        self.0.source_event_id
+        self.inner.source_event_id
+    }
+
+    /// Returns the timestamp of the last transform buffer enqueue operation, if it exists.
+    #[must_use]
+    pub fn last_transform_timestamp(&self) -> Option<Instant> {
+        self.last_transform_timestamp
+    }
+
+    /// Sets the transform enqueue timestamp to the provided value.
+    pub fn set_last_transform_timestamp(&mut self, timestamp: Instant) {
+        self.last_transform_timestamp = Some(timestamp);
     }
 }
 
@@ -260,7 +283,10 @@ impl Default for Inner {
 
 impl Default for EventMetadata {
     fn default() -> Self {
-        Self(Arc::new(Inner::default()))
+        Self {
+            inner: Arc::new(Inner::default()),
+            last_transform_timestamp: None,
+        }
     }
 }
 
@@ -276,7 +302,7 @@ impl ByteSizeOf for EventMetadata {
         // NOTE we don't count the `str` here because it's allocated somewhere
         // else. We're just moving around the pointer, which is already captured
         // by `ByteSizeOf::size_of`.
-        self.0.finalizers.allocated_bytes()
+        self.inner.finalizers.allocated_bytes()
     }
 }
 
@@ -342,6 +368,7 @@ impl EventMetadata {
     /// If a Datadog API key is not set in `self`, the one from `other` will be used.
     /// If a Splunk HEC token is not set in `self`, the one from `other` will be used.
     pub fn merge(&mut self, other: Self) {
+        let other_timestamp = other.last_transform_timestamp;
         let inner = self.get_mut();
         let other = other.into_owned();
         inner.finalizers.merge(other.finalizers);
@@ -351,11 +378,24 @@ impl EventMetadata {
         if inner.source_event_id.is_none() {
             inner.source_event_id = other.source_event_id;
         }
+
+        // Keep the earliest `last_transform_timestamp` for accurate latency measurement.
+        match (self.last_transform_timestamp, other_timestamp) {
+            (Some(self_ts), Some(other_ts)) => {
+                if other_ts < self_ts {
+                    self.last_transform_timestamp = Some(other_ts);
+                }
+            }
+            (None, Some(other_ts)) => {
+                self.last_transform_timestamp = Some(other_ts);
+            }
+            _ => {}
+        }
     }
 
     /// Update the finalizer(s) status.
     pub fn update_status(&self, status: EventStatus) {
-        self.0.finalizers.update_status(status);
+        self.inner.finalizers.update_status(status);
     }
 
     /// Update the finalizers' sources.
@@ -365,7 +405,7 @@ impl EventMetadata {
 
     /// Gets a reference to the event finalizers.
     pub fn finalizers(&self) -> &EventFinalizers {
-        &self.0.finalizers
+        &self.inner.finalizers
     }
 
     /// Add a new event finalizer to the existing list of event finalizers.
@@ -385,7 +425,7 @@ impl EventMetadata {
 
     /// Get the schema definition.
     pub fn schema_definition(&self) -> &Arc<schema::Definition> {
-        &self.0.schema_definition
+        &self.inner.schema_definition
     }
 
     /// Set the schema definition.
diff --git a/lib/vector-core/src/event/proto.rs b/lib/vector-core/src/event/proto.rs
index 6a4796906..21311267e 100644
--- a/lib/vector-core/src/event/proto.rs
+++ b/lib/vector-core/src/event/proto.rs
@@ -677,18 +677,22 @@ impl From<Metadata> for EventMetadata {
             }
         };
 
-        EventMetadata(Arc::new(Inner {
-            value: metadata_value.unwrap_or_else(|| vrl::value::Value::Object(ObjectMap::new())),
-            secrets: secrets.unwrap_or_default(),
-            finalizers: EventFinalizers::default(),
-            source_id,
-            source_type: source_type.map(Into::into),
-            upstream_id,
-            schema_definition: default_schema_definition(),
-            dropped_fields: ObjectMap::new(),
-            datadog_origin_metadata,
-            source_event_id,
-        }))
+        EventMetadata {
+            inner: Arc::new(Inner {
+                value: metadata_value
+                    .unwrap_or_else(|| vrl::value::Value::Object(ObjectMap::new())),
+                secrets: secrets.unwrap_or_default(),
+                finalizers: EventFinalizers::default(),
+                source_id,
+                source_type: source_type.map(Into::into),
+                upstream_id,
+                schema_definition: default_schema_definition(),
+                dropped_fields: ObjectMap::new(),
+                datadog_origin_metadata,
+                source_event_id,
+            }),
+            last_transform_timestamp: None,
+        }
     }
 }
 
diff --git a/lib/vector-core/src/latency.rs b/lib/vector-core/src/latency.rs
new file mode 100644
index 000000000..536a8ef53
--- /dev/null
+++ b/lib/vector-core/src/latency.rs
@@ -0,0 +1,59 @@
+use std::time::Instant;
+
+use metrics::{Histogram, gauge, histogram};
+use vector_common::stats::EwmaGauge;
+
+use crate::event::EventArray;
+
+const COMPONENT_LATENCY: &str = "component_latency_seconds";
+const COMPONENT_LATENCY_MEAN: &str = "component_latency_mean_seconds";
+const DEFAULT_LATENCY_EWMA_ALPHA: f64 = 0.9;
+
+#[derive(Debug)]
+pub struct LatencyRecorder {
+    histogram: Histogram,
+    gauge: EwmaGauge,
+}
+
+impl LatencyRecorder {
+    pub fn new(ewma_alpha: Option<f64>) -> Self {
+        Self {
+            histogram: histogram!(COMPONENT_LATENCY),
+            gauge: EwmaGauge::new(
+                gauge!(COMPONENT_LATENCY_MEAN),
+                ewma_alpha.or(Some(DEFAULT_LATENCY_EWMA_ALPHA)),
+            ),
+        }
+    }
+
+    pub fn on_send(&self, events: &mut EventArray, now: Instant) {
+        let mut sum = 0.0;
+        let mut count = 0usize;
+
+        // Since all of the events in the array will most likely have entered and exited the
+        // component at close to the same time, we average all the latencies over the entire array
+        // and record it just once in the EWMA-backed gauge. If we were to record each latency
+        // individually, the gauge would effectively just reflect the latest array's latency,
+        // eliminating the utility of the EWMA averaging. However, we record the individual
+        // latencies in the histogram to get a more granular view of the latency distribution.
+        for mut event in events.iter_events_mut() {
+            let metadata = event.metadata_mut();
+            if let Some(previous) = metadata.last_transform_timestamp() {
+                let latency = now.saturating_duration_since(previous).as_secs_f64();
+                sum += latency;
+                count += 1;
+                self.histogram.record(latency);
+            }
+
+            metadata.set_last_transform_timestamp(now);
+        }
+        if count > 0 {
+            #[expect(
+                clippy::cast_precision_loss,
+                reason = "losing precision is acceptable here"
+            )]
+            let mean = sum / count as f64;
+            self.gauge.record(mean);
+        }
+    }
+}
diff --git a/lib/vector-core/src/lib.rs b/lib/vector-core/src/lib.rs
index e12b4dc04..ad1b36d1d 100644
--- a/lib/vector-core/src/lib.rs
+++ b/lib/vector-core/src/lib.rs
@@ -31,6 +31,7 @@ pub mod config;
 pub mod event;
 pub mod fanout;
 pub mod ipallowlist;
+pub mod latency;
 pub mod metrics;
 pub mod partition;
 pub mod schema;
diff --git a/lib/vector-core/src/transform/outputs.rs b/lib/vector-core/src/transform/outputs.rs
index e9a4dcf2f..7918fb008 100644
--- a/lib/vector-core/src/transform/outputs.rs
+++ b/lib/vector-core/src/transform/outputs.rs
@@ -220,6 +220,19 @@ impl TransformOutputsBuf {
     pub fn take_all_named(&mut self) -> HashMap<String, OutputBuffer> {
         std::mem::take(&mut self.named_buffers)
     }
+
+    /// Applies `f` to each [`EventArray`] currently buffered in this outputs buffer.
+    ///
+    /// This is useful for cross-cutting instrumentation (e.g. latency timestamp propagation)
+    /// that needs mutable access to the buffered arrays before they are sent.
+    pub fn for_each_array_mut(&mut self, mut f: impl FnMut(&mut EventArray)) {
+        if let Some(primary) = self.primary_buffer.as_mut() {
+            primary.for_each_array_mut(&mut f);
+        }
+        for buf in self.named_buffers.values_mut() {
+            buf.for_each_array_mut(&mut f);
+        }
+    }
 }
 
 impl ByteSizeOf for TransformOutputsBuf {
@@ -295,6 +308,13 @@ impl OutputBuffer {
         self.0.drain(..).flat_map(EventArray::into_events)
     }
 
+    /// Applies `f` to each [`EventArray`] currently held by this buffer.
+    pub fn for_each_array_mut(&mut self, mut f: impl FnMut(&mut EventArray)) {
+        for array in &mut self.0 {
+            f(array);
+        }
+    }
+
     async fn send(
         &mut self,
         output: &mut Fanout,
diff --git a/lib/vector-lib/src/lib.rs b/lib/vector-lib/src/lib.rs
index 3400a03c2..82bb67cf9 100644
--- a/lib/vector-lib/src/lib.rs
+++ b/lib/vector-lib/src/lib.rs
@@ -21,8 +21,8 @@ pub use vector_config::impl_generate_config_from_default;
 pub use vector_core::compile_vrl;
 pub use vector_core::{
     EstimatedJsonEncodedSizeOf, buckets, default_data_dir, emit, event, fanout, ipallowlist,
-    metric_tags, metrics, partition, quantiles, register, samples, schema, serde, sink, source,
-    source_sender, tcp, tls, transform,
+    latency, metric_tags, metrics, partition, quantiles, register, samples, schema, serde, sink,
+    source, source_sender, tcp, tls, transform,
 };
 pub use vector_lookup as lookup;
 pub use vector_stream as stream;
diff --git a/lib/vector-top/Cargo.toml b/lib/vector-top/Cargo.toml
index 8e494cc58..c0da764b4 100644
--- a/lib/vector-top/Cargo.toml
+++ b/lib/vector-top/Cargo.toml
@@ -21,6 +21,7 @@ crossterm = { version = "0.29.0", default-features = false, features = ["event-s
 unit-prefix = { version = "0.5.2", default-features = false, features = ["std"] }
 num-format = { version = "0.4.4", default-features = false, features = ["with-num-bigint"] }
 ratatui = { version = "0.30.0", default-features = false, features = ["crossterm", "layout-cache"] }
+regex.workspace = true
 vector-common = { path = "../vector-common" }
 vector-api-client = { path = "../vector-api-client" }
 
diff --git a/lib/vector-top/src/dashboard.rs b/lib/vector-top/src/dashboard.rs
index 418ee0831..c4078ebde 100644
--- a/lib/vector-top/src/dashboard.rs
+++ b/lib/vector-top/src/dashboard.rs
@@ -3,7 +3,7 @@ use std::{io::stdout, time::Duration};
 use crossterm::{
     ExecutableCommand,
     cursor::Show,
-    event::{DisableMouseCapture, EnableMouseCapture, KeyCode},
+    event::{DisableMouseCapture, EnableMouseCapture},
     execute,
     terminal::{EnterAlternateScreen, LeaveAlternateScreen, disable_raw_mode, enable_raw_mode},
     tty::IsTty,
@@ -12,14 +12,22 @@ use num_format::{Locale, ToFormattedString};
 use ratatui::{
     Frame, Terminal,
     backend::CrosstermBackend,
-    layout::{Alignment, Constraint, Layout, Rect},
-    style::{Color, Modifier, Style},
+    layout::{Alignment, Constraint, Flex, Layout, Position, Rect},
+    style::{Color, Modifier, Style, Stylize},
     text::{Line, Span},
-    widgets::{Block, Borders, Cell, Paragraph, Row, Table, Wrap},
+    widgets::{
+        Block, Borders, Cell, Clear, List, ListItem, ListState, Padding, Paragraph, Row, Scrollbar,
+        ScrollbarOrientation, ScrollbarState, Table, TableState, Wrap,
+    },
 };
 use tokio::sync::oneshot;
 use unit_prefix::NumberPrefix;
 
+use crate::{
+    input::{InputMode, handle_input},
+    state::{ComponentRow, FilterColumn, FilterMenuState, SortColumn},
+};
+
 use super::{
     events::capture_key_press,
     state::{self, ConnectionStatus},
@@ -29,6 +37,12 @@ pub const fn is_allocation_tracing_enabled() -> bool {
     cfg!(feature = "allocation-tracing")
 }
 
+macro_rules! row_comparator {
+    ($field:ident) => {
+        |l: &ComponentRow, r: &ComponentRow| l.$field.cmp(&r.$field)
+    };
+}
+
 /// Format metrics, with thousands separation
 trait ThousandsFormatter {
     fn thousands_format(&self) -> String;
@@ -129,18 +143,36 @@ const NUM_COLUMNS: usize = if is_allocation_tracing_enabled() {
     9
 };
 
+pub mod columns {
+    pub const ID: &str = "ID";
+    pub const OUTPUT: &str = "Output";
+    pub const KIND: &str = "Kind";
+    pub const TYPE: &str = "Type";
+    pub const EVENTS_IN: &str = "Events In";
+    pub const EVENTS_IN_TOTAL: &str = "Events In Total";
+    pub const BYTES_IN: &str = "Bytes In";
+    pub const BYTES_IN_TOTAL: &str = "Bytes In Total";
+    pub const EVENTS_OUT: &str = "Events Out";
+    pub const EVENTS_OUT_TOTAL: &str = "Events Out Total";
+    pub const BYTES_OUT: &str = "Bytes Out";
+    pub const BYTES_OUT_TOTAL: &str = "Bytes Out Total";
+    pub const ERRORS: &str = "Errors";
+    #[cfg(feature = "allocation-tracing")]
+    pub const MEMORY_USED: &str = "Memory Used";
+}
+
 static HEADER: [&str; NUM_COLUMNS] = [
-    "ID",
-    "Output",
-    "Kind",
-    "Type",
-    "Events In",
-    "Bytes In",
-    "Events Out",
-    "Bytes Out",
-    "Errors",
+    columns::ID,
+    columns::OUTPUT,
+    columns::KIND,
+    columns::TYPE,
+    columns::EVENTS_IN,
+    columns::BYTES_IN,
+    columns::EVENTS_OUT,
+    columns::BYTES_OUT,
+    columns::ERRORS,
     #[cfg(feature = "allocation-tracing")]
-    "Memory Used",
+    columns::MEMORY_USED,
 ];
 
 struct Widgets<'a> {
@@ -210,12 +242,79 @@ impl<'a> Widgets<'a> {
         // Header columns
         let header = HEADER
             .iter()
-            .map(|s| Cell::from(*s).style(Style::default().add_modifier(Modifier::BOLD)))
+            .map(|s| {
+                let mut content_line = Line::from(*s);
+                let c = Cell::default().style(Style::default().add_modifier(Modifier::BOLD));
+                if state.filter_state.column.matches_header(s)
+                    && let Some(pattern) = state.filter_state.pattern.as_ref().map(|p| p.as_str())
+                {
+                    content_line.push_span(" ");
+                    let filter_span =
+                        Span::styled(format!("/{pattern}/"), Style::default().fg(Color::Yellow));
+                    content_line.push_span(filter_span);
+                };
+                if state
+                    .sort_state
+                    .column
+                    .map(|c| c.matches_header(s))
+                    .unwrap_or_default()
+                {
+                    content_line.push_span(if state.sort_state.reverse {
+                        " ▼"
+                    } else {
+                        " ▲"
+                    });
+                };
+                c.content(content_line)
+            })
             .collect::<Vec<_>>();
 
         // Data columns
         let mut items = Vec::new();
-        for (_, r) in state.components.iter() {
+        let mut sorted = state.components.iter().collect::<Vec<_>>();
+        if let Some(column) = state.sort_state.column {
+            let sort_fn = match column {
+                SortColumn::Id => row_comparator!(key),
+                SortColumn::Kind => row_comparator!(kind),
+                SortColumn::Type => row_comparator!(component_type),
+                SortColumn::EventsIn => row_comparator!(received_events_throughput_sec),
+                SortColumn::EventsInTotal => row_comparator!(received_events_total),
+                SortColumn::BytesIn => row_comparator!(received_bytes_throughput_sec),
+                SortColumn::BytesInTotal => row_comparator!(received_bytes_total),
+                SortColumn::EventsOut => row_comparator!(sent_events_throughput_sec),
+                SortColumn::EventsOutTotal => row_comparator!(sent_events_total),
+                SortColumn::BytesOut => row_comparator!(sent_bytes_throughput_sec),
+                SortColumn::BytesOutTotal => row_comparator!(sent_bytes_total),
+                SortColumn::Errors => row_comparator!(errors),
+                #[cfg(feature = "allocation-tracing")]
+                SortColumn::MemoryUsed => row_comparator!(allocated_bytes),
+            };
+            if state.sort_state.reverse {
+                sorted.sort_by(|a, b| sort_fn(a.1, b.1).reverse())
+            } else {
+                sorted.sort_by(|a, b| sort_fn(a.1, b.1));
+            }
+        }
+
+        for (_, r) in sorted.into_iter().filter(|(_, r)| {
+            let column = state.filter_state.column;
+            if let Some(regex) = &state.filter_state.pattern {
+                match column {
+                    FilterColumn::Id => {
+                        regex.is_match(r.key.id()) || r.key.id().contains(regex.as_str())
+                    }
+                    FilterColumn::Kind => {
+                        regex.is_match(&r.kind) || r.kind.contains(regex.as_str())
+                    }
+                    FilterColumn::Type => {
+                        regex.is_match(&r.component_type)
+                            || r.component_type.contains(regex.as_str())
+                    }
+                }
+            } else {
+                true
+            }
+        }) {
             let mut data = vec![
                 r.key.id().to_string(),
                 if !r.has_displayable_outputs() {
@@ -310,7 +409,34 @@ impl<'a> Widgets<'a> {
             .header(Row::new(header).bottom_margin(1))
             .block(Block::default().borders(Borders::ALL).title("Components"))
             .column_spacing(2);
-        f.render_widget(w, area);
+        f.render_stateful_widget(
+            w,
+            area,
+            // We don't need selection, so just create a table state for the scroll
+            &mut TableState::new().with_offset(state.ui.scroll),
+        );
+        // Skip the border + header row + 1 row of padding as well as the bottom border
+        let scrollbar_area = Rect::new(area.x, area.y + 3, area.width, area.height - 4);
+        f.render_stateful_widget(
+            Scrollbar::new(ScrollbarOrientation::VerticalRight)
+                .begin_symbol(Some("↑"))
+                .end_symbol(Some("↓")),
+            scrollbar_area,
+            &mut ScrollbarState::new(
+                // Maximum allowed scroll value
+                // We calculate it like this, because scrollbar usually accounts for full
+                // overscroll, but we want scrolling to stop when last available item is visible and
+                // at the bottom of the table.
+                state
+                    .components
+                    .len()
+                    .saturating_sub(scrollbar_area.height.into())
+                    // 1 is also added, because ScrollBar removes 1, to ensure last item is visible
+                    // when overscrolling - we avoid overscroll, so this is useless to us.
+                    .saturating_add(1),
+            )
+            .position(state.ui.scroll),
+        );
     }
 
     /// Alerts the user to resize the window to view columns
@@ -323,9 +449,109 @@ impl<'a> Widgets<'a> {
         f.render_widget(w, area);
     }
 
+    /// Renders a box showing instructions on how to use `vector top`.
+    fn help_box(&self, f: &mut Frame, area: Rect) {
+        let text = vec![
+            Line::from("General").bold(),
+            Line::from("ESC, q => quit (or close window)"),
+            Line::from("↓, j => scroll down by 1 row"),
+            Line::from("↑, k => scroll up by 1 row"),
+            Line::from("→, PageDown, CTRL+f => scroll down by 1 page"),
+            Line::from("←, PageUp, CTRL+b => scroll up by 1 page"),
+            Line::from("End, G => scroll to bottom"),
+            Line::from("Home, g => scroll to top"),
+            Line::from("F1, ? => toggle this help window"),
+            Line::from("1-9 => sort by column"),
+            Line::from("F6, s => toggle sort menu"),
+            Line::from("F7, r => toggle ascending/descending sort"),
+            Line::from("F4, f, / => toggle filter menu"),
+            Line::default(),
+            Line::from("Sort menu").bold(),
+            Line::from("↑, Shift+Tab, k => move sort column selection up"),
+            Line::from("↓, Tab, j => move sort column selection down"),
+            Line::from("Enter => confirm sort selection"),
+            Line::from("F6, s => toggle sort menu"),
+            Line::default(),
+            Line::from("Filter menu").bold(),
+            Line::from("↑, Shift+Tab => move filter column selection up"),
+            Line::from("↓, Tab => move filter column selection down"),
+            Line::from("Enter => confirm filter selection"),
+            Line::from("F4 => toggle sort menu"),
+        ];
+
+        let block = Block::default()
+            .borders(Borders::ALL)
+            .border_style(Style::default())
+            .padding(Padding::proportional(2))
+            .title("Help");
+        let w = Paragraph::new(text)
+            .block(block)
+            .style(Style::default().fg(Color::Gray))
+            .alignment(Alignment::Left);
+
+        f.render_widget(Clear, area);
+        f.render_widget(w, area);
+    }
+
+    /// Renders a box with sorting options.
+    fn sort_box(&self, f: &mut Frame, area: Rect, mut list_state: ListState) {
+        f.render_widget(Clear, area);
+        let w = List::new(
+            SortColumn::items()
+                .into_iter()
+                .map(|h| ListItem::new(Line::from(h))),
+        )
+        .block(
+            Block::default()
+                .padding(Padding::proportional(2))
+                .borders(Borders::ALL)
+                .title("Sort by"),
+        )
+        .highlight_style(Style::new().reversed());
+        f.render_stateful_widget(w, area, &mut list_state);
+    }
+
+    /// Renders a box with filtering options.
+    fn filter_box(&self, f: &mut Frame, area: Rect, filter_menu_state: &FilterMenuState) {
+        f.render_widget(Clear, area);
+        let w = List::new(
+            FilterColumn::items()
+                .into_iter()
+                .map(|h| ListItem::new(Line::from(h))),
+        )
+        .block(Block::default().borders(Borders::ALL).title("Filter by"))
+        .highlight_style(Style::new().reversed());
+        let (top, bottom) = {
+            (
+                Rect::new(area.x, area.y, area.width, area.height / 2),
+                Rect::new(
+                    area.x,
+                    area.y + area.height / 2,
+                    area.width,
+                    area.height / 2,
+                ),
+            )
+        };
+        f.render_stateful_widget(w, top, &mut filter_menu_state.column_selection.clone());
+        f.render_widget(
+            Paragraph::new(filter_menu_state.input.clone()).block(
+                Block::default()
+                    .borders(Borders::ALL)
+                    .title("Filter pattern"),
+            ),
+            bottom,
+        );
+        f.set_cursor_position(Position::new(
+            bottom.x + 1 + filter_menu_state.input.len() as u16,
+            bottom.y + 1,
+        ));
+    }
+
     /// Renders a box showing instructions on how to exit from `vector top`.
     fn quit_box(&self, f: &mut Frame, area: Rect) {
-        let text = vec![Line::from("To quit, press ESC or 'q'")];
+        let text = vec![Line::from(
+            "To quit, press ESC or 'q'; Press F1 or '?' for help",
+        )];
 
         let block = Block::default()
             .borders(Borders::ALL)
@@ -355,6 +581,37 @@ impl<'a> Widgets<'a> {
         }
 
         self.quit_box(f, rects[2]);
+
+        // Render help, sort and filter over other items
+        if state.ui.help_visible {
+            let [area] = Layout::horizontal([Constraint::Length(64)])
+                .flex(Flex::Center)
+                .areas(size);
+            let [area] = Layout::vertical([Constraint::Length(32)])
+                .flex(Flex::Center)
+                .areas(area);
+            self.help_box(f, area);
+        }
+
+        if state.ui.sort_visible {
+            let [area] = Layout::horizontal([Constraint::Length(64)])
+                .flex(Flex::Center)
+                .areas(size);
+            let [area] = Layout::vertical([Constraint::Length(32)])
+                .flex(Flex::Center)
+                .areas(area);
+            self.sort_box(f, area, state.ui.sort_menu_state);
+        }
+
+        if state.ui.filter_visible {
+            let [area] = Layout::horizontal([Constraint::Length(64)])
+                .flex(Flex::Center)
+                .areas(size);
+            let [area] = Layout::vertical([Constraint::Length(12)])
+                .flex(Flex::Center)
+                .areas(area);
+            self.filter_box(f, area, &state.ui.filter_menu_state);
+        }
     }
 }
 
@@ -372,6 +629,7 @@ pub async fn init_dashboard<'a>(
     url: &'a str,
     interval: u32,
     human_metrics: bool,
+    event_tx: state::EventTx,
     mut state_rx: state::StateRx,
     mut shutdown_rx: oneshot::Receiver<()>,
 ) -> Result<(), Box<dyn std::error::Error>> {
@@ -395,16 +653,27 @@ pub async fn init_dashboard<'a>(
     terminal.clear()?;
 
     let widgets = Widgets::new(title, url, interval, human_metrics);
+    let mut input_mode = InputMode::Top;
 
     loop {
         tokio::select! {
             Some(state) = state_rx.recv() => {
+                if state.ui.filter_visible {
+                    input_mode = InputMode::FilterInput;
+                } else if state.ui.sort_visible {
+                    input_mode = InputMode::SortMenu;
+                } else if state.ui.help_visible {
+                    input_mode = InputMode::HelpMenu;
+                } else {
+                    input_mode = InputMode::Top;
+                }
                 terminal.draw(|f| widgets.draw(f, state))?;
             },
             k = key_press_rx.recv() => {
-                if let KeyCode::Esc | KeyCode::Char('q') = k.unwrap() {
+                let k = k.unwrap();
+                if handle_input(input_mode, k, &event_tx, &terminal).await {
                     _ = key_press_kill_tx.send(());
-                    break
+                    break;
                 }
             }
             _ = &mut shutdown_rx => {
diff --git a/lib/vector-top/src/events.rs b/lib/vector-top/src/events.rs
index 5696e696c..6864966f6 100644
--- a/lib/vector-top/src/events.rs
+++ b/lib/vector-top/src/events.rs
@@ -1,10 +1,10 @@
-use crossterm::event::{Event, EventStream, KeyCode};
+use crossterm::event::{Event, EventStream, KeyEvent};
 use futures::StreamExt;
 use tokio::sync::{mpsc, oneshot};
 
 /// Capture keyboard input, and send it upstream via a channel. This is used for interaction
 /// with the dashboard, and exiting from `vector top`.
-pub fn capture_key_press() -> (mpsc::UnboundedReceiver<KeyCode>, oneshot::Sender<()>) {
+pub fn capture_key_press() -> (mpsc::UnboundedReceiver<KeyEvent>, oneshot::Sender<()>) {
     let (tx, rx) = mpsc::unbounded_channel();
     let (kill_tx, mut kill_rx) = oneshot::channel();
 
@@ -17,7 +17,7 @@ pub fn capture_key_press() -> (mpsc::UnboundedReceiver<KeyCode>, oneshot::Sender
                 _ = &mut kill_rx => return,
                 Some(Ok(event)) = events.next() => {
                      if let Event::Key(k) = event {
-                        _ = tx.clone().send(k.code);
+                        _ = tx.clone().send(k);
                     };
                 }
             }
diff --git a/lib/vector-top/src/input.rs b/lib/vector-top/src/input.rs
new file mode 100644
index 000000000..874a0f5be
--- /dev/null
+++ b/lib/vector-top/src/input.rs
@@ -0,0 +1,229 @@
+use crossterm::event::{KeyCode, KeyEvent, KeyModifiers};
+use ratatui::{Terminal, prelude::Backend};
+
+use crate::state::{self, EventType, SortColumn, UiEventType};
+
+#[derive(Debug, Clone, Copy)]
+pub(crate) enum InputMode {
+    Top,
+    HelpMenu,
+    FilterInput,
+    SortMenu,
+}
+
+/// Handles keyboard input for top
+///
+/// Returns true if input handling is done (quit is requested)
+pub(crate) async fn handle_input<B: Backend>(
+    mode: InputMode,
+    key_event: KeyEvent,
+    event_tx: &state::EventTx,
+    terminal: &Terminal<B>,
+) -> bool {
+    match mode {
+        InputMode::Top => handle_top_input(key_event, event_tx, terminal).await,
+        InputMode::HelpMenu => handle_help_input(key_event, event_tx, terminal).await,
+        InputMode::FilterInput => handle_filter_input(key_event, event_tx, terminal).await,
+        InputMode::SortMenu => handle_sort_input(key_event, event_tx, terminal).await,
+    }
+}
+
+async fn handle_top_input<B: Backend>(
+    key_event: KeyEvent,
+    event_tx: &state::EventTx,
+    terminal: &Terminal<B>,
+) -> bool {
+    match key_event.code {
+        KeyCode::Esc | KeyCode::Char('q') => {
+            return true;
+        }
+        KeyCode::Up | KeyCode::Char('k') => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::Scroll(
+                    -1,
+                    terminal.size().unwrap_or_default(),
+                )))
+                .await;
+        }
+        KeyCode::Down | KeyCode::Char('j') => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::Scroll(
+                    1,
+                    terminal.size().unwrap_or_default(),
+                )))
+                .await;
+        }
+        KeyCode::End | KeyCode::Char('G') => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::Scroll(
+                    isize::MAX,
+                    terminal.size().unwrap_or_default(),
+                )))
+                .await;
+        }
+        KeyCode::Home | KeyCode::Char('g') => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::Scroll(
+                    isize::MIN,
+                    terminal.size().unwrap_or_default(),
+                )))
+                .await;
+        }
+        KeyCode::Left | KeyCode::PageUp => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ScrollPage(
+                    -1,
+                    terminal.size().unwrap_or_default(),
+                )))
+                .await;
+        }
+        KeyCode::Char('b') if key_event.modifiers.intersects(KeyModifiers::CONTROL) => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ScrollPage(
+                    -1,
+                    terminal.size().unwrap_or_default(),
+                )))
+                .await;
+        }
+        KeyCode::Right | KeyCode::PageDown => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ScrollPage(
+                    1,
+                    terminal.size().unwrap_or_default(),
+                )))
+                .await;
+        }
+        KeyCode::Char('f') if key_event.modifiers.intersects(KeyModifiers::CONTROL) => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ScrollPage(
+                    1,
+                    terminal.size().unwrap_or_default(),
+                )))
+                .await;
+        }
+        KeyCode::Char('?') | KeyCode::F(1) => {
+            let _ = event_tx.send(EventType::Ui(UiEventType::ToggleHelp)).await;
+        }
+        KeyCode::Char('s') | KeyCode::F(6) => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ToggleSortMenu))
+                .await;
+        }
+        KeyCode::Char('r') | KeyCode::F(7) => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ToggleSortDirection))
+                .await;
+        }
+        KeyCode::Char(d) if d.is_ascii_digit() => {
+            let col = match d {
+                '1' => SortColumn::Id,
+                '3' => SortColumn::Kind,
+                '4' => SortColumn::Type,
+                '5' => SortColumn::EventsInTotal,
+                '6' => SortColumn::BytesInTotal,
+                '7' => SortColumn::EventsOutTotal,
+                '8' => SortColumn::BytesOutTotal,
+                '9' => SortColumn::Errors,
+                #[cfg(feature = "allocation-tracing")]
+                '0' => SortColumn::MemoryUsed,
+                _ => return false,
+            };
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::SortByColumn(col)))
+                .await;
+        }
+        KeyCode::F(4) | KeyCode::Char('f') | KeyCode::Char('/') => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ToggleFilterMenu))
+                .await;
+        }
+        _ => (),
+    }
+    false
+}
+
+async fn handle_help_input<B: Backend>(
+    key_event: KeyEvent,
+    event_tx: &state::EventTx,
+    terminal: &Terminal<B>,
+) -> bool {
+    match key_event.code {
+        KeyCode::Esc => {
+            let _ = event_tx.send(EventType::Ui(UiEventType::ToggleHelp)).await;
+        }
+        _ => return handle_top_input(key_event, event_tx, terminal).await,
+    }
+    false
+}
+
+async fn handle_sort_input<B: Backend>(
+    key_event: KeyEvent,
+    event_tx: &state::EventTx,
+    terminal: &Terminal<B>,
+) -> bool {
+    match key_event.code {
+        KeyCode::Esc => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ToggleSortMenu))
+                .await;
+        }
+        KeyCode::Up | KeyCode::BackTab | KeyCode::Char('k') => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::SortSelection(-1)))
+                .await;
+        }
+        KeyCode::Down | KeyCode::Tab | KeyCode::Char('j') => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::SortSelection(1)))
+                .await;
+        }
+        KeyCode::Enter => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::SortConfirmation))
+                .await;
+        }
+        _ => return handle_top_input(key_event, event_tx, terminal).await,
+    }
+    false
+}
+
+async fn handle_filter_input<B: Backend>(
+    key_event: KeyEvent,
+    event_tx: &state::EventTx,
+    terminal: &Terminal<B>,
+) -> bool {
+    match key_event.code {
+        KeyCode::Esc => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::ToggleFilterMenu))
+                .await;
+        }
+        KeyCode::BackTab | KeyCode::Up => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::FilterColumnSelection(-1)))
+                .await;
+        }
+        KeyCode::Tab | KeyCode::Down => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::FilterColumnSelection(1)))
+                .await;
+        }
+        KeyCode::Backspace => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::FilterBackspace))
+                .await;
+        }
+        KeyCode::Enter => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::FilterConfirmation))
+                .await;
+        }
+        KeyCode::Char(any) => {
+            let _ = event_tx
+                .send(EventType::Ui(UiEventType::FilterInput(any)))
+                .await;
+        }
+        _ => return handle_top_input(key_event, event_tx, terminal).await,
+    }
+    false
+}
diff --git a/lib/vector-top/src/lib.rs b/lib/vector-top/src/lib.rs
index c3580a464..7f4f9b8a8 100644
--- a/lib/vector-top/src/lib.rs
+++ b/lib/vector-top/src/lib.rs
@@ -1,5 +1,6 @@
 //! Top subcommand
 pub mod dashboard;
 pub mod events;
+mod input;
 pub mod metrics;
 pub mod state;
diff --git a/lib/vector-top/src/state.rs b/lib/vector-top/src/state.rs
index 4bf3ffb8e..e4c535f78 100644
--- a/lib/vector-top/src/state.rs
+++ b/lib/vector-top/src/state.rs
@@ -1,18 +1,24 @@
 use std::{
     collections::{BTreeMap, HashMap},
+    str::FromStr,
     time::Duration,
 };
 
 use chrono::{DateTime, Local};
 use ratatui::{
+    layout::Size,
     style::{Color, Style},
     text::Span,
+    widgets::ListState,
 };
+use regex::Regex;
 use tokio::sync::mpsc;
 use vector_common::internal_event::DEFAULT_OUTPUT;
 
 use vector_common::config::ComponentKey;
 
+use crate::dashboard::columns;
+
 type IdentifiedMetric = (ComponentKey, i64);
 
 #[derive(Debug)]
@@ -45,6 +51,37 @@ pub enum EventType {
     ComponentAdded(ComponentRow),
     ComponentRemoved(ComponentKey),
     ConnectionUpdated(ConnectionStatus),
+    Ui(UiEventType),
+}
+
+#[derive(Debug)]
+pub enum UiEventType {
+    // Scroll up (-) or down (+). Also passes the window size for correct max scroll calculation.
+    Scroll(isize, Size),
+    // Scroll up (-) or down (+) by a whole page. Also passes the window size for page size and max scroll calculation.
+    ScrollPage(isize, Size),
+    // Toggles help window. Also closes other windows.
+    ToggleHelp,
+    // Toggles sort menu. Also closes other windows.
+    ToggleSortMenu,
+    // Toggles sort direction.
+    ToggleSortDirection,
+    // Change sort selection up (-) or down (+).
+    SortSelection(isize),
+    // Change sort selection to a specific column.
+    SortByColumn(SortColumn),
+    // Confirms current sort selection.
+    SortConfirmation,
+    // Toggles filter menu. Also closes other windows.
+    ToggleFilterMenu,
+    // Change filter column selection left (-) or right (+).
+    FilterColumnSelection(isize),
+    // Adds input to filter string.
+    FilterInput(char),
+    // Removes a character from the end of the filter string.
+    FilterBackspace,
+    // Confirms current filter selection.
+    FilterConfirmation,
 }
 
 #[derive(Debug, Copy, Clone)]
@@ -82,17 +119,253 @@ pub struct State {
     pub connection_status: ConnectionStatus,
     pub uptime: Duration,
     pub components: BTreeMap<ComponentKey, ComponentRow>,
+    pub sort_state: SortState,
+    pub filter_state: FilterState,
+    pub ui: UiState,
+}
+
+#[derive(Debug, Clone, Copy)]
+pub enum SortColumn {
+    Id = 0,
+    Kind = 1,
+    Type = 2,
+    EventsIn = 3,
+    EventsInTotal = 4,
+    BytesIn = 5,
+    BytesInTotal = 6,
+    EventsOut = 7,
+    EventsOutTotal = 8,
+    BytesOut = 9,
+    BytesOutTotal = 10,
+    Errors = 11,
+    #[cfg(feature = "allocation-tracing")]
+    MemoryUsed = 12,
+}
+
+#[derive(Debug, Default, Clone, Copy)]
+pub enum FilterColumn {
+    #[default]
+    Id = 0,
+    Kind = 1,
+    Type = 2,
+}
+
+impl SortColumn {
+    pub fn matches_header(&self, header: &str) -> bool {
+        match self {
+            SortColumn::Id => header == columns::ID,
+            SortColumn::Kind => header == columns::KIND,
+            SortColumn::Type => header == columns::TYPE,
+            SortColumn::EventsIn | SortColumn::EventsInTotal => header == columns::EVENTS_IN,
+            SortColumn::BytesIn | SortColumn::BytesInTotal => header == columns::BYTES_IN,
+            SortColumn::EventsOut | SortColumn::EventsOutTotal => header == columns::EVENTS_OUT,
+            SortColumn::BytesOut | SortColumn::BytesOutTotal => header == columns::BYTES_OUT,
+            SortColumn::Errors => header == columns::ERRORS,
+            #[cfg(feature = "allocation-tracing")]
+            SortColumn::MemoryUsed => header == columns::MEMORY_USED,
+        }
+    }
+
+    pub fn items() -> Vec<&'static str> {
+        vec![
+            columns::ID,
+            columns::KIND,
+            columns::TYPE,
+            columns::EVENTS_IN,
+            columns::EVENTS_IN_TOTAL,
+            columns::BYTES_IN,
+            columns::BYTES_IN_TOTAL,
+            columns::EVENTS_OUT,
+            columns::EVENTS_OUT_TOTAL,
+            columns::BYTES_OUT,
+            columns::BYTES_OUT_TOTAL,
+            columns::ERRORS,
+            #[cfg(feature = "allocation-tracing")]
+            columns::MEMORY_USED,
+        ]
+    }
+}
+
+impl FilterColumn {
+    pub fn matches_header(&self, header: &str) -> bool {
+        match self {
+            FilterColumn::Id => header == columns::ID,
+            FilterColumn::Kind => header == columns::KIND,
+            FilterColumn::Type => header == columns::TYPE,
+        }
+    }
+
+    pub fn items() -> Vec<&'static str> {
+        vec![columns::ID, columns::KIND, columns::TYPE]
+    }
+}
+
+impl From<usize> for SortColumn {
+    fn from(value: usize) -> Self {
+        match value {
+            1 => SortColumn::Kind,
+            2 => SortColumn::Type,
+            3 => SortColumn::EventsIn,
+            4 => SortColumn::EventsInTotal,
+            5 => SortColumn::BytesIn,
+            6 => SortColumn::BytesInTotal,
+            7 => SortColumn::EventsOut,
+            8 => SortColumn::EventsOutTotal,
+            9 => SortColumn::BytesOut,
+            10 => SortColumn::BytesOutTotal,
+            11 => SortColumn::Errors,
+            #[cfg(feature = "allocation-tracing")]
+            12 => SortColumn::MemoryUsed,
+            _ => SortColumn::Id,
+        }
+    }
+}
+
+impl From<usize> for FilterColumn {
+    fn from(value: usize) -> Self {
+        match value {
+            1 => FilterColumn::Kind,
+            2 => FilterColumn::Type,
+            _ => FilterColumn::Id,
+        }
+    }
+}
+
+impl FromStr for SortColumn {
+    type Err = String;
+
+    fn from_str(s: &str) -> Result<Self, Self::Err> {
+        if let Some((index, _)) = Self::items()
+            .iter()
+            .enumerate()
+            .find(|(_, item)| item.eq_ignore_ascii_case(s))
+        {
+            Ok(index.into())
+        } else {
+            Err("Unknown sort field".to_string())
+        }
+    }
+}
+
+impl FromStr for FilterColumn {
+    type Err = String;
+
+    fn from_str(s: &str) -> Result<Self, Self::Err> {
+        if let Some((index, _)) = Self::items()
+            .iter()
+            .enumerate()
+            .find(|(_, item)| item.eq_ignore_ascii_case(s))
+        {
+            Ok(index.into())
+        } else {
+            Err("Unknown filter field".to_string())
+        }
+    }
+}
+
+#[derive(Debug, Default, Clone)]
+pub struct SortState {
+    pub column: Option<SortColumn>,
+    pub reverse: bool,
+}
+
+#[derive(Debug, Default, Clone)]
+pub struct FilterState {
+    pub column: FilterColumn,
+    pub pattern: Option<Regex>,
+}
+
+#[derive(Debug, Default, Clone)]
+pub struct UiState {
+    pub scroll: usize,
+    pub help_visible: bool,
+    pub sort_visible: bool,
+    pub sort_menu_state: ListState,
+    pub filter_visible: bool,
+    pub filter_menu_state: FilterMenuState,
+}
+
+#[derive(Debug, Clone)]
+pub struct FilterMenuState {
+    pub input: String,
+    pub column_selection: ListState,
+}
+
+impl Default for FilterMenuState {
+    fn default() -> Self {
+        Self {
+            input: Default::default(),
+            column_selection: ListState::default().with_selected(Some(0)),
+        }
+    }
+}
+
+impl UiState {
+    /// Returns the height of components display box in rows, based on provided [`Size`].
+    /// Calculates by deducting rows used for header and footer.
+    pub fn components_box_height(area: Size) -> u16 {
+        // Currently hardcoded (10 is the number of rows the header and footer take up)
+        area.height.saturating_sub(10)
+    }
+
+    /// Returns the maximum scroll value
+    pub fn max_scroll(area: Size, components_count: usize) -> usize {
+        components_count.saturating_sub(Self::components_box_height(area).into())
+    }
+
+    /// Changes current scroll by provided diff in rows. Uses [`Size`] to limit scroll,
+    /// so that scrolling down is possible until the last component is visible.
+    pub fn scroll(&mut self, diff: isize, area: Size, components_count: usize) {
+        let max_scroll = Self::max_scroll(area, components_count);
+        self.scroll = self.scroll.saturating_add_signed(diff);
+        if self.scroll > max_scroll {
+            self.scroll = max_scroll;
+        }
+    }
+
+    /// Changes current scroll by provided diff in pages. Uses [`Size`] to limit scroll,
+    /// and to calculate number of rows a page contains.
+    pub fn scroll_page(&mut self, diff: isize, area: Size, components_count: usize) {
+        self.scroll(
+            diff * (Self::components_box_height(area) as isize),
+            area,
+            components_count,
+        );
+    }
 }
 
 impl State {
-    pub const fn new(components: BTreeMap<ComponentKey, ComponentRow>) -> Self {
+    pub fn new(components: BTreeMap<ComponentKey, ComponentRow>) -> Self {
         Self {
             connection_status: ConnectionStatus::Pending,
             uptime: Duration::from_secs(0),
             components,
+            ui: UiState::default(),
+            sort_state: SortState::default(),
+            filter_state: FilterState::default(),
         }
     }
+
+    pub fn apply_sort_state_to_ui(&mut self) {
+        self.ui
+            .sort_menu_state
+            .select(self.sort_state.column.map(|c| c as usize));
+    }
+
+    pub fn apply_filter_state_to_ui(&mut self) {
+        self.ui
+            .filter_menu_state
+            .column_selection
+            .select(Some(self.filter_state.column as usize));
+        self.ui.filter_menu_state.input = self
+            .filter_state
+            .pattern
+            .as_ref()
+            .map(|r| r.as_str().to_string())
+            .unwrap_or("".to_string());
+    }
 }
+
 pub type EventTx = mpsc::Sender<EventType>;
 pub type EventRx = mpsc::Receiver<EventType>;
 pub type StateRx = mpsc::Receiver<State>;
@@ -143,15 +416,19 @@ impl ComponentRow {
 /// Takes the receiver `EventRx` channel, and returns a `StateRx` state receiver. This
 /// represents the single destination for handling subscriptions and returning 'immutable' state
 /// for re-rendering the dashboard. This approach uses channels vs. mutexes.
-pub async fn updater(mut event_rx: EventRx) -> StateRx {
+pub async fn updater(mut event_rx: EventRx, mut state: State) -> StateRx {
     let (tx, rx) = mpsc::channel(20);
 
-    let mut state = State::new(BTreeMap::new());
     tokio::spawn(async move {
         while let Some(event_type) = event_rx.recv().await {
             match event_type {
                 EventType::InitializeState(new_state) => {
+                    let old_state = state;
                     state = new_state;
+                    // Keep filters, sort and UI states
+                    state.filter_state = old_state.filter_state;
+                    state.sort_state = old_state.sort_state;
+                    state.ui = old_state.ui;
                 }
                 EventType::ReceivedBytesTotals(rows) => {
                     for (key, v) in rows {
@@ -253,6 +530,7 @@ pub async fn updater(mut event_rx: EventRx) -> StateRx {
                 EventType::UptimeChanged(uptime) => {
                     state.uptime = Duration::from_secs_f64(uptime);
                 }
+                EventType::Ui(ui_event_type) => handle_ui_event(ui_event_type, &mut state),
             }
 
             // Send updated map to listeners
@@ -262,3 +540,86 @@ pub async fn updater(mut event_rx: EventRx) -> StateRx {
 
     rx
 }
+
+fn handle_ui_event(event: UiEventType, state: &mut State) {
+    match event {
+        UiEventType::Scroll(diff, area) => {
+            state.ui.scroll(diff, area, state.components.len());
+        }
+        UiEventType::ScrollPage(diff, area) => {
+            state.ui.scroll_page(diff, area, state.components.len());
+        }
+        UiEventType::ToggleHelp => {
+            state.ui.help_visible = !state.ui.help_visible;
+            if state.ui.help_visible {
+                state.ui.sort_visible = false;
+                state.ui.filter_visible = false;
+            }
+        }
+        UiEventType::ToggleSortMenu => {
+            state.ui.sort_visible = !state.ui.sort_visible;
+            if state.ui.sort_visible {
+                state.apply_sort_state_to_ui();
+                state.ui.help_visible = false;
+                state.ui.filter_visible = false;
+            }
+        }
+        UiEventType::ToggleSortDirection => state.sort_state.reverse = !state.sort_state.reverse,
+        UiEventType::SortSelection(diff) => {
+            let next = state.ui.sort_menu_state.selected().map_or(0, |s| {
+                s.saturating_add_signed(diff)
+                    .min(SortColumn::items().len() - 1)
+            });
+            state.ui.sort_menu_state.select(Some(next));
+        }
+        UiEventType::SortByColumn(col) => state.sort_state.column = Some(col),
+        UiEventType::SortConfirmation => {
+            if let Some(selected) = state.ui.sort_menu_state.selected() {
+                state.sort_state.column = Some(selected.into())
+            }
+            state.ui.sort_visible = false;
+        }
+        UiEventType::ToggleFilterMenu => {
+            state.ui.filter_visible = !state.ui.filter_visible;
+            if state.ui.filter_visible {
+                state.apply_filter_state_to_ui();
+                state.ui.help_visible = false;
+                state.ui.sort_visible = false;
+            }
+        }
+        UiEventType::FilterColumnSelection(diff) => {
+            let next = state
+                .ui
+                .filter_menu_state
+                .column_selection
+                .selected()
+                .map_or(0, |s| {
+                    s.saturating_add_signed(diff)
+                        .min(FilterColumn::items().len() - 1)
+                });
+            state
+                .ui
+                .filter_menu_state
+                .column_selection
+                .select(Some(next));
+        }
+        UiEventType::FilterInput(c) => {
+            state.ui.filter_menu_state.input.push(c);
+        }
+        UiEventType::FilterBackspace => {
+            let _ = state.ui.filter_menu_state.input.pop();
+        }
+        UiEventType::FilterConfirmation => {
+            if state.ui.filter_menu_state.input.is_empty() {
+                state.filter_state.pattern = None;
+            } else {
+                // display errors (https://github.com/vectordotdev/vector/issues/24620)?
+                state.filter_state.pattern = Regex::new(&state.ui.filter_menu_state.input).ok();
+            }
+            if let Some(selected) = state.ui.filter_menu_state.column_selection.selected() {
+                state.filter_state.column = selected.into()
+            }
+            state.ui.filter_visible = false;
+        }
+    }
+}
diff --git a/lib/vector-vrl-metrics/Cargo.toml b/lib/vector-vrl-metrics/Cargo.toml
index 6998448b6..6f6a30b66 100644
--- a/lib/vector-vrl-metrics/Cargo.toml
+++ b/lib/vector-vrl-metrics/Cargo.toml
@@ -14,3 +14,4 @@ vector-core = { path = "../vector-core", default-features = false, features = ["
 vector-common = { path = "../vector-common", default-features = false }
 tokio.workspace = true
 tokio-stream.workspace = true
+vector-vrl-category.workspace = true
diff --git a/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs b/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
index 85c6c1377..949b2c831 100644
--- a/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
+++ b/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
@@ -1,4 +1,6 @@
 use std::collections::BTreeMap;
+use std::sync::LazyLock;
+use vector_vrl_category::Category;
 use vrl::prelude::expression::Expr;
 use vrl::value;
 
@@ -8,6 +10,33 @@ use crate::common::resolve_tags;
 use crate::common::validate_tags;
 use crate::common::{Error, MetricsStorage};
 
+static DEFAULT_TAGS: LazyLock<Value> = LazyLock::new(|| Value::Object(BTreeMap::new()));
+static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
+    vec![
+        Parameter {
+            keyword: "function",
+            kind: kind::BYTES,
+            required: true,
+            description: "The metric name to search.",
+            default: None,
+        },
+        Parameter {
+            keyword: "key",
+            kind: kind::BYTES,
+            required: true,
+            description: "The metric name to aggregate.",
+            default: None,
+        },
+        Parameter {
+            keyword: "tags",
+            kind: kind::OBJECT,
+            required: false,
+            description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
+            default: Some(&DEFAULT_TAGS),
+        },
+    ]
+});
+
 fn aggregate_metrics(
     metrics_storage: &MetricsStorage,
     function: &Bytes,
@@ -54,27 +83,16 @@ impl Function for AggregateVectorMetrics {
         )
     }
 
+    fn category(&self) -> &'static str {
+        Category::Metrics.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::FLOAT | kind::NULL
+    }
+
     fn parameters(&self) -> &'static [Parameter] {
-        &[
-            Parameter {
-                keyword: "function",
-                kind: kind::BYTES,
-                required: true,
-                description: "The metric name to search.",
-            },
-            Parameter {
-                keyword: "key",
-                kind: kind::BYTES,
-                required: true,
-                description: "The metric name to aggregate.",
-            },
-            Parameter {
-                keyword: "tags",
-                kind: kind::OBJECT,
-                required: false,
-                description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
-            },
-        ]
+        &PARAMETERS
     }
 
     fn examples(&self) -> &'static [Example] {
diff --git a/lib/vector-vrl-metrics/src/find_vector_metrics.rs b/lib/vector-vrl-metrics/src/find_vector_metrics.rs
index 78a014065..4db90e4df 100644
--- a/lib/vector-vrl-metrics/src/find_vector_metrics.rs
+++ b/lib/vector-vrl-metrics/src/find_vector_metrics.rs
@@ -1,4 +1,5 @@
-use std::collections::BTreeMap;
+use std::{collections::BTreeMap, sync::LazyLock};
+use vector_vrl_category::Category;
 use vrl::prelude::expression::Expr;
 
 use vrl::prelude::*;
@@ -22,6 +23,27 @@ fn find_metrics(
     ))
 }
 
+static DEFAULT_TAGS: LazyLock<Value> = LazyLock::new(|| Value::Object(BTreeMap::new()));
+
+static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
+    vec![
+        Parameter {
+            keyword: "key",
+            kind: kind::BYTES,
+            required: true,
+            description: "The metric name to search.",
+            default: None,
+        },
+        Parameter {
+            keyword: "tags",
+            kind: kind::OBJECT,
+            required: false,
+            description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
+            default: Some(&DEFAULT_TAGS),
+        },
+    ]
+});
+
 #[derive(Clone, Copy, Debug)]
 pub struct FindVectorMetrics;
 
@@ -37,21 +59,16 @@ impl Function for FindVectorMetrics {
         )
     }
 
+    fn category(&self) -> &'static str {
+        Category::Metrics.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::ARRAY
+    }
+
     fn parameters(&self) -> &'static [Parameter] {
-        &[
-            Parameter {
-                keyword: "key",
-                kind: kind::BYTES,
-                required: true,
-                description: "The metric name to search.",
-            },
-            Parameter {
-                keyword: "tags",
-                kind: kind::OBJECT,
-                required: false,
-                description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
-            },
-        ]
+        &PARAMETERS
     }
 
     fn examples(&self) -> &'static [Example] {
diff --git a/lib/vector-vrl-metrics/src/get_vector_metric.rs b/lib/vector-vrl-metrics/src/get_vector_metric.rs
index 67fa6349e..21193b3e8 100644
--- a/lib/vector-vrl-metrics/src/get_vector_metric.rs
+++ b/lib/vector-vrl-metrics/src/get_vector_metric.rs
@@ -1,5 +1,6 @@
-use std::collections::BTreeMap;
+use std::{collections::BTreeMap, sync::LazyLock};
 
+use vector_vrl_category::Category;
 use vrl::prelude::{expression::Expr, *};
 
 use crate::common::{
@@ -19,6 +20,27 @@ fn get_metric(
     Ok(value)
 }
 
+static DEFAULT_TAGS: LazyLock<Value> = LazyLock::new(|| Value::Object(BTreeMap::new()));
+
+static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
+    vec![
+        Parameter {
+            keyword: "key",
+            kind: kind::BYTES,
+            required: true,
+            description: "The metric name to search.",
+            default: None,
+        },
+        Parameter {
+            keyword: "tags",
+            kind: kind::OBJECT,
+            required: false,
+            description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
+            default: Some(&DEFAULT_TAGS),
+        },
+    ]
+});
+
 #[derive(Clone, Copy, Debug)]
 pub struct GetVectorMetric;
 
@@ -34,21 +56,16 @@ impl Function for GetVectorMetric {
         )
     }
 
+    fn category(&self) -> &'static str {
+        Category::Metrics.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::OBJECT | kind::NULL
+    }
+
     fn parameters(&self) -> &'static [Parameter] {
-        &[
-            Parameter {
-                keyword: "key",
-                kind: kind::BYTES,
-                required: true,
-                description: "The metric name to search.",
-            },
-            Parameter {
-                keyword: "tags",
-                kind: kind::OBJECT,
-                required: false,
-                description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
-            },
-        ]
+        &PARAMETERS
     }
 
     fn examples(&self) -> &'static [Example] {
diff --git a/lib/vector-vrl/category/Cargo.toml b/lib/vector-vrl/category/Cargo.toml
new file mode 100644
index 000000000..dafb938de
--- /dev/null
+++ b/lib/vector-vrl/category/Cargo.toml
@@ -0,0 +1,10 @@
+[package]
+name = "vector-vrl-category"
+version = "0.1.0"
+authors = ["Vector Contributors <vector@datadoghq.com>"]
+edition = "2024"
+publish = false
+license = "MPL-2.0"
+
+[dependencies]
+strum = { version = "0.27", features = ["derive"] }
diff --git a/lib/vector-vrl/category/src/lib.rs b/lib/vector-vrl/category/src/lib.rs
new file mode 100644
index 000000000..bd1df2097
--- /dev/null
+++ b/lib/vector-vrl/category/src/lib.rs
@@ -0,0 +1,16 @@
+use strum::AsRefStr;
+
+/// Category classification for Vector-specific VRL functions.
+///
+/// This enum complements the categories defined in the VRL stdlib,
+/// providing Vector-specific categories for enrichment, metrics, and event functions.
+#[derive(Debug, Clone, Copy, AsRefStr)]
+#[strum(serialize_all = "PascalCase")]
+pub enum Category {
+    /// Enrichment table operations
+    Enrichment,
+    /// Event metadata and secret management
+    Event,
+    /// Internal Vector metrics operations
+    Metrics,
+}
diff --git a/lib/vector-vrl/functions/Cargo.toml b/lib/vector-vrl/functions/Cargo.toml
index 4e630b951..973c32e56 100644
--- a/lib/vector-vrl/functions/Cargo.toml
+++ b/lib/vector-vrl/functions/Cargo.toml
@@ -12,6 +12,7 @@ vrl.workspace = true
 enrichment = { path = "../../enrichment" }
 dnstap-parser = { path = "../../dnstap-parser", optional = true }
 vector-vrl-metrics = { path = "../../vector-vrl-metrics", optional = true }
+vector-vrl-category.workspace = true
 
 [features]
 default = []
diff --git a/lib/vector-vrl/functions/src/get_secret.rs b/lib/vector-vrl/functions/src/get_secret.rs
index 49152f31d..0f3a787d8 100644
--- a/lib/vector-vrl/functions/src/get_secret.rs
+++ b/lib/vector-vrl/functions/src/get_secret.rs
@@ -1,3 +1,4 @@
+use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 fn get_secret(ctx: &mut Context, key: Value) -> std::result::Result<Value, ExpressionError> {
@@ -21,12 +22,21 @@ impl Function for GetSecret {
         "Returns the value of the given secret from an event."
     }
 
+    fn category(&self) -> &'static str {
+        Category::Event.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::BYTES | kind::NULL
+    }
+
     fn parameters(&self) -> &'static [Parameter] {
         &[Parameter {
             keyword: "key",
             kind: kind::BYTES,
             required: true,
             description: "The name of the secret.",
+            default: None,
         }]
     }
 
diff --git a/lib/vector-vrl/functions/src/remove_secret.rs b/lib/vector-vrl/functions/src/remove_secret.rs
index 47c75b459..47d5c2004 100644
--- a/lib/vector-vrl/functions/src/remove_secret.rs
+++ b/lib/vector-vrl/functions/src/remove_secret.rs
@@ -1,3 +1,4 @@
+use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 fn remove_secret(ctx: &mut Context, key: Value) -> std::result::Result<Value, ExpressionError> {
@@ -18,12 +19,21 @@ impl Function for RemoveSecret {
         "Removes a secret from an event."
     }
 
+    fn category(&self) -> &'static str {
+        Category::Event.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::NULL
+    }
+
     fn parameters(&self) -> &'static [Parameter] {
         &[Parameter {
             keyword: "key",
             kind: kind::BYTES,
             required: true,
             description: "The name of the secret to remove.",
+            default: None,
         }]
     }
 
diff --git a/lib/vector-vrl/functions/src/set_secret.rs b/lib/vector-vrl/functions/src/set_secret.rs
index 598822c1f..4fba86197 100644
--- a/lib/vector-vrl/functions/src/set_secret.rs
+++ b/lib/vector-vrl/functions/src/set_secret.rs
@@ -1,3 +1,4 @@
+use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 fn set_secret(
@@ -25,6 +26,14 @@ impl Function for SetSecret {
         "Sets the given secret in the event."
     }
 
+    fn category(&self) -> &'static str {
+        Category::Event.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::NULL
+    }
+
     fn parameters(&self) -> &'static [Parameter] {
         &[
             Parameter {
@@ -32,12 +41,14 @@ impl Function for SetSecret {
                 kind: kind::BYTES,
                 required: true,
                 description: "The name of the secret.",
+                default: None,
             },
             Parameter {
                 keyword: "secret",
                 kind: kind::BYTES,
                 required: true,
                 description: "The secret value.",
+                default: None,
             },
         ]
     }
diff --git a/lib/vector-vrl/functions/src/set_semantic_meaning.rs b/lib/vector-vrl/functions/src/set_semantic_meaning.rs
index 86bbb6336..13fe6239c 100644
--- a/lib/vector-vrl/functions/src/set_semantic_meaning.rs
+++ b/lib/vector-vrl/functions/src/set_semantic_meaning.rs
@@ -3,6 +3,7 @@ use std::{
     ops::{Deref, DerefMut},
 };
 
+use vector_vrl_category::Category;
 use vrl::{
     diagnostic::Label,
     path::{OwnedTargetPath, PathPrefix},
@@ -46,6 +47,14 @@ impl Function for SetSemanticMeaning {
         "}
     }
 
+    fn category(&self) -> &'static str {
+        Category::Event.as_ref()
+    }
+
+    fn return_kind(&self) -> u16 {
+        kind::NULL
+    }
+
     fn parameters(&self) -> &'static [Parameter] {
         &[
             Parameter {
@@ -53,12 +62,14 @@ impl Function for SetSemanticMeaning {
                 kind: kind::ANY,
                 required: true,
                 description: "The path of the value that is assigned a meaning.",
+                default: None,
             },
             Parameter {
                 keyword: "meaning",
                 kind: kind::BYTES,
                 required: true,
                 description: "The name of the meaning to assign.",
+                default: None,
             },
         ]
     }
diff --git a/src/config/compiler.rs b/src/config/compiler.rs
index ecb90f7e3..f493f8c84 100644
--- a/src/config/compiler.rs
+++ b/src/config/compiler.rs
@@ -36,7 +36,7 @@ pub fn compile(mut builder: ConfigBuilder) -> Result<(Config, Vec<String>), Vec<
         errors.extend(output_errors);
     }
 
-    if let Err(alpha_errors) = validation::check_buffer_utilization_ewma_alpha(&builder) {
+    if let Err(alpha_errors) = validation::check_values(&builder) {
         errors.extend(alpha_errors);
     }
 
diff --git a/src/config/validation.rs b/src/config/validation.rs
index 0b629ea90..35519a93c 100644
--- a/src/config/validation.rs
+++ b/src/config/validation.rs
@@ -11,14 +11,28 @@ use super::{
 };
 use crate::config::schema;
 
-/// Minimum value (exclusive) for `utilization_ewma_alpha`.
+/// Minimum value (exclusive) for EWMA alpha options.
 /// The alpha value must be strictly greater than this value.
 const EWMA_ALPHA_MIN: f64 = 0.0;
 
-/// Maximum value (exclusive) for `utilization_ewma_alpha`.
+/// Maximum value (exclusive) for EWMA alpha options.
 /// The alpha value must be strictly less than this value.
 const EWMA_ALPHA_MAX: f64 = 1.0;
 
+/// Validates an optional EWMA alpha value and returns an error message if invalid.
+/// Returns `None` if the value is `None` or valid, otherwise returns an error message.
+fn validate_ewma_alpha(alpha: Option<f64>, field_name: &str) -> Option<String> {
+    if let Some(alpha) = alpha
+        && !(alpha > EWMA_ALPHA_MIN && alpha < EWMA_ALPHA_MAX)
+    {
+        Some(format!(
+            "Global `{field_name}` must be between 0 and 1 exclusive (0 < alpha < 1), got {alpha}"
+        ))
+    } else {
+        None
+    }
+}
+
 /// Check that provide + topology config aren't present in the same builder, which is an error.
 pub fn check_provider(config: &ConfigBuilder) -> Result<(), Vec<String>> {
     if config.provider.is_some()
@@ -155,17 +169,25 @@ pub fn check_resources(config: &ConfigBuilder) -> Result<(), Vec<String>> {
     }
 }
 
-/// Validates that `buffer_utilization_ewma_alpha` value is within the valid range (0 < alpha < 1)
-/// for the global configuration.
-pub fn check_buffer_utilization_ewma_alpha(config: &ConfigBuilder) -> Result<(), Vec<String>> {
-    if let Some(alpha) = config.global.buffer_utilization_ewma_alpha
-        && (alpha <= EWMA_ALPHA_MIN || alpha >= EWMA_ALPHA_MAX)
+/// Validates that `*_ewma_alpha` values are within the valid range (0 < alpha < 1).
+pub fn check_values(config: &ConfigBuilder) -> Result<(), Vec<String>> {
+    let mut errors = Vec::new();
+
+    if let Some(error) = validate_ewma_alpha(
+        config.global.buffer_utilization_ewma_alpha,
+        "buffer_utilization_ewma_alpha",
+    ) {
+        errors.push(error);
+    }
+    if let Some(error) = validate_ewma_alpha(config.global.latency_ewma_alpha, "latency_ewma_alpha")
     {
-        Err(vec![format!(
-            "Global `buffer_utilization_ewma_alpha` must be between 0 and 1 exclusive (0 < alpha < 1), got {alpha}"
-        )])
-    } else {
+        errors.push(error);
+    }
+
+    if errors.is_empty() {
         Ok(())
+    } else {
+        Err(errors)
     }
 }
 
diff --git a/src/sinks/prometheus/remote_write/config.rs b/src/sinks/prometheus/remote_write/config.rs
index 97560a777..d0dce3908 100644
--- a/src/sinks/prometheus/remote_write/config.rs
+++ b/src/sinks/prometheus/remote_write/config.rs
@@ -178,9 +178,14 @@ impl SinkConfig for RemoteWriteConfig {
             None => None,
         };
 
+        let healthcheck_endpoint = match cx.healthcheck.uri {
+            Some(uri) => uri.uri,
+            None => endpoint.clone(),
+        };
+
         let healthcheck = healthcheck(
             client.clone(),
-            endpoint.clone(),
+            healthcheck_endpoint,
             self.compression,
             auth.clone(),
         )
diff --git a/src/sources/host_metrics/filesystem.rs b/src/sources/host_metrics/filesystem.rs
index f6fb50ed5..def1680d6 100644
--- a/src/sources/host_metrics/filesystem.rs
+++ b/src/sources/host_metrics/filesystem.rs
@@ -2,6 +2,8 @@ use futures::StreamExt;
 use heim::units::information::byte;
 #[cfg(not(windows))]
 use heim::units::ratio::ratio;
+#[cfg(unix)]
+use nix::sys::statvfs::statvfs;
 use vector_lib::{configurable::configurable_component, metric_tags};
 
 use super::{FilterList, HostMetrics, default_all_devices, example_devices, filter_result};
@@ -128,8 +130,32 @@ impl HostMetrics {
                     output.gauge(
                         "filesystem_used_ratio",
                         usage.ratio().get::<ratio>() as f64,
-                        tags,
+                        tags.clone(),
                     );
+
+                    // inode metrics via a second statvfs call - heim's Usage wraps
+                    // libc::statvfs internally but doesn't expose inode fields
+                    // (f_files, f_ffree). the kernel caches statvfs for local
+                    // filesystems so the overhead is negligible, but network mounts
+                    // may pay a small extra cost.
+                    #[cfg(unix)]
+                    if let Ok(stat) = statvfs(partition.mount_point()) {
+                        let inodes_total = stat.files() as f64;
+                        let inodes_free = stat.files_free() as f64;
+                        let inodes_used = (inodes_total - inodes_free).max(0.0);
+                        let inodes_used_ratio = if inodes_total > 0.0 {
+                            inodes_used / inodes_total
+                        } else {
+                            0.0
+                        };
+
+                        output.gauge("filesystem_inodes_total", inodes_total, tags.clone());
+                        output.gauge("filesystem_inodes_free", inodes_free, tags.clone());
+                        output.gauge("filesystem_inodes_used", inodes_used, tags.clone());
+                        output.gauge("filesystem_inodes_used_ratio", inodes_used_ratio, tags);
+                    }
+                    #[cfg(windows)]
+                    drop(tags);
                 }
             }
             Err(error) => {
@@ -161,17 +187,40 @@ mod tests {
             .await;
         let metrics = buffer.metrics;
         assert!(!metrics.is_empty());
-        assert!(metrics.len().is_multiple_of(4));
         assert!(all_gauges(&metrics));
 
-        // There are exactly three filesystem_* names
-        for name in &[
+        // Base metrics (these are always present)
+        let base_metrics = [
             "filesystem_free_bytes",
             "filesystem_total_bytes",
             "filesystem_used_bytes",
             "filesystem_used_ratio",
-        ] {
-            assert_eq!(count_name(&metrics, name), metrics.len() / 4, "name={name}");
+        ];
+
+        // Each filesystem should have all 4 base metrics
+        let num_filesystems = count_name(&metrics, "filesystem_free_bytes");
+        assert!(num_filesystems > 0);
+        for name in &base_metrics {
+            assert_eq!(count_name(&metrics, name), num_filesystems, "name={name}");
+        }
+
+        // Inode metrics are present for filesystems that support statvfs
+        // (some virtual filesystems like /proc, /sys might not)
+        let inode_metrics = [
+            "filesystem_inodes_total",
+            "filesystem_inodes_free",
+            "filesystem_inodes_used",
+            "filesystem_inodes_used_ratio",
+        ];
+        let num_inode_total = count_name(&metrics, "filesystem_inodes_total");
+        assert!(
+            num_inode_total > 0,
+            "Expected at least one filesystem to report inode metrics"
+        );
+
+        // For filesystems that report inodes, all 4 inode metrics should be present
+        for name in &inode_metrics {
+            assert_eq!(count_name(&metrics, name), num_inode_total, "name={name}");
         }
 
         // They should all have "filesystem" and "mountpoint" tags
diff --git a/src/test_util/mock/sinks/completion.rs b/src/test_util/mock/sinks/completion.rs
new file mode 100644
index 000000000..a390f33c9
--- /dev/null
+++ b/src/test_util/mock/sinks/completion.rs
@@ -0,0 +1,95 @@
+use std::sync::{Arc, Mutex};
+
+use async_trait::async_trait;
+use futures_util::{FutureExt, StreamExt, future, stream::BoxStream};
+use tokio::sync::oneshot::Sender;
+use vector_lib::{
+    config::{AcknowledgementsConfig, Input},
+    configurable::configurable_component,
+    event::Event,
+    sink::{StreamSink, VectorSink},
+};
+
+use crate::{
+    config::{SinkConfig, SinkContext},
+    sinks::Healthcheck,
+};
+
+/// Configuration for the `test_completion` sink.
+#[configurable_component(sink("test_completion", "Test (completion)."))]
+#[derive(Clone, Debug, Default)]
+pub struct CompletionSinkConfig {
+    #[serde(skip)]
+    expected: usize,
+
+    #[serde(skip)]
+    completion_tx: Arc<Mutex<Option<Sender<bool>>>>,
+}
+
+impl_generate_config_from_default!(CompletionSinkConfig);
+
+impl CompletionSinkConfig {
+    pub fn new(expected: usize, completion_tx: Sender<bool>) -> Self {
+        Self {
+            expected,
+            completion_tx: Arc::new(Mutex::new(Some(completion_tx))),
+        }
+    }
+}
+
+#[async_trait]
+#[typetag::serde(name = "test_completion")]
+impl SinkConfig for CompletionSinkConfig {
+    async fn build(&self, _cx: SinkContext) -> crate::Result<(VectorSink, Healthcheck)> {
+        let completion_tx = self
+            .completion_tx
+            .lock()
+            .expect("completion sink mutex poisoned")
+            .take();
+
+        let sink = CompletionSink {
+            remaining: self.expected,
+            completion_tx,
+        };
+        let healthcheck = future::ready(Ok(())).boxed();
+
+        Ok((VectorSink::from_event_streamsink(sink), healthcheck))
+    }
+
+    fn input(&self) -> Input {
+        Input::all()
+    }
+
+    fn acknowledgements(&self) -> &AcknowledgementsConfig {
+        &AcknowledgementsConfig::DEFAULT
+    }
+}
+
+struct CompletionSink {
+    remaining: usize,
+    completion_tx: Option<Sender<bool>>,
+}
+
+#[async_trait]
+impl StreamSink<Event> for CompletionSink {
+    async fn run(mut self: Box<Self>, mut input: BoxStream<'_, Event>) -> Result<(), ()> {
+        while let Some(event) = input.next().await {
+            drop(event);
+
+            if self.remaining > 0 {
+                self.remaining -= 1;
+                if self.remaining == 0
+                    && let Some(tx) = self.completion_tx.take()
+                {
+                    let _ = tx.send(true);
+                }
+            }
+        }
+
+        if let Some(tx) = self.completion_tx.take() {
+            let _ = tx.send(self.remaining == 0);
+        }
+
+        Ok(())
+    }
+}
diff --git a/src/test_util/mock/sinks/mod.rs b/src/test_util/mock/sinks/mod.rs
index 862d74036..226391007 100644
--- a/src/test_util/mock/sinks/mod.rs
+++ b/src/test_util/mock/sinks/mod.rs
@@ -4,6 +4,9 @@ pub use self::backpressure::BackpressureSinkConfig;
 mod basic;
 pub use self::basic::BasicSinkConfig;
 
+mod completion;
+pub use self::completion::CompletionSinkConfig;
+
 mod error;
 pub use self::error::ErrorSinkConfig;
 
diff --git a/src/test_util/mock/transforms/noop.rs b/src/test_util/mock/transforms/noop.rs
index 83506160c..355480ddf 100644
--- a/src/test_util/mock/transforms/noop.rs
+++ b/src/test_util/mock/transforms/noop.rs
@@ -1,7 +1,7 @@
-use std::pin::Pin;
+use std::{pin::Pin, time::Duration};
 
 use async_trait::async_trait;
-use futures_util::Stream;
+use futures_util::{Stream, StreamExt as _};
 use vector_lib::{
     config::{DataType, Input, TransformOutput},
     configurable::configurable_component,
@@ -19,17 +19,31 @@ use crate::config::{GenerateConfig, OutputId, TransformConfig, TransformContext}
 pub struct NoopTransformConfig {
     #[configurable(derived)]
     transform_type: TransformType,
+
+    /// Optional per-event/array delay, in milliseconds.
+    ///
+    /// This is intended for tests that need deterministic, non-zero component latency.
+    #[serde(default, skip_serializing_if = "Option::is_none")]
+    delay_ms: Option<u64>,
 }
 
 impl GenerateConfig for NoopTransformConfig {
     fn generate_config() -> toml::Value {
         toml::Value::try_from(&Self {
             transform_type: TransformType::Function,
+            delay_ms: None,
         })
         .unwrap()
     }
 }
 
+impl NoopTransformConfig {
+    pub fn with_delay_ms(mut self, delay_ms: u64) -> Self {
+        self.delay_ms = Some(delay_ms);
+        self
+    }
+}
+
 #[async_trait]
 #[typetag::serde(name = "test_noop")]
 impl TransformConfig for NoopTransformConfig {
@@ -52,37 +66,55 @@ impl TransformConfig for NoopTransformConfig {
     }
 
     async fn build(&self, _: &TransformContext) -> crate::Result<Transform> {
+        let delay = self.delay_ms.map(Duration::from_millis);
         match self.transform_type {
-            TransformType::Function => Ok(Transform::Function(Box::new(NoopTransform))),
-            TransformType::Synchronous => Ok(Transform::Synchronous(Box::new(NoopTransform))),
-            TransformType::Task => Ok(Transform::Task(Box::new(NoopTransform))),
+            TransformType::Function => Ok(Transform::Function(Box::new(NoopTransform { delay }))),
+            TransformType::Synchronous => {
+                Ok(Transform::Synchronous(Box::new(NoopTransform { delay })))
+            }
+            TransformType::Task => Ok(Transform::Task(Box::new(NoopTransform { delay }))),
         }
     }
 }
 
 impl From<TransformType> for NoopTransformConfig {
     fn from(transform_type: TransformType) -> Self {
-        Self { transform_type }
+        Self {
+            transform_type,
+            delay_ms: None,
+        }
     }
 }
 
 #[derive(Clone)]
-struct NoopTransform;
+struct NoopTransform {
+    delay: Option<Duration>,
+}
 
 impl FunctionTransform for NoopTransform {
     fn transform(&mut self, output: &mut OutputBuffer, event: Event) {
+        if let Some(delay) = self.delay {
+            std::thread::sleep(delay);
+        }
         output.push(event);
     }
 }
 
 impl<T> TaskTransform<T> for NoopTransform
 where
-    T: EventContainer + 'static,
+    T: EventContainer + Send + 'static,
 {
     fn transform(
         self: Box<Self>,
         task: Pin<Box<dyn futures_util::Stream<Item = T> + Send>>,
     ) -> Pin<Box<dyn Stream<Item = T> + Send>> {
-        Box::pin(task)
+        if let Some(delay) = self.delay {
+            Box::pin(task.then(move |item| async move {
+                tokio::time::sleep(delay).await;
+                item
+            }))
+        } else {
+            Box::pin(task)
+        }
     }
 }
diff --git a/src/top/cmd.rs b/src/top/cmd.rs
index f63b2e313..641006378 100644
--- a/src/top/cmd.rs
+++ b/src/top/cmd.rs
@@ -1,14 +1,16 @@
+use std::collections::BTreeMap;
 use std::time::Duration;
 
 use chrono::Local;
 use futures_util::future::join_all;
+use regex::Regex;
 use tokio::sync::{mpsc, oneshot};
 use vector_lib::api_client::{Client, connect_subscription_client};
 
 use vector_lib::top::{
     dashboard::{init_dashboard, is_tty},
     metrics,
-    state::{self, ConnectionStatus, EventType},
+    state::{self, ConnectionStatus, EventType, State},
 };
 
 /// Delay (in milliseconds) before attempting to reconnect to the Vector API
@@ -53,11 +55,20 @@ pub async fn cmd(opts: &super::Opts) -> exitcode::ExitCode {
 pub async fn top(opts: &super::Opts, client: Client, dashboard_title: &str) -> exitcode::ExitCode {
     // Channel for updating state via event messages
     let (tx, rx) = tokio::sync::mpsc::channel(20);
-    let state_rx = state::updater(rx).await;
+    let mut starting_state = State::new(BTreeMap::new());
+    starting_state.sort_state.column = opts.sort_field;
+    starting_state.sort_state.reverse = opts.sort_desc;
+    starting_state.filter_state.column = opts.filter_field;
+    starting_state.filter_state.pattern = opts
+        .filter_value
+        .as_deref()
+        .map(Regex::new)
+        .and_then(Result::ok);
+    let state_rx = state::updater(rx, starting_state).await;
     // Channel for shutdown signal
     let (shutdown_tx, shutdown_rx) = oneshot::channel::<()>();
 
-    let connection = tokio::spawn(subscription(opts.clone(), client, tx, shutdown_tx));
+    let connection = tokio::spawn(subscription(opts.clone(), client, tx.clone(), shutdown_tx));
 
     // Initialize the dashboard
     match init_dashboard(
@@ -65,6 +76,7 @@ pub async fn top(opts: &super::Opts, client: Client, dashboard_title: &str) -> e
         opts.url().as_str(),
         opts.interval,
         opts.human_metrics,
+        tx,
         state_rx,
         shutdown_rx,
     )
diff --git a/src/top/mod.rs b/src/top/mod.rs
index 025596718..5fc101f7b 100644
--- a/src/top/mod.rs
+++ b/src/top/mod.rs
@@ -6,6 +6,7 @@ use glob::Pattern;
 
 pub use cmd::{cmd, top};
 use url::Url;
+use vector_lib::top::state::{FilterColumn, SortColumn};
 
 use crate::config::api::default_graphql_url;
 
@@ -34,6 +35,24 @@ pub struct Opts {
     /// Components IDs to observe (comma-separated; accepts glob patterns)
     #[arg(default_value = "*", value_delimiter(','), short = 'c', long)]
     components: Vec<Pattern>,
+
+    /// Field to sort values to by default (can be changed while running).
+    #[arg(short = 's', long)]
+    sort_field: Option<SortColumn>,
+
+    /// Sort descending instead of ascending.
+    #[arg(long, default_value_t = false)]
+    sort_desc: bool,
+
+    /// Field to filter values by default (can be changed while running).
+    #[arg(default_value = "id", long)]
+    filter_field: FilterColumn,
+
+    /// Filter to apply to the chosen field (ID by default).
+    ///
+    /// This accepts Regex patterns.
+    #[arg(short = 'f', long)]
+    filter_value: Option<String>,
 }
 
 impl Opts {
diff --git a/src/topology/builder.rs b/src/topology/builder.rs
index ba19f1dc6..0a8e7e2e9 100644
--- a/src/topology/builder.rs
+++ b/src/topology/builder.rs
@@ -22,10 +22,11 @@ use vector_lib::{
         BufferType, WhenFull,
         topology::{
             builder::TopologyBuilder,
-            channel::{BufferReceiver, BufferSender, ChannelMetricMetadata},
+            channel::{BufferReceiver, BufferSender, ChannelMetricMetadata, LimitedReceiver},
         },
     },
     internal_event::{self, CountByteSize, EventsSent, InternalEventHandle as _, Registered},
+    latency::LatencyRecorder,
     schema::Definition,
     source_sender::{CHUNK_SIZE, SourceSenderItem},
     transform::update_runtime_schema_definition,
@@ -272,34 +273,13 @@ impl<'a> Builder<'a> {
             let mut schema_definitions = HashMap::with_capacity(source_outputs.len());
 
             for output in source_outputs.into_iter() {
-                let mut rx = builder.add_source_output(output.clone(), key.clone());
+                let rx = builder.add_source_output(output.clone(), key.clone());
 
-                let (mut fanout, control) = Fanout::new();
+                let (fanout, control) = Fanout::new();
                 let source_type = source.inner.get_component_name();
                 let source = Arc::new(key.clone());
 
-                let pump = async move {
-                    debug!("Source pump starting.");
-
-                    while let Some(SourceSenderItem {
-                        events: mut array,
-                        send_reference,
-                    }) = rx.next().await
-                    {
-                        array.set_output_id(&source);
-                        array.set_source_type(source_type);
-                        fanout
-                            .send(array, Some(send_reference))
-                            .await
-                            .map_err(|e| {
-                                debug!("Source pump finished with an error.");
-                                TaskError::wrapped(e)
-                            })?;
-                    }
-
-                    debug!("Source pump finished normally.");
-                    Ok(TaskOutput::Source)
-                };
+                let pump = run_source_output_pump(rx, fanout, source, source_type);
 
                 pumps.push(pump.instrument(span.clone()));
                 controls.insert(
@@ -536,7 +516,7 @@ impl<'a> Builder<'a> {
                 .insert(key.clone(), (input_tx, node.inputs.clone()));
 
             let (transform_task, transform_outputs) =
-                build_transform(transform, node, input_rx, &self.utilization_registry);
+                self.build_transform(transform, node, input_rx);
 
             self.outputs.extend(transform_outputs);
             self.tasks.insert(key.clone(), transform_task);
@@ -730,6 +710,201 @@ impl<'a> Builder<'a> {
             self.detach_triggers.insert(key.clone(), trigger);
         }
     }
+
+    fn build_transform(
+        &self,
+        transform: Transform,
+        node: TransformNode,
+        input_rx: BufferReceiver<EventArray>,
+    ) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
+        match transform {
+            // TODO: avoid the double boxing for function transforms here
+            Transform::Function(t) => self.build_sync_transform(Box::new(t), node, input_rx),
+            Transform::Synchronous(t) => self.build_sync_transform(t, node, input_rx),
+            Transform::Task(t) => self.build_task_transform(
+                t,
+                input_rx,
+                node.input_details.data_type(),
+                node.typetag,
+                &node.key,
+                &node.outputs,
+            ),
+        }
+    }
+
+    fn build_sync_transform(
+        &self,
+        t: Box<dyn SyncTransform>,
+        node: TransformNode,
+        input_rx: BufferReceiver<EventArray>,
+    ) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
+        let (outputs, controls) = TransformOutputs::new(node.outputs, &node.key);
+
+        let sender = self
+            .utilization_registry
+            .add_component(node.key.clone(), gauge!("utilization"));
+        let runner = Runner::new(
+            t,
+            input_rx,
+            sender,
+            node.input_details.data_type(),
+            outputs,
+            LatencyRecorder::new(self.config.global.latency_ewma_alpha),
+        );
+        let transform = if node.enable_concurrency {
+            runner.run_concurrently().boxed()
+        } else {
+            runner.run_inline().boxed()
+        };
+
+        let transform = async move {
+            debug!("Synchronous transform starting.");
+
+            match transform.await {
+                Ok(v) => {
+                    debug!("Synchronous transform finished normally.");
+                    Ok(v)
+                }
+                Err(e) => {
+                    debug!("Synchronous transform finished with an error.");
+                    Err(e)
+                }
+            }
+        };
+
+        let mut output_controls = HashMap::new();
+        for (name, control) in controls {
+            let id = name
+                .map(|name| OutputId::from((&node.key, name)))
+                .unwrap_or_else(|| OutputId::from(&node.key));
+            output_controls.insert(id, control);
+        }
+
+        let task = Task::new(node.key.clone(), node.typetag, transform);
+
+        (task, output_controls)
+    }
+
+    fn build_task_transform(
+        &self,
+        t: Box<dyn TaskTransform<EventArray>>,
+        input_rx: BufferReceiver<EventArray>,
+        input_type: DataType,
+        typetag: &str,
+        key: &ComponentKey,
+        outputs: &[TransformOutput],
+    ) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
+        let (mut fanout, control) = Fanout::new();
+
+        let sender = self
+            .utilization_registry
+            .add_component(key.clone(), gauge!("utilization"));
+        let input_rx = wrap(sender, key.clone(), input_rx.into_stream());
+
+        let events_received = register!(EventsReceived);
+        let filtered = input_rx
+            .filter(move |events| ready(filter_events_type(events, input_type)))
+            .inspect(move |events| {
+                events_received.emit(CountByteSize(
+                    events.len(),
+                    events.estimated_json_encoded_size_of(),
+                ))
+            });
+        let events_sent = register!(EventsSent::from(internal_event::Output(None)));
+        let output_id = Arc::new(OutputId {
+            component: key.clone(),
+            port: None,
+        });
+        let latency_recorder = LatencyRecorder::new(self.config.global.latency_ewma_alpha);
+
+        // Task transforms can only write to the default output, so only a single schema def map is needed
+        let schema_definition_map = outputs
+            .iter()
+            .find(|x| x.port.is_none())
+            .expect("output for default port required for task transforms")
+            .log_schema_definitions
+            .clone()
+            .into_iter()
+            .map(|(key, value)| (key, Arc::new(value)))
+            .collect();
+
+        let stream = t
+            .transform(Box::pin(filtered))
+            .map(move |mut events| {
+                for event in events.iter_events_mut() {
+                    update_runtime_schema_definition(event, &output_id, &schema_definition_map);
+                }
+                let now = Instant::now();
+                latency_recorder.on_send(&mut events, now);
+                (events, now)
+            })
+            .inspect(move |(events, _): &(EventArray, Instant)| {
+                events_sent.emit(CountByteSize(
+                    events.len(),
+                    events.estimated_json_encoded_size_of(),
+                ));
+            });
+        let transform = async move {
+            debug!("Task transform starting.");
+
+            match fanout.send_stream(stream).await {
+                Ok(()) => {
+                    debug!("Task transform finished normally.");
+                    Ok(TaskOutput::Transform)
+                }
+                Err(e) => {
+                    debug!("Task transform finished with an error.");
+                    Err(TaskError::wrapped(e))
+                }
+            }
+        }
+        .boxed();
+
+        let mut outputs = HashMap::new();
+        outputs.insert(OutputId::from(key), control);
+
+        let task = Task::new(key.clone(), typetag, transform);
+
+        (task, outputs)
+    }
+}
+
+async fn run_source_output_pump(
+    mut rx: LimitedReceiver<SourceSenderItem>,
+    mut fanout: Fanout,
+    source: Arc<ComponentKey>,
+    source_type: &'static str,
+) -> TaskResult {
+    debug!("Source pump starting.");
+
+    while let Some(SourceSenderItem {
+        events: mut array,
+        send_reference,
+    }) = rx.next().await
+    {
+        // Even though we have a `send_reference` timestamp above, that reference time is when
+        // the events were enqueued in the `SourceSender`, not when they were pulled out of the
+        // `rx` stream on this end. Since those times can be quite different (due to blocking
+        // inherent to the fanout send operation), we set the `last_transform_timestamp` to the
+        // current time instead to get an accurate reference for when the events started waiting
+        // for the first transform.
+        let now = Instant::now();
+        array.for_each_metadata_mut(|metadata| {
+            metadata.set_source_id(Arc::clone(&source));
+            metadata.set_source_type(source_type);
+            metadata.set_last_transform_timestamp(now);
+        });
+        fanout
+            .send(array, Some(send_reference))
+            .await
+            .map_err(|e| {
+                debug!("Source pump finished with an error.");
+                TaskError::wrapped(e)
+            })?;
+    }
+
+    debug!("Source pump finished normally.");
+    Ok(TaskOutput::Source)
 }
 
 pub async fn reload_enrichment_tables(config: &Config) {
@@ -940,80 +1115,13 @@ impl TransformNode {
     }
 }
 
-fn build_transform(
-    transform: Transform,
-    node: TransformNode,
-    input_rx: BufferReceiver<EventArray>,
-    utilization_registry: &UtilizationRegistry,
-) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
-    match transform {
-        // TODO: avoid the double boxing for function transforms here
-        Transform::Function(t) => {
-            build_sync_transform(Box::new(t), node, input_rx, utilization_registry)
-        }
-        Transform::Synchronous(t) => build_sync_transform(t, node, input_rx, utilization_registry),
-        Transform::Task(t) => build_task_transform(
-            t,
-            input_rx,
-            node.input_details.data_type(),
-            node.typetag,
-            &node.key,
-            &node.outputs,
-            utilization_registry,
-        ),
-    }
-}
-
-fn build_sync_transform(
-    t: Box<dyn SyncTransform>,
-    node: TransformNode,
-    input_rx: BufferReceiver<EventArray>,
-    utilization_registry: &UtilizationRegistry,
-) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
-    let (outputs, controls) = TransformOutputs::new(node.outputs, &node.key);
-
-    let sender = utilization_registry.add_component(node.key.clone(), gauge!("utilization"));
-    let runner = Runner::new(t, input_rx, sender, node.input_details.data_type(), outputs);
-    let transform = if node.enable_concurrency {
-        runner.run_concurrently().boxed()
-    } else {
-        runner.run_inline().boxed()
-    };
-
-    let transform = async move {
-        debug!("Synchronous transform starting.");
-
-        match transform.await {
-            Ok(v) => {
-                debug!("Synchronous transform finished normally.");
-                Ok(v)
-            }
-            Err(e) => {
-                debug!("Synchronous transform finished with an error.");
-                Err(e)
-            }
-        }
-    };
-
-    let mut output_controls = HashMap::new();
-    for (name, control) in controls {
-        let id = name
-            .map(|name| OutputId::from((&node.key, name)))
-            .unwrap_or_else(|| OutputId::from(&node.key));
-        output_controls.insert(id, control);
-    }
-
-    let task = Task::new(node.key.clone(), node.typetag, transform);
-
-    (task, output_controls)
-}
-
 struct Runner {
     transform: Box<dyn SyncTransform>,
     input_rx: Option<BufferReceiver<EventArray>>,
     input_type: DataType,
     outputs: TransformOutputs,
     timer_tx: UtilizationComponentSender,
+    latency_recorder: LatencyRecorder,
     events_received: Registered<EventsReceived>,
 }
 
@@ -1024,6 +1132,7 @@ impl Runner {
         timer_tx: UtilizationComponentSender,
         input_type: DataType,
         outputs: TransformOutputs,
+        latency_recorder: LatencyRecorder,
     ) -> Self {
         Self {
             transform,
@@ -1031,6 +1140,7 @@ impl Runner {
             input_type,
             outputs,
             timer_tx,
+            latency_recorder,
             events_received: register!(EventsReceived),
         }
     }
@@ -1046,6 +1156,8 @@ impl Runner {
 
     async fn send_outputs(&mut self, outputs_buf: &mut TransformOutputsBuf) -> crate::Result<()> {
         self.timer_tx.try_send_start_wait();
+        let now = Instant::now();
+        outputs_buf.for_each_array_mut(|array| self.latency_recorder.on_send(array, now));
         self.outputs.send(outputs_buf).await
     }
 
@@ -1095,8 +1207,7 @@ impl Runner {
 
                 result = in_flight.next(), if !in_flight.is_empty() => {
                     match result {
-                        Some(Ok(outputs_buf)) => {
-                            let mut outputs_buf: TransformOutputsBuf = outputs_buf;
+                        Some(Ok(mut outputs_buf)) => {
                             self.send_outputs(&mut outputs_buf).await
                                 .map_err(TaskError::wrapped)?;
                         }
@@ -1141,81 +1252,3 @@ impl Runner {
         Ok(TaskOutput::Transform)
     }
 }
-
-fn build_task_transform(
-    t: Box<dyn TaskTransform<EventArray>>,
-    input_rx: BufferReceiver<EventArray>,
-    input_type: DataType,
-    typetag: &str,
-    key: &ComponentKey,
-    outputs: &[TransformOutput],
-    utilization_registry: &UtilizationRegistry,
-) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
-    let (mut fanout, control) = Fanout::new();
-
-    let sender = utilization_registry.add_component(key.clone(), gauge!("utilization"));
-    let input_rx = wrap(sender, key.clone(), input_rx.into_stream());
-
-    let events_received = register!(EventsReceived);
-    let filtered = input_rx
-        .filter(move |events| ready(filter_events_type(events, input_type)))
-        .inspect(move |events| {
-            events_received.emit(CountByteSize(
-                events.len(),
-                events.estimated_json_encoded_size_of(),
-            ))
-        });
-    let events_sent = register!(EventsSent::from(internal_event::Output(None)));
-    let output_id = Arc::new(OutputId {
-        component: key.clone(),
-        port: None,
-    });
-
-    // Task transforms can only write to the default output, so only a single schema def map is needed
-    let schema_definition_map = outputs
-        .iter()
-        .find(|x| x.port.is_none())
-        .expect("output for default port required for task transforms")
-        .log_schema_definitions
-        .clone()
-        .into_iter()
-        .map(|(key, value)| (key, Arc::new(value)))
-        .collect();
-
-    let stream = t
-        .transform(Box::pin(filtered))
-        .map(move |mut events| {
-            for event in events.iter_events_mut() {
-                update_runtime_schema_definition(event, &output_id, &schema_definition_map);
-            }
-            (events, Instant::now())
-        })
-        .inspect(move |(events, _): &(EventArray, Instant)| {
-            events_sent.emit(CountByteSize(
-                events.len(),
-                events.estimated_json_encoded_size_of(),
-            ));
-        });
-    let transform = async move {
-        debug!("Task transform starting.");
-
-        match fanout.send_stream(stream).await {
-            Ok(()) => {
-                debug!("Task transform finished normally.");
-                Ok(TaskOutput::Transform)
-            }
-            Err(e) => {
-                debug!("Task transform finished with an error.");
-                Err(TaskError::wrapped(e))
-            }
-        }
-    }
-    .boxed();
-
-    let mut outputs = HashMap::new();
-    outputs.insert(OutputId::from(key), control);
-
-    let task = Task::new(key.clone(), typetag, transform);
-
-    (task, outputs)
-}
diff --git a/src/topology/test/compliance.rs b/src/topology/test/compliance.rs
index 5f3783055..b54fa3af2 100644
--- a/src/topology/test/compliance.rs
+++ b/src/topology/test/compliance.rs
@@ -19,6 +19,19 @@ use crate::{
     topology::RunningTopology,
 };
 
+const TEST_SOURCE_COMPONENT_ID: &str = "in";
+const TEST_UPSTREAM_COMPONENT_ID: &str = "transform";
+const TEST_SOURCE_TYPE: &str = "unit_test";
+
+fn set_expected_source_metadata(event: &mut Event) {
+    event.set_source_id(Arc::new(ComponentKey::from(TEST_SOURCE_COMPONENT_ID)));
+    event.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
+    event.set_source_type(TEST_SOURCE_TYPE);
+    event
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+}
+
 async fn create_topology(
     event: Event,
     transform_type: TransformType,
@@ -58,11 +71,7 @@ async fn test_function_transform_single_event() {
         let mut events = events.into_events().collect::<Vec<_>>();
         assert_eq!(events.len(), 1);
 
-        original_event.set_source_id(Arc::new(ComponentKey::from("in")));
-        original_event.set_upstream_id(Arc::new(OutputId::from("transform")));
-        original_event
-            .metadata_mut()
-            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+        set_expected_source_metadata(&mut original_event);
 
         let event = events.remove(0);
         assert_eq!(original_event, event);
@@ -83,11 +92,7 @@ async fn test_sync_transform_single_event() {
         let mut events = events.into_events().collect::<Vec<_>>();
         assert_eq!(events.len(), 1);
 
-        original_event.set_source_id(Arc::new(ComponentKey::from("in")));
-        original_event.set_upstream_id(Arc::new(OutputId::from("transform")));
-        original_event
-            .metadata_mut()
-            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+        set_expected_source_metadata(&mut original_event);
 
         let event = events.remove(0);
         assert_eq!(original_event, event);
@@ -107,11 +112,7 @@ async fn test_task_transform_single_event() {
         let mut events = events.into_events().collect::<Vec<_>>();
         assert_eq!(events.len(), 1);
 
-        original_event.set_source_id(Arc::new(ComponentKey::from("in")));
-        original_event.set_upstream_id(Arc::new(OutputId::from("transform")));
-        original_event
-            .metadata_mut()
-            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+        set_expected_source_metadata(&mut original_event);
 
         let event = events.remove(0);
         assert_eq!(original_event, event);
diff --git a/src/topology/test/latency_metrics.rs b/src/topology/test/latency_metrics.rs
new file mode 100644
index 000000000..6b4dac1a4
--- /dev/null
+++ b/src/topology/test/latency_metrics.rs
@@ -0,0 +1,147 @@
+use std::time::Instant;
+use tokio::{
+    sync::oneshot,
+    time::{Duration, timeout},
+};
+use vector_lib::metrics::Controller;
+
+use crate::{
+    config::Config,
+    event::{Event, LogEvent, Metric, MetricValue},
+    test_util::{
+        mock::{
+            basic_source,
+            sinks::CompletionSinkConfig,
+            transforms::{NoopTransformConfig, TransformType},
+        },
+        start_topology, trace_init,
+    },
+};
+
+const EVENT_COUNT: usize = 100;
+const TRANSFORM_DELAY_MS: u64 = 10;
+const SOURCE_ID: &str = "latency_source";
+const TRANSFORM_ID: &str = "latency_delay";
+const TRANSFORM_TYPE: &str = "test_noop";
+const TRANSFORM_KIND: &str = "transform";
+const SINK_ID: &str = "latency_sink";
+
+struct LatencyTestRun {
+    metrics: Vec<Metric>,
+    elapsed_time: f64,
+}
+
+#[tokio::test]
+async fn component_latency_metrics_emitted() {
+    let run = run_latency_topology().await;
+
+    assert_histogram_count(
+        &run.metrics,
+        "component_latency_seconds",
+        has_component_tags,
+    );
+    assert_gauge_range(
+        &run.metrics,
+        "component_latency_mean_seconds",
+        has_component_tags,
+        TRANSFORM_DELAY_MS as f64 / 1000.0,
+        run.elapsed_time,
+    );
+}
+
+async fn run_latency_topology() -> LatencyTestRun {
+    trace_init();
+
+    let controller = Controller::get().expect("metrics controller");
+    controller.reset();
+
+    let (mut source_tx, source_config) = basic_source();
+    let transform_config =
+        NoopTransformConfig::from(TransformType::Task).with_delay_ms(TRANSFORM_DELAY_MS);
+    let (sink_done_tx, sink_done_rx) = oneshot::channel();
+    let sink_config = CompletionSinkConfig::new(EVENT_COUNT, sink_done_tx);
+
+    let mut config = Config::builder();
+    config.add_source(SOURCE_ID, source_config);
+    config.add_transform(TRANSFORM_ID, &[SOURCE_ID], transform_config);
+    config.add_sink(SINK_ID, &[TRANSFORM_ID], sink_config);
+
+    let start_time = Instant::now();
+    let (topology, _) = start_topology(config.build().unwrap(), false).await;
+
+    for idx in 0..EVENT_COUNT {
+        let event = Event::Log(LogEvent::from(format!("payload-{idx}")));
+        source_tx.send_event(event).await.unwrap();
+    }
+
+    drop(source_tx);
+
+    let completed = timeout(Duration::from_secs(5), sink_done_rx)
+        .await
+        .expect("timed out waiting for completion sink to finish")
+        .expect("completion sink sender dropped");
+    assert!(
+        completed,
+        "completion sink finished before receiving all events"
+    );
+
+    topology.stop().await;
+    let elapsed_time = start_time.elapsed().as_secs_f64();
+
+    LatencyTestRun {
+        metrics: controller.capture_metrics(),
+        elapsed_time,
+    }
+}
+
+fn assert_histogram_count(metrics: &[Metric], metric_name: &str, tags_match: fn(&Metric) -> bool) {
+    let histogram = metrics
+        .iter()
+        .find(|metric| metric.name() == metric_name && tags_match(metric))
+        .unwrap_or_else(|| panic!("{metric_name} histogram missing"));
+
+    match histogram.value() {
+        MetricValue::AggregatedHistogram { count, .. } => {
+            assert_eq!(
+                *count, EVENT_COUNT as u64,
+                "histogram count should match number of events"
+            );
+        }
+        other => panic!("expected aggregated histogram, got {other:?}"),
+    }
+}
+
+fn assert_gauge_range(
+    metrics: &[Metric],
+    metric_name: &str,
+    tags_match: fn(&Metric) -> bool,
+    expected_min: f64,
+    elapsed_time: f64,
+) {
+    let gauge = metrics
+        .iter()
+        .find(|metric| metric.name() == metric_name && tags_match(metric))
+        .unwrap_or_else(|| panic!("{metric_name} gauge missing"));
+
+    match gauge.value() {
+        MetricValue::Gauge { value } => {
+            assert!(
+                *value >= expected_min,
+                "expected mean latency to be >= {expected_min}, got {value}"
+            );
+            assert!(
+                *value < elapsed_time,
+                "expected mean latency ({value}) to be less than elapsed time ({elapsed_time})"
+            );
+        }
+        other => panic!("expected gauge metric, got {other:?}"),
+    }
+}
+
+fn has_component_tags(metric: &Metric) -> bool {
+    metric.tags().is_some_and(|tags| {
+        tags.get("component_id") == Some(TRANSFORM_ID)
+            && tags.get("component_type") == Some(TRANSFORM_TYPE)
+            && tags.get("component_kind") == Some(TRANSFORM_KIND)
+    })
+}
diff --git a/src/topology/test/mod.rs b/src/topology/test/mod.rs
index 738a73125..f4ccf9741 100644
--- a/src/topology/test/mod.rs
+++ b/src/topology/test/mod.rs
@@ -39,6 +39,7 @@ mod crash;
 mod doesnt_reload;
 #[cfg(all(feature = "sources-http_server", feature = "sinks-http"))]
 mod end_to_end;
+mod latency_metrics;
 #[cfg(all(
     feature = "sources-prometheus",
     feature = "sinks-prometheus",
@@ -84,6 +85,18 @@ fn into_message_stream(array: SourceSenderItem) -> impl futures::Stream<Item = S
     stream::iter(array.events.into_events().map(into_message))
 }
 
+const TEST_UPSTREAM_COMPONENT_ID: &str = "test";
+const TEST_BASIC_SOURCE_TYPE: &str = "test_basic";
+
+fn set_expected_source_metadata(event: &mut Event, source_component_id: &str) {
+    event.set_source_id(Arc::new(ComponentKey::from(source_component_id)));
+    event.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
+    event.set_source_type(TEST_BASIC_SOURCE_TYPE);
+    event
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+}
+
 #[tokio::test]
 async fn topology_shutdown_while_active() {
     trace_init();
@@ -158,11 +171,7 @@ async fn topology_source_and_sink() {
 
     let res = out1.flat_map(into_event_stream).collect::<Vec<_>>().await;
 
-    event.set_source_id(Arc::new(ComponentKey::from("in1")));
-    event.set_upstream_id(Arc::new(OutputId::from("test")));
-    event
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event, "in1");
 
     assert_eq!(vec![event], res);
 }
@@ -195,18 +204,8 @@ async fn topology_multiple_sources() {
 
     topology.stop().await;
 
-    event1.set_source_id(Arc::new(ComponentKey::from("in1")));
-    event2.set_source_id(Arc::new(ComponentKey::from("in2")));
-
-    event1.set_upstream_id(Arc::new(OutputId::from("test")));
-    event1
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
-
-    event2.set_upstream_id(Arc::new(OutputId::from("test")));
-    event2
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event1, "in1");
+    set_expected_source_metadata(&mut event2, "in2");
 
     assert_eq!(out_event1, Some(event1.into()));
     assert_eq!(out_event2, Some(event2.into()));
@@ -241,12 +240,7 @@ async fn topology_multiple_sinks() {
     let res2 = out2.flat_map(into_event_stream).collect::<Vec<_>>().await;
 
     // We should see that both sinks got the exact same event:
-    event.set_source_id(Arc::new(ComponentKey::from("in1")));
-
-    event.set_upstream_id(Arc::new(OutputId::from("test")));
-    event
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event, "in1");
 
     let expected = vec![event];
     assert_eq!(expected, res1);
@@ -321,12 +315,7 @@ async fn topology_remove_one_source() {
     drop(in2);
     topology.stop().await;
 
-    event1.set_source_id(Arc::new(ComponentKey::from("in1")));
-
-    event1.set_upstream_id(Arc::new(OutputId::from("test")));
-    event1
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event1, "in1");
 
     let res = h_out1.await.unwrap();
     assert_eq!(vec![event1], res);
@@ -365,12 +354,7 @@ async fn topology_remove_one_sink() {
     let res1 = out1.flat_map(into_event_stream).collect::<Vec<_>>().await;
     let res2 = out2.flat_map(into_event_stream).collect::<Vec<_>>().await;
 
-    event.set_source_id(Arc::new(ComponentKey::from("in1")));
-
-    event.set_upstream_id(Arc::new(OutputId::from("test")));
-    event
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event, "in1");
 
     assert_eq!(vec![event], res1);
     assert_eq!(Vec::<Event>::new(), res2);
@@ -481,11 +465,7 @@ async fn topology_swap_source() {
     // as we've removed it from the topology prior to the sends.
     assert_eq!(Vec::<Event>::new(), res1);
 
-    event2.set_source_id(Arc::new(ComponentKey::from("in2")));
-    event2.set_upstream_id(Arc::new(OutputId::from("test")));
-    event2
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event2, "in2");
 
     assert_eq!(vec![event2], res2);
 }
@@ -598,11 +578,7 @@ async fn topology_swap_sink() {
     // the new sink, which _was_ rebuilt:
     assert_eq!(Vec::<Event>::new(), res1);
 
-    event1.set_source_id(Arc::new(ComponentKey::from("in1")));
-    event1.set_upstream_id(Arc::new(OutputId::from("test")));
-    event1
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event1, "in1");
     assert_eq!(vec![event1], res2);
 }
 
@@ -710,17 +686,8 @@ async fn topology_rebuild_connected() {
 
     let res = h_out1.await.unwrap();
 
-    event1.set_source_id(Arc::new(ComponentKey::from("in1")));
-    event2.set_source_id(Arc::new(ComponentKey::from("in1")));
-
-    event1.set_upstream_id(Arc::new(OutputId::from("test")));
-    event2.set_upstream_id(Arc::new(OutputId::from("test")));
-    event1
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
-    event2
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event1, "in1");
+    set_expected_source_metadata(&mut event2, "in1");
 
     assert_eq!(vec![event1, event2], res);
 }
@@ -773,11 +740,7 @@ async fn topology_rebuild_connected_transform() {
     let res2 = h_out2.await.unwrap();
     assert_eq!(Vec::<Event>::new(), res1);
 
-    event.set_source_id(Arc::new(ComponentKey::from("in1")));
-    event.set_upstream_id(Arc::new(OutputId::from("test")));
-    event
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    set_expected_source_metadata(&mut event, "in1");
 
     assert_eq!(vec![event], res2);
 }
diff --git a/src/transforms/dedupe/config.rs b/src/transforms/dedupe/config.rs
index 2161c656f..5a4e728e7 100644
--- a/src/transforms/dedupe/config.rs
+++ b/src/transforms/dedupe/config.rs
@@ -104,6 +104,20 @@ mod tests {
         },
     };
 
+    const TEST_SOURCE_COMPONENT_ID: &str = "in";
+    const TEST_UPSTREAM_COMPONENT_ID: &str = "transform";
+    const TEST_SOURCE_TYPE: &str = "unit_test_stream";
+
+    fn set_expected_metadata(event: &mut Event) {
+        event.set_source_id(Arc::new(ComponentKey::from(TEST_SOURCE_COMPONENT_ID)));
+        event.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
+        event.set_source_type(TEST_SOURCE_TYPE);
+        // The schema definition is copied from the source for dedupe.
+        event
+            .metadata_mut()
+            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    }
+
     #[test]
     fn generate_config() {
         crate::test_util::test_generate_config::<DedupeConfig>();
@@ -181,12 +195,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
-            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event1
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             // Second event differs in matched field so should be output even though it
@@ -194,12 +203,7 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event2.set_source_id(Arc::new(ComponentKey::from("in")));
-            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event2
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event2);
             assert_eq!(new_event, event2);
 
             // Third event has the same value for "matched" as first event, so it should be dropped.
@@ -241,12 +245,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
-            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event1
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             // Second event has a different matched field name with the same value,
@@ -254,12 +253,7 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event2.set_source_id(Arc::new(ComponentKey::from("in")));
-            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event2
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event2);
             assert_eq!(new_event, event2);
 
             drop(tx);
@@ -304,12 +298,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
-            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event1
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             // Second event is the same just with different field order, so it
@@ -354,13 +343,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
-            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
-
-            // the schema definition is copied from the source for dedupe
-            event1
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             // Second event gets output because it's not a dupe. This causes the first
@@ -368,12 +351,7 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event2.set_source_id(Arc::new(ComponentKey::from("in")));
-            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event2
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event2);
 
             assert_eq!(new_event, event2);
 
@@ -382,7 +360,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             drop(tx);
@@ -432,13 +410,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
-            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
-
-            // the schema definition is copied from the source for dedupe
-            event1
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             // Second time the event gets dropped because it's a dupe.
@@ -450,7 +422,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             drop(tx);
@@ -491,12 +463,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
-            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event1
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             // Second event should also get passed through even though the string
@@ -504,12 +471,7 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event2.set_source_id(Arc::new(ComponentKey::from("in")));
-            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event2
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event2);
             assert_eq!(new_event, event2);
 
             drop(tx);
@@ -554,12 +516,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
-            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event1
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             // Second event should also get passed through even though the string
@@ -567,12 +524,7 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event2.set_source_id(Arc::new(ComponentKey::from("in")));
-            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event2
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event2);
             assert_eq!(new_event, event2);
 
             drop(tx);
@@ -610,12 +562,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event1.set_source_id(Arc::new(ComponentKey::from("in")));
-            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event1
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event1);
             assert_eq!(new_event, event1);
 
             // Second event should also get passed through as null is different than
@@ -623,12 +570,7 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            event2.set_source_id(Arc::new(ComponentKey::from("in")));
-            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
-            // the schema definition is copied from the source for dedupe
-            event2
-                .metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut event2);
             assert_eq!(new_event, event2);
 
             drop(tx);
diff --git a/src/transforms/filter.rs b/src/transforms/filter.rs
index a2e8318b4..152681e9e 100644
--- a/src/transforms/filter.rs
+++ b/src/transforms/filter.rs
@@ -116,6 +116,19 @@ mod test {
         transforms::test::create_topology,
     };
 
+    const TEST_SOURCE_COMPONENT_ID: &str = "in";
+    const TEST_UPSTREAM_COMPONENT_ID: &str = "transform";
+    const TEST_SOURCE_TYPE: &str = "unit_test_stream";
+
+    fn set_expected_metadata(event: &mut Event) {
+        event.set_source_id(Arc::new(ComponentKey::from(TEST_SOURCE_COMPONENT_ID)));
+        event.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
+        event.set_source_type(TEST_SOURCE_TYPE);
+        event
+            .metadata_mut()
+            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    }
+
     #[test]
     fn generate_config() {
         crate::test_util::test_generate_config::<super::FilterConfig>();
@@ -133,10 +146,7 @@ mod test {
             let mut log = Event::from(LogEvent::from("message"));
             tx.send(log.clone()).await.unwrap();
 
-            log.set_source_id(Arc::new(ComponentKey::from("in")));
-            log.set_upstream_id(Arc::new(OutputId::from("transform")));
-            log.metadata_mut()
-                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+            set_expected_metadata(&mut log);
 
             assert_eq!(out.recv().await.unwrap(), log);
 
diff --git a/src/transforms/log_to_metric.rs b/src/transforms/log_to_metric.rs
index a7fb90e1c..a9bb70790 100644
--- a/src/transforms/log_to_metric.rs
+++ b/src/transforms/log_to_metric.rs
@@ -959,6 +959,7 @@ mod tests {
     use std::{sync::Arc, time::Duration};
 
     use chrono::{DateTime, Timelike, Utc, offset::TimeZone};
+    use similar_asserts::assert_eq;
     use tokio::sync::mpsc;
     use tokio_stream::wrappers::ReceiverStream;
     use vector_lib::{
@@ -978,6 +979,11 @@ mod tests {
         transforms::test::create_topology,
     };
 
+    const TEST_SOURCE_COMPONENT_ID: &str = "in";
+    const TEST_UPSTREAM_COMPONENT_ID: &str = "transform";
+    const TEST_SOURCE_TYPE: &str = "unit_test_stream";
+    const TEST_NAMESPACE: &str = "test_namespace";
+
     #[test]
     fn generate_config() {
         crate::test_util::test_generate_config::<LogToMetricConfig>();
@@ -1006,6 +1012,12 @@ mod tests {
         log
     }
 
+    fn set_test_source_metadata(metadata: &mut EventMetadata) {
+        metadata.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
+        metadata.set_source_id(Arc::new(ComponentKey::from(TEST_SOURCE_COMPONENT_ID)));
+        metadata.set_source_type(TEST_SOURCE_TYPE);
+    }
+
     async fn do_transform(config: LogToMetricConfig, event: Event) -> Option<Event> {
         assert_transform_compliance(async move {
             let (tx, rx) = mpsc::channel(1);
@@ -1072,8 +1084,7 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
         let metric = do_transform(config, event).await.unwrap();
 
         assert_eq!(
@@ -1115,8 +1126,7 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1170,8 +1180,7 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1226,8 +1235,7 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap().into_metric();
         let tags = metric.tags().expect("Metric should have tags");
@@ -1351,8 +1359,7 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1407,8 +1414,7 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
         let metric = do_transform(config, event).await.unwrap();
 
         assert_eq!(
@@ -1448,8 +1454,7 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1490,8 +1495,7 @@ mod tests {
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
 
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1586,8 +1590,7 @@ mod tests {
 
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let output = do_transform_multiple_events(config, event, 2).await;
 
@@ -1652,8 +1655,7 @@ mod tests {
 
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let output = do_transform_multiple_events(config, event, 2).await;
 
@@ -1706,8 +1708,7 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1748,8 +1749,7 @@ mod tests {
 
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1791,8 +1791,7 @@ mod tests {
 
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        set_test_source_metadata(&mut metadata);
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1814,7 +1813,7 @@ mod tests {
     //  Metric Metadata Tests
     //
     fn create_log_event(json_str: &str) -> Event {
-        create_log_event_with_namespace(json_str, Some("test_namespace"))
+        create_log_event_with_namespace(json_str, Some(TEST_NAMESPACE))
     }
 
     fn create_log_event_with_namespace(json_str: &str, namespace: Option<&str>) -> Event {
@@ -1827,8 +1826,7 @@ mod tests {
         }
 
         let mut metadata = EventMetadata::default();
-        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
-        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        set_test_source_metadata(&mut metadata);
 
         Event::Log(LogEvent::from_parts(log_value, metadata.clone()))
     }
@@ -1861,7 +1859,7 @@ mod tests {
                 MetricValue::Gauge { value: 990.0 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some("test_namespace"))
+            .with_namespace(Some(TEST_NAMESPACE))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -1938,7 +1936,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some("test_namespace"))
+            .with_namespace(Some(TEST_NAMESPACE))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -1997,7 +1995,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some("test_namespace"))
+            .with_namespace(Some(TEST_NAMESPACE))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -2056,7 +2054,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some("test_namespace"))
+            .with_namespace(Some(TEST_NAMESPACE))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -2117,7 +2115,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some("test_namespace"))
+            .with_namespace(Some(TEST_NAMESPACE))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -2154,7 +2152,7 @@ mod tests {
                 MetricValue::Counter { value: 10.0 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some("test_namespace"))
+            .with_namespace(Some(TEST_NAMESPACE))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -2193,7 +2191,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some("test_namespace"))
+            .with_namespace(Some(TEST_NAMESPACE))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
diff --git a/website/cue/reference/components/sinks/prometheus_remote_write.cue b/website/cue/reference/components/sinks/prometheus_remote_write.cue
index 92904077b..5b9bb9267 100644
--- a/website/cue/reference/components/sinks/prometheus_remote_write.cue
+++ b/website/cue/reference/components/sinks/prometheus_remote_write.cue
@@ -15,7 +15,10 @@ components: sinks: prometheus_remote_write: {
 	features: {
 		auto_generated:   true
 		acknowledgements: true
-		healthcheck: enabled: true
+		healthcheck: {
+			enabled:  true
+			uses_uri: true
+		}
 		send: {
 			batch: {
 				enabled:      true
diff --git a/website/cue/reference/components/sources/internal_metrics.cue b/website/cue/reference/components/sources/internal_metrics.cue
index 946544a70..7aa7834cd 100644
--- a/website/cue/reference/components/sources/internal_metrics.cue
+++ b/website/cue/reference/components/sources/internal_metrics.cue
@@ -273,6 +273,28 @@ components: sources: internal_metrics: {
 				reason: _reason
 			}
 		}
+		component_latency_seconds: {
+			description: """
+				The elapsed time, in fractional seconds, that an event spends in a single transform.
+
+				This includes both the time spent queued in the transform’s input buffer and the time spent executing the transform itself.
+				"""
+			type:              "histogram"
+			default_namespace: "vector"
+			tags:              _internal_metrics_tags
+		}
+		component_latency_mean_seconds: {
+			description: """
+				The mean elapsed time, in fractional seconds, that an event spends in a single transform.
+
+				This includes both the time spent queued in the transform’s input buffer and the time spent executing the transform itself.
+
+				This value is smoothed over time using an exponentially weighted moving average (EWMA).
+				"""
+			type:              "gauge"
+			default_namespace: "vector"
+			tags:              _internal_metrics_tags
+		}
 		buffer_byte_size: {
 			description:        "The number of bytes currently in the buffer."
 			type:               "gauge"
diff --git a/website/cue/reference/components/transforms.cue b/website/cue/reference/components/transforms.cue
index 6c907ecc7..e06591510 100644
--- a/website/cue/reference/components/transforms.cue
+++ b/website/cue/reference/components/transforms.cue
@@ -15,11 +15,13 @@ components: transforms: [Name=string]: {
 	telemetry: metrics: {
 		component_discarded_events_total:     components.sources.internal_metrics.output.metrics.component_discarded_events_total
 		component_errors_total:               components.sources.internal_metrics.output.metrics.component_errors_total
+		component_latency_mean_seconds:       components.sources.internal_metrics.output.metrics.component_latency_mean_seconds
+		component_latency_seconds:            components.sources.internal_metrics.output.metrics.component_latency_seconds
+		component_received_event_bytes_total: components.sources.internal_metrics.output.metrics.component_received_event_bytes_total
 		component_received_events_count:      components.sources.internal_metrics.output.metrics.component_received_events_count
 		component_received_events_total:      components.sources.internal_metrics.output.metrics.component_received_events_total
-		component_received_event_bytes_total: components.sources.internal_metrics.output.metrics.component_received_event_bytes_total
-		component_sent_events_total:          components.sources.internal_metrics.output.metrics.component_sent_events_total
 		component_sent_event_bytes_total:     components.sources.internal_metrics.output.metrics.component_sent_event_bytes_total
+		component_sent_events_total:          components.sources.internal_metrics.output.metrics.component_sent_events_total
 		transform_buffer_max_byte_size:       components.sources.internal_metrics.output.metrics.transform_buffer_max_byte_size
 		transform_buffer_max_event_size:      components.sources.internal_metrics.output.metrics.transform_buffer_max_event_size
 		transform_buffer_max_size_bytes:      components.sources.internal_metrics.output.metrics.transform_buffer_max_size_bytes
diff --git a/website/cue/reference/generated/configuration.cue b/website/cue/reference/generated/configuration.cue
index 6bf097aa7..4c6678bde 100644
--- a/website/cue/reference/generated/configuration.cue
+++ b/website/cue/reference/generated/configuration.cue
@@ -706,11 +706,11 @@ generated: configuration: configuration: {
 			The alpha value for the exponential weighted moving average (EWMA) of source and transform
 			buffer utilization metrics.
 
-			This value specifies how much of the existing value is retained when each update is made.
-			Values closer to 1.0 result in the value adjusting slower to changes. The default value of
-			0.9 is equivalent to a "half life" of 6-7 measurements.
+			This controls how quickly the `*_buffer_utilization_mean` gauges respond to new
+			observations. Values closer to 1.0 retain more of the previous value, leading to slower
+			adjustments. The default value of 0.9 is equivalent to a "half life" of 6-7 measurements.
 
-			Must be between 0 and 1 exclusive (0 < alpha < 1).
+			Must be between 0 and 1 exclusively (0 < alpha < 1).
 			"""
 		required: false
 		type: float: {}
@@ -832,6 +832,20 @@ generated: configuration: configuration: {
 		required: false
 		type: float: {}
 	}
+	latency_ewma_alpha: {
+		description: """
+			The alpha value for the exponential weighted moving average (EWMA) of transform latency
+			metrics.
+
+			This controls how quickly the `component_latency_mean_seconds` gauge responds to new
+			observations. Values closer to 1.0 retain more of the previous value, leading to slower
+			adjustments. The default value of 0.9 is equivalent to a "half life" of 6-7 measurements.
+
+			Must be between 0 and 1 exclusively (0 < alpha < 1).
+			"""
+		required: false
+		type: float: {}
+	}
 	log_schema: {
 		common: false
 		description: """
diff --git a/website/package.json b/website/package.json
index 81b4c64bd..fec13b066 100644
--- a/website/package.json
+++ b/website/package.json
@@ -2,9 +2,9 @@
   "type": "module",
   "private": true,
   "scripts": {
-    "typesense-index": "ts-node scripts/typesense-index.ts",
-    "typesense-sync:preview": "NODE_ENV=preview ts-node scripts/typesense-sync.ts",
-    "typesense-sync:production": "NODE_ENV=production ts-node scripts/typesense-sync.ts",
+    "typesense-index": "tsx scripts/typesense-index.ts",
+    "typesense-sync:preview": "NODE_ENV=preview tsx scripts/typesense-sync.ts",
+    "typesense-sync:production": "NODE_ENV=production tsx scripts/typesense-sync.ts",
     "config-examples": "node scripts/create-config-examples.js",
     "lighthouse": "lighthouse --output html --output-path ./reports/lighthouse.html --view https://vector.dev"
   },
@@ -54,10 +54,10 @@
     "tailwindcss": "^2.2.4",
     "tocbot": "^4.12.2",
     "topojson-client": "^3.1.0",
-    "ts-node": "^10.9.2",
+    "tsx": "^4.21.0",
     "typescript": "^5.9.3",
     "typesense": "^1.8.2",
-    "typesense-sync": "https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v1.1.0.tgz"
+    "typesense-sync": "https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v2.0.0.tgz"
   },
   "browserslist": [
     "since 2017-06"
diff --git a/website/scripts/typesense-index.ts b/website/scripts/typesense-index.ts
index a80f1f343..7b14f07d0 100644
--- a/website/scripts/typesense-index.ts
+++ b/website/scripts/typesense-index.ts
@@ -1,11 +1,15 @@
 import chalk from "chalk";
-import cheerio from "cheerio";
+import * as cheerio from "cheerio";
 import { Element } from "domhandler";
 import dotEnv from "dotenv-defaults";
 import fs from "fs";
 import glob from "glob-promise";
 import path from "path";
 import crypto from "crypto";
+import { fileURLToPath } from "url";
+
+const __filename = fileURLToPath(import.meta.url);
+const __dirname = path.dirname(__filename);
 
 dotEnv.config();
 
diff --git a/website/scripts/typesense-sync.ts b/website/scripts/typesense-sync.ts
index fd08850d1..65f867de7 100644
--- a/website/scripts/typesense-sync.ts
+++ b/website/scripts/typesense-sync.ts
@@ -1,19 +1,19 @@
-const { typesenseSync } = require('typesense-sync');
-const { saveSettings } = require('typesense-sync/settings');
-const tsConfig = require('../typesense.config.json');
+// Will be running in the CI/CD pipeline
+import process from 'node:process';
+import { typesenseSync } from 'typesense-sync';
+const configFilePath = './typesense.config.json';
 
-const syncCollection = async () => {
-  const promises: Promise<any>[] = []
+const config = {
+  configFilePath,
+  shouldSaveSettings: true,
+};
 
-  for (const collection of tsConfig.collections) {
-    console.log(`Updating collection ${collection.name}`)
-    promises.push(typesenseSync(collection.name, collection.file_path))
-  }
+typesenseSync(config)
+  .then(() => {
+    console.log('Typesense sync completed');
+  })
+  .catch((err) => {
+    console.error('An error occurred', err);
+    process.exit(1);
+  });
 
-  return await Promise.all(promises)
-}
-
-saveSettings()
-  .then(() => syncCollection())
-  .then(() => console.log('Typesense sync completed'))
-  .catch(error => console.log('An error occurred', error))
diff --git a/website/typesense.config.json b/website/typesense.config.json
index 42b3acf11..89b40e66a 100644
--- a/website/typesense.config.json
+++ b/website/typesense.config.json
@@ -15,7 +15,9 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string"
+            "type": "string",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -26,7 +28,9 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string"
+            "type": "string",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -37,7 +41,9 @@
             "optional": true,
             "sort": true,
             "stem": false,
-            "type": "int64"
+            "type": "int64",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -48,7 +54,9 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string"
+              "type": "string",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -59,7 +67,9 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string"
+            "type": "string",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -70,7 +80,9 @@
             "optional": true,
             "sort": true,
             "stem": false,
-            "type": "int64"
+            "type": "int64",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -81,7 +93,9 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string"
+            "type": "string",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -92,7 +106,9 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string[]"
+            "type": "string[]",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -103,7 +119,9 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string"
+            "type": "string",
+            "stem_dictionary": "",
+            "store": true
           },
           {
             "facet": false,
@@ -114,7 +132,9 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string[]"
+            "type": "string[]",
+            "stem_dictionary": "",
+            "store": true
           }
         ]
       },
diff --git a/website/yarn.lock b/website/yarn.lock
index d5853b441..fb7d5ede8 100644
--- a/website/yarn.lock
+++ b/website/yarn.lock
@@ -1066,13 +1066,6 @@
   resolved "https://registry.npmjs.org/@colors/colors/-/colors-1.6.0.tgz"
   integrity sha512-Ir+AOibqzrIsL6ajt3Rz3LskB7OiMVHqltZmspbW/TJuTVuyOMirVqAkjfY6JISiLHgyNqicAC8AyHHGzNd/dA==
 
-"@cspotcode/source-map-support@^0.8.0":
-  version "0.8.1"
-  resolved "https://registry.yarnpkg.com/@cspotcode/source-map-support/-/source-map-support-0.8.1.tgz#00629c35a688e05a88b1cda684fb9d5e73f000a1"
-  integrity sha512-IchNf6dN4tHoMFIn/7OE8LWZ19Y6q/67Bmf6vnGREv8RSbBVb9LPJxEcnwrcwX6ixSvaiGoomAUvu4YSxXrVgw==
-  dependencies:
-    "@jridgewell/trace-mapping" "0.3.9"
-
 "@dabh/diagnostics@^2.0.2":
   version "2.0.3"
   resolved "https://registry.npmjs.org/@dabh/diagnostics/-/diagnostics-2.0.3.tgz"
@@ -1174,6 +1167,136 @@
   resolved "https://registry.yarnpkg.com/@emotion/utils/-/utils-1.4.2.tgz#6df6c45881fcb1c412d6688a311a98b7f59c1b52"
   integrity sha512-3vLclRofFziIa3J2wDh9jjbkUz9qk5Vi3IZ/FSTKViB0k+ef0fPV7dYrUIugbgupYDx7v9ud/SjrtEP8Y4xLoA==
 
+"@esbuild/aix-ppc64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/aix-ppc64/-/aix-ppc64-0.27.3.tgz#815b39267f9bffd3407ea6c376ac32946e24f8d2"
+  integrity sha512-9fJMTNFTWZMh5qwrBItuziu834eOCUcEqymSH7pY+zoMVEZg3gcPuBNxH1EvfVYe9h0x/Ptw8KBzv7qxb7l8dg==
+
+"@esbuild/android-arm64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/android-arm64/-/android-arm64-0.27.3.tgz#19b882408829ad8e12b10aff2840711b2da361e8"
+  integrity sha512-YdghPYUmj/FX2SYKJ0OZxf+iaKgMsKHVPF1MAq/P8WirnSpCStzKJFjOjzsW0QQ7oIAiccHdcqjbHmJxRb/dmg==
+
+"@esbuild/android-arm@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/android-arm/-/android-arm-0.27.3.tgz#90be58de27915efa27b767fcbdb37a4470627d7b"
+  integrity sha512-i5D1hPY7GIQmXlXhs2w8AWHhenb00+GxjxRncS2ZM7YNVGNfaMxgzSGuO8o8SJzRc/oZwU2bcScvVERk03QhzA==
+
+"@esbuild/android-x64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/android-x64/-/android-x64-0.27.3.tgz#d7dcc976f16e01a9aaa2f9b938fbec7389f895ac"
+  integrity sha512-IN/0BNTkHtk8lkOM8JWAYFg4ORxBkZQf9zXiEOfERX/CzxW3Vg1ewAhU7QSWQpVIzTW+b8Xy+lGzdYXV6UZObQ==
+
+"@esbuild/darwin-arm64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/darwin-arm64/-/darwin-arm64-0.27.3.tgz#9f6cac72b3a8532298a6a4493ed639a8988e8abd"
+  integrity sha512-Re491k7ByTVRy0t3EKWajdLIr0gz2kKKfzafkth4Q8A5n1xTHrkqZgLLjFEHVD+AXdUGgQMq+Godfq45mGpCKg==
+
+"@esbuild/darwin-x64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/darwin-x64/-/darwin-x64-0.27.3.tgz#ac61d645faa37fd650340f1866b0812e1fb14d6a"
+  integrity sha512-vHk/hA7/1AckjGzRqi6wbo+jaShzRowYip6rt6q7VYEDX4LEy1pZfDpdxCBnGtl+A5zq8iXDcyuxwtv3hNtHFg==
+
+"@esbuild/freebsd-arm64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/freebsd-arm64/-/freebsd-arm64-0.27.3.tgz#b8625689d73cf1830fe58c39051acdc12474ea1b"
+  integrity sha512-ipTYM2fjt3kQAYOvo6vcxJx3nBYAzPjgTCk7QEgZG8AUO3ydUhvelmhrbOheMnGOlaSFUoHXB6un+A7q4ygY9w==
+
+"@esbuild/freebsd-x64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/freebsd-x64/-/freebsd-x64-0.27.3.tgz#07be7dd3c9d42fe0eccd2ab9f9ded780bc53bead"
+  integrity sha512-dDk0X87T7mI6U3K9VjWtHOXqwAMJBNN2r7bejDsc+j03SEjtD9HrOl8gVFByeM0aJksoUuUVU9TBaZa2rgj0oA==
+
+"@esbuild/linux-arm64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-arm64/-/linux-arm64-0.27.3.tgz#bf31918fe5c798586460d2b3d6c46ed2c01ca0b6"
+  integrity sha512-sZOuFz/xWnZ4KH3YfFrKCf1WyPZHakVzTiqji3WDc0BCl2kBwiJLCXpzLzUBLgmp4veFZdvN5ChW4Eq/8Fc2Fg==
+
+"@esbuild/linux-arm@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-arm/-/linux-arm-0.27.3.tgz#28493ee46abec1dc3f500223cd9f8d2df08f9d11"
+  integrity sha512-s6nPv2QkSupJwLYyfS+gwdirm0ukyTFNl3KTgZEAiJDd+iHZcbTPPcWCcRYH+WlNbwChgH2QkE9NSlNrMT8Gfw==
+
+"@esbuild/linux-ia32@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-ia32/-/linux-ia32-0.27.3.tgz#750752a8b30b43647402561eea764d0a41d0ee29"
+  integrity sha512-yGlQYjdxtLdh0a3jHjuwOrxQjOZYD/C9PfdbgJJF3TIZWnm/tMd/RcNiLngiu4iwcBAOezdnSLAwQDPqTmtTYg==
+
+"@esbuild/linux-loong64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-loong64/-/linux-loong64-0.27.3.tgz#a5a92813a04e71198c50f05adfaf18fc1e95b9ed"
+  integrity sha512-WO60Sn8ly3gtzhyjATDgieJNet/KqsDlX5nRC5Y3oTFcS1l0KWba+SEa9Ja1GfDqSF1z6hif/SkpQJbL63cgOA==
+
+"@esbuild/linux-mips64el@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-mips64el/-/linux-mips64el-0.27.3.tgz#deb45d7fd2d2161eadf1fbc593637ed766d50bb1"
+  integrity sha512-APsymYA6sGcZ4pD6k+UxbDjOFSvPWyZhjaiPyl/f79xKxwTnrn5QUnXR5prvetuaSMsb4jgeHewIDCIWljrSxw==
+
+"@esbuild/linux-ppc64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-ppc64/-/linux-ppc64-0.27.3.tgz#6f39ae0b8c4d3d2d61a65b26df79f6e12a1c3d78"
+  integrity sha512-eizBnTeBefojtDb9nSh4vvVQ3V9Qf9Df01PfawPcRzJH4gFSgrObw+LveUyDoKU3kxi5+9RJTCWlj4FjYXVPEA==
+
+"@esbuild/linux-riscv64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-riscv64/-/linux-riscv64-0.27.3.tgz#4c5c19c3916612ec8e3915187030b9df0b955c1d"
+  integrity sha512-3Emwh0r5wmfm3ssTWRQSyVhbOHvqegUDRd0WhmXKX2mkHJe1SFCMJhagUleMq+Uci34wLSipf8Lagt4LlpRFWQ==
+
+"@esbuild/linux-s390x@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-s390x/-/linux-s390x-0.27.3.tgz#9ed17b3198fa08ad5ccaa9e74f6c0aff7ad0156d"
+  integrity sha512-pBHUx9LzXWBc7MFIEEL0yD/ZVtNgLytvx60gES28GcWMqil8ElCYR4kvbV2BDqsHOvVDRrOxGySBM9Fcv744hw==
+
+"@esbuild/linux-x64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/linux-x64/-/linux-x64-0.27.3.tgz#12383dcbf71b7cf6513e58b4b08d95a710bf52a5"
+  integrity sha512-Czi8yzXUWIQYAtL/2y6vogER8pvcsOsk5cpwL4Gk5nJqH5UZiVByIY8Eorm5R13gq+DQKYg0+JyQoytLQas4dA==
+
+"@esbuild/netbsd-arm64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/netbsd-arm64/-/netbsd-arm64-0.27.3.tgz#dd0cb2fa543205fcd931df44f4786bfcce6df7d7"
+  integrity sha512-sDpk0RgmTCR/5HguIZa9n9u+HVKf40fbEUt+iTzSnCaGvY9kFP0YKBWZtJaraonFnqef5SlJ8/TiPAxzyS+UoA==
+
+"@esbuild/netbsd-x64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/netbsd-x64/-/netbsd-x64-0.27.3.tgz#028ad1807a8e03e155153b2d025b506c3787354b"
+  integrity sha512-P14lFKJl/DdaE00LItAukUdZO5iqNH7+PjoBm+fLQjtxfcfFE20Xf5CrLsmZdq5LFFZzb5JMZ9grUwvtVYzjiA==
+
+"@esbuild/openbsd-arm64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/openbsd-arm64/-/openbsd-arm64-0.27.3.tgz#e3c16ff3490c9b59b969fffca87f350ffc0e2af5"
+  integrity sha512-AIcMP77AvirGbRl/UZFTq5hjXK+2wC7qFRGoHSDrZ5v5b8DK/GYpXW3CPRL53NkvDqb9D+alBiC/dV0Fb7eJcw==
+
+"@esbuild/openbsd-x64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/openbsd-x64/-/openbsd-x64-0.27.3.tgz#c5a4693fcb03d1cbecbf8b422422468dfc0d2a8b"
+  integrity sha512-DnW2sRrBzA+YnE70LKqnM3P+z8vehfJWHXECbwBmH/CU51z6FiqTQTHFenPlHmo3a8UgpLyH3PT+87OViOh1AQ==
+
+"@esbuild/openharmony-arm64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/openharmony-arm64/-/openharmony-arm64-0.27.3.tgz#082082444f12db564a0775a41e1991c0e125055e"
+  integrity sha512-NinAEgr/etERPTsZJ7aEZQvvg/A6IsZG/LgZy+81wON2huV7SrK3e63dU0XhyZP4RKGyTm7aOgmQk0bGp0fy2g==
+
+"@esbuild/sunos-x64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/sunos-x64/-/sunos-x64-0.27.3.tgz#5ab036c53f929e8405c4e96e865a424160a1b537"
+  integrity sha512-PanZ+nEz+eWoBJ8/f8HKxTTD172SKwdXebZ0ndd953gt1HRBbhMsaNqjTyYLGLPdoWHy4zLU7bDVJztF5f3BHA==
+
+"@esbuild/win32-arm64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/win32-arm64/-/win32-arm64-0.27.3.tgz#38de700ef4b960a0045370c171794526e589862e"
+  integrity sha512-B2t59lWWYrbRDw/tjiWOuzSsFh1Y/E95ofKz7rIVYSQkUYBjfSgf6oeYPNWHToFRr2zx52JKApIcAS/D5TUBnA==
+
+"@esbuild/win32-ia32@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/win32-ia32/-/win32-ia32-0.27.3.tgz#451b93dc03ec5d4f38619e6cd64d9f9eff06f55c"
+  integrity sha512-QLKSFeXNS8+tHW7tZpMtjlNb7HKau0QDpwm49u0vUp9y1WOF+PEzkU84y9GqYaAVW8aH8f3GcBck26jh54cX4Q==
+
+"@esbuild/win32-x64@0.27.3":
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/@esbuild/win32-x64/-/win32-x64-0.27.3.tgz#0eaf705c941a218a43dba8e09f1df1d6cd2f1f17"
+  integrity sha512-4uJGhsxuptu3OcpVAzli+/gWusVGwZZHTlS63hh++ehExkVT8SgiEf7/uC/PclrPPkLhZqGgCTjd0VWLo6xMqA==
+
 "@fullhuman/postcss-purgecss@^4.0.3":
   version "4.1.3"
   resolved "https://registry.npmjs.org/@fullhuman/postcss-purgecss/-/postcss-purgecss-4.1.3.tgz"
@@ -1202,28 +1325,20 @@
     "@jridgewell/gen-mapping" "^0.3.5"
     "@jridgewell/trace-mapping" "^0.3.24"
 
-"@jridgewell/resolve-uri@^3.0.3", "@jridgewell/resolve-uri@^3.1.0":
+"@jridgewell/resolve-uri@^3.1.0":
   version "3.1.2"
   resolved "https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz"
   integrity sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==
 
-"@jridgewell/sourcemap-codec@^1.4.10", "@jridgewell/sourcemap-codec@^1.5.0":
-  version "1.5.5"
-  resolved "https://registry.yarnpkg.com/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.5.tgz#6912b00d2c631c0d15ce1a7ab57cd657f2a8f8ba"
-  integrity sha512-cYQ9310grqxueWbl+WuIUIaiUaDcj7WOq5fVhEljNVgRfOUhY9fy2zTvfoqWsnebh8Sl70VScFbICvJnLKB0Og==
-
 "@jridgewell/sourcemap-codec@^1.4.14", "@jridgewell/sourcemap-codec@^1.4.15":
   version "1.5.0"
   resolved "https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.0.tgz"
   integrity sha512-gv3ZRaISU3fjPAgNsriBRqGWQL6quFx04YMPW/zD8XMLsU32mhCCbfbO6KZFLjvYpCZ8zyDEgqsgf+PwPaM7GQ==
 
-"@jridgewell/trace-mapping@0.3.9":
-  version "0.3.9"
-  resolved "https://registry.yarnpkg.com/@jridgewell/trace-mapping/-/trace-mapping-0.3.9.tgz#6534fd5933a53ba7cbf3a17615e273a0d1273ff9"
-  integrity sha512-3Belt6tdc8bPgAtbcmdtNJlirVoTmEb5e2gC94PnkwEW9jI6CAHUeoG85tjWP5WquqfavoMtMwiG4P926ZKKuQ==
-  dependencies:
-    "@jridgewell/resolve-uri" "^3.0.3"
-    "@jridgewell/sourcemap-codec" "^1.4.10"
+"@jridgewell/sourcemap-codec@^1.5.0":
+  version "1.5.5"
+  resolved "https://registry.yarnpkg.com/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.5.tgz#6912b00d2c631c0d15ce1a7ab57cd657f2a8f8ba"
+  integrity sha512-cYQ9310grqxueWbl+WuIUIaiUaDcj7WOq5fVhEljNVgRfOUhY9fy2zTvfoqWsnebh8Sl70VScFbICvJnLKB0Og==
 
 "@jridgewell/trace-mapping@^0.3.24", "@jridgewell/trace-mapping@^0.3.28":
   version "0.3.31"
@@ -1368,26 +1483,6 @@
     lodash.merge "^4.6.2"
     lodash.uniq "^4.5.0"
 
-"@tsconfig/node10@^1.0.7":
-  version "1.0.12"
-  resolved "https://registry.yarnpkg.com/@tsconfig/node10/-/node10-1.0.12.tgz#be57ceac1e4692b41be9de6be8c32a106636dba4"
-  integrity sha512-UCYBaeFvM11aU2y3YPZ//O5Rhj+xKyzy7mvcIoAjASbigy8mHMryP5cK7dgjlz2hWxh1g5pLw084E0a/wlUSFQ==
-
-"@tsconfig/node12@^1.0.7":
-  version "1.0.11"
-  resolved "https://registry.yarnpkg.com/@tsconfig/node12/-/node12-1.0.11.tgz#ee3def1f27d9ed66dac6e46a295cffb0152e058d"
-  integrity sha512-cqefuRsh12pWyGsIoBKJA9luFu3mRxCA+ORZvA4ktLSzIuCUtWVxGIuXigEwO5/ywWFMZ2QEGKWvkZG1zDMTag==
-
-"@tsconfig/node14@^1.0.0":
-  version "1.0.3"
-  resolved "https://registry.yarnpkg.com/@tsconfig/node14/-/node14-1.0.3.tgz#e4386316284f00b98435bf40f72f75a09dabf6c1"
-  integrity sha512-ysT8mhdixWK6Hw3i1V2AeRqZ5WfXg1G43mqoYlM2nc6388Fq5jcXyr5mRsqViLx/GJYdoL0bfXD8nmF+Zn/Iow==
-
-"@tsconfig/node16@^1.0.2":
-  version "1.0.4"
-  resolved "https://registry.yarnpkg.com/@tsconfig/node16/-/node16-1.0.4.tgz#0b92dcc0cc1c81f6f306a381f28e31b1a56536e9"
-  integrity sha512-vxhUy4J8lyeyinH7Azl1pdd43GJhZH/tP2weN8TntQblOY+A0XbT8DJk1/oCPuOOyg/Ja757rG0CgHcWC8OfMA==
-
 "@types/dotenv-defaults@^2.0.1":
   version "2.0.4"
   resolved "https://registry.npmjs.org/@types/dotenv-defaults/-/dotenv-defaults-2.0.4.tgz"
@@ -1479,23 +1574,11 @@ acorn-walk@^7.0.0:
   resolved "https://registry.npmjs.org/acorn-walk/-/acorn-walk-7.2.0.tgz"
   integrity sha512-OPdCF6GsMIP+Az+aWfAAOEt2/+iVDKE7oy6lJ098aoe59oAmK76qV6Gw60SbZ8jHuG2wH058GF4pLFbYamYrVA==
 
-acorn-walk@^8.1.1:
-  version "8.3.4"
-  resolved "https://registry.yarnpkg.com/acorn-walk/-/acorn-walk-8.3.4.tgz#794dd169c3977edf4ba4ea47583587c5866236b7"
-  integrity sha512-ueEepnujpqee2o5aIYnvHU6C0A42MNdsIDeqy5BydrkuC5R1ZuUFnm27EeFJGoEHJQgn3uleRvmTXaJgfXbt4g==
-  dependencies:
-    acorn "^8.11.0"
-
 acorn@^7.0.0:
   version "7.4.1"
   resolved "https://registry.npmjs.org/acorn/-/acorn-7.4.1.tgz"
   integrity sha512-nQyp0o1/mNdbTO1PO6kHkwSrmgZ0MT/jCCpNiwbUjGoRN4dlBhqJtoQuCnEOKzgTVwg0ZWiCoQy6SxMebQVh8A==
 
-acorn@^8.11.0, acorn@^8.4.1:
-  version "8.15.0"
-  resolved "https://registry.yarnpkg.com/acorn/-/acorn-8.15.0.tgz#a360898bc415edaac46c8241f6383975b930b816"
-  integrity sha512-NZyJarBfL7nWwIq+FDL6Zp/yHEhePMNnnJ0y3qfieCrmNvYct8uvtiV41UvlSe6apAfk0fY1FbWx+NwfmpvtTg==
-
 alpinejs@^2.5, alpinejs@^2.8.2:
   version "2.8.2"
   resolved "https://registry.npmjs.org/alpinejs/-/alpinejs-2.8.2.tgz"
@@ -1521,11 +1604,6 @@ anymatch@~3.1.2:
     normalize-path "^3.0.0"
     picomatch "^2.0.4"
 
-arg@^4.1.0:
-  version "4.1.3"
-  resolved "https://registry.npmjs.org/arg/-/arg-4.1.3.tgz"
-  integrity sha512-58S9QDqG0Xx27YwPSt9fJxivjYl432YCwfDMfZ+71RAqUrZef7LrKQZ3LHLOwCS4FLNBplP533Zx895SeOCHvA==
-
 arg@^5.0.1:
   version "5.0.2"
   resolved "https://registry.yarnpkg.com/arg/-/arg-5.0.2.tgz#c81433cc427c92c4dcf4865142dbca6f15acd59c"
@@ -1563,12 +1641,12 @@ autoprefixer@^10.2.5:
     postcss-value-parser "^4.2.0"
 
 axios@^1.6.0, axios@^1.8.4:
-  version "1.13.2"
-  resolved "https://registry.yarnpkg.com/axios/-/axios-1.13.2.tgz#9ada120b7b5ab24509553ec3e40123521117f687"
-  integrity sha512-VPk9ebNqPcy5lRGuSlKx752IlDatOjT9paPlm8A7yOuW2Fbvp4X3JznJtT4f0GzGLLiWE9W8onz51SqLYwzGaA==
+  version "1.13.5"
+  resolved "https://registry.yarnpkg.com/axios/-/axios-1.13.5.tgz#5e464688fa127e11a660a2c49441c009f6567a43"
+  integrity sha512-cz4ur7Vb0xS4/KUN0tPWe44eqxrIu31me+fbang3ijiNscE129POzipJJA6zniq2C/Z6sJCjMimjS8Lc/GAs8Q==
   dependencies:
-    follow-redirects "^1.15.6"
-    form-data "^4.0.4"
+    follow-redirects "^1.15.11"
+    form-data "^4.0.5"
     proxy-from-env "^1.1.0"
 
 babel-plugin-macros@^3.1.0:
@@ -1929,11 +2007,6 @@ cosmiconfig@^7.0.0, cosmiconfig@^7.0.1:
     path-type "^4.0.0"
     yaml "^1.10.0"
 
-create-require@^1.1.0:
-  version "1.1.1"
-  resolved "https://registry.npmjs.org/create-require/-/create-require-1.1.1.tgz"
-  integrity sha512-dcKFX3jn0MpIaXjisoRvexIJVEKzaq7z2rZKxf+MSr9TkdmHmsU4m2lcLojrj/FHl8mk5VxMmYA+ftRkP/3oKQ==
-
 crypto@^1.0.1:
   version "1.0.1"
   resolved "https://registry.npmjs.org/crypto/-/crypto-1.0.1.tgz"
@@ -2047,11 +2120,6 @@ didyoumean@^1.2.2:
   resolved "https://registry.npmjs.org/didyoumean/-/didyoumean-1.2.2.tgz"
   integrity sha512-gxtyfqMg7GKyhQmb056K7M3xszy/myH8w+B4RT+QXBQsvAOdc3XymqDDPHx1BgPgsdAA5SIifona89YtRATDzw==
 
-diff@^4.0.1:
-  version "4.0.2"
-  resolved "https://registry.npmjs.org/diff/-/diff-4.0.2.tgz"
-  integrity sha512-58lmxKSA4BNyLz+HHMUzlOEpg09FV+ev6ZMe3vJihgdxzgcwZ8VoEEPmALCZG9LmqfVoNMMKpttIYTVG6uDY7A==
-
 dir-glob@^3.0.1:
   version "3.0.1"
   resolved "https://registry.npmjs.org/dir-glob/-/dir-glob-3.0.1.tgz"
@@ -2212,6 +2280,38 @@ es-set-tostringtag@^2.1.0:
     has-tostringtag "^1.0.2"
     hasown "^2.0.2"
 
+esbuild@~0.27.0:
+  version "0.27.3"
+  resolved "https://registry.yarnpkg.com/esbuild/-/esbuild-0.27.3.tgz#5859ca8e70a3af956b26895ce4954d7e73bd27a8"
+  integrity sha512-8VwMnyGCONIs6cWue2IdpHxHnAjzxnw2Zr7MkVxB2vjmQ2ivqGFb4LEG3SMnv0Gb2F/G/2yA8zUaiL1gywDCCg==
+  optionalDependencies:
+    "@esbuild/aix-ppc64" "0.27.3"
+    "@esbuild/android-arm" "0.27.3"
+    "@esbuild/android-arm64" "0.27.3"
+    "@esbuild/android-x64" "0.27.3"
+    "@esbuild/darwin-arm64" "0.27.3"
+    "@esbuild/darwin-x64" "0.27.3"
+    "@esbuild/freebsd-arm64" "0.27.3"
+    "@esbuild/freebsd-x64" "0.27.3"
+    "@esbuild/linux-arm" "0.27.3"
+    "@esbuild/linux-arm64" "0.27.3"
+    "@esbuild/linux-ia32" "0.27.3"
+    "@esbuild/linux-loong64" "0.27.3"
+    "@esbuild/linux-mips64el" "0.27.3"
+    "@esbuild/linux-ppc64" "0.27.3"
+    "@esbuild/linux-riscv64" "0.27.3"
+    "@esbuild/linux-s390x" "0.27.3"
+    "@esbuild/linux-x64" "0.27.3"
+    "@esbuild/netbsd-arm64" "0.27.3"
+    "@esbuild/netbsd-x64" "0.27.3"
+    "@esbuild/openbsd-arm64" "0.27.3"
+    "@esbuild/openbsd-x64" "0.27.3"
+    "@esbuild/openharmony-arm64" "0.27.3"
+    "@esbuild/sunos-x64" "0.27.3"
+    "@esbuild/win32-arm64" "0.27.3"
+    "@esbuild/win32-ia32" "0.27.3"
+    "@esbuild/win32-x64" "0.27.3"
+
 escalade@^3.1.1, escalade@^3.2.0:
   version "3.2.0"
   resolved "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz"
@@ -2282,15 +2382,15 @@ fn.name@1.x.x:
   resolved "https://registry.npmjs.org/fn.name/-/fn.name-1.1.0.tgz"
   integrity sha512-GRnmB5gPyJpAhTQdSZTSp9uaPSvl09KoYcMQtsB9rQoOmzs9dH6ffeccH+Z+cv6P68Hu5bC6JjRh4Ah/mHSNRw==
 
-follow-redirects@^1.15.6:
-  version "1.15.6"
-  resolved "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.15.6.tgz"
-  integrity sha512-wWN62YITEaOpSK584EZXJafH1AGpO8RVgElfkuXbTOrPX4fIfOyEpW/CsiNd8JdYrAoOvafRTOEnvsO++qCqFA==
+follow-redirects@^1.15.11:
+  version "1.15.11"
+  resolved "https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.15.11.tgz#777d73d72a92f8ec4d2e410eb47352a56b8e8340"
+  integrity sha512-deG2P0JfjrTxl50XGCDyfI97ZGVCxIpfKYmfyrQ54n5FO/0gfIES8C/Psl6kWVDolizcaaxZJnTS0QSMxvnsBQ==
 
-form-data@^4.0.4:
-  version "4.0.4"
-  resolved "https://registry.npmjs.org/form-data/-/form-data-4.0.4.tgz"
-  integrity sha512-KrGhL9Q4zjj0kiUt5OO4Mr/A/jlI2jDYs5eHBpYHPcBEVSiipAvn2Ko2HnPe20rmcuuvMHNdZFp+4IlGTMF0Ow==
+form-data@^4.0.5:
+  version "4.0.5"
+  resolved "https://registry.yarnpkg.com/form-data/-/form-data-4.0.5.tgz#b49e48858045ff4cbf6b03e1805cebcad3679053"
+  integrity sha512-8RipRLol37bNs2bhoV67fiTEvdTrbMUYcFTiy3+wuuOnUog2QBHCZWXDRijWQfAkhBj2Uf5UnVaiWwA5vdd82w==
   dependencies:
     asynckit "^0.4.0"
     combined-stream "^1.0.8"
@@ -2342,6 +2442,11 @@ fsevents@~2.3.2:
   resolved "https://registry.npmjs.org/fsevents/-/fsevents-2.3.2.tgz"
   integrity sha512-xiqMQR4xAeHTuB9uWm+fFRcIOgKBMiOBP+eXiyT7jsgVCq1bkVygt00oASowB7EdtpOHaaPgKt812P9ab+DDKA==
 
+fsevents@~2.3.3:
+  version "2.3.3"
+  resolved "https://registry.yarnpkg.com/fsevents/-/fsevents-2.3.3.tgz#cac6407785d03675a2a5e1a5305c697b347d90d6"
+  integrity sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==
+
 function-bind@^1.1.2:
   version "1.1.2"
   resolved "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz"
@@ -2386,6 +2491,13 @@ get-stdin@^8.0.0:
   resolved "https://registry.npmjs.org/get-stdin/-/get-stdin-8.0.0.tgz"
   integrity sha512-sY22aA6xchAzprjyqmSEQv4UbAAzRN0L2dQB0NlN5acTTK9Don6nhoc3eAbUnpZiCANAMfd/+40kVdKfFygohg==
 
+get-tsconfig@^4.7.5:
+  version "4.13.6"
+  resolved "https://registry.yarnpkg.com/get-tsconfig/-/get-tsconfig-4.13.6.tgz#2fbfda558a98a691a798f123afd95915badce876"
+  integrity sha512-shZT/QMiSHc/YBLxxOkMtgSid5HFoauqCE3/exfsEcwg1WkeqjG+V40yBbBrsD+jW2HDXcs28xOfcbm2jI8Ddw==
+  dependencies:
+    resolve-pkg-maps "^1.0.0"
+
 glob-parent@^5.1.2, glob-parent@~5.1.2:
   version "5.1.2"
   resolved "https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz"
@@ -2751,11 +2863,6 @@ make-dir@^2.1.0:
     pify "^4.0.1"
     semver "^5.6.0"
 
-make-error@^1.1.1:
-  version "1.3.6"
-  resolved "https://registry.npmjs.org/make-error/-/make-error-1.3.6.tgz"
-  integrity sha512-s8UhlNe7vPKomQhC1qFelMokr/Sc3AgNbso3n74mVPA5LTZwkB9NlXf4XPamLxJE8h0gh73rM94xvwRT2CVInw==
-
 math-intrinsics@^1.1.0:
   version "1.1.0"
   resolved "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz"
@@ -3294,6 +3401,11 @@ resolve-from@^4.0.0:
   resolved "https://registry.npmjs.org/resolve-from/-/resolve-from-4.0.0.tgz"
   integrity sha512-pb/MYmXstAkysRFx8piNI1tGFNQIFA3vkE3Gq4EuA1dF6gHp/+vgZqsCGJapvy8N3Q+4o7FwvquPJcnZ7RYy4g==
 
+resolve-pkg-maps@^1.0.0:
+  version "1.0.0"
+  resolved "https://registry.yarnpkg.com/resolve-pkg-maps/-/resolve-pkg-maps-1.0.0.tgz#616b3dc2c57056b5588c31cdf4b3d64db133720f"
+  integrity sha512-seS2Tj26TBVOC2NIc2rOe2y2ZO7efxITtLZcGSOnHHNOQ7CkiUBfw0Iw2ck6xkIhPwLhKNLS8BO+hEpngQlqzw==
+
 resolve@^1.1.7, resolve@^1.14.2, resolve@^1.19.0, resolve@^1.20.0, resolve@^1.22.10:
   version "1.22.11"
   resolved "https://registry.yarnpkg.com/resolve/-/resolve-1.22.11.tgz#aad857ce1ffb8bfa9b0b1ac29f1156383f68c262"
@@ -3585,44 +3697,34 @@ ts-easing@^0.2.0:
   resolved "https://registry.npmjs.org/ts-easing/-/ts-easing-0.2.0.tgz"
   integrity sha512-Z86EW+fFFh/IFB1fqQ3/+7Zpf9t2ebOAxNI/V6Wo7r5gqiqtxmgTlQ1qbqQcjLKYeSHPTsEmvlJUDg/EuL0uHQ==
 
-ts-node@^10.9.2:
-  version "10.9.2"
-  resolved "https://registry.yarnpkg.com/ts-node/-/ts-node-10.9.2.tgz#70f021c9e185bccdca820e26dc413805c101c71f"
-  integrity sha512-f0FFpIdcHgn8zcPSbf1dRevwt047YMnaiJM3u2w2RewrB+fob/zePZcrOyQoLMMO7aBIddLcQIEK5dYjkLnGrQ==
-  dependencies:
-    "@cspotcode/source-map-support" "^0.8.0"
-    "@tsconfig/node10" "^1.0.7"
-    "@tsconfig/node12" "^1.0.7"
-    "@tsconfig/node14" "^1.0.0"
-    "@tsconfig/node16" "^1.0.2"
-    acorn "^8.4.1"
-    acorn-walk "^8.1.1"
-    arg "^4.1.0"
-    create-require "^1.1.0"
-    diff "^4.0.1"
-    make-error "^1.1.1"
-    v8-compile-cache-lib "^3.0.1"
-    yn "3.1.1"
-
 tslib@^2.1.0, tslib@^2.3.0, tslib@^2.6.2:
   version "2.8.1"
   resolved "https://registry.npmjs.org/tslib/-/tslib-2.8.1.tgz"
   integrity sha512-oJFu94HQb+KVduSUQL7wnpmqnfmLsOA/nAh6b6EH0wCEoK0/mPeXU6c3wKDV83MkOuHPRHtSXKKU99IBazS/2w==
 
+tsx@^4.21.0:
+  version "4.21.0"
+  resolved "https://registry.yarnpkg.com/tsx/-/tsx-4.21.0.tgz#32aa6cf17481e336f756195e6fe04dae3e6308b1"
+  integrity sha512-5C1sg4USs1lfG0GFb2RLXsdpXqBSEhAaA/0kPL01wxzpMqLILNxIxIOKiILz+cdg/pLnOUxFYOR5yhHU666wbw==
+  dependencies:
+    esbuild "~0.27.0"
+    get-tsconfig "^4.7.5"
+  optionalDependencies:
+    fsevents "~2.3.3"
+
 typescript@^5.9.3:
   version "5.9.3"
   resolved "https://registry.yarnpkg.com/typescript/-/typescript-5.9.3.tgz#5b4f59e15310ab17a216f5d6cf53ee476ede670f"
   integrity sha512-jl1vZzPDinLr9eUt3J/t7V6FgNEw9QjvBPdysz9KfQDD41fQrC2Y4vKQdiaUpFT4bXlb1RHhLpp8wtm6M5TgSw==
 
-"typesense-sync@https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v1.1.0.tgz":
-  version "1.1.0"
-  resolved "https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v1.1.0.tgz"
-  integrity sha512-w1b1aWpiXHOF/VK9SpzsH1B4yVRcyn0EJGQDEoDL5a6R/iD3y+HvG93UGBKlwYCnjv3E5woxXOgYNIE0MqP2Kg==
+"typesense-sync@https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v2.0.0.tgz":
+  version "2.0.0"
+  resolved "https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v2.0.0.tgz#eb267fc50527ed69ce272cef03ca61017bb20f9c"
   dependencies:
     "@babel/runtime" "^7.25.0"
-    crypto "^1.0.1"
     dotenv "^16.4.5"
-    typesense "^2.0.3"
+    lodash "^4.17.21"
+    typesense "^3.0.1"
     winston "^3.14.2"
     yargs "^17.7.2"
 
@@ -3634,10 +3736,10 @@ typesense@^1.8.2:
     axios "^1.6.0"
     loglevel "^1.8.1"
 
-typesense@^2.0.3:
-  version "2.1.0"
-  resolved "https://registry.npmjs.org/typesense/-/typesense-2.1.0.tgz"
-  integrity sha512-a/IRTL+dRXlpRDU4UodyGj8hl5xBz3nKihVRd/KfSFAfFPGcpdX6lxIgwdXy3O6VLNNiEsN8YwIsPHQPVT0vNw==
+typesense@^3.0.1:
+  version "3.0.1"
+  resolved "https://registry.yarnpkg.com/typesense/-/typesense-3.0.1.tgz#fbbfef3383d983fb58e97b78a236047843cc69ca"
+  integrity sha512-aRzuDQlwR7s2sWw+JiR3CufrMWpzH5UAJ4XlybYczD02QPy5jCsEQiueqUu0Wiai4zW/RGYRruF3XrdEXPgPJA==
   dependencies:
     axios "^1.8.4"
     loglevel "^1.8.1"
@@ -3711,11 +3813,6 @@ util@^0.10.3:
   dependencies:
     inherits "2.0.3"
 
-v8-compile-cache-lib@^3.0.1:
-  version "3.0.1"
-  resolved "https://registry.yarnpkg.com/v8-compile-cache-lib/-/v8-compile-cache-lib-3.0.1.tgz#6336e8d71965cb3d35a1bbb7868445a7c05264bf"
-  integrity sha512-wa7YjyUGfNZngI/vtK0UHAN+lgDCxBPCylVXGp0zu59Fz5aiGtNXaq3DhIov063MorB+VfufLh3JlF2KdTK3xg==
-
 whatwg-encoding@^3.1.1:
   version "3.1.1"
   resolved "https://registry.yarnpkg.com/whatwg-encoding/-/whatwg-encoding-3.1.1.tgz#d0f4ef769905d426e1688f3e34381a99b60b76e5"
@@ -3823,8 +3920,3 @@ yargs@^17.7.2:
     string-width "^4.2.3"
     y18n "^5.0.5"
     yargs-parser "^21.1.1"
-
-yn@3.1.1:
-  version "3.1.1"
-  resolved "https://registry.npmjs.org/yn/-/yn-3.1.1.tgz"
-  integrity sha512-Ux4ygGWsu2c7isFWe8Yu1YluJmqVhxqK2cLXNQA5AcC3QfbGNpM7fu0Y8b/z16pXLnFxZYvWhd3fhBY9DLmC6Q==
