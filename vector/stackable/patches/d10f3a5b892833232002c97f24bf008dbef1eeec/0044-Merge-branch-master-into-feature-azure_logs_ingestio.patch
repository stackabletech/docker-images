From 437d6a15f26318fdb1df5523be3a8c0d482650bc Mon Sep 17 00:00:00 2001
From: Jed Laundry <jlaundry@jlaundry.com>
Date: Sat, 7 Feb 2026 07:01:30 +1300
Subject: Merge branch 'master' into feature-azure_logs_ingestion

---
 .github/ISSUE_TEMPLATE/minor-release.md       |    3 +-
 .github/actions/install-vdev/action.yml       |    4 +-
 .github/actions/setup/action.yml              |    6 +-
 .github/audit.yml                             |   18 -
 .github/workflows/build-test-runner.yml       |    4 +-
 .github/workflows/changelog.yaml              |    4 +-
 .github/workflows/changes.yml                 |    6 +-
 .github/workflows/ci-integration-review.yml   |    4 +-
 .github/workflows/cli.yml                     |    2 +-
 .github/workflows/compilation-timings.yml     |   10 +-
 .github/workflows/component_features.yml      |    2 +-
 .github/workflows/cross.yml                   |    4 +-
 .github/workflows/deny.yml                    |    2 +-
 .github/workflows/environment.yml             |    4 +-
 .../gardener_remove_waiting_author.yml        |    2 +-
 .github/workflows/install-sh.yml              |    2 +-
 .github/workflows/integration-test.yml        |    4 +-
 .github/workflows/integration.yml             |    4 +-
 .github/workflows/k8s_e2e.yml                 |    4 +-
 .github/workflows/msrv.yml                    |    2 +-
 .github/workflows/protobuf.yml                |    2 +-
 .github/workflows/publish-homebrew.yml        |    2 +-
 .github/workflows/publish.yml                 |   40 +-
 .github/workflows/regression.yml              |   22 +-
 .github/workflows/scorecard.yml               |    4 +-
 .github/workflows/test-make-command.yml       |    2 +-
 .github/workflows/test.yml                    |   24 +-
 .github/workflows/unit_mac.yml                |    2 +-
 .github/workflows/unit_windows.yml            |    2 +-
 .github/workflows/vdev_publish.yml            |    2 +-
 Cargo.lock                                    |  380 ++--
 Cargo.toml                                    |   17 +-
 LICENSE-3rdparty.csv                          |   15 +-
 ...ckhouse_arrow_complex_types.enhancement.md |    3 +
 .../optimize-websocket-source.enhancement.md  |    3 +
 lib/codecs/Cargo.toml                         |    5 +-
 lib/codecs/src/encoding/format/arrow.rs       | 2008 ++++++-----------
 .../src/vrl_functions/parse_dnstap.rs         |    2 +
 .../src/find_enrichment_table_records.rs      |    5 +
 .../src/get_enrichment_table_record.rs        |    5 +
 lib/file-source-common/Cargo.toml             |    2 +-
 lib/vector-api-client/Cargo.toml              |    4 +-
 lib/vector-buffers/Cargo.toml                 |    2 +-
 lib/vector-common/Cargo.toml                  |    2 +-
 lib/vector-core/Cargo.toml                    |    4 +-
 lib/vector-core/src/transform/mod.rs          |  365 +--
 lib/vector-core/src/transform/outputs.rs      |  358 +++
 lib/vector-tap/Cargo.toml                     |    4 +-
 lib/vector-top/Cargo.toml                     |    2 +-
 lib/vector-vrl-metrics/Cargo.toml             |    4 +-
 .../src/aggregate_vector_metrics.rs           |    3 +
 .../src/find_vector_metrics.rs                |    2 +
 .../src/get_vector_metric.rs                  |    2 +
 lib/vector-vrl/functions/src/get_secret.rs    |    1 +
 lib/vector-vrl/functions/src/remove_secret.rs |    1 +
 lib/vector-vrl/functions/src/set_secret.rs    |    2 +
 .../functions/src/set_semantic_meaning.rs     |    2 +
 src/providers/http.rs                         |   32 +-
 src/sinks/clickhouse/arrow/parser.rs          |  917 ++++----
 src/sinks/clickhouse/arrow/schema.rs          |   70 +-
 src/sinks/clickhouse/integration_tests.rs     |  571 ++++-
 src/sinks/util/encoding.rs                    |    1 +
 src/sources/websocket/source.rs               |    9 +-
 src/transforms/sample/transform.rs            |   75 +-
 vdev/Cargo.toml                               |    2 +-
 .../reference/components/sinks/clickhouse.cue |    3 -
 66 files changed, 2490 insertions(+), 2586 deletions(-)
 delete mode 100644 .github/audit.yml
 create mode 100644 changelog.d/24074_clickhouse_arrow_complex_types.enhancement.md
 create mode 100644 changelog.d/optimize-websocket-source.enhancement.md
 create mode 100644 lib/vector-core/src/transform/outputs.rs

diff --git a/.github/ISSUE_TEMPLATE/minor-release.md b/.github/ISSUE_TEMPLATE/minor-release.md
index bdf7150d1..bcecf7508 100644
--- a/.github/ISSUE_TEMPLATE/minor-release.md
+++ b/.github/ISSUE_TEMPLATE/minor-release.md
@@ -92,11 +92,12 @@ Automated steps include:
     - Refer to the internal releasing doc to monitor the deployment.
 - [ ] Release Linux packages. Refer to the internal releasing doc.
 - [ ] Release updated Helm chart. See [releasing Helm chart](https://github.com/vectordotdev/helm-charts/blob/develop/RELEASING.md).
+- [ ] Release Homebrew. Refer to the internal releasing doc.
 - [ ] Create internal Docker images. Refer to the internal releasing doc.
+- [ ] Update the latest [release tag](https://github.com/vectordotdev/vector/release) description with the release announcement.
 - [ ] Create a new PR with title starting as `chore(releasing):`
   - [ ] Cherry-pick any release commits from the release branch that are not on `master`, to `master`.
   - [ ] Run `cargo vdev build manifests` and commit changes.
   - [ ] Bump the release number in the `Cargo.toml` on master to the next minor release.
   - [ ] Also, update `Cargo.lock` with: `cargo update -p vector`.
   - [ ] If there is a VRL version update, revert it and make it track the git `main` branch and then run `cargo update -p vrl`.
-- [ ] Kick-off post-mortems for any regressions resolved by the release.
diff --git a/.github/actions/install-vdev/action.yml b/.github/actions/install-vdev/action.yml
index 8c5947c07..7b35d99d5 100644
--- a/.github/actions/install-vdev/action.yml
+++ b/.github/actions/install-vdev/action.yml
@@ -17,7 +17,7 @@ runs:
     - name: Cache vdev binary
       id: cache-vdev
       if: ${{ inputs.skip-cache != 'true' }}
-      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
+      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
       with:
         path: ~/.cargo/bin/vdev
         key: ${{ runner.os }}-vdev-${{ hashFiles('vdev/**', 'Cargo.toml', 'Cargo.lock') }}
@@ -33,7 +33,7 @@ runs:
 
     - name: Save vdev to cache
       if: ${{ inputs.skip-cache == 'true' }}
-      uses: actions/cache/save@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
+      uses: actions/cache/save@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
       with:
         path: ~/.cargo/bin/vdev
         key: ${{ runner.os }}-vdev-${{ hashFiles('vdev/**', 'Cargo.toml', 'Cargo.lock') }}
diff --git a/.github/actions/setup/action.yml b/.github/actions/setup/action.yml
index 06b5bcfe6..d8c5e8c02 100644
--- a/.github/actions/setup/action.yml
+++ b/.github/actions/setup/action.yml
@@ -83,7 +83,7 @@ runs:
     - name: Check vdev cache status
       id: check-vdev-cache
       if: ${{ inputs.vdev == 'true' }}
-      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
+      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
       with:
         path: ~/.cargo/bin/vdev
         key: ${{ runner.os }}-vdev-${{ hashFiles('vdev/**', 'Cargo.toml', 'Cargo.lock') }}
@@ -148,7 +148,7 @@ runs:
 
     - name: Cache Cargo registry, index, and git DB
       if: ${{ inputs.cargo-cache == 'true' || env.NEEDS_RUST == 'true' || env.VDEV_NEEDS_COMPILE == 'true' }}
-      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
+      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
       with:
         path: |
           ~/.cargo/registry/index/
@@ -244,7 +244,7 @@ runs:
 
     - name: Cache prepare.sh binaries
       id: cache-prepare-binaries
-      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
+      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
       with:
         path: |
           ~/.cargo/bin/cargo-deb
diff --git a/.github/audit.yml b/.github/audit.yml
deleted file mode 100644
index 2544a1bad..000000000
--- a/.github/audit.yml
+++ /dev/null
@@ -1,18 +0,0 @@
-name: Security audit
-
-on:
-  schedule:
-    - cron: '0 * * * *'
-  push:
-    branches:
-      - master
-
-jobs:
-  security_audit:
-    runs-on: ubuntu-24.04
-    steps:
-      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
-      # TODO: replace this action - abandoned since 2020
-      - uses: actions-rs/audit-check@35b7b53b1e25b55642157ac01b4adceb5b9ebef3 # v1.2.0
-        with:
-          token: ${{ secrets.GITHUB_TOKEN }}
diff --git a/.github/workflows/build-test-runner.yml b/.github/workflows/build-test-runner.yml
index f8fc383e4..82e247c6f 100644
--- a/.github/workflows/build-test-runner.yml
+++ b/.github/workflows/build-test-runner.yml
@@ -24,7 +24,7 @@ jobs:
   build:
     runs-on: ubuntu-24.04
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.checkout_ref || inputs.commit_sha }}
 
@@ -33,7 +33,7 @@ jobs:
           vdev: true
 
       - name: Login to GitHub Container Registry
-        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
+        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
         with:
           registry: ghcr.io
           username: ${{ github.actor }}
diff --git a/.github/workflows/changelog.yaml b/.github/workflows/changelog.yaml
index d7b94723f..e1d6beffd 100644
--- a/.github/workflows/changelog.yaml
+++ b/.github/workflows/changelog.yaml
@@ -45,7 +45,7 @@ jobs:
           exit 0
 
       # Checkout changelog script and changelog.d/ from master
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         if: env.SHOULD_RUN == 'true'
         with:
           ref: master
@@ -55,7 +55,7 @@ jobs:
           sparse-checkout-cone-mode: false
 
       # Checkout PR's changelog.d/ into tmp/
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         if: env.SHOULD_RUN == 'true'
         with:
           repository: ${{ github.event.pull_request.head.repo.full_name }}
diff --git a/.github/workflows/changes.yml b/.github/workflows/changes.yml
index 31dd60615..49e9fd19e 100644
--- a/.github/workflows/changes.yml
+++ b/.github/workflows/changes.yml
@@ -182,7 +182,7 @@ jobs:
       unit_mac-yml: ${{ steps.filter.outputs.unit_mac-yml }}
       unit_windows-yml: ${{ steps.filter.outputs.unit_windows-yml }}
     steps:
-    - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+    - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
     - uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3.0.2
       id: filter
@@ -326,7 +326,7 @@ jobs:
       webhdfs: ${{ steps.filter.outputs.webhdfs }}
       any: ${{ steps.detect-changes.outputs.any }}
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
       - uses: ./.github/actions/setup
         with:
@@ -418,7 +418,7 @@ jobs:
       opentelemetry-metrics: ${{ steps.filter.outputs.opentelemetry-metrics }}
       any: ${{ steps.detect-changes.outputs.any }}
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
       - uses: ./.github/actions/setup
         with:
diff --git a/.github/workflows/ci-integration-review.yml b/.github/workflows/ci-integration-review.yml
index 79958b5a9..d634a9839 100644
--- a/.github/workflows/ci-integration-review.yml
+++ b/.github/workflows/ci-integration-review.yml
@@ -105,7 +105,7 @@ jobs:
           "redis", "splunk", "webhdfs"
         ]
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           submodules: "recursive"
           ref: ${{ github.event.review.commit_id }}
@@ -141,7 +141,7 @@ jobs:
           "datadog-logs", "datadog-metrics", "opentelemetry-logs", "opentelemetry-metrics"
         ]
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           submodules: "recursive"
           ref: ${{ github.event.review.commit_id }}
diff --git a/.github/workflows/cli.yml b/.github/workflows/cli.yml
index 19b63afa4..9c6728fb2 100644
--- a/.github/workflows/cli.yml
+++ b/.github/workflows/cli.yml
@@ -16,7 +16,7 @@ jobs:
     runs-on: ubuntu-24.04
     timeout-minutes: 30
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
       - uses: ./.github/actions/setup
diff --git a/.github/workflows/compilation-timings.yml b/.github/workflows/compilation-timings.yml
index 4f147fdc1..dec323e81 100644
--- a/.github/workflows/compilation-timings.yml
+++ b/.github/workflows/compilation-timings.yml
@@ -16,7 +16,7 @@ jobs:
     runs-on: ubuntu-24.04-8core
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
@@ -32,7 +32,7 @@ jobs:
       PROFILE: debug
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
@@ -43,7 +43,7 @@ jobs:
     runs-on: ubuntu-24.04-8core
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
@@ -54,7 +54,7 @@ jobs:
     runs-on: ubuntu-24.04-8core
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
@@ -67,7 +67,7 @@ jobs:
     runs-on: ubuntu-24.04-8core
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
diff --git a/.github/workflows/component_features.yml b/.github/workflows/component_features.yml
index 04492c62c..78abccaa0 100644
--- a/.github/workflows/component_features.yml
+++ b/.github/workflows/component_features.yml
@@ -35,7 +35,7 @@ jobs:
     timeout-minutes: 180 # Usually takes 2h but this prevents it from hanging for an indefinite time
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/cross.yml b/.github/workflows/cross.yml
index 0ea0b45cb..64da74d3f 100644
--- a/.github/workflows/cross.yml
+++ b/.github/workflows/cross.yml
@@ -31,11 +31,11 @@ jobs:
           - arm-unknown-linux-musleabi
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
-      - uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5.0.1
+      - uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
         name: Cache Cargo registry + index
         with:
           path: |
diff --git a/.github/workflows/deny.yml b/.github/workflows/deny.yml
index b55c0bc65..2a9696cc3 100644
--- a/.github/workflows/deny.yml
+++ b/.github/workflows/deny.yml
@@ -52,7 +52,7 @@ jobs:
       CARGO_INCREMENTAL: 0
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/environment.yml b/.github/workflows/environment.yml
index 2efb01513..129e4c661 100644
--- a/.github/workflows/environment.yml
+++ b/.github/workflows/environment.yml
@@ -36,7 +36,7 @@ jobs:
     timeout-minutes: 30
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
@@ -45,7 +45,7 @@ jobs:
       - name: Set up Docker Buildx
         uses: docker/setup-buildx-action@8d2750c68a42422c14e847fe6c8ac0403b4cbd6f # v3.12.0
       - name: Login to DockerHub
-        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
+        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
         if: env.SHOULD_PUBLISH == 'true'
         with:
           username: ${{ secrets.CI_DOCKER_USERNAME }}
diff --git a/.github/workflows/gardener_remove_waiting_author.yml b/.github/workflows/gardener_remove_waiting_author.yml
index 6366cc7b0..b431395b7 100644
--- a/.github/workflows/gardener_remove_waiting_author.yml
+++ b/.github/workflows/gardener_remove_waiting_author.yml
@@ -16,7 +16,7 @@ jobs:
     runs-on: ubuntu-24.04
     timeout-minutes: 5
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: actions-ecosystem/action-remove-labels@2ce5d41b4b6aa8503e285553f75ed56e0a40bae0 # v1.3.0
         with:
           labels: "meta: awaiting author"
diff --git a/.github/workflows/install-sh.yml b/.github/workflows/install-sh.yml
index 873a3a3d6..69777ead1 100644
--- a/.github/workflows/install-sh.yml
+++ b/.github/workflows/install-sh.yml
@@ -23,7 +23,7 @@ jobs:
     timeout-minutes: 5
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/integration-test.yml b/.github/workflows/integration-test.yml
index 8b62da0a0..838c922e6 100644
--- a/.github/workflows/integration-test.yml
+++ b/.github/workflows/integration-test.yml
@@ -39,13 +39,13 @@ jobs:
 
       - name: (PR comment) Checkout PR branch
         if: ${{ github.event_name == 'issue_comment' }}
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ steps.comment-branch.outputs.head_ref }}
 
       - name: Checkout branch
         if: ${{ github.event_name != 'issue_comment' }}
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
       - run: bash scripts/environment/prepare.sh --modules=rustup,datadog-ci
 
diff --git a/.github/workflows/integration.yml b/.github/workflows/integration.yml
index de2cc01c2..94e8a9466 100644
--- a/.github/workflows/integration.yml
+++ b/.github/workflows/integration.yml
@@ -79,7 +79,7 @@ jobs:
         ]
     timeout-minutes: 90
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           submodules: "recursive"
 
@@ -140,7 +140,7 @@ jobs:
     timeout-minutes: 90
 
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           submodules: "recursive"
 
diff --git a/.github/workflows/k8s_e2e.yml b/.github/workflows/k8s_e2e.yml
index 9b019590d..fc36a0ed8 100644
--- a/.github/workflows/k8s_e2e.yml
+++ b/.github/workflows/k8s_e2e.yml
@@ -78,7 +78,7 @@ jobs:
       DISABLE_MOLD: true
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
       - run: sudo -E bash scripts/ci-free-disk-space.sh
@@ -171,7 +171,7 @@ jobs:
       fail-fast: false
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/msrv.yml b/.github/workflows/msrv.yml
index 4d08fdb20..5bc98ced1 100644
--- a/.github/workflows/msrv.yml
+++ b/.github/workflows/msrv.yml
@@ -22,7 +22,7 @@ jobs:
     runs-on: ubuntu-24.04
     timeout-minutes: 45
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.checkout_ref }}
       - uses: ./.github/actions/setup
diff --git a/.github/workflows/protobuf.yml b/.github/workflows/protobuf.yml
index 6e56ea14c..06e8411a8 100644
--- a/.github/workflows/protobuf.yml
+++ b/.github/workflows/protobuf.yml
@@ -22,7 +22,7 @@ jobs:
     timeout-minutes: 5
     steps:
       # Run `git checkout`
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       # Install the `buf` CLI
       - uses: bufbuild/buf-setup-action@a47c93e0b1648d5651a065437926377d060baa99 # v1.50.0
         with:
diff --git a/.github/workflows/publish-homebrew.yml b/.github/workflows/publish-homebrew.yml
index 4d17069c3..117fe087c 100644
--- a/.github/workflows/publish-homebrew.yml
+++ b/.github/workflows/publish-homebrew.yml
@@ -28,7 +28,7 @@ jobs:
       GITHUB_TOKEN: ${{ secrets.HOMEBREW_PAT }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref || github.ref_name }}
 
diff --git a/.github/workflows/publish.yml b/.github/workflows/publish.yml
index fc9dae047..3e33c5811 100644
--- a/.github/workflows/publish.yml
+++ b/.github/workflows/publish.yml
@@ -37,7 +37,7 @@ jobs:
       vector_release_channel: ${{ steps.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Generate publish metadata
@@ -55,7 +55,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -81,7 +81,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -107,7 +107,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -135,7 +135,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -163,7 +163,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -191,7 +191,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -219,7 +219,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -247,7 +247,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -290,7 +290,7 @@ jobs:
             exit 1
           fi
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (macOS-specific)
@@ -324,7 +324,7 @@ jobs:
       RELEASE_BUILDER: "true"
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Windows-specific)
@@ -394,7 +394,7 @@ jobs:
       - name: Fix Git safe directories issue when in containers (actions/checkout#760)
         run: git config --global --add safe.directory /__w/vector/vector
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (x86_64-unknown-linux-gnu)
@@ -445,7 +445,7 @@ jobs:
       - name: Fix Git safe directories issue when in containers (actions/checkout#760)
         run: git config --global --add safe.directory /__w/vector/vector
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (x86_64-unknown-linux-gnu)
@@ -474,7 +474,7 @@ jobs:
             runner: macos-14
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (${{ matrix.target }})
@@ -508,16 +508,16 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Login to DockerHub
-        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
+        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
         with:
           username: ${{ secrets.CI_DOCKER_USERNAME }}
           password: ${{ secrets.CI_DOCKER_PASSWORD }}
       - name: Login to GitHub Container Registry
-        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
+        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
         with:
           registry: ghcr.io
           username: ${{ github.actor }}
@@ -607,7 +607,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (aarch64-unknown-linux-gnu)
@@ -692,7 +692,7 @@ jobs:
       VECTOR_VERSION: ${{ needs.generate-publish-metadata.outputs.vector_version }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (aarch64-unknown-linux-gnu)
@@ -775,7 +775,7 @@ jobs:
       VECTOR_VERSION: ${{ needs.generate-publish-metadata.outputs.vector_version }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (aarch64-unknown-linux-gnu)
diff --git a/.github/workflows/regression.yml b/.github/workflows/regression.yml
index 14c45a827..0c462b27d 100644
--- a/.github/workflows/regression.yml
+++ b/.github/workflows/regression.yml
@@ -51,7 +51,7 @@ jobs:
       smp-version: ${{ steps.experimental-meta.outputs.SMP_CRATE_VERSION }}
     steps:
       - name: Checkout repository
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           fetch-depth: 0  # need to pull repository history to find merge bases
 
@@ -117,7 +117,7 @@ jobs:
     outputs:
       source_changed: ${{ steps.filter.outputs.SOURCE_CHANGED }}
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
       - name: Collect file changes
         id: changes
@@ -192,9 +192,9 @@ jobs:
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
 
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ needs.resolve-inputs.outputs.baseline-sha }}
           path: baseline-vector
@@ -231,9 +231,9 @@ jobs:
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
 
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ needs.resolve-inputs.outputs.comparison-sha }}
           path: comparison-vector
@@ -314,7 +314,7 @@ jobs:
         uses: aws-actions/amazon-ecr-login@062b18b96a7aff071d4dc91bc00c4c1a7945b076 # v2.0.1
 
       - name: Docker Login to ECR
-        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
+        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
         with:
           registry: ${{ steps.login-ecr.outputs.registry }}
 
@@ -354,7 +354,7 @@ jobs:
         uses: aws-actions/amazon-ecr-login@062b18b96a7aff071d4dc91bc00c4c1a7945b076 # v2.0.1
 
       - name: Docker Login to ECR
-        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
+        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
         with:
           registry: ${{ steps.login-ecr.outputs.registry }}
 
@@ -373,7 +373,7 @@ jobs:
       - upload-baseline-image-to-ecr
       - upload-comparison-image-to-ecr
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ needs.resolve-inputs.outputs.comparison-sha }}
 
@@ -448,7 +448,7 @@ jobs:
       - should-run-gate
       - resolve-inputs
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
       - name: Configure AWS Credentials
         uses: aws-actions/configure-aws-credentials@61815dcd50bd041e203e49132bacad1fd04d2708 # v5.1.1
@@ -485,7 +485,7 @@ jobs:
       - submit-job
       - resolve-inputs
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ needs.resolve-inputs.outputs.comparison-sha }}
 
diff --git a/.github/workflows/scorecard.yml b/.github/workflows/scorecard.yml
index 1ad210543..a6482fb83 100644
--- a/.github/workflows/scorecard.yml
+++ b/.github/workflows/scorecard.yml
@@ -32,7 +32,7 @@ jobs:
 
     steps:
       - name: "Checkout code"
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           persist-credentials: false
 
@@ -68,6 +68,6 @@ jobs:
       # Upload the results to GitHub's code scanning dashboard (optional).
       # Commenting out will disable upload of results to your repo's Code Scanning dashboard
       - name: "Upload to code-scanning"
-        uses: github/codeql-action/upload-sarif@5d4e8d1aca955e8d8589aabd499c5cae939e33c7 # v4.31.9
+        uses: github/codeql-action/upload-sarif@b20883b0cd1f46c72ae0ba6d1090936928f9fa30 # v4.32.0
         with:
           sarif_file: results.sarif
diff --git a/.github/workflows/test-make-command.yml b/.github/workflows/test-make-command.yml
index fe565bc48..3f2d483ed 100644
--- a/.github/workflows/test-make-command.yml
+++ b/.github/workflows/test-make-command.yml
@@ -28,7 +28,7 @@ jobs:
       CARGO_INCREMENTAL: 0
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/test.yml b/.github/workflows/test.yml
index ed5c8f764..114321ae2 100644
--- a/.github/workflows/test.yml
+++ b/.github/workflows/test.yml
@@ -30,7 +30,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -42,7 +42,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -56,7 +56,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -83,7 +83,7 @@ jobs:
     if: ${{ needs.changes.outputs.scripts == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           cargo-cache: false
@@ -97,7 +97,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           cargo-cache: false
@@ -111,7 +111,7 @@ jobs:
     if: ${{ needs.changes.outputs.dependencies == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           cargo-cache: false
@@ -126,7 +126,7 @@ jobs:
     if: ${{ needs.changes.outputs.cue == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -139,7 +139,7 @@ jobs:
     if: ${{ needs.changes.outputs.markdown == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -152,7 +152,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.component_docs == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -167,7 +167,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -179,7 +179,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.cue == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -193,7 +193,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.dependencies == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
       - uses: ./.github/actions/setup
         with:
           rust: true
diff --git a/.github/workflows/unit_mac.yml b/.github/workflows/unit_mac.yml
index e69390f2b..b177b018d 100644
--- a/.github/workflows/unit_mac.yml
+++ b/.github/workflows/unit_mac.yml
@@ -20,7 +20,7 @@ jobs:
       CARGO_INCREMENTAL: 0
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/unit_windows.yml b/.github/workflows/unit_windows.yml
index 0c2eeae07..831eec042 100644
--- a/.github/workflows/unit_windows.yml
+++ b/.github/workflows/unit_windows.yml
@@ -18,7 +18,7 @@ jobs:
     timeout-minutes: 60
     steps:
       - name: Checkout branch
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/vdev_publish.yml b/.github/workflows/vdev_publish.yml
index 5ef36fb36..155db01ec 100644
--- a/.github/workflows/vdev_publish.yml
+++ b/.github/workflows/vdev_publish.yml
@@ -19,7 +19,7 @@ jobs:
 
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
+        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
 
       - name: Bootstrap runner environment (Ubuntu)
         if: startsWith(matrix.os, 'ubuntu')
diff --git a/Cargo.lock b/Cargo.lock
index 953cffb65..3d3c4ce86 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -12,15 +12,6 @@ dependencies = [
  "regex",
 ]
 
-[[package]]
-name = "addr2line"
-version = "0.24.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "dfbe277e56a376000877090da837660b4427aad530e3028d44e0bffe4f89a1c1"
-dependencies = [
- "gimli",
-]
-
 [[package]]
 name = "adler2"
 version = "2.0.0"
@@ -412,7 +403,7 @@ version = "56.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e003216336f70446457e280807a73899dd822feaf02087d31febca1363e2fccc"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "half",
  "num",
 ]
@@ -679,7 +670,7 @@ dependencies = [
  "async-stream",
  "async-trait",
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "fnv",
  "futures-timer",
@@ -733,7 +724,7 @@ version = "7.0.17"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "34ecdaff7c9cffa3614a9f9999bf9ee4c3078fe3ce4d6a6e161736b56febf2de"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "indexmap 2.12.0",
  "serde",
  "serde_json",
@@ -817,7 +808,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "08f6da6d49a956424ca4e28fe93656f790d748b469eaccbc7488fec545315180"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures 0.3.31",
  "memchr",
  "nkeys",
@@ -993,7 +984,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "hex",
  "http 1.3.1",
@@ -1032,7 +1023,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "http-body 0.4.6",
@@ -1085,7 +1076,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1108,7 +1099,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1131,7 +1122,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1155,7 +1146,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1178,7 +1169,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "regex-lite",
@@ -1204,7 +1195,7 @@ dependencies = [
  "aws-smithy-types",
  "aws-smithy-xml",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "hex",
  "hmac",
@@ -1235,7 +1226,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "regex-lite",
@@ -1280,7 +1271,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1303,7 +1294,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1326,7 +1317,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1368,7 +1359,7 @@ dependencies = [
  "aws-smithy-http",
  "aws-smithy-runtime-api",
  "aws-smithy-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "form_urlencoded",
  "hex",
  "hmac",
@@ -1399,7 +1390,7 @@ checksum = "b65d21e1ba6f2cdec92044f904356a19f5ad86961acf015741106cdfafd747c0"
 dependencies = [
  "aws-smithy-http",
  "aws-smithy-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "crc32c",
  "crc32fast",
  "crc64fast-nvme",
@@ -1421,7 +1412,7 @@ checksum = "c41172a5393f54e26d6b1bfbfce5d0abaa5c46870a1641c1c1899b527f8b6388"
 dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "flate2",
  "futures-util",
  "http 0.2.9",
@@ -1437,7 +1428,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "604c7aec361252b8f1c871a7641d5e0ba3a7f5a586e51b66bc9510a5519594d9"
 dependencies = [
  "aws-smithy-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "crc32fast",
 ]
 
@@ -1450,7 +1441,7 @@ dependencies = [
  "aws-smithy-eventstream",
  "aws-smithy-runtime-api",
  "aws-smithy-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "bytes-utils",
  "futures-core",
  "http 0.2.9",
@@ -1523,7 +1514,7 @@ dependencies = [
  "aws-smithy-observability",
  "aws-smithy-runtime-api",
  "aws-smithy-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "http 1.3.1",
@@ -1543,7 +1534,7 @@ checksum = "07f5e0fc8a6b3f2303f331b94504bbf754d85488f402d6f1dd7a6080f99afe56"
 dependencies = [
  "aws-smithy-async",
  "aws-smithy-types",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "http 0.2.9",
  "http 1.3.1",
  "pin-project-lite",
@@ -1559,7 +1550,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d498595448e43de7f4296b7b7a18a8a02c61ec9349128c80a368f7c3b4ab11a8"
 dependencies = [
  "base64-simd",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "bytes-utils",
  "futures-core",
  "http 0.2.9",
@@ -1610,7 +1601,7 @@ dependencies = [
  "async-trait",
  "axum-core 0.3.4",
  "bitflags 1.3.2",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-util",
  "http 0.2.9",
  "http-body 0.4.6",
@@ -1638,7 +1629,7 @@ checksum = "3a6c9af12842a67734c9a2e355436e5d03b22383ed60cf13cd0c18fbfe3dcbcf"
 dependencies = [
  "async-trait",
  "axum-core 0.4.5",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-util",
  "http 1.3.1",
  "http-body 1.0.0",
@@ -1664,7 +1655,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "759fa577a247914fd3f7f76d62972792636412fbfd634cd452f6a385a74d2d2c"
 dependencies = [
  "async-trait",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-util",
  "http 0.2.9",
  "http-body 0.4.6",
@@ -1681,7 +1672,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "09f2bd6146b97ae3359fa0cc6d6b376d9539582c7b4220f041a33ec24c226199"
 dependencies = [
  "async-trait",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-util",
  "http 1.3.1",
  "http-body 1.0.0",
@@ -1703,7 +1694,7 @@ dependencies = [
  "async-lock 3.4.0",
  "async-trait",
  "azure_core_macros",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures 0.3.31",
  "openssl",
  "pin-project",
@@ -1781,21 +1772,6 @@ dependencies = [
  "tokio",
 ]
 
-[[package]]
-name = "backtrace"
-version = "0.3.75"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6806a6321ec58106fea15becdad98371e28d92ccbc7c8f1b3b6dd724fe8f1002"
-dependencies = [
- "addr2line",
- "cfg-if",
- "libc",
- "miniz_oxide",
- "object",
- "rustc-demangle",
- "windows-targets 0.52.6",
-]
-
 [[package]]
 name = "base16"
 version = "0.2.1"
@@ -1986,7 +1962,7 @@ checksum = "8796b390a5b4c86f9f2e8173a68c2791f4fa6b038b84e96dbc01c016d1e6722c"
 dependencies = [
  "base64 0.22.1",
  "bollard-stubs",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "futures-core",
  "futures-util",
@@ -2190,6 +2166,20 @@ name = "bytemuck"
 version = "1.21.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ef657dfab802224e671f5818e9a4935f9b1957ed18e58292690cc39e7a4092a3"
+dependencies = [
+ "bytemuck_derive",
+]
+
+[[package]]
+name = "bytemuck_derive"
+version = "1.10.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "f9abbd1bc6865053c427f7198e6af43bfdedc55ab791faed4fbd361d789575ff"
+dependencies = [
+ "proc-macro2 1.0.101",
+ "quote 1.0.40",
+ "syn 2.0.106",
+]
 
 [[package]]
 name = "byteorder"
@@ -2209,9 +2199,9 @@ dependencies = [
 
 [[package]]
 name = "bytes"
-version = "1.10.1"
+version = "1.11.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d71b6127be86fdcfddb610f7182ac57211d4b18a3e9c82eb2d17662f2227ad6a"
+checksum = "1e748733b7cbc798e1434b6ac524f0c1ff2ab456fe201501e6497c8417a4fc33"
 dependencies = [
  "serde",
 ]
@@ -2222,7 +2212,7 @@ version = "0.1.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e47d3a8076e283f3acd27400535992edb3ba4b5bb72f8891ad8fbe7932a7d4b9"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "either",
 ]
 
@@ -2432,9 +2422,9 @@ dependencies = [
 
 [[package]]
 name = "clap"
-version = "4.5.53"
+version = "4.5.56"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c9e340e012a1bf4935f5282ed1436d1489548e8f72308207ea5df0e23d2d03f8"
+checksum = "a75ca66430e33a14957acc24c5077b503e7d374151b2b4b3a10c83b4ceb4be0e"
 dependencies = [
  "clap_builder",
  "clap_derive",
@@ -2452,9 +2442,9 @@ dependencies = [
 
 [[package]]
 name = "clap_builder"
-version = "4.5.53"
+version = "4.5.56"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d76b5d13eaa18c901fd2f7fca939fefe3a0727a953561fefdf3b2922b8569d00"
+checksum = "793207c7fa6300a0608d1080b858e5fdbe713cdc1c8db9fb17777d8a13e63df0"
 dependencies = [
  "anstream",
  "anstyle",
@@ -2474,9 +2464,9 @@ dependencies = [
 
 [[package]]
 name = "clap_derive"
-version = "4.5.49"
+version = "4.5.55"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2a0b5487afeab2deb2ff4e03a807ad1a03ac532ff5a2cee5d86884440c7f7671"
+checksum = "a92793da1a46a5f2a02a6f4c46c6496b28c43638adea8306fcb0caa1634f24e5"
 dependencies = [
  "heck 0.5.0",
  "proc-macro2 1.0.101",
@@ -2526,7 +2516,7 @@ dependencies = [
  "apache-avro 0.20.0",
  "arrow",
  "async-trait",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "csv-core",
  "derivative",
@@ -2548,6 +2538,7 @@ dependencies = [
  "rust_decimal",
  "serde",
  "serde-aux",
+ "serde_arrow",
  "serde_json",
  "serde_with",
  "similar-asserts",
@@ -2616,7 +2607,7 @@ version = "4.6.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "35ed6e9d84f0b51a7f52daf1c7d71dd136fd7a3f41a8462b8cdb8c78d920fad4"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-core",
  "memchr",
  "pin-project-lite",
@@ -3539,7 +3530,7 @@ version = "0.1.0"
 dependencies = [
  "anyhow",
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "chrono-tz",
  "dnsmsg-parser",
@@ -3591,7 +3582,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a11dd7f04a6a6d2aea0153c6e31f5ea7af8b2efdf52cdaeea7a9a592c7fefef9"
 dependencies = [
  "bumpalo",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "domain-macros",
  "futures-util",
  "hashbrown 0.14.5",
@@ -4050,7 +4041,7 @@ name = "file-source"
 version = "0.1.0"
 dependencies = [
  "async-compression",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "file-source-common",
  "futures 0.3.31",
@@ -4072,7 +4063,7 @@ version = "0.1.0"
 dependencies = [
  "async-compression",
  "bstr 1.12.0",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "crc",
  "dashmap",
@@ -4423,17 +4414,11 @@ dependencies = [
  "wasm-bindgen",
 ]
 
-[[package]]
-name = "gimli"
-version = "0.31.1"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "07e28edb80900c19c28f1072f2e8aeca7fa06b23cd4169cefe1af5aa3260783f"
-
 [[package]]
 name = "git2"
-version = "0.20.2"
+version = "0.20.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2deb07a133b1520dc1a5690e9bd08950108873d7ed5de38dcc74d3b5ebffa110"
+checksum = "7b88256088d75a56f8ecfa070513a775dd9107f6530ef14919dac831af9cfe2b"
 dependencies = [
  "bitflags 2.10.0",
  "libc",
@@ -4637,7 +4622,7 @@ version = "0.3.26"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "81fe527a889e1532da5c525686d96d4c2e74cdd345badf8dfef9f6b39dd5f5e8"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fnv",
  "futures-core",
  "futures-sink",
@@ -4657,7 +4642,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f3c0b69cfcb4e1b9f1bf2f53f95f766e4661169728ec61cd3fe5a0166f2d1386"
 dependencies = [
  "atomic-waker",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fnv",
  "futures-core",
  "futures-sink",
@@ -4675,6 +4660,7 @@ version = "2.4.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6dd08c532ae367adf81c312a4580bc67f1d0fe8bc9c460520283f4c0ff277888"
 dependencies = [
+ "bytemuck",
  "cfg-if",
  "crunchy",
  "num-traits",
@@ -4763,7 +4749,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "06683b93020a07e3dbcf5f8c0f6d40080d725bea7936fc01ad345c01b97dc270"
 dependencies = [
  "base64 0.21.7",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "headers-core",
  "http 0.2.9",
  "httpdate",
@@ -5061,7 +5047,7 @@ version = "0.2.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "bd6effc99afb63425aff9b05836f029929e345a6148a14b7ecd5ab67af944482"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fnv",
  "itoa",
 ]
@@ -5072,7 +5058,7 @@ version = "1.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f4a85d31aea989eead29a3aaf9e1115a180df8282431156e533de47660892565"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fnv",
  "itoa",
 ]
@@ -5083,7 +5069,7 @@ version = "0.4.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7ceab25649e9960c0311ea418d17bee82c0dcec1bd053b5f9a66e265a693bed2"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "http 0.2.9",
  "pin-project-lite",
 ]
@@ -5094,7 +5080,7 @@ version = "1.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1cac85db508abc24a2e48553ba12a996e87244a0395ce011e62b37158745d643"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "http 1.3.1",
 ]
 
@@ -5104,7 +5090,7 @@ version = "0.1.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "793429d76616a256bcb62c2a2ec2bed781c8307e797e2598c50010f2bee2544f"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-util",
  "http 1.3.1",
  "http-body 1.0.0",
@@ -5151,7 +5137,7 @@ version = "0.14.32"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "41dfc780fdec9373c01bae43289ea34c972e40ee3c9f6b3c8801a35f35586ce7"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-channel",
  "futures-core",
  "futures-util",
@@ -5176,7 +5162,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "eb3aa54a13a0dfe7fbe3a59e0c76093041720fdc77b110cc0fc260fafb4dc51e"
 dependencies = [
  "atomic-waker",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-channel",
  "futures-core",
  "h2 0.4.12",
@@ -5250,7 +5236,7 @@ version = "0.9.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ca815a891b24fdfb243fa3239c86154392b0953ee584aa1a2a1f66d20cbe75cc"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures 0.3.31",
  "headers",
  "http 0.2.9",
@@ -5327,7 +5313,7 @@ version = "0.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d6183ddfa99b85da61a140bea0efc93fdf56ceaa041b37d553518030827f9905"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "hyper 0.14.32",
  "native-tls",
  "tokio",
@@ -5340,7 +5326,7 @@ version = "0.6.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "70206fc6890eaca9fde8a0bf71caa2ddfc9fe045ac9e5c70df101a7dbde866e0"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "http-body-util",
  "hyper 1.7.0",
  "hyper-util",
@@ -5357,7 +5343,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3c6995591a8f1380fcb4ba966a252a4b29188d51d2b89e3a252f5305be65aea8"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-channel",
  "futures-core",
  "futures-util",
@@ -5608,7 +5594,7 @@ version = "2.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "22fa7ee6be451ea0b1912b962c91c8380835e97cf1584a77e18264e908448dcb"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "log",
  "nom 7.1.3",
  "smallvec",
@@ -5687,17 +5673,6 @@ dependencies = [
  "windows-sys 0.48.0",
 ]
 
-[[package]]
-name = "io-uring"
-version = "0.7.9"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "d93587f37623a1a17d94ef2bc9ada592f5465fe7732084ab7beefabe5c77c0c4"
-dependencies = [
- "bitflags 2.10.0",
- "cfg-if",
- "libc",
-]
-
 [[package]]
 name = "iovec"
 version = "0.1.4"
@@ -5951,7 +5926,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6d9455388f4977de4d0934efa9f7d36296295537d774574113a20f6082de03da"
 dependencies = [
  "base64 0.13.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "serde",
  "serde-value",
@@ -6050,7 +6025,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d81336eb3a5b10a40c97a5a97ad66622e92bad942ce05ee789edd730aa4f8603"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "either",
  "futures 0.3.31",
@@ -6274,9 +6249,9 @@ dependencies = [
 
 [[package]]
 name = "libgit2-sys"
-version = "0.18.2+1.9.1"
+version = "0.18.3+1.9.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1c42fe03df2bd3c53a3a9c7317ad91d80c81cd1fb0caec8d7cc4cd2bfa10c222"
+checksum = "c9b3acc4b91781bb0b3386669d325163746af5f6e4f73e6d2d630e09a35f3487"
 dependencies = [
  "cc",
  "libc",
@@ -6600,6 +6575,21 @@ dependencies = [
  "libc",
 ]
 
+[[package]]
+name = "marrow"
+version = "0.2.5"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "ea734fcb7619dfcc47a396f7bf0c72571ccc8c18ae7236ae028d485b27424b74"
+dependencies = [
+ "arrow-array",
+ "arrow-buffer",
+ "arrow-data",
+ "arrow-schema",
+ "bytemuck",
+ "half",
+ "serde",
+]
+
 [[package]]
 name = "match_cfg"
 version = "0.1.0"
@@ -6946,7 +6936,7 @@ version = "3.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a15d522be0a9c3e46fd2632e272d178f56387bdb5c9fbb3a36c649062e9b5219"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "encoding_rs",
  "futures-util",
  "http 1.3.1",
@@ -7061,7 +7051,7 @@ version = "0.8.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "16c903aa70590cb93691bf97a767c8d1d6122d2cc9070433deb3bbf36ce8bd23"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures 0.3.31",
  "libc",
  "log",
@@ -7497,22 +7487,13 @@ dependencies = [
  "objc2-core-foundation",
 ]
 
-[[package]]
-name = "object"
-version = "0.36.7"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "62948e14d923ea95ea2c7c86c71013138b66525b86bdc08d2dcc262bdb497b87"
-dependencies = [
- "memchr",
-]
-
 [[package]]
 name = "octseq"
 version = "0.5.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "126c3ca37c9c44cec575247f43a3e4374d8927684f129d2beeb0d2cef262fe12"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "serde",
  "smallvec",
 ]
@@ -7579,7 +7560,7 @@ dependencies = [
  "anyhow",
  "backon",
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "futures 0.3.31",
  "getrandom 0.2.15",
@@ -7686,7 +7667,7 @@ dependencies = [
 name = "opentelemetry-proto"
 version = "0.1.0"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "glob",
  "hex",
@@ -7952,20 +7933,21 @@ dependencies = [
 
 [[package]]
 name = "phf"
-version = "0.11.2"
+version = "0.12.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ade2d8b8f33c7333b51bcf0428d37e217e9f32192ae4772156f65063b8ce03dc"
+checksum = "913273894cec178f401a31ec4b656318d95473527be05c0752cc41cdc32be8b7"
 dependencies = [
- "phf_shared 0.11.2",
+ "phf_shared 0.12.1",
 ]
 
 [[package]]
 name = "phf"
-version = "0.12.1"
+version = "0.13.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "913273894cec178f401a31ec4b656318d95473527be05c0752cc41cdc32be8b7"
+checksum = "c1562dc717473dbaa4c1f85a36410e03c047b2e7df7f45ee938fbef64ae7fadf"
 dependencies = [
- "phf_shared 0.12.1",
+ "phf_shared 0.13.1",
+ "serde",
 ]
 
 [[package]]
@@ -7979,18 +7961,18 @@ dependencies = [
 
 [[package]]
 name = "phf_shared"
-version = "0.11.2"
+version = "0.12.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "90fcb95eef784c2ac79119d1dd819e162b5da872ce6f3c3abe1e8ca1c082f72b"
+checksum = "06005508882fb681fd97892ecff4b7fd0fee13ef1aa569f8695dae7ab9099981"
 dependencies = [
- "siphasher 0.3.11",
+ "siphasher 1.0.1",
 ]
 
 [[package]]
 name = "phf_shared"
-version = "0.12.1"
+version = "0.13.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "06005508882fb681fd97892ecff4b7fd0fee13ef1aa569f8695dae7ab9099981"
+checksum = "e57fef6bc5981e38c2ce2d63bfa546861309f875b8a75f092d1d54ae2d64f266"
 dependencies = [
  "siphasher 1.0.1",
 ]
@@ -8182,13 +8164,13 @@ dependencies = [
 
 [[package]]
 name = "postgres-protocol"
-version = "0.6.8"
+version = "0.6.10"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "76ff0abab4a9b844b93ef7b81f1efc0a366062aaef2cd702c76256b5dc075c54"
+checksum = "3ee9dd5fe15055d2b6806f4736aa0c9637217074e224bbec46d4041b91bb9491"
 dependencies = [
  "base64 0.22.1",
  "byteorder",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fallible-iterator",
  "hmac",
  "md-5",
@@ -8200,11 +8182,11 @@ dependencies = [
 
 [[package]]
 name = "postgres-types"
-version = "0.2.9"
+version = "0.2.12"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "613283563cd90e1dfc3518d548caee47e0e725455ed619881f5cf21f36de4b48"
+checksum = "54b858f82211e84682fecd373f68e1ceae642d8d751a1ebd13f33de6257b3e20"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "fallible-iterator",
  "postgres-protocol",
@@ -8429,7 +8411,7 @@ version = "0.11.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0b82eaa1d779e9a4bc1c3217db8ffbeabaae1dca241bf70183242128d48681cd"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "prost-derive 0.11.9",
 ]
 
@@ -8439,7 +8421,7 @@ version = "0.12.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "deb1435c188b76130da55f17a466d252ff7b1418b2ad3e037d127b94e3411f29"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "prost-derive 0.12.6",
 ]
 
@@ -8449,7 +8431,7 @@ version = "0.13.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2796faa41db3ec313a31f7624d9286acf277b52de526150b7e69f3debf891ee5"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "prost-derive 0.13.5",
 ]
 
@@ -8459,7 +8441,7 @@ version = "0.11.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "119533552c9a7ffacc21e099c24a0ac8bb19c2a2a3f363de84cd9b844feab270"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "heck 0.4.1",
  "itertools 0.10.5",
  "lazy_static",
@@ -8481,7 +8463,7 @@ version = "0.12.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "22505a5c94da8e3b7c2996394d1c933236c4d743e81a410bcca4e6989fc066a4"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "heck 0.5.0",
  "itertools 0.12.1",
  "log",
@@ -8649,7 +8631,7 @@ checksum = "6cee616af00383c461f9ceb0067d15dee68e7d313ae47dbd7f8543236aed7ee9"
 dependencies = [
  "async-channel 2.3.1",
  "async-trait",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "crc",
  "data-url",
@@ -8768,7 +8750,7 @@ version = "0.11.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "62e96808277ec6f97351a2380e6c25114bc9e67037775464979f3037c92d05ef"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "pin-project-lite",
  "quinn-proto",
  "quinn-udp",
@@ -8786,7 +8768,7 @@ version = "0.11.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a2fe5ef3495d7d2e377ff17b1a8ce2ee2ec2a18cde8b6ad6619d65d0701c135d"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "getrandom 0.2.15",
  "rand 0.8.5",
  "ring",
@@ -9097,7 +9079,7 @@ checksum = "7cd3650deebc68526b304898b192fa4102a4ef0b9ada24da096559cb60e0eef8"
 dependencies = [
  "arc-swap",
  "backon",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "cfg-if",
  "combine 4.6.6",
  "futures-channel",
@@ -9277,7 +9259,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "78bf93c4af7a8bb7d879d51cebe797356ff10ae8516ace542b5182d9dcac10b2"
 dependencies = [
  "base64 0.21.7",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "encoding_rs",
  "futures-core",
  "futures-util",
@@ -9321,7 +9303,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "eddd3ca559203180a307f12d114c268abf583f59b03cb906fd0b3ff8646c1147"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "cookie",
  "cookie_store",
  "encoding_rs",
@@ -9449,7 +9431,7 @@ checksum = "2297bf9c81a3f0dc96bc9521370b88f054168c29826a75e89c55ff196e7ed6a1"
 dependencies = [
  "bitvec",
  "bytecheck",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "hashbrown 0.12.3",
  "ptr_meta",
  "rend",
@@ -9584,7 +9566,7 @@ version = "0.24.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e1568e15fab2d546f940ed3a21f48bbbd1c494c90c99c4481339364a497f94a9"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "flume 0.11.0",
  "futures-util",
  "log",
@@ -9604,7 +9586,7 @@ checksum = "35affe401787a9bd846712274d97654355d21b2a2c092a3139aabe31e9022282"
 dependencies = [
  "arrayvec",
  "borsh",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "num-traits",
  "rand 0.8.5",
  "rkyv",
@@ -9612,12 +9594,6 @@ dependencies = [
  "serde_json",
 ]
 
-[[package]]
-name = "rustc-demangle"
-version = "0.1.26"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "56f7d92ca342cea22a06f2121d944b4fd82af56988c270852495420f961d4ace"
-
 [[package]]
 name = "rustc-hash"
 version = "2.1.1"
@@ -10070,6 +10046,21 @@ dependencies = [
  "serde",
 ]
 
+[[package]]
+name = "serde_arrow"
+version = "0.13.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "038967a6dda16f5c6ca5b6e1afec9cd2361d39f0db681ca338ac5f0ccece6469"
+dependencies = [
+ "arrow-array",
+ "arrow-schema",
+ "bytemuck",
+ "chrono",
+ "half",
+ "marrow",
+ "serde",
+]
+
 [[package]]
 name = "serde_bytes"
 version = "0.11.19"
@@ -10620,7 +10611,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ee6798b1838b6a0f69c007c133b8df5866302197e404e8b6ee8ed3e3a5e68dc6"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "crc",
  "crossbeam-queue",
@@ -10698,7 +10689,7 @@ dependencies = [
  "base64 0.22.1",
  "bitflags 2.10.0",
  "byteorder",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "crc",
  "digest",
@@ -11352,23 +11343,20 @@ checksum = "1f3ccbac311fea05f86f61904b462b55fb3df8837a366dfc601a0161d0532f20"
 
 [[package]]
 name = "tokio"
-version = "1.47.1"
+version = "1.49.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "89e49afdadebb872d3145a5638b59eb0691ea23e46ca484037cfab3b76b95038"
+checksum = "72a2903cd7736441aac9df9d7688bd0ce48edccaadf181c3b90be801e81d3d86"
 dependencies = [
- "backtrace",
- "bytes 1.10.1",
- "io-uring",
+ "bytes 1.11.1",
  "libc",
  "mio",
  "parking_lot 0.12.4",
  "pin-project-lite",
  "signal-hook-registry",
- "slab",
  "socket2 0.6.0",
  "tokio-macros",
  "tracing 0.1.41",
- "windows-sys 0.59.0",
+ "windows-sys 0.61.0",
 ]
 
 [[package]]
@@ -11394,9 +11382,9 @@ dependencies = [
 
 [[package]]
 name = "tokio-macros"
-version = "2.5.0"
+version = "2.6.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6e06d43f1345a3bcd39f6a56dbb7dcab2ba47e68e8ac134855e7e2bdbaf8cab8"
+checksum = "af407857209536a95c8e56f8231ef2c2e2aff839b22e07a1ffcbc617e9db9fa5"
 dependencies = [
  "proc-macro2 1.0.101",
  "quote 1.0.40",
@@ -11426,25 +11414,25 @@ dependencies = [
 
 [[package]]
 name = "tokio-postgres"
-version = "0.7.13"
+version = "0.7.15"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6c95d533c83082bb6490e0189acaa0bbeef9084e60471b696ca6988cd0541fb0"
+checksum = "2b40d66d9b2cfe04b628173409368e58247e8eddbbd3b0e6c6ba1d09f20f6c9e"
 dependencies = [
  "async-trait",
  "byteorder",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "fallible-iterator",
  "futures-channel",
  "futures-util",
  "log",
  "parking_lot 0.12.4",
  "percent-encoding",
- "phf 0.11.2",
+ "phf 0.13.1",
  "pin-project-lite",
  "postgres-protocol",
  "postgres-types",
  "rand 0.9.2",
- "socket2 0.5.10",
+ "socket2 0.6.0",
  "tokio",
  "tokio-util",
  "whoami",
@@ -11494,9 +11482,9 @@ dependencies = [
 
 [[package]]
 name = "tokio-stream"
-version = "0.1.17"
+version = "0.1.18"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "eca58d7bba4a75707817a2c44174253f9236b2d5fbd055602e9d5c07c139a047"
+checksum = "32da49809aab5c3bc678af03902d4ccddea2a87d028d86392a4b1560c6906c70"
 dependencies = [
  "futures-core",
  "pin-project-lite",
@@ -11506,12 +11494,10 @@ dependencies = [
 
 [[package]]
 name = "tokio-test"
-version = "0.4.4"
+version = "0.4.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2468baabc3311435b55dd935f702f42cd1b8abb7e754fb7dfb16bd36aa88f9f7"
+checksum = "3f6d24790a10a7af737693a3e8f1d03faef7e6ca0cc99aae5066f533766de545"
 dependencies = [
- "async-stream",
- "bytes 1.10.1",
  "futures-core",
  "tokio",
  "tokio-stream",
@@ -11547,7 +11533,7 @@ name = "tokio-util"
 version = "0.7.13"
 source = "git+https://github.com/vectordotdev/tokio?branch=tokio-util-0.7.13-framed-read-continue-on-error#b4bdfda8fe8aa24eba36de0d60063b14f30c7fe7"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-core",
  "futures-io",
  "futures-sink",
@@ -11563,7 +11549,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f591660438b3038dd04d16c938271c79e7e06260ad2ea2885a4861bfb238605d"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-core",
  "futures-sink",
  "http 1.3.1",
@@ -11678,7 +11664,7 @@ dependencies = [
  "async-trait",
  "axum 0.6.20",
  "base64 0.21.7",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "flate2",
  "h2 0.3.26",
  "http 0.2.9",
@@ -11711,7 +11697,7 @@ dependencies = [
  "async-trait",
  "axum 0.7.5",
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "h2 0.4.12",
  "http 1.3.1",
  "http-body 1.0.0",
@@ -11804,7 +11790,7 @@ checksum = "61c5bb1d698276a2443e5ecfabc1008bf15a36c12e6a7176e7bf089ea9131140"
 dependencies = [
  "async-compression",
  "bitflags 2.10.0",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-core",
  "futures-util",
  "http 0.2.9",
@@ -11826,7 +11812,7 @@ checksum = "1e9cd434a998747dd2c4276bc96ee2e0c7a2eadf3cae88e52be55a05fa9053f5"
 dependencies = [
  "base64 0.21.7",
  "bitflags 2.10.0",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "http 1.3.1",
  "http-body 1.0.0",
  "http-body-util",
@@ -11845,7 +11831,7 @@ checksum = "d4e6559d53cc268e5031cd8429d05415bc4cb4aefc4aa5d6cc35fbf5b924a1f8"
 dependencies = [
  "async-compression",
  "bitflags 2.10.0",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-core",
  "futures-util",
  "http 1.3.1",
@@ -12085,7 +12071,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9e3dac10fd62eaf6617d3a904ae222845979aec67c615d1c842b4002c7666fb9"
 dependencies = [
  "byteorder",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "data-encoding",
  "http 0.2.9",
  "httparse",
@@ -12104,7 +12090,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9ef1a641ea34f399a848dea702823bbecfb4c486f911735368f1f137cb8257e1"
 dependencies = [
  "byteorder",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "data-encoding",
  "http 1.3.1",
  "httparse",
@@ -12175,7 +12161,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "44f91ea93fdd5fd4985fcc0a197ed8e8da18705912bef63c9b9b3148d6f35510"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures 0.3.31",
  "quick-xml 0.38.4",
  "serde",
@@ -12554,7 +12540,7 @@ dependencies = [
  "bloomy",
  "bollard",
  "byteorder",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "bytesize",
  "cfg-if",
  "chrono",
@@ -12746,7 +12732,7 @@ dependencies = [
  "async-stream",
  "async-trait",
  "bytecheck",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "clap",
  "crc32fast",
  "criterion",
@@ -12787,7 +12773,7 @@ name = "vector-common"
 version = "0.1.0"
 dependencies = [
  "async-stream",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "chrono",
  "crossbeam-utils",
  "derivative",
@@ -12877,7 +12863,7 @@ dependencies = [
  "async-trait",
  "base64 0.22.1",
  "bitmask-enum",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "cfg-if",
  "chrono",
  "chrono-tz",
@@ -13128,7 +13114,7 @@ checksum = "6a02e4885ed3bc0f2de90ea6dd45ebcbb66dacffe03547fadbb0eeae2770887d"
 [[package]]
 name = "vrl"
 version = "0.30.0"
-source = "git+https://github.com/vectordotdev/vrl.git?branch=main#ee45ff6caf724ff91ff7fac825baa7267fd7aab7"
+source = "git+https://github.com/vectordotdev/vrl.git?branch=main#1f2c963ed13c30a8897c3555f82e85c772cd5643"
 dependencies = [
  "aes",
  "aes-siv",
@@ -13137,7 +13123,7 @@ dependencies = [
  "base16",
  "base62",
  "base64-simd",
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "cbc",
  "cfb-mode",
  "cfg-if",
@@ -13291,7 +13277,7 @@ version = "0.3.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "4378d202ff965b011c64817db11d5829506d3404edeadb61f190d111da3f231c"
 dependencies = [
- "bytes 1.10.1",
+ "bytes 1.11.1",
  "futures-channel",
  "futures-util",
  "headers",
diff --git a/Cargo.toml b/Cargo.toml
index 698ce21b8..2ea5a610a 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -142,11 +142,11 @@ arc-swap = { version = "1.7.1", default-features = false }
 async-stream = { version = "0.3.6", default-features = false }
 async-trait = { version = "0.1.89", default-features = false }
 base64 = { version = "0.22.1", default-features = false }
-bytes = { version = "1.10.1", default-features = false, features = ["serde"] }
+bytes = { version = "1.11.1", default-features = false, features = ["serde"] }
 cfg-if = { version = "1.0.3", default-features = false }
 chrono = { version = "0.4.41", default-features = false, features = ["clock", "serde"] }
 chrono-tz = { version = "0.10.4", default-features = false, features = ["serde"] }
-clap = { version = "4.5.53", default-features = false, features = ["derive", "error-context", "env", "help", "std", "string", "usage", "wrap_help"] }
+clap = { version = "4.5.56", default-features = false, features = ["derive", "error-context", "env", "help", "std", "string", "usage", "wrap_help"] }
 clap_complete = "4.5.65"
 colored = { version = "3.0.0", default-features = false }
 const-str = { version = "1.0.0", default-features = false }
@@ -191,7 +191,10 @@ serde_yaml = { version = "0.9.34", default-features = false }
 snafu = { version = "0.8.9", default-features = false, features = ["futures", "std"] }
 socket2 = { version = "0.5.10", default-features = false }
 tempfile = "3.23.0"
-tokio = { version = "1.45.1", default-features = false }
+tokio = { version = "1.49.0", default-features = false }
+tokio-stream = { version = "0.1.18", default-features = false }
+tokio-test = "0.4.5"
+tokio-tungstenite = { version = "0.20.1", default-features = false }
 toml = { version = "0.9.8", default-features = false, features = ["serde", "display", "parse"] }
 tonic = { version = "0.11", default-features = false, features = ["transport", "codegen", "prost", "tls", "tls-roots", "gzip"] }
 tonic-build = { version = "0.11", default-features = false, features = ["transport", "prost"] }
@@ -245,7 +248,7 @@ async-trait.workspace = true
 futures.workspace = true
 tokio = { workspace = true, features = ["full"] }
 tokio-openssl = { version = "0.6.5", default-features = false }
-tokio-stream = { version = "0.1.17", default-features = false, features = ["net", "sync", "time"] }
+tokio-stream = { workspace = true, features = ["net", "sync", "time"] }
 tokio-util = { version = "0.7", default-features = false, features = ["io", "time"] }
 console-subscriber = { version = "0.4.1", default-features = false, optional = true }
 
@@ -422,7 +425,7 @@ strip-ansi-escapes = { version = "0.2.1", default-features = false }
 syslog = { version = "6.1.1", default-features = false, optional = true }
 tikv-jemallocator = { version = "0.6.0", default-features = false, features = ["unprefixed_malloc_on_supported_platforms"], optional = true }
 tokio-postgres = { version = "0.7.13", default-features = false, features = ["runtime", "with-chrono-0_4"], optional = true }
-tokio-tungstenite = { version = "0.20.1", default-features = false, features = ["connect"], optional = true }
+tokio-tungstenite = { workspace = true, features = ["connect"], optional = true }
 toml.workspace = true
 hickory-proto = { workspace = true, optional = true }
 tonic = { workspace = true, optional = true }
@@ -478,7 +481,7 @@ similar-asserts = "1.7.0"
 tempfile.workspace = true
 test-generator = "0.3.1"
 tokio = { workspace = true, features = ["test-util"] }
-tokio-test = "0.4.4"
+tokio-test.workspace = true
 tower-test = "0.4.0"
 vector-lib = { workspace = true, features = ["test"] }
 vrl.workspace = true
@@ -855,7 +858,7 @@ sinks-azure_logs_ingestion = ["dep:azure_core", "dep:azure_identity"]
 sinks-azure_monitor_logs = []
 sinks-blackhole = []
 sinks-chronicle = []
-sinks-clickhouse = ["dep:rust_decimal", "codecs-arrow"]
+sinks-clickhouse = ["dep:nom", "dep:rust_decimal", "codecs-arrow"]
 sinks-console = []
 sinks-databend = ["dep:databend-client"]
 sinks-datadog_events = []
diff --git a/LICENSE-3rdparty.csv b/LICENSE-3rdparty.csv
index eeec9b14f..e72f2c3f9 100644
--- a/LICENSE-3rdparty.csv
+++ b/LICENSE-3rdparty.csv
@@ -1,6 +1,5 @@
 Component,Origin,License,Copyright
 Inflector,https://github.com/whatisinternet/inflector,BSD-2-Clause,Josh Teeter<joshteeter@gmail.com>
-addr2line,https://github.com/gimli-rs/addr2line,Apache-2.0 OR MIT,The addr2line Authors
 adler2,https://github.com/oyvindln/adler2,0BSD OR MIT OR Apache-2.0,"Jonas Schievink <jonasschievink@gmail.com>, oyvindln <oyvindln@users.noreply.github.com>"
 adler32,https://github.com/remram44/adler32-rs,Zlib,Remi Rampin <remirampin@gmail.com>
 aead,https://github.com/RustCrypto/traits,MIT OR Apache-2.0,RustCrypto Developers
@@ -108,7 +107,6 @@ azure_identity,https://github.com/azure/azure-sdk-for-rust,MIT,Microsoft
 azure_storage_blob,https://github.com/azure/azure-sdk-for-rust,MIT,Microsoft
 backoff,https://github.com/ihrwein/backoff,MIT OR Apache-2.0,Tibor Benke <ihrwein@gmail.com>
 backon,https://github.com/Xuanwo/backon,Apache-2.0,The backon Authors
-backtrace,https://github.com/rust-lang/backtrace-rs,MIT OR Apache-2.0,The Rust Project Developers
 base16,https://github.com/thomcc/rust-base16,CC0-1.0,Thom Chiovoloni <tchiovoloni@mozilla.com>
 base16ct,https://github.com/RustCrypto/formats/tree/master/base16ct,Apache-2.0 OR MIT,RustCrypto Developers
 base62,https://github.com/fbernier/base62,MIT,"Franois Bernier <frankbernier@gmail.com>, Chai T. Rex <ChaiTRex@users.noreply.github.com>"
@@ -142,6 +140,7 @@ bytecheck,https://github.com/djkoloski/bytecheck,MIT,David Koloski <djkoloski@gm
 bytecheck_derive,https://github.com/djkoloski/bytecheck,MIT,David Koloski <djkoloski@gmail.com>
 bytecount,https://github.com/llogiq/bytecount,Apache-2.0 OR MIT,"Andre Bogus <bogusandre@gmail.de>, Joshua Landau <joshua@landau.ws>"
 bytemuck,https://github.com/Lokathor/bytemuck,Zlib OR Apache-2.0 OR MIT,Lokathor <zefria@gmail.com>
+bytemuck_derive,https://github.com/Lokathor/bytemuck,Zlib OR Apache-2.0 OR MIT,Lokathor <zefria@gmail.com>
 byteorder,https://github.com/BurntSushi/byteorder,Unlicense OR MIT,Andrew Gallant <jamslam@gmail.com>
 bytes,https://github.com/carllerche/bytes,MIT,Carl Lerche <me@carllerche.com>
 bytes,https://github.com/tokio-rs/bytes,MIT,"Carl Lerche <me@carllerche.com>, Sean McArthur <sean@seanmonstar.com>"
@@ -310,7 +309,6 @@ futures-timer,https://github.com/async-rs/futures-timer,MIT OR Apache-2.0,Alex C
 futures-util,https://github.com/rust-lang/futures-rs,MIT OR Apache-2.0,The futures-util Authors
 generic-array,https://github.com/fizyk20/generic-array,MIT,"Bartomiej Kamiski <fizyk20@gmail.com>, Aaron Trent <novacrazy@gmail.com>"
 getrandom,https://github.com/rust-random/getrandom,MIT OR Apache-2.0,The Rand Project Developers
-gimli,https://github.com/gimli-rs/gimli,MIT OR Apache-2.0,The gimli Authors
 glob,https://github.com/rust-lang/glob,MIT OR Apache-2.0,The Rust Project Developers
 gloo-timers,https://github.com/rustwasm/gloo/tree/master/crates/timers,MIT OR Apache-2.0,Rust and WebAssembly Working Group
 goauth,https://github.com/durch/rust-goauth,MIT,Drazen Urch <github@drazenur.ch>
@@ -393,7 +391,6 @@ instability,https://github.com/ratatui-org/instability,MIT,"Stephen M. Coakley <
 instant,https://github.com/sebcrozet/instant,BSD-3-Clause,sebcrozet <developer@crozet.re>
 inventory,https://github.com/dtolnay/inventory,MIT OR Apache-2.0,David Tolnay <dtolnay@gmail.com>
 io-lifetimes,https://github.com/sunfishcode/io-lifetimes,Apache-2.0 WITH LLVM-exception OR Apache-2.0 OR MIT,Dan Gohman <dev@sunfishcode.online>
-io-uring,https://github.com/tokio-rs/io-uring,MIT OR Apache-2.0,quininer <quininer@live.com>
 iovec,https://github.com/carllerche/iovec,MIT OR Apache-2.0,Carl Lerche <me@carllerche.com>
 ipconfig,https://github.com/liranringel/ipconfig,MIT OR Apache-2.0,Liran Ringel <liranringel@gmail.com>
 ipcrypt-rs,https://github.com/jedisct1/rust-ipcrypt2,ISC,Frank Denis <github@pureftpd.org>
@@ -459,6 +456,7 @@ macro_magic_core,https://github.com/sam0x17/macro_magic,MIT,The macro_magic_core
 macro_magic_core_macros,https://github.com/sam0x17/macro_magic,MIT,The macro_magic_core_macros Authors
 macro_magic_macros,https://github.com/sam0x17/macro_magic,MIT,The macro_magic_macros Authors
 malloc_buf,https://github.com/SSheldon/malloc_buf,MIT,Steven Sheldon
+marrow,https://github.com/chmp/marrow,MIT,Christopher Prohm <mail@cprohm.de>
 match_cfg,https://github.com/gnzlbg/match_cfg,MIT OR Apache-2.0,gnzlbg <gonzalobg88@gmail.com>
 matchers,https://github.com/hawkw/matchers,MIT,Eliza Weisman <eliza@buoyant.io>
 matchit,https://github.com/ibraheemdev/matchit,MIT AND BSD-3-Clause,Ibraheem Ahmed <ibraheem@ibraheem.ca>
@@ -524,7 +522,6 @@ oauth2,https://github.com/ramosbugs/oauth2-rs,MIT OR Apache-2.0,"Alex Crichton <
 objc,http://github.com/SSheldon/rust-objc,MIT,Steven Sheldon
 objc2-core-foundation,https://github.com/madsmtm/objc2,Zlib OR Apache-2.0 OR MIT,The objc2-core-foundation Authors
 objc2-io-kit,https://github.com/madsmtm/objc2,Zlib OR Apache-2.0 OR MIT,The objc2-io-kit Authors
-object,https://github.com/gimli-rs/object,Apache-2.0 OR MIT,The object Authors
 octseq,https://github.com/NLnetLabs/octets/,BSD-3-Clause,NLnet Labs <rust-team@nlnetlabs.nl>
 ofb,https://github.com/RustCrypto/block-modes,MIT OR Apache-2.0,RustCrypto Developers
 once_cell,https://github.com/matklad/once_cell,MIT OR Apache-2.0,Aleksey Kladov <aleksey.kladov@gmail.com>
@@ -576,8 +573,8 @@ polling,https://github.com/smol-rs/polling,Apache-2.0 OR MIT,"Stjepan Glavina <s
 poly1305,https://github.com/RustCrypto/universal-hashes,Apache-2.0 OR MIT,RustCrypto Developers
 portable-atomic,https://github.com/taiki-e/portable-atomic,Apache-2.0 OR MIT,The portable-atomic Authors
 postgres-openssl,https://github.com/sfackler/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
-postgres-protocol,https://github.com/sfackler/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
-postgres-types,https://github.com/sfackler/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
+postgres-protocol,https://github.com/rust-postgres/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
+postgres-types,https://github.com/rust-postgres/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
 powerfmt,https://github.com/jhpratt/powerfmt,MIT OR Apache-2.0,Jacob Pratt <jacob@jhpratt.dev>
 ppv-lite86,https://github.com/cryptocorrosion/cryptocorrosion,MIT OR Apache-2.0,The CryptoCorrosion Contributors
 prettydiff,https://github.com/romankoblov/prettydiff,MIT,Roman Koblov <penpen938@me.com>
@@ -659,7 +656,6 @@ roxmltree,https://github.com/RazrFalcon/roxmltree,MIT OR Apache-2.0,Yevhenii Rei
 rsa,https://github.com/RustCrypto/RSA,MIT OR Apache-2.0,"RustCrypto Developers, dignifiedquire <dignifiedquire@gmail.com>"
 rumqttc,https://github.com/bytebeamio/rumqtt,Apache-2.0,tekjar <raviteja@bytebeam.io>
 rust_decimal,https://github.com/paupino/rust-decimal,MIT,Paul Mason <paul@form1.co.nz>
-rustc-demangle,https://github.com/rust-lang/rustc-demangle,MIT OR Apache-2.0,Alex Crichton <alex@alexcrichton.com>
 rustc-hash,https://github.com/rust-lang/rustc-hash,Apache-2.0 OR MIT,The Rust Project Developers
 rustc_version,https://github.com/djc/rustc-version-rs,MIT OR Apache-2.0,The rustc_version Authors
 rustc_version_runtime,https://github.com/seppo0010/rustc-version-runtime-rs,MIT,Sebastian Waisbrot <seppo0010@gmail.com>
@@ -692,6 +688,7 @@ serde,https://github.com/serde-rs/serde,MIT OR Apache-2.0,"Erick Tryzelaar <eric
 serde-aux,https://github.com/iddm/serde-aux,MIT,Victor Polevoy <maintainer@vpolevoy.com>
 serde-toml-merge,https://github.com/jdrouet/serde-toml-merge,MIT,Jeremie Drouet <jeremie.drouet@gmail.com>
 serde-value,https://github.com/arcnmx/serde-value,MIT,arcnmx
+serde_arrow,https://github.com/chmp/serde_arrow,MIT,Christopher Prohm <mail@cprohm.de>
 serde_bytes,https://github.com/serde-rs/bytes,MIT OR Apache-2.0,David Tolnay <dtolnay@gmail.com>
 serde_core,https://github.com/serde-rs/serde,MIT OR Apache-2.0,"Erick Tryzelaar <erick.tryzelaar@gmail.com>, David Tolnay <dtolnay@gmail.com>"
 serde_derive,https://github.com/serde-rs/serde,MIT OR Apache-2.0,"Erick Tryzelaar <erick.tryzelaar@gmail.com>, David Tolnay <dtolnay@gmail.com>"
@@ -784,7 +781,7 @@ tokio-io-timeout,https://github.com/sfackler/tokio-io-timeout,MIT OR Apache-2.0,
 tokio-macros,https://github.com/tokio-rs/tokio,MIT,Tokio Contributors <team@tokio.rs>
 tokio-native-tls,https://github.com/tokio-rs/tls,MIT,Tokio Contributors <team@tokio.rs>
 tokio-openssl,https://github.com/tokio-rs/tokio-openssl,MIT OR Apache-2.0,Alex Crichton <alex@alexcrichton.com>
-tokio-postgres,https://github.com/sfackler/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
+tokio-postgres,https://github.com/rust-postgres/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
 tokio-retry,https://github.com/srijs/rust-tokio-retry,MIT,Sam Rijs <srijs@airpost.net>
 tokio-rustls,https://github.com/rustls/tokio-rustls,MIT OR Apache-2.0,The tokio-rustls Authors
 tokio-stream,https://github.com/tokio-rs/tokio,MIT,Tokio Contributors <team@tokio.rs>
diff --git a/changelog.d/24074_clickhouse_arrow_complex_types.enhancement.md b/changelog.d/24074_clickhouse_arrow_complex_types.enhancement.md
new file mode 100644
index 000000000..5a5d6c65e
--- /dev/null
+++ b/changelog.d/24074_clickhouse_arrow_complex_types.enhancement.md
@@ -0,0 +1,3 @@
+The `clickhouse` sink now supports complex data types (Array, Map, and Tuple) when using the `arrow_stream` format.
+
+authors: benjamin-awd
diff --git a/changelog.d/optimize-websocket-source.enhancement.md b/changelog.d/optimize-websocket-source.enhancement.md
new file mode 100644
index 000000000..9745ed1eb
--- /dev/null
+++ b/changelog.d/optimize-websocket-source.enhancement.md
@@ -0,0 +1,3 @@
+Small optimization to the `websocket` source performance by avoiding getting a new time for every event in an array.
+
+authors: bruceg
diff --git a/lib/codecs/Cargo.toml b/lib/codecs/Cargo.toml
index d8ef9e105..8b3cbac1b 100644
--- a/lib/codecs/Cargo.toml
+++ b/lib/codecs/Cargo.toml
@@ -14,7 +14,7 @@ path = "tests/bin/generate-avro-fixtures.rs"
 
 [dependencies]
 apache-avro = { version = "0.20.0", default-features = false }
-arrow = { version = "56.2.0", default-features = false, features = ["ipc"] }
+arrow = { version = "56.2.0", default-features = false, features = ["ipc"], optional = true }
 async-trait.workspace = true
 bytes.workspace = true
 chrono.workspace = true
@@ -38,6 +38,7 @@ regex.workspace = true
 serde.workspace = true
 serde_with = { version = "3.14.0", default-features = false, features = ["std", "macros", "chrono_0_4"] }
 serde_json.workspace = true
+serde_arrow = { version = "0.13", features = ["arrow-56"], optional = true }
 serde-aux = { version = "4.5", optional = true }
 smallvec = { version = "1", default-features = false, features = ["union"] }
 snafu.workspace = true
@@ -67,7 +68,7 @@ uuid.workspace = true
 vrl.workspace = true
 
 [features]
-arrow = []
+arrow = ["dep:arrow", "dep:serde_arrow"]
 opentelemetry = ["dep:opentelemetry-proto"]
 syslog = ["dep:syslog_loose", "dep:strum", "dep:derive_more", "dep:serde-aux", "dep:toml"]
 test = []
diff --git a/lib/codecs/src/encoding/format/arrow.rs b/lib/codecs/src/encoding/format/arrow.rs
index 3c2d3863f..4812b4f57 100644
--- a/lib/codecs/src/encoding/format/arrow.rs
+++ b/lib/codecs/src/encoding/format/arrow.rs
@@ -5,26 +5,19 @@
 //! a continuous stream of record batches without a file footer.
 
 use arrow::{
-    array::{
-        ArrayRef, BinaryBuilder, BooleanBuilder, Decimal128Builder, Decimal256Builder,
-        Float32Builder, Float64Builder, Int8Builder, Int16Builder, Int32Builder, Int64Builder,
-        StringBuilder, TimestampMicrosecondBuilder, TimestampMillisecondBuilder,
-        TimestampNanosecondBuilder, TimestampSecondBuilder, UInt8Builder, UInt16Builder,
-        UInt32Builder, UInt64Builder,
-    },
-    datatypes::{DataType, Schema, TimeUnit, i256},
+    array::ArrayRef,
+    compute::{CastOptions, cast_with_options},
+    datatypes::{DataType, Field, Fields, Schema, SchemaRef, TimeUnit},
     ipc::writer::StreamWriter,
     record_batch::RecordBatch,
 };
 use async_trait::async_trait;
 use bytes::{BufMut, Bytes, BytesMut};
 use chrono::{DateTime, Utc};
-use rust_decimal::Decimal;
 use snafu::Snafu;
 use std::sync::Arc;
 use vector_config::configurable_component;
-
-use vector_core::event::{Event, Value};
+use vector_core::event::{Event, LogEvent, Value};
 
 /// Provides Arrow schema for encoding.
 ///
@@ -98,7 +91,7 @@ impl ArrowStreamSerializerConfig {
 /// Arrow IPC stream batch serializer that holds the schema
 #[derive(Clone, Debug)]
 pub struct ArrowStreamSerializer {
-    schema: Arc<Schema>,
+    schema: SchemaRef,
 }
 
 impl ArrowStreamSerializer {
@@ -111,20 +104,20 @@ impl ArrowStreamSerializer {
         // If allow_nullable_fields is enabled, transform the schema once here
         // instead of on every batch encoding
         let schema = if config.allow_nullable_fields {
-            Schema::new_with_metadata(
-                schema
-                    .fields()
-                    .iter()
-                    .map(|f| Arc::new(make_field_nullable(f)))
-                    .collect::<Vec<_>>(),
-                schema.metadata().clone(),
-            )
+            let nullable_fields: Fields = schema
+                .fields()
+                .iter()
+                .map(|f| make_field_nullable(f))
+                .collect::<Result<Vec<_>, _>>()
+                .map_err(|e| vector_common::Error::from(e.to_string()))?
+                .into();
+            Schema::new_with_metadata(nullable_fields, schema.metadata().clone())
         } else {
             schema
         };
 
         Ok(Self {
-            schema: Arc::new(schema),
+            schema: SchemaRef::new(schema),
         })
     }
 }
@@ -176,19 +169,6 @@ pub enum ArrowEncodingError {
         message: String,
     },
 
-    /// Unsupported Arrow data type for field
-    #[snafu(display(
-        "Unsupported Arrow data type for field '{}': {:?}",
-        field_name,
-        data_type
-    ))]
-    UnsupportedType {
-        /// The field name
-        field_name: String,
-        /// The unsupported data type
-        data_type: DataType,
-    },
-
     /// Null value encountered for non-nullable field
     #[snafu(display("Null value for non-nullable field '{}'", field_name))]
     NullConstraint {
@@ -202,6 +182,35 @@ pub enum ArrowEncodingError {
         /// The underlying IO error
         source: std::io::Error,
     },
+
+    /// Serde Arrow serialization error
+    #[snafu(display("Serde Arrow error: {}", source))]
+    SerdeArrow {
+        /// The underlying serde_arrow error
+        source: serde_arrow::Error,
+    },
+
+    /// Timestamp value overflows the representable range
+    #[snafu(display(
+        "Timestamp overflow for field '{}': value '{}' cannot be represented as i64 nanoseconds",
+        field_name,
+        timestamp
+    ))]
+    TimestampOverflow {
+        /// The field name
+        field_name: String,
+        /// The timestamp value that overflowed
+        timestamp: String,
+    },
+
+    /// Invalid Map schema structure
+    #[snafu(display("Invalid Map schema for field '{}': {}", field_name, reason))]
+    InvalidMapSchema {
+        /// The field name
+        field_name: String,
+        /// Description of the schema violation
+        reason: String,
+    },
 }
 
 impl From<std::io::Error> for ArrowEncodingError {
@@ -213,7 +222,7 @@ impl From<std::io::Error> for ArrowEncodingError {
 /// Encodes a batch of events into Arrow IPC streaming format
 pub fn encode_events_to_arrow_ipc_stream(
     events: &[Event],
-    schema: Option<Arc<Schema>>,
+    schema: Option<SchemaRef>,
 ) -> Result<Bytes, ArrowEncodingError> {
     if events.is_empty() {
         return Err(ArrowEncodingError::NoEvents);
@@ -235,417 +244,156 @@ pub fn encode_events_to_arrow_ipc_stream(
 }
 
 /// Recursively makes a Field and all its nested fields nullable
-fn make_field_nullable(field: &arrow::datatypes::Field) -> arrow::datatypes::Field {
+fn make_field_nullable(field: &Field) -> Result<Field, ArrowEncodingError> {
     let new_data_type = match field.data_type() {
-        DataType::List(inner_field) => DataType::List(Arc::new(make_field_nullable(inner_field))),
-        DataType::Struct(fields) => {
-            DataType::Struct(fields.iter().map(|f| make_field_nullable(f)).collect())
-        }
-        DataType::Map(inner_field, sorted) => {
-            DataType::Map(Arc::new(make_field_nullable(inner_field)), *sorted)
-        }
-        other => other.clone(),
-    };
-
-    field
-        .clone()
-        .with_data_type(new_data_type)
-        .with_nullable(true)
-}
-
-/// Builds an Arrow RecordBatch from events
-fn build_record_batch(
-    schema: Arc<Schema>,
-    events: &[Event],
-) -> Result<RecordBatch, ArrowEncodingError> {
-    let num_fields = schema.fields().len();
-    let mut columns: Vec<ArrayRef> = Vec::with_capacity(num_fields);
+        DataType::List(inner_field) => DataType::List(make_field_nullable(inner_field)?.into()),
+        DataType::Struct(fields) => DataType::Struct(
+            fields
+                .iter()
+                .map(|f| make_field_nullable(f))
+                .collect::<Result<Vec<_>, _>>()?
+                .into(),
+        ),
+        DataType::Map(inner, sorted) => {
+            // A Map's inner field is a "entries" Struct<Key, Value>
+            let DataType::Struct(fields) = inner.data_type() else {
+                return Err(ArrowEncodingError::InvalidMapSchema {
+                    field_name: field.name().to_string(),
+                    reason: format!("inner type must be Struct, found {:?}", inner.data_type()),
+                });
+            };
 
-    for field in schema.fields() {
-        let field_name = field.name();
-        let nullable = field.is_nullable();
-        let array: ArrayRef = match field.data_type() {
-            DataType::Timestamp(time_unit, _) => {
-                build_timestamp_array(events, field_name, *time_unit, nullable)?
-            }
-            DataType::Utf8 => build_string_array(events, field_name, nullable)?,
-            DataType::Int8 => build_int8_array(events, field_name, nullable)?,
-            DataType::Int16 => build_int16_array(events, field_name, nullable)?,
-            DataType::Int32 => build_int32_array(events, field_name, nullable)?,
-            DataType::Int64 => build_int64_array(events, field_name, nullable)?,
-            DataType::UInt8 => build_uint8_array(events, field_name, nullable)?,
-            DataType::UInt16 => build_uint16_array(events, field_name, nullable)?,
-            DataType::UInt32 => build_uint32_array(events, field_name, nullable)?,
-            DataType::UInt64 => build_uint64_array(events, field_name, nullable)?,
-            DataType::Float32 => build_float32_array(events, field_name, nullable)?,
-            DataType::Float64 => build_float64_array(events, field_name, nullable)?,
-            DataType::Boolean => build_boolean_array(events, field_name, nullable)?,
-            DataType::Binary => build_binary_array(events, field_name, nullable)?,
-            DataType::Decimal128(precision, scale) => {
-                build_decimal128_array(events, field_name, *precision, *scale, nullable)?
-            }
-            DataType::Decimal256(precision, scale) => {
-                build_decimal256_array(events, field_name, *precision, *scale, nullable)?
-            }
-            other_type => {
-                return Err(ArrowEncodingError::UnsupportedType {
-                    field_name: field_name.into(),
-                    data_type: other_type.clone(),
+            if fields.len() != 2 {
+                return Err(ArrowEncodingError::InvalidMapSchema {
+                    field_name: field.name().to_string(),
+                    reason: format!("expected 2 fields (key, value), found {}", fields.len()),
                 });
             }
-        };
+            let key_field = &fields[0];
+            let value_field = &fields[1];
 
-        columns.push(array);
-    }
+            let new_struct_fields: Fields =
+                [key_field.clone(), make_field_nullable(value_field)?.into()].into();
 
-    RecordBatch::try_new(schema, columns)
-        .map_err(|source| ArrowEncodingError::RecordBatchCreation { source })
-}
+            // Reconstruct the inner "entries" field
+            // The inner field itself must be non-nullable (only the Map wrapper is nullable)
+            let new_inner_field = inner
+                .as_ref()
+                .clone()
+                .with_data_type(DataType::Struct(new_struct_fields))
+                .with_nullable(false);
 
-/// Macro to handle appending null or returning an error for non-nullable fields.
-macro_rules! handle_null_constraints {
-    ($builder:expr, $nullable:expr, $field_name:expr) => {{
-        if !$nullable {
-            return Err(ArrowEncodingError::NullConstraint {
-                field_name: $field_name.into(),
-            });
-        }
-        $builder.append_null();
-    }};
-}
-
-/// Macro to generate a `build_*_array` function for primitive types.
-macro_rules! define_build_primitive_array_fn {
-    (
-        $fn_name:ident, // The function name (e.g., build_int8_array)
-        $builder_ty:ty, // The builder type (e.g., Int8Builder)
-        // One or more match arms for valid Value types
-        $( $value_pat:pat $(if $guard:expr)? => $append_expr:expr ),+
-    ) => {
-        fn $fn_name(
-            events: &[Event],
-            field_name: &str,
-            nullable: bool,
-        ) -> Result<ArrayRef, ArrowEncodingError> {
-            let mut builder = <$builder_ty>::with_capacity(events.len());
-
-            for event in events {
-                if let Event::Log(log) = event {
-                    match log.get(field_name) {
-                        $(
-                            $value_pat $(if $guard)? => builder.append_value($append_expr),
-                        )+
-                        // All other patterns are treated as null/invalid
-                        _ => handle_null_constraints!(builder, nullable, field_name),
-                    }
-                }
-            }
-            Ok(Arc::new(builder.finish()))
+            DataType::Map(new_inner_field.into(), *sorted)
         }
+        other => other.clone(),
     };
-}
 
-fn extract_timestamp(value: &Value) -> Option<DateTime<Utc>> {
-    match value {
-        Value::Timestamp(ts) => Some(*ts),
-        Value::Bytes(bytes) => std::str::from_utf8(bytes)
-            .ok()
-            .and_then(|s| chrono::DateTime::parse_from_rfc3339(s).ok())
-            .map(|dt| dt.with_timezone(&Utc)),
-        _ => None,
-    }
+    Ok(field
+        .clone()
+        .with_data_type(new_data_type)
+        .with_nullable(true))
 }
 
-fn build_timestamp_array(
+/// Build an Arrow RecordBatch from a slice of events using the provided schema.
+fn build_record_batch(
+    schema: SchemaRef,
     events: &[Event],
-    field_name: &str,
-    time_unit: TimeUnit,
-    nullable: bool,
-) -> Result<ArrayRef, ArrowEncodingError> {
-    macro_rules! build_array {
-        ($builder:ty, $converter:expr) => {{
-            let mut builder = <$builder>::with_capacity(events.len());
-            for event in events {
-                if let Event::Log(log) = event {
-                    let value_to_append = log.get(field_name).and_then(|value| {
-                        // First, try to extract it as a native or string timestamp
-                        if let Some(ts) = extract_timestamp(value) {
-                            $converter(&ts)
-                        }
-                        // Else, fall back to a raw integer
-                        else if let Value::Integer(i) = value {
-                            Some(*i)
-                        }
-                        // Else, it's an unsupported type (e.g., Bool, Float)
-                        else {
-                            None
-                        }
-                    });
-
-                    if value_to_append.is_none() && !nullable {
-                        return Err(ArrowEncodingError::NullConstraint {
-                            field_name: field_name.into(),
-                        });
-                    }
-
-                    builder.append_option(value_to_append);
-                }
-            }
-            Ok(Arc::new(builder.finish()))
-        }};
-    }
-
-    match time_unit {
-        TimeUnit::Second => {
-            build_array!(TimestampSecondBuilder, |ts: &DateTime<Utc>| Some(
-                ts.timestamp()
-            ))
-        }
-        TimeUnit::Millisecond => {
-            build_array!(TimestampMillisecondBuilder, |ts: &DateTime<Utc>| Some(
-                ts.timestamp_millis()
-            ))
-        }
-        TimeUnit::Microsecond => {
-            build_array!(TimestampMicrosecondBuilder, |ts: &DateTime<Utc>| Some(
-                ts.timestamp_micros()
-            ))
-        }
-        TimeUnit::Nanosecond => {
-            build_array!(TimestampNanosecondBuilder, |ts: &DateTime<Utc>| ts
-                .timestamp_nanos_opt())
+) -> Result<RecordBatch, ArrowEncodingError> {
+    let log_events: Vec<LogEvent> = events
+        .iter()
+        .filter_map(Event::maybe_as_log)
+        .map(|log| convert_timestamps(log, &schema))
+        .collect::<Result<Vec<_>, _>>()?;
+
+    let batch = serde_arrow::to_record_batch(schema.fields(), &log_events).map_err(|source| {
+        // serde_arrow doesn't expose structured error variants (see
+        // https://docs.rs/serde_arrow/latest/serde_arrow/enum.Error.html), so we string-match on
+        // the message to detect null constraint violations, then find the actual field ourselves.
+        if source.message().contains("non-nullable")
+            && let Some(field_name) = find_null_field(&log_events, &schema)
+        {
+            return ArrowEncodingError::NullConstraint { field_name };
         }
-    }
-}
-
-fn build_string_array(
-    events: &[Event],
-    field_name: &str,
-    nullable: bool,
-) -> Result<ArrayRef, ArrowEncodingError> {
-    let mut builder = StringBuilder::with_capacity(events.len(), 0);
-
-    for event in events {
-        if let Event::Log(log) = event {
-            let mut appended = false;
-            if let Some(value) = log.get(field_name) {
-                match value {
-                    Value::Bytes(bytes) => {
-                        // Attempt direct UTF-8 conversion first, fallback to lossy
-                        match std::str::from_utf8(bytes) {
-                            Ok(s) => builder.append_value(s),
-                            Err(_) => builder.append_value(&String::from_utf8_lossy(bytes)),
-                        }
-                        appended = true;
-                    }
-                    Value::Object(obj) => {
-                        if let Ok(s) = serde_json::to_string(&obj) {
-                            builder.append_value(s);
-                            appended = true;
-                        }
-                    }
-                    Value::Array(arr) => {
-                        if let Ok(s) = serde_json::to_string(&arr) {
-                            builder.append_value(s);
-                            appended = true;
-                        }
-                    }
-                    _ => {
-                        builder.append_value(&value.to_string_lossy());
-                        appended = true;
-                    }
-                }
-            }
-
-            if !appended {
-                handle_null_constraints!(builder, nullable, field_name);
+        ArrowEncodingError::SerdeArrow { source }
+    })?;
+
+    // Post-process: use Arrow's cast for any remaining type mismatches.
+    // serde_arrow serializes Vector's Value types using fixed Arrow types (e.g., Int64
+    // for all integers, Float64 for floats, LargeUtf8 for strings), but the target schema
+    // may specify narrower types. Arrow's cast handles these conversions safely.
+    let columns: Result<Vec<ArrayRef>, _> = batch
+        .columns()
+        .iter()
+        .zip(schema.fields())
+        .map(|(col, field)| {
+            if col.data_type() == field.data_type() {
+                Ok(col.clone())
+            } else {
+                cast_with_options(col, field.data_type(), &CastOptions::default())
+                    .map_err(|source| ArrowEncodingError::RecordBatchCreation { source })
             }
-        }
-    }
+        })
+        .collect();
 
-    Ok(Arc::new(builder.finish()))
+    RecordBatch::try_new(schema, columns?)
+        .map_err(|source| ArrowEncodingError::RecordBatchCreation { source })
 }
 
-define_build_primitive_array_fn!(
-    build_int8_array,
-    Int8Builder,
-    Some(Value::Integer(i)) if *i >= i8::MIN as i64 && *i <= i8::MAX as i64 => *i as i8
-);
-
-define_build_primitive_array_fn!(
-    build_int16_array,
-    Int16Builder,
-    Some(Value::Integer(i)) if *i >= i16::MIN as i64 && *i <= i16::MAX as i64 => *i as i16
-);
-
-define_build_primitive_array_fn!(
-    build_int32_array,
-    Int32Builder,
-    Some(Value::Integer(i)) if *i >= i32::MIN as i64 && *i <= i32::MAX as i64 => *i as i32
-);
-
-define_build_primitive_array_fn!(
-    build_int64_array,
-    Int64Builder,
-    Some(Value::Integer(i)) => *i
-);
-
-define_build_primitive_array_fn!(
-    build_uint8_array,
-    UInt8Builder,
-    Some(Value::Integer(i)) if *i >= 0 && *i <= u8::MAX as i64 => *i as u8
-);
-
-define_build_primitive_array_fn!(
-    build_uint16_array,
-    UInt16Builder,
-    Some(Value::Integer(i)) if *i >= 0 && *i <= u16::MAX as i64 => *i as u16
-);
-
-define_build_primitive_array_fn!(
-    build_uint32_array,
-    UInt32Builder,
-    Some(Value::Integer(i)) if *i >= 0 && *i <= u32::MAX as i64 => *i as u32
-);
-
-define_build_primitive_array_fn!(
-    build_uint64_array,
-    UInt64Builder,
-    Some(Value::Integer(i)) if *i >= 0 => *i as u64
-);
-
-define_build_primitive_array_fn!(
-    build_float32_array,
-    Float32Builder,
-    Some(Value::Float(f)) => f.into_inner() as f32,
-    Some(Value::Integer(i)) => *i as f32
-);
-
-define_build_primitive_array_fn!(
-    build_float64_array,
-    Float64Builder,
-    Some(Value::Float(f)) => f.into_inner(),
-    Some(Value::Integer(i)) => *i as f64
-);
-
-define_build_primitive_array_fn!(
-    build_boolean_array,
-    BooleanBuilder,
-    Some(Value::Boolean(b)) => *b
-);
-
-fn build_binary_array(
-    events: &[Event],
-    field_name: &str,
-    nullable: bool,
-) -> Result<ArrayRef, ArrowEncodingError> {
-    let mut builder = BinaryBuilder::with_capacity(events.len(), 0);
-
-    for event in events {
-        if let Event::Log(log) = event {
-            match log.get(field_name) {
-                Some(Value::Bytes(bytes)) => builder.append_value(bytes),
-                _ => handle_null_constraints!(builder, nullable, field_name),
+/// Find which non-nullable field has a missing value (called only on error).
+fn find_null_field(events: &[LogEvent], schema: &SchemaRef) -> Option<String> {
+    for field in schema.fields() {
+        if !field.is_nullable() {
+            let name = field.name();
+            if events
+                .iter()
+                .any(|e| e.get(lookup::event_path!(name)).is_none())
+            {
+                return Some(name.to_string());
             }
         }
     }
-
-    Ok(Arc::new(builder.finish()))
+    None
 }
 
-fn build_decimal128_array(
-    events: &[Event],
-    field_name: &str,
-    precision: u8,
-    scale: i8,
-    nullable: bool,
-) -> Result<ArrayRef, ArrowEncodingError> {
-    let mut builder = Decimal128Builder::with_capacity(events.len())
-        .with_precision_and_scale(precision, scale)
-        .map_err(|_| ArrowEncodingError::UnsupportedType {
-            field_name: field_name.into(),
-            data_type: DataType::Decimal128(precision, scale),
-        })?;
-
-    let target_scale = scale.unsigned_abs() as u32;
-
-    for event in events {
-        if let Event::Log(log) = event {
-            let mut appended = false;
-            match log.get(field_name) {
-                Some(Value::Float(f)) => {
-                    if let Ok(mut decimal) = Decimal::try_from(f.into_inner()) {
-                        decimal.rescale(target_scale);
-                        let mantissa = decimal.mantissa();
-                        builder.append_value(mantissa);
-                        appended = true;
-                    }
-                }
-                Some(Value::Integer(i)) => {
-                    let mut decimal = Decimal::from(*i);
-                    decimal.rescale(target_scale);
-                    let mantissa = decimal.mantissa();
-                    builder.append_value(mantissa);
-                    appended = true;
-                }
-                _ => {}
-            }
+/// Convert Value::Timestamp to Value::Integer for timestamp columns.
+///
+/// This is necessary because serde_arrow's string parsing expects specific formats
+/// based on the timezone setting, but Vector's timestamps always serialize as RFC 3339
+/// with 'Z' suffix. Converting to i64 directly avoids this format mismatch.
+fn convert_timestamps(
+    event: &LogEvent,
+    schema: &SchemaRef,
+) -> Result<LogEvent, ArrowEncodingError> {
+    let mut result = event.clone();
 
-            if !appended {
-                handle_null_constraints!(builder, nullable, field_name);
+    for field in schema.fields() {
+        if let DataType::Timestamp(unit, _) = field.data_type() {
+            let field_name = field.name().as_str();
+
+            if let Some(Value::Timestamp(ts)) = event.get(lookup::event_path!(field_name)) {
+                let val = timestamp_to_unit(ts, unit).ok_or_else(|| {
+                    ArrowEncodingError::TimestampOverflow {
+                        field_name: field_name.to_string(),
+                        timestamp: ts.to_rfc3339(),
+                    }
+                })?;
+                result.insert(field_name, Value::Integer(val));
             }
         }
     }
 
-    Ok(Arc::new(builder.finish()))
+    Ok(result)
 }
 
-fn build_decimal256_array(
-    events: &[Event],
-    field_name: &str,
-    precision: u8,
-    scale: i8,
-    nullable: bool,
-) -> Result<ArrayRef, ArrowEncodingError> {
-    let mut builder = Decimal256Builder::with_capacity(events.len())
-        .with_precision_and_scale(precision, scale)
-        .map_err(|_| ArrowEncodingError::UnsupportedType {
-            field_name: field_name.into(),
-            data_type: DataType::Decimal256(precision, scale),
-        })?;
-
-    let target_scale = scale.unsigned_abs() as u32;
-
-    for event in events {
-        if let Event::Log(log) = event {
-            let mut appended = false;
-            match log.get(field_name) {
-                Some(Value::Float(f)) => {
-                    if let Ok(mut decimal) = Decimal::try_from(f.into_inner()) {
-                        decimal.rescale(target_scale);
-                        let mantissa = decimal.mantissa();
-                        // rust_decimal does not support i256 natively so we upcast here
-                        builder.append_value(i256::from_i128(mantissa));
-                        appended = true;
-                    }
-                }
-                Some(Value::Integer(i)) => {
-                    let mut decimal = Decimal::from(*i);
-                    decimal.rescale(target_scale);
-                    let mantissa = decimal.mantissa();
-                    builder.append_value(i256::from_i128(mantissa));
-                    appended = true;
-                }
-                _ => {}
-            }
-
-            if !appended {
-                handle_null_constraints!(builder, nullable, field_name);
-            }
-        }
+/// Convert a DateTime<Utc> to i64 in the specified Arrow TimeUnit.
+/// Returns None if the value would overflow (only possible for nanoseconds).
+fn timestamp_to_unit(ts: &DateTime<Utc>, unit: &TimeUnit) -> Option<i64> {
+    match unit {
+        TimeUnit::Second => Some(ts.timestamp()),
+        TimeUnit::Millisecond => Some(ts.timestamp_millis()),
+        TimeUnit::Microsecond => Some(ts.timestamp_micros()),
+        TimeUnit::Nanosecond => ts.timestamp_nanos_opt(),
     }
-
-    Ok(Arc::new(builder.finish()))
 }
 
 #[cfg(test)]
@@ -657,1015 +405,575 @@ mod tests {
             TimestampMicrosecondArray, TimestampMillisecondArray, TimestampNanosecondArray,
             TimestampSecondArray,
         },
-        datatypes::Field,
         ipc::reader::StreamReader,
     };
-    use chrono::Utc;
     use std::io::Cursor;
-    use vector_core::event::LogEvent;
-
-    #[test]
-    fn test_encode_all_types() {
-        let mut log = LogEvent::default();
-        log.insert("string_field", "test");
-        log.insert("int8_field", 127);
-        log.insert("int16_field", 32000);
-        log.insert("int32_field", 1000000);
-        log.insert("int64_field", 42);
-        log.insert("float32_field", 3.15);
-        log.insert("float64_field", 3.15);
-        log.insert("bool_field", true);
-        log.insert("bytes_field", bytes::Bytes::from("binary"));
-        log.insert("timestamp_field", Utc::now());
-
-        let events = vec![Event::Log(log)];
-
-        let schema = Arc::new(Schema::new(vec![
-            Field::new("string_field", DataType::Utf8, true),
-            Field::new("int8_field", DataType::Int8, true),
-            Field::new("int16_field", DataType::Int16, true),
-            Field::new("int32_field", DataType::Int32, true),
-            Field::new("int64_field", DataType::Int64, true),
-            Field::new("float32_field", DataType::Float32, true),
-            Field::new("float64_field", DataType::Float64, true),
-            Field::new("bool_field", DataType::Boolean, true),
-            Field::new("bytes_field", DataType::Binary, true),
-            Field::new(
-                "timestamp_field",
-                DataType::Timestamp(TimeUnit::Millisecond, None),
-                true,
-            ),
-        ]));
 
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
+    /// Helper to encode events and return the decoded RecordBatch
+    fn encode_and_decode(
+        events: Vec<Event>,
+        schema: SchemaRef,
+    ) -> Result<RecordBatch, Box<dyn std::error::Error>> {
+        let bytes = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)))?;
         let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 1);
-        assert_eq!(batch.num_columns(), 10);
+        let mut reader = StreamReader::try_new(cursor, None)?;
+        Ok(reader.next().unwrap()?)
+    }
 
-        // Verify string field
-        assert_eq!(
-            batch
-                .column(0)
-                .as_any()
-                .downcast_ref::<StringArray>()
-                .unwrap()
-                .value(0),
-            "test"
-        );
+    /// Create a simple event from key-value pairs
+    fn create_event<V>(fields: Vec<(&str, V)>) -> Event
+    where
+        V: Into<Value>,
+    {
+        let mut log = LogEvent::default();
+        for (key, value) in fields {
+            log.insert(key, value.into());
+        }
+        Event::Log(log)
+    }
+
+    /// Assert a primitive value at a specific column and row
+    macro_rules! assert_primitive_value {
+        ($batch:expr, $col:expr, $row:expr, $array_type:ty, $expected:expr) => {
+            assert_eq!(
+                $batch
+                    .column($col)
+                    .as_any()
+                    .downcast_ref::<$array_type>()
+                    .unwrap()
+                    .value($row),
+                $expected
+            )
+        };
+    }
 
-        // Verify int8 field
-        assert_eq!(
-            batch
-                .column(1)
-                .as_any()
-                .downcast_ref::<arrow::array::Int8Array>()
-                .unwrap()
-                .value(0),
-            127
-        );
+    mod comprehensive {
+        use super::*;
+
+        #[test]
+        fn test_encode_all_types() {
+            use arrow::array::{
+                Decimal128Array, ListArray, MapArray, UInt8Array, UInt16Array, UInt32Array,
+                UInt64Array,
+            };
+            use vrl::value::ObjectMap;
+
+            let now = Utc::now();
+
+            // Create a struct (tuple) value with unnamed fields
+            let mut tuple_value = ObjectMap::new();
+            tuple_value.insert("f0".into(), Value::Bytes("nested_str".into()));
+            tuple_value.insert("f1".into(), Value::Integer(999));
+
+            // Create a named struct (named tuple) value
+            let mut named_tuple_value = ObjectMap::new();
+            named_tuple_value.insert("category".into(), Value::Bytes("test_category".into()));
+            named_tuple_value.insert("tag".into(), Value::Bytes("test_tag".into()));
+
+            // Create a list value
+            let list_value = Value::Array(vec![
+                Value::Integer(1),
+                Value::Integer(2),
+                Value::Integer(3),
+            ]);
+
+            // Create a map value
+            let mut map_value = ObjectMap::new();
+            map_value.insert("key1".into(), Value::Integer(100));
+            map_value.insert("key2".into(), Value::Integer(200));
+
+            let mut log = LogEvent::default();
+            // Primitive types
+            log.insert("string_field", "test");
+            log.insert("int8_field", 127);
+            log.insert("int16_field", 32000);
+            log.insert("int32_field", 1000000);
+            log.insert("int64_field", 42);
+            log.insert("uint8_field", 255);
+            log.insert("uint16_field", 65535);
+            log.insert("uint32_field", 4000000);
+            log.insert("uint64_field", 9000000000_i64);
+            log.insert("float32_field", 3.15);
+            log.insert("float64_field", 3.15);
+            log.insert("bool_field", true);
+            log.insert("bytes_field", bytes::Bytes::from("binary"));
+            log.insert("timestamp_field", now);
+            log.insert("decimal_field", 99.99);
+            // Complex types
+            log.insert("list_field", list_value);
+            log.insert("struct_field", Value::Object(tuple_value));
+            log.insert("named_struct_field", Value::Object(named_tuple_value));
+            log.insert("map_field", Value::Object(map_value));
+
+            let events = vec![Event::Log(log)];
+
+            // Build schema with all supported types
+            let struct_fields = arrow::datatypes::Fields::from(vec![
+                Field::new("f0", DataType::Utf8, true),
+                Field::new("f1", DataType::Int64, true),
+            ]);
+
+            let named_struct_fields = arrow::datatypes::Fields::from(vec![
+                Field::new("category", DataType::Utf8, true),
+                Field::new("tag", DataType::Utf8, true),
+            ]);
+
+            let map_entries = Field::new(
+                "entries",
+                DataType::Struct(arrow::datatypes::Fields::from(vec![
+                    Field::new("keys", DataType::Utf8, false),
+                    Field::new("values", DataType::Int64, true),
+                ])),
+                false,
+            );
 
-        // Verify int16 field
-        assert_eq!(
-            batch
-                .column(2)
-                .as_any()
-                .downcast_ref::<arrow::array::Int16Array>()
-                .unwrap()
-                .value(0),
-            32000
-        );
+            let schema = SchemaRef::new(Schema::new(vec![
+                Field::new("string_field", DataType::Utf8, true),
+                Field::new("int8_field", DataType::Int8, true),
+                Field::new("int16_field", DataType::Int16, true),
+                Field::new("int32_field", DataType::Int32, true),
+                Field::new("int64_field", DataType::Int64, true),
+                Field::new("uint8_field", DataType::UInt8, true),
+                Field::new("uint16_field", DataType::UInt16, true),
+                Field::new("uint32_field", DataType::UInt32, true),
+                Field::new("uint64_field", DataType::UInt64, true),
+                Field::new("float32_field", DataType::Float32, true),
+                Field::new("float64_field", DataType::Float64, true),
+                Field::new("bool_field", DataType::Boolean, true),
+                Field::new("bytes_field", DataType::Binary, true),
+                Field::new(
+                    "timestamp_field",
+                    DataType::Timestamp(TimeUnit::Millisecond, None),
+                    true,
+                ),
+                Field::new("decimal_field", DataType::Decimal128(10, 2), true),
+                Field::new(
+                    "list_field",
+                    DataType::List(Field::new("item", DataType::Int64, true).into()),
+                    true,
+                ),
+                Field::new("struct_field", DataType::Struct(struct_fields), true),
+                Field::new(
+                    "named_struct_field",
+                    DataType::Struct(named_struct_fields),
+                    true,
+                ),
+                Field::new("map_field", DataType::Map(map_entries.into(), false), true),
+            ]));
+
+            let batch = encode_and_decode(events, schema).expect("Failed to encode");
+
+            assert_eq!(batch.num_rows(), 1);
+            assert_eq!(batch.num_columns(), 19);
+
+            // Verify all primitive types
+            assert_eq!(
+                batch
+                    .column(0)
+                    .as_any()
+                    .downcast_ref::<StringArray>()
+                    .unwrap()
+                    .value(0),
+                "test"
+            );
+            assert_primitive_value!(batch, 1, 0, arrow::array::Int8Array, 127);
+            assert_primitive_value!(batch, 2, 0, arrow::array::Int16Array, 32000);
+            assert_primitive_value!(batch, 3, 0, arrow::array::Int32Array, 1000000);
+            assert_primitive_value!(batch, 4, 0, Int64Array, 42);
+            assert_primitive_value!(batch, 5, 0, UInt8Array, 255);
+            assert_primitive_value!(batch, 6, 0, UInt16Array, 65535);
+            assert_primitive_value!(batch, 7, 0, UInt32Array, 4000000);
+            assert_primitive_value!(batch, 8, 0, UInt64Array, 9000000000);
+            assert!(
+                (batch
+                    .column(9)
+                    .as_any()
+                    .downcast_ref::<arrow::array::Float32Array>()
+                    .unwrap()
+                    .value(0)
+                    - 3.15)
+                    .abs()
+                    < 0.001
+            );
+            assert!(
+                (batch
+                    .column(10)
+                    .as_any()
+                    .downcast_ref::<Float64Array>()
+                    .unwrap()
+                    .value(0)
+                    - 3.15)
+                    .abs()
+                    < 0.001
+            );
+            assert!(
+                batch
+                    .column(11)
+                    .as_any()
+                    .downcast_ref::<BooleanArray>()
+                    .unwrap()
+                    .value(0)
+            );
+            assert_primitive_value!(batch, 12, 0, BinaryArray, b"binary");
+            assert_primitive_value!(
+                batch,
+                13,
+                0,
+                TimestampMillisecondArray,
+                now.timestamp_millis()
+            );
+            assert_primitive_value!(batch, 14, 0, Decimal128Array, 9999);
 
-        // Verify int32 field
-        assert_eq!(
-            batch
-                .column(3)
+            let list_array = batch
+                .column(15)
                 .as_any()
-                .downcast_ref::<arrow::array::Int32Array>()
-                .unwrap()
-                .value(0),
-            1000000
-        );
-
-        // Verify int64 field
-        assert_eq!(
-            batch
-                .column(4)
+                .downcast_ref::<ListArray>()
+                .unwrap();
+            assert!(!list_array.is_null(0));
+            let list_value = list_array.value(0);
+            assert_eq!(list_value.len(), 3);
+            let int_array = list_value.as_any().downcast_ref::<Int64Array>().unwrap();
+            assert_eq!(int_array.value(0), 1);
+            assert_eq!(int_array.value(1), 2);
+            assert_eq!(int_array.value(2), 3);
+
+            // Verify struct field (unnamed)
+            let struct_array = batch
+                .column(16)
                 .as_any()
-                .downcast_ref::<Int64Array>()
-                .unwrap()
-                .value(0),
-            42
-        );
-
-        // Verify float32 field
-        assert!(
-            (batch
-                .column(5)
+                .downcast_ref::<arrow::array::StructArray>()
+                .unwrap();
+            assert!(!struct_array.is_null(0));
+            assert_primitive_value!(struct_array, 0, 0, StringArray, "nested_str");
+            assert_primitive_value!(struct_array, 1, 0, Int64Array, 999);
+
+            // Verify named struct field (named tuple)
+            let named_struct_array = batch
+                .column(17)
                 .as_any()
-                .downcast_ref::<arrow::array::Float32Array>()
-                .unwrap()
-                .value(0)
-                - 3.15)
-                .abs()
-                < 0.001
-        );
-
-        // Verify float64 field
-        assert!(
-            (batch
-                .column(6)
-                .as_any()
-                .downcast_ref::<Float64Array>()
-                .unwrap()
-                .value(0)
-                - 3.15)
-                .abs()
-                < 0.001
-        );
-
-        // Verify boolean field
-        assert!(
-            batch
-                .column(7)
-                .as_any()
-                .downcast_ref::<BooleanArray>()
-                .unwrap()
-                .value(0),
-            "{}",
-            true
-        );
-
-        // Verify binary field
-        assert_eq!(
-            batch
-                .column(8)
-                .as_any()
-                .downcast_ref::<BinaryArray>()
-                .unwrap()
-                .value(0),
-            b"binary"
-        );
-
-        // Verify timestamp field
-        assert!(
-            !batch
-                .column(9)
+                .downcast_ref::<arrow::array::StructArray>()
+                .unwrap();
+            assert!(!named_struct_array.is_null(0));
+            assert_primitive_value!(named_struct_array, 0, 0, StringArray, "test_category");
+            assert_primitive_value!(named_struct_array, 1, 0, StringArray, "test_tag");
+
+            // Verify map field
+            let map_array = batch
+                .column(18)
                 .as_any()
-                .downcast_ref::<TimestampMillisecondArray>()
-                .unwrap()
-                .is_null(0)
-        );
-    }
-
-    #[test]
-    fn test_encode_null_values() {
-        let mut log1 = LogEvent::default();
-        log1.insert("field_a", 1);
-        // field_b is missing
-
-        let mut log2 = LogEvent::default();
-        log2.insert("field_b", 2);
-        // field_a is missing
-
-        let events = vec![Event::Log(log1), Event::Log(log2)];
-
-        let schema = Arc::new(Schema::new(vec![
-            Field::new("field_a", DataType::Int64, true),
-            Field::new("field_b", DataType::Int64, true),
-        ]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 2);
-
-        let field_a = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<Int64Array>()
-            .unwrap();
-        assert_eq!(field_a.value(0), 1);
-        assert!(field_a.is_null(1));
-
-        let field_b = batch
-            .column(1)
-            .as_any()
-            .downcast_ref::<Int64Array>()
-            .unwrap();
-        assert!(field_b.is_null(0));
-        assert_eq!(field_b.value(1), 2);
+                .downcast_ref::<MapArray>()
+                .unwrap();
+            assert!(!map_array.is_null(0));
+            let map_value = map_array.value(0);
+            assert_eq!(map_value.len(), 2);
+        }
     }
 
-    #[test]
-    fn test_encode_type_mismatches() {
-        let mut log1 = LogEvent::default();
-        log1.insert("field", 42); // Integer
-
-        let mut log2 = LogEvent::default();
-        log2.insert("field", 3.15); // Float - type mismatch!
+    mod error_handling {
+        use super::*;
 
-        let events = vec![Event::Log(log1), Event::Log(log2)];
+        #[test]
+        fn test_encode_without_schema_fails() {
+            let events = vec![create_event(vec![("message", "hello")])];
 
-        // Schema expects Int64
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "field",
-            DataType::Int64,
-            true,
-        )]));
+            let result = encode_events_to_arrow_ipc_stream(&events, None);
+            assert!(result.is_err());
+            assert!(matches!(
+                result.unwrap_err(),
+                ArrowEncodingError::NoSchemaProvided
+            ));
+        }
 
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
+        #[test]
+        fn test_encode_empty_events() {
+            let events: Vec<Event> = vec![];
+            let result = encode_events_to_arrow_ipc_stream(&events, None);
+            assert!(result.is_err());
+            assert!(matches!(result.unwrap_err(), ArrowEncodingError::NoEvents));
+        }
 
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 2);
-
-        let field_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<Int64Array>()
-            .unwrap();
-        assert_eq!(field_array.value(0), 42);
-        assert!(field_array.is_null(1)); // Type mismatch becomes null
+        #[test]
+        fn test_null_constraint_error() {
+            let events = vec![create_event(vec![("other_field", "value")])];
+
+            let schema = SchemaRef::new(Schema::new(vec![Field::new(
+                "required_field",
+                DataType::Utf8,
+                false, // non-nullable
+            )]));
+
+            let result = encode_events_to_arrow_ipc_stream(&events, Some(schema));
+            assert!(result.is_err());
+            match result.unwrap_err() {
+                ArrowEncodingError::NullConstraint { field_name } => {
+                    assert_eq!(field_name, "required_field");
+                }
+                other => panic!("Expected NullConstraint error, got: {:?}", other),
+            }
+        }
     }
 
-    #[test]
-    fn test_encode_complex_json_values() {
-        use serde_json::json;
-
-        let mut log = LogEvent::default();
-        log.insert(
-            "object_field",
-            json!({"key": "value", "nested": {"count": 42}}),
-        );
-        log.insert("array_field", json!([1, 2, 3]));
-
-        let events = vec![Event::Log(log)];
-
-        let schema = Arc::new(Schema::new(vec![
-            Field::new("object_field", DataType::Utf8, true),
-            Field::new("array_field", DataType::Utf8, true),
-        ]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
+    mod temporal_types {
+        use super::*;
+
+        #[test]
+        fn test_encode_timestamp_precisions() {
+            let now = Utc::now();
+            let mut log = LogEvent::default();
+            log.insert("ts_second", now);
+            log.insert("ts_milli", now);
+            log.insert("ts_micro", now);
+            log.insert("ts_nano", now);
+
+            let events = vec![Event::Log(log)];
+
+            let schema = SchemaRef::new(Schema::new(vec![
+                Field::new(
+                    "ts_second",
+                    DataType::Timestamp(TimeUnit::Second, None),
+                    true,
+                ),
+                Field::new(
+                    "ts_milli",
+                    DataType::Timestamp(TimeUnit::Millisecond, None),
+                    true,
+                ),
+                Field::new(
+                    "ts_micro",
+                    DataType::Timestamp(TimeUnit::Microsecond, None),
+                    true,
+                ),
+                Field::new(
+                    "ts_nano",
+                    DataType::Timestamp(TimeUnit::Nanosecond, None),
+                    true,
+                ),
+            ]));
+
+            let batch = encode_and_decode(events, schema).unwrap();
+
+            assert_eq!(batch.num_rows(), 1);
+            assert_eq!(batch.num_columns(), 4);
+
+            let ts_second = batch
+                .column(0)
+                .as_any()
+                .downcast_ref::<TimestampSecondArray>()
+                .unwrap();
+            assert!(!ts_second.is_null(0));
+            assert_eq!(ts_second.value(0), now.timestamp());
 
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 1);
-
-        let object_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<StringArray>()
-            .unwrap();
-        let object_str = object_array.value(0);
-        assert!(object_str.contains("key"));
-        assert!(object_str.contains("value"));
-
-        let array_array = batch
-            .column(1)
-            .as_any()
-            .downcast_ref::<StringArray>()
-            .unwrap();
-        let array_str = array_array.value(0);
-        assert_eq!(array_str, "[1,2,3]");
-    }
+            let ts_milli = batch
+                .column(1)
+                .as_any()
+                .downcast_ref::<TimestampMillisecondArray>()
+                .unwrap();
+            assert!(!ts_milli.is_null(0));
+            assert_eq!(ts_milli.value(0), now.timestamp_millis());
 
-    #[test]
-    fn test_encode_unsupported_type() {
-        let mut log = LogEvent::default();
-        log.insert("field", "value");
-
-        let events = vec![Event::Log(log)];
-
-        // Use an unsupported type
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "field",
-            DataType::Duration(TimeUnit::Millisecond),
-            true,
-        )]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(schema));
-        assert!(result.is_err());
-        assert!(matches!(
-            result.unwrap_err(),
-            ArrowEncodingError::UnsupportedType { .. }
-        ));
-    }
+            let ts_micro = batch
+                .column(2)
+                .as_any()
+                .downcast_ref::<TimestampMicrosecondArray>()
+                .unwrap();
+            assert!(!ts_micro.is_null(0));
+            assert_eq!(ts_micro.value(0), now.timestamp_micros());
 
-    #[test]
-    fn test_encode_without_schema_fails() {
-        let mut log1 = LogEvent::default();
-        log1.insert("message", "hello");
+            let ts_nano = batch
+                .column(3)
+                .as_any()
+                .downcast_ref::<TimestampNanosecondArray>()
+                .unwrap();
+            assert!(!ts_nano.is_null(0));
+            assert_eq!(ts_nano.value(0), now.timestamp_nanos_opt().unwrap());
+        }
 
-        let events = vec![Event::Log(log1)];
+        #[test]
+        fn test_encode_mixed_timestamp_string_native_and_integer() {
+            // Test mixing RFC3339 string timestamps, native Timestamp values, and integers.
+            // Note: String timestamps require the schema to have Some("UTC") timezone for
+            // serde_arrow to parse them correctly. Native Value::Timestamp values are
+            // converted to integers internally, so they work with any timezone setting.
+            let now = Utc::now();
 
-        let result = encode_events_to_arrow_ipc_stream(&events, None);
-        assert!(result.is_err());
-        assert!(matches!(
-            result.unwrap_err(),
-            ArrowEncodingError::NoSchemaProvided
-        ));
-    }
+            let mut log1 = LogEvent::default();
+            log1.insert("ts", "2025-10-22T10:18:44.256Z"); // RFC3339 String
 
-    #[test]
-    fn test_encode_empty_events() {
-        let events: Vec<Event> = vec![];
-        let result = encode_events_to_arrow_ipc_stream(&events, None);
-        assert!(result.is_err());
-        assert!(matches!(result.unwrap_err(), ArrowEncodingError::NoEvents));
-    }
+            let mut log2 = LogEvent::default();
+            log2.insert("ts", now); // Native Timestamp
 
-    #[test]
-    fn test_encode_timestamp_precisions() {
-        let now = Utc::now();
-        let mut log = LogEvent::default();
-        log.insert("ts_second", now);
-        log.insert("ts_milli", now);
-        log.insert("ts_micro", now);
-        log.insert("ts_nano", now);
+            let mut log3 = LogEvent::default();
+            log3.insert("ts", 1729594724256000000_i64); // Integer (nanoseconds)
 
-        let events = vec![Event::Log(log)];
+            let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
 
-        let schema = Arc::new(Schema::new(vec![
-            Field::new(
-                "ts_second",
-                DataType::Timestamp(TimeUnit::Second, None),
-                true,
-            ),
-            Field::new(
-                "ts_milli",
-                DataType::Timestamp(TimeUnit::Millisecond, None),
-                true,
-            ),
-            Field::new(
-                "ts_micro",
-                DataType::Timestamp(TimeUnit::Microsecond, None),
-                true,
-            ),
-            Field::new(
-                "ts_nano",
-                DataType::Timestamp(TimeUnit::Nanosecond, None),
+            // Use Some("UTC") to enable serde_arrow's RFC3339 string parsing
+            let schema = SchemaRef::new(Schema::new(vec![Field::new(
+                "ts",
+                DataType::Timestamp(TimeUnit::Nanosecond, Some("UTC".into())),
                 true,
-            ),
-        ]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 1);
-        assert_eq!(batch.num_columns(), 4);
-
-        let ts_second = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<TimestampSecondArray>()
-            .unwrap();
-        assert!(!ts_second.is_null(0));
-        assert_eq!(ts_second.value(0), now.timestamp());
-
-        let ts_milli = batch
-            .column(1)
-            .as_any()
-            .downcast_ref::<TimestampMillisecondArray>()
-            .unwrap();
-        assert!(!ts_milli.is_null(0));
-        assert_eq!(ts_milli.value(0), now.timestamp_millis());
-
-        let ts_micro = batch
-            .column(2)
-            .as_any()
-            .downcast_ref::<TimestampMicrosecondArray>()
-            .unwrap();
-        assert!(!ts_micro.is_null(0));
-        assert_eq!(ts_micro.value(0), now.timestamp_micros());
-
-        let ts_nano = batch
-            .column(3)
-            .as_any()
-            .downcast_ref::<TimestampNanosecondArray>()
-            .unwrap();
-        assert!(!ts_nano.is_null(0));
-        assert_eq!(ts_nano.value(0), now.timestamp_nanos_opt().unwrap());
-    }
+            )]));
 
-    #[test]
-    fn test_encode_mixed_timestamp_string_and_native() {
-        // Test mixing string timestamps with native Timestamp values
-        let mut log1 = LogEvent::default();
-        log1.insert("ts", "2025-10-22T10:18:44.256Z"); // String
+            let batch = encode_and_decode(events, schema).unwrap();
 
-        let mut log2 = LogEvent::default();
-        log2.insert("ts", Utc::now()); // Native Timestamp
+            assert_eq!(batch.num_rows(), 3);
 
-        let mut log3 = LogEvent::default();
-        log3.insert("ts", 1729594724256000000_i64); // Integer (nanoseconds)
-
-        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
-
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "ts",
-            DataType::Timestamp(TimeUnit::Nanosecond, None),
-            true,
-        )]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 3);
-
-        let ts_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<TimestampNanosecondArray>()
-            .unwrap();
-
-        // All three should be non-null
-        assert!(!ts_array.is_null(0));
-        assert!(!ts_array.is_null(1));
-        assert!(!ts_array.is_null(2));
-
-        // First one should match the parsed string
-        let expected = chrono::DateTime::parse_from_rfc3339("2025-10-22T10:18:44.256Z")
-            .unwrap()
-            .timestamp_nanos_opt()
-            .unwrap();
-        assert_eq!(ts_array.value(0), expected);
-
-        // Third one should match the integer
-        assert_eq!(ts_array.value(2), 1729594724256000000_i64);
-    }
-
-    #[test]
-    fn test_encode_invalid_string_timestamp() {
-        // Test that invalid timestamp strings become null
-        let mut log1 = LogEvent::default();
-        log1.insert("timestamp", "not-a-timestamp");
-
-        let mut log2 = LogEvent::default();
-        log2.insert("timestamp", "2025-10-22T10:18:44.256Z"); // Valid
-
-        let mut log3 = LogEvent::default();
-        log3.insert("timestamp", "2025-99-99T99:99:99Z"); // Invalid
-
-        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
-
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "timestamp",
-            DataType::Timestamp(TimeUnit::Nanosecond, None),
-            true,
-        )]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 3);
-
-        let ts_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<TimestampNanosecondArray>()
-            .unwrap();
-
-        // Invalid timestamps should be null
-        assert!(ts_array.is_null(0));
-        assert!(!ts_array.is_null(1)); // Valid one
-        assert!(ts_array.is_null(2));
-    }
-
-    #[test]
-    fn test_encode_decimal128_from_integer() {
-        use arrow::array::Decimal128Array;
-
-        let mut log = LogEvent::default();
-        // Store quantity as integer: 1000
-        log.insert("quantity", 1000_i64);
-
-        let events = vec![Event::Log(log)];
-
-        // Decimal(10, 3) - will represent 1000 as 1000.000
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "quantity",
-            DataType::Decimal128(10, 3),
-            true,
-        )]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 1);
-
-        let decimal_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<Decimal128Array>()
-            .unwrap();
-
-        assert!(!decimal_array.is_null(0));
-        // 1000 with scale 3 = 1000 * 10^3 = 1000000
-        assert_eq!(decimal_array.value(0), 1000000_i128);
-    }
-
-    #[test]
-    fn test_encode_decimal256() {
-        use arrow::array::Decimal256Array;
-
-        let mut log = LogEvent::default();
-        // Very large precision number
-        log.insert("big_value", 123456789.123456_f64);
-
-        let events = vec![Event::Log(log)];
-
-        // Decimal256(50, 6) - high precision decimal
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "big_value",
-            DataType::Decimal256(50, 6),
-            true,
-        )]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 1);
-
-        let decimal_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<Decimal256Array>()
-            .unwrap();
-
-        assert!(!decimal_array.is_null(0));
-        // Value should be non-null and encoded
-        let value = decimal_array.value(0);
-        assert!(value.to_i128().is_some());
-    }
-
-    #[test]
-    fn test_encode_decimal_null_values() {
-        use arrow::array::Decimal128Array;
-
-        let mut log1 = LogEvent::default();
-        log1.insert("price", 99.99_f64);
-
-        let log2 = LogEvent::default();
-        // No price field - should be null
-
-        let mut log3 = LogEvent::default();
-        log3.insert("price", 50.00_f64);
-
-        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
-
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "price",
-            DataType::Decimal128(10, 2),
-            true,
-        )]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 3);
-
-        let decimal_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<Decimal128Array>()
-            .unwrap();
-
-        // First row: 99.99
-        assert!(!decimal_array.is_null(0));
-        assert_eq!(decimal_array.value(0), 9999_i128);
-
-        // Second row: null
-        assert!(decimal_array.is_null(1));
-
-        // Third row: 50.00
-        assert!(!decimal_array.is_null(2));
-        assert_eq!(decimal_array.value(2), 5000_i128);
-    }
-
-    #[test]
-    fn test_encode_unsigned_integer_types() {
-        use arrow::array::{UInt8Array, UInt16Array, UInt32Array, UInt64Array};
-
-        let mut log = LogEvent::default();
-        log.insert("uint8_field", 255_i64);
-        log.insert("uint16_field", 65535_i64);
-        log.insert("uint32_field", 4294967295_i64);
-        log.insert("uint64_field", 9223372036854775807_i64);
+            let ts_array = batch
+                .column(0)
+                .as_any()
+                .downcast_ref::<TimestampNanosecondArray>()
+                .unwrap();
 
-        let events = vec![Event::Log(log)];
+            // All three should be non-null
+            assert!(!ts_array.is_null(0));
+            assert!(!ts_array.is_null(1));
+            assert!(!ts_array.is_null(2));
 
-        let schema = Arc::new(Schema::new(vec![
-            Field::new("uint8_field", DataType::UInt8, true),
-            Field::new("uint16_field", DataType::UInt16, true),
-            Field::new("uint32_field", DataType::UInt32, true),
-            Field::new("uint64_field", DataType::UInt64, true),
-        ]));
+            // First one should match the parsed RFC3339 string
+            let expected = chrono::DateTime::parse_from_rfc3339("2025-10-22T10:18:44.256Z")
+                .unwrap()
+                .timestamp_nanos_opt()
+                .unwrap();
+            assert_eq!(ts_array.value(0), expected);
 
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
+            // Second one should match the native timestamp
+            assert_eq!(ts_array.value(1), now.timestamp_nanos_opt().unwrap());
 
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 1);
-        assert_eq!(batch.num_columns(), 4);
-
-        // Verify uint8
-        let uint8_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<UInt8Array>()
-            .unwrap();
-        assert_eq!(uint8_array.value(0), 255_u8);
-
-        // Verify uint16
-        let uint16_array = batch
-            .column(1)
-            .as_any()
-            .downcast_ref::<UInt16Array>()
-            .unwrap();
-        assert_eq!(uint16_array.value(0), 65535_u16);
-
-        // Verify uint32
-        let uint32_array = batch
-            .column(2)
-            .as_any()
-            .downcast_ref::<UInt32Array>()
-            .unwrap();
-        assert_eq!(uint32_array.value(0), 4294967295_u32);
-
-        // Verify uint64
-        let uint64_array = batch
-            .column(3)
-            .as_any()
-            .downcast_ref::<UInt64Array>()
-            .unwrap();
-        assert_eq!(uint64_array.value(0), 9223372036854775807_u64);
+            // Third one should match the integer
+            assert_eq!(ts_array.value(2), 1729594724256000000_i64);
+        }
     }
 
-    #[test]
-    fn test_encode_unsigned_integers_with_null_and_overflow() {
-        use arrow::array::{UInt8Array, UInt32Array};
-
-        let mut log1 = LogEvent::default();
-        log1.insert("uint8_field", 100_i64);
-        log1.insert("uint32_field", 1000_i64);
-
-        let mut log2 = LogEvent::default();
-        log2.insert("uint8_field", 300_i64); // Overflow - should be null
-        log2.insert("uint32_field", -1_i64); // Negative - should be null
+    mod config_tests {
+        use super::*;
+        use tokio_util::codec::Encoder;
 
-        let log3 = LogEvent::default();
-        // Missing fields - should be null
+        #[test]
+        fn test_config_allow_nullable_fields_overrides_schema() {
+            let mut log1 = LogEvent::default();
+            log1.insert("strict_field", 42);
+            let log2 = LogEvent::default();
+            let events = vec![Event::Log(log1), Event::Log(log2)];
 
-        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
+            let schema = Schema::new(vec![Field::new("strict_field", DataType::Int64, false)]);
 
-        let schema = Arc::new(Schema::new(vec![
-            Field::new("uint8_field", DataType::UInt8, true),
-            Field::new("uint32_field", DataType::UInt32, true),
-        ]));
+            let mut config = ArrowStreamSerializerConfig::new(schema);
+            config.allow_nullable_fields = true;
 
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
+            let mut serializer =
+                ArrowStreamSerializer::new(config).expect("Failed to create serializer");
 
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 3);
-
-        // Check uint8 column
-        let uint8_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<UInt8Array>()
-            .unwrap();
-        assert_eq!(uint8_array.value(0), 100_u8); // Valid
-        assert!(uint8_array.is_null(1)); // Overflow
-        assert!(uint8_array.is_null(2)); // Missing
-
-        // Check uint32 column
-        let uint32_array = batch
-            .column(1)
-            .as_any()
-            .downcast_ref::<UInt32Array>()
-            .unwrap();
-        assert_eq!(uint32_array.value(0), 1000_u32); // Valid
-        assert!(uint32_array.is_null(1)); // Negative
-        assert!(uint32_array.is_null(2)); // Missing
-    }
+            let mut buffer = BytesMut::new();
+            serializer
+                .encode(events, &mut buffer)
+                .expect("Encoding should succeed when allow_nullable_fields is true");
 
-    #[test]
-    fn test_encode_non_nullable_field_with_null_value() {
-        // Test that encoding fails when a non-nullable field encounters a null value
-        let mut log1 = LogEvent::default();
-        log1.insert("required_field", 42);
+            let cursor = Cursor::new(buffer);
+            let mut reader = StreamReader::try_new(cursor, None).expect("Failed to create reader");
+            let batch = reader.next().unwrap().expect("Failed to read batch");
 
-        let log2 = LogEvent::default();
-        // log2 is missing required_field - should cause an error
+            assert_eq!(batch.num_rows(), 2);
 
-        let events = vec![Event::Log(log1), Event::Log(log2)];
-
-        // Create schema with non-nullable field
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "required_field",
-            DataType::Int64,
-            false, // Not nullable
-        )]));
+            let binding = batch.schema();
+            let output_field = binding.field(0);
+            assert!(
+                output_field.is_nullable(),
+                "The output schema field should have been transformed to nullable=true"
+            );
 
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(schema));
-        assert!(result.is_err());
+            let array = batch
+                .column(0)
+                .as_any()
+                .downcast_ref::<Int64Array>()
+                .unwrap();
 
-        match result.unwrap_err() {
-            ArrowEncodingError::NullConstraint { field_name } => {
-                assert_eq!(field_name, "required_field");
-            }
-            other => panic!("Expected NullConstraint error, got: {:?}", other),
+            assert_eq!(array.value(0), 42);
+            assert!(!array.is_null(0));
+            assert!(
+                array.is_null(1),
+                "The missing value should be encoded as null"
+            );
         }
-    }
-
-    #[test]
-    fn test_encode_non_nullable_string_field_with_missing_value() {
-        // Test that encoding fails for non-nullable string field
-        let mut log1 = LogEvent::default();
-        log1.insert("name", "Alice");
 
-        let mut log2 = LogEvent::default();
-        log2.insert("name", "Bob");
+        #[test]
+        fn test_make_field_nullable_with_nested_types() {
+            let inner_struct_field = Field::new("nested_field", DataType::Int64, false);
+            let inner_struct =
+                DataType::Struct(arrow::datatypes::Fields::from(vec![inner_struct_field]));
+            let list_field = Field::new("item", inner_struct, false);
+            let list_type = DataType::List(list_field.into());
+            let outer_field = Field::new("inner_list", list_type, false);
+            let outer_struct = DataType::Struct(arrow::datatypes::Fields::from(vec![outer_field]));
 
-        let log3 = LogEvent::default();
-        // log3 is missing name field
+            let original_field = Field::new("root", outer_struct, false);
+            let nullable_field = make_field_nullable(&original_field).unwrap();
 
-        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
+            assert!(
+                nullable_field.is_nullable(),
+                "Root field should be nullable"
+            );
 
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "name",
-            DataType::Utf8,
-            false, // Not nullable
-        )]));
+            if let DataType::Struct(root_fields) = nullable_field.data_type() {
+                let inner_list_field = &root_fields[0];
+                assert!(inner_list_field.is_nullable());
 
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(schema));
-        assert!(result.is_err());
+                if let DataType::List(list_item_field) = inner_list_field.data_type() {
+                    assert!(list_item_field.is_nullable());
 
-        match result.unwrap_err() {
-            ArrowEncodingError::NullConstraint { field_name } => {
-                assert_eq!(field_name, "name");
+                    if let DataType::Struct(inner_struct_fields) = list_item_field.data_type() {
+                        let nested_field = &inner_struct_fields[0];
+                        assert!(nested_field.is_nullable());
+                    } else {
+                        panic!("Expected Struct type for list items");
+                    }
+                } else {
+                    panic!("Expected List type for inner_list");
+                }
+            } else {
+                panic!("Expected Struct type for root field");
             }
-            other => panic!("Expected NullConstraint error, got: {:?}", other),
         }
-    }
-
-    #[test]
-    fn test_encode_non_nullable_field_all_values_present() {
-        // Test that encoding succeeds when all values are present for non-nullable field
-        let mut log1 = LogEvent::default();
-        log1.insert("id", 1);
-
-        let mut log2 = LogEvent::default();
-        log2.insert("id", 2);
-
-        let mut log3 = LogEvent::default();
-        log3.insert("id", 3);
 
-        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
+        #[test]
+        fn test_make_field_nullable_with_map_type() {
+            let key_field = Field::new("key", DataType::Utf8, false);
+            let value_field = Field::new("value", DataType::Int64, false);
+            let entries_struct =
+                DataType::Struct(arrow::datatypes::Fields::from(vec![key_field, value_field]));
+            let entries_field = Field::new("entries", entries_struct, false);
+            let map_type = DataType::Map(entries_field.into(), false);
 
-        let schema = Arc::new(Schema::new(vec![Field::new(
-            "id",
-            DataType::Int64,
-            false, // Not nullable
-        )]));
-
-        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
-        assert!(result.is_ok());
-
-        let bytes = result.unwrap();
-        let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None).unwrap();
-        let batch = reader.next().unwrap().unwrap();
-
-        assert_eq!(batch.num_rows(), 3);
-
-        let id_array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<Int64Array>()
-            .unwrap();
-
-        assert_eq!(id_array.value(0), 1);
-        assert_eq!(id_array.value(1), 2);
-        assert_eq!(id_array.value(2), 3);
-        assert!(!id_array.is_null(0));
-        assert!(!id_array.is_null(1));
-        assert!(!id_array.is_null(2));
-    }
+            let original_field = Field::new("my_map", map_type, false);
+            let nullable_field = make_field_nullable(&original_field).unwrap();
 
-    #[test]
-    fn test_config_allow_nullable_fields_overrides_schema() {
-        use tokio_util::codec::Encoder;
-
-        // Create events: One valid, one missing the "required" field
-        let mut log1 = LogEvent::default();
-        log1.insert("strict_field", 42);
-        let log2 = LogEvent::default();
-        let events = vec![Event::Log(log1), Event::Log(log2)];
-
-        let schema = Schema::new(vec![Field::new("strict_field", DataType::Int64, false)]);
-
-        let mut config = ArrowStreamSerializerConfig::new(schema);
-        config.allow_nullable_fields = true;
-
-        let mut serializer =
-            ArrowStreamSerializer::new(config).expect("Failed to create serializer");
-
-        let mut buffer = BytesMut::new();
-        serializer
-            .encode(events, &mut buffer)
-            .expect("Encoding should succeed when allow_nullable_fields is true");
-
-        let cursor = Cursor::new(buffer);
-        let mut reader = StreamReader::try_new(cursor, None).expect("Failed to create reader");
-        let batch = reader.next().unwrap().expect("Failed to read batch");
-
-        assert_eq!(batch.num_rows(), 2);
-
-        let binding = batch.schema();
-        let output_field = binding.field(0);
-        assert!(
-            output_field.is_nullable(),
-            "The output schema field should have been transformed to nullable=true"
-        );
-
-        let array = batch
-            .column(0)
-            .as_any()
-            .downcast_ref::<Int64Array>()
-            .unwrap();
-
-        assert_eq!(array.value(0), 42);
-        assert!(!array.is_null(0));
-        assert!(
-            array.is_null(1),
-            "The missing value should be encoded as null"
-        );
-    }
-
-    #[test]
-    fn test_make_field_nullable_with_nested_types() {
-        // Test that make_field_nullable recursively handles List and Struct types
-
-        // Create a nested structure: Struct containing a List of Structs
-        // struct { inner_list: [{ nested_field: Int64 }] }
-        let inner_struct_field = Field::new("nested_field", DataType::Int64, false);
-        let inner_struct =
-            DataType::Struct(arrow::datatypes::Fields::from(vec![inner_struct_field]));
-        let list_field = Field::new("item", inner_struct, false);
-        let list_type = DataType::List(Arc::new(list_field));
-        let outer_field = Field::new("inner_list", list_type, false);
-        let outer_struct = DataType::Struct(arrow::datatypes::Fields::from(vec![outer_field]));
-
-        let original_field = Field::new("root", outer_struct, false);
-
-        // Apply make_field_nullable
-        let nullable_field = make_field_nullable(&original_field);
-
-        // Verify root field is nullable
-        assert!(
-            nullable_field.is_nullable(),
-            "Root field should be nullable"
-        );
-
-        // Verify nested struct is nullable
-        if let DataType::Struct(root_fields) = nullable_field.data_type() {
-            let inner_list_field = &root_fields[0];
             assert!(
-                inner_list_field.is_nullable(),
-                "inner_list field should be nullable"
+                nullable_field.is_nullable(),
+                "Root map field should be nullable"
             );
 
-            // Verify list element is nullable
-            if let DataType::List(list_item_field) = inner_list_field.data_type() {
+            if let DataType::Map(entries_field, _sorted) = nullable_field.data_type() {
                 assert!(
-                    list_item_field.is_nullable(),
-                    "List item field should be nullable"
+                    !entries_field.is_nullable(),
+                    "Map entries field should be non-nullable"
                 );
 
-                // Verify inner struct fields are nullable
-                if let DataType::Struct(inner_struct_fields) = list_item_field.data_type() {
-                    let nested_field = &inner_struct_fields[0];
+                if let DataType::Struct(struct_fields) = entries_field.data_type() {
+                    let key_field = &struct_fields[0];
+                    let value_field = &struct_fields[1];
                     assert!(
-                        nested_field.is_nullable(),
-                        "nested_field should be nullable"
+                        !key_field.is_nullable(),
+                        "Map key field should be non-nullable"
+                    );
+                    assert!(
+                        value_field.is_nullable(),
+                        "Map value field should be nullable"
                     );
                 } else {
-                    panic!("Expected Struct type for list items");
+                    panic!("Expected Struct type for map entries");
                 }
             } else {
-                panic!("Expected List type for inner_list");
-            }
-        } else {
-            panic!("Expected Struct type for root field");
-        }
-    }
-
-    #[test]
-    fn test_make_field_nullable_with_map_type() {
-        // Test that make_field_nullable handles Map types
-        // Map is internally represented as List<Struct<key, value>>
-
-        // Create a map: Map<Utf8, Int64>
-        // Internally: List<Struct<entries: {key: Utf8, value: Int64}>>
-        let key_field = Field::new("key", DataType::Utf8, false);
-        let value_field = Field::new("value", DataType::Int64, false);
-        let entries_struct =
-            DataType::Struct(arrow::datatypes::Fields::from(vec![key_field, value_field]));
-        let entries_field = Field::new("entries", entries_struct, false);
-        let map_type = DataType::Map(Arc::new(entries_field), false);
-
-        let original_field = Field::new("my_map", map_type, false);
-
-        // Apply make_field_nullable
-        let nullable_field = make_field_nullable(&original_field);
-
-        // Verify root field is nullable
-        assert!(
-            nullable_field.is_nullable(),
-            "Root map field should be nullable"
-        );
-
-        // Verify map entries are nullable
-        if let DataType::Map(entries_field, _sorted) = nullable_field.data_type() {
-            assert!(
-                entries_field.is_nullable(),
-                "Map entries field should be nullable"
-            );
-
-            // Verify the struct inside the map is nullable
-            if let DataType::Struct(struct_fields) = entries_field.data_type() {
-                let key_field = &struct_fields[0];
-                let value_field = &struct_fields[1];
-                assert!(key_field.is_nullable(), "Map key field should be nullable");
-                assert!(
-                    value_field.is_nullable(),
-                    "Map value field should be nullable"
-                );
-            } else {
-                panic!("Expected Struct type for map entries");
+                panic!("Expected Map type for my_map field");
             }
-        } else {
-            panic!("Expected Map type for my_map field");
         }
     }
 }
diff --git a/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs b/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
index f61ef30c8..24c8e3f7a 100644
--- a/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
+++ b/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
@@ -23,11 +23,13 @@ impl Function for ParseDnstap {
                 keyword: "value",
                 kind: kind::BYTES,
                 required: true,
+                description: "The base64 encoded representation of the DNSTAP data to parse.",
             },
             Parameter {
                 keyword: "lowercase_hostnames",
                 kind: kind::BOOLEAN,
                 required: false,
+                description: "Whether to turn all hostnames found in resulting data lowercase, for consistency.",
             },
         ]
     }
diff --git a/lib/enrichment/src/find_enrichment_table_records.rs b/lib/enrichment/src/find_enrichment_table_records.rs
index 1b016aa90..a604024a7 100644
--- a/lib/enrichment/src/find_enrichment_table_records.rs
+++ b/lib/enrichment/src/find_enrichment_table_records.rs
@@ -64,26 +64,31 @@ impl Function for FindEnrichmentTableRecords {
                 keyword: "table",
                 kind: kind::BYTES,
                 required: true,
+                description: "The [enrichment table](/docs/reference/glossary/#enrichment-tables) to search.",
             },
             Parameter {
                 keyword: "condition",
                 kind: kind::OBJECT,
                 required: true,
+                description: "The condition to search on. Since the condition is used at boot time to create indices into the data, these conditions must be statically defined.",
             },
             Parameter {
                 keyword: "select",
                 kind: kind::ARRAY,
                 required: false,
+                description: "A subset of fields from the enrichment table to return. If not specified, all fields are returned.",
             },
             Parameter {
                 keyword: "case_sensitive",
                 kind: kind::BOOLEAN,
                 required: false,
+                description: "Whether text fields need to match cases exactly.",
             },
             Parameter {
                 keyword: "wildcard",
                 kind: kind::BYTES,
                 required: false,
+                description: "Value to use for wildcard matching in the search.",
             },
         ]
     }
diff --git a/lib/enrichment/src/get_enrichment_table_record.rs b/lib/enrichment/src/get_enrichment_table_record.rs
index ef2103702..d347162e5 100644
--- a/lib/enrichment/src/get_enrichment_table_record.rs
+++ b/lib/enrichment/src/get_enrichment_table_record.rs
@@ -62,26 +62,31 @@ impl Function for GetEnrichmentTableRecord {
                 keyword: "table",
                 kind: kind::BYTES,
                 required: true,
+                description: "The [enrichment table](/docs/reference/glossary/#enrichment-tables) to search.",
             },
             Parameter {
                 keyword: "condition",
                 kind: kind::OBJECT,
                 required: true,
+                description: "The condition to search on. Since the condition is used at boot time to create indices into the data, these conditions must be statically defined.",
             },
             Parameter {
                 keyword: "select",
                 kind: kind::ARRAY,
                 required: false,
+                description: "A subset of fields from the enrichment table to return. If not specified, all fields are returned.",
             },
             Parameter {
                 keyword: "case_sensitive",
                 kind: kind::BOOLEAN,
                 required: false,
+                description: "Whether the text fields match the case exactly.",
             },
             Parameter {
                 keyword: "wildcard",
                 kind: kind::BYTES,
                 required: false,
+                description: "Value to use for wildcard matching in the search.",
             },
         ]
     }
diff --git a/lib/file-source-common/Cargo.toml b/lib/file-source-common/Cargo.toml
index 3b912196c..31730d15e 100644
--- a/lib/file-source-common/Cargo.toml
+++ b/lib/file-source-common/Cargo.toml
@@ -18,7 +18,7 @@ crc = "3.3.0"
 serde = { version = "1.0", default-features = false, features = ["derive"] }
 serde_json = { version = "1.0.143", default-features = false }
 bstr = { version = "1.12", default-features = false }
-bytes = { version = "1.10.1", default-features = false, features = ["serde"] }
+bytes = { version = "1.11.1", default-features = false, features = ["serde"] }
 dashmap = { version = "6.1", default-features = false }
 async-compression = { version = "0.4.27", features = ["tokio", "gzip"] }
 vector-common = { path = "../vector-common", default-features = false }
diff --git a/lib/vector-api-client/Cargo.toml b/lib/vector-api-client/Cargo.toml
index 5b3b32908..3fbc200af 100644
--- a/lib/vector-api-client/Cargo.toml
+++ b/lib/vector-api-client/Cargo.toml
@@ -18,14 +18,14 @@ anyhow.workspace = true
 # Tokio / Futures
 futures.workspace = true
 tokio = { workspace = true, features = ["macros", "rt", "sync"] }
-tokio-stream = { version = "0.1.17", default-features = false, features = ["sync"] }
+tokio-stream = { workspace = true, features = ["sync"] }
 
 # GraphQL
 graphql_client = { version = "0.14.0", default-features = false, features = ["graphql_query_derive"] }
 
 # HTTP / WebSockets
 reqwest = { version = "0.11.26", default-features = false, features = ["json"] }
-tokio-tungstenite = { version = "0.20.1", default-features = false, features = ["connect", "rustls"] }
+tokio-tungstenite = { workspace = true, features = ["connect", "rustls"] }
 
 # External libs
 chrono.workspace = true
diff --git a/lib/vector-buffers/Cargo.toml b/lib/vector-buffers/Cargo.toml
index bb99e14b8..c89500fad 100644
--- a/lib/vector-buffers/Cargo.toml
+++ b/lib/vector-buffers/Cargo.toml
@@ -47,7 +47,7 @@ quickcheck = "1.0"
 rand.workspace = true
 serde_yaml.workspace = true
 temp-dir = "0.1.16"
-tokio-test = "0.4.4"
+tokio-test.workspace = true
 tracing-fluent-assertions = { version = "0.3" }
 tracing-subscriber = { workspace = true, features = ["env-filter", "fmt", "registry", "std", "ansi"] }
 
diff --git a/lib/vector-common/Cargo.toml b/lib/vector-common/Cargo.toml
index 3604dfa94..06f3a2f87 100644
--- a/lib/vector-common/Cargo.toml
+++ b/lib/vector-common/Cargo.toml
@@ -35,7 +35,7 @@ tokenize = []
 
 [dependencies]
 async-stream = "0.3.6"
-bytes = { version = "1.10.1", default-features = false, optional = true }
+bytes = { version = "1.11.1", default-features = false, optional = true }
 chrono.workspace = true
 crossbeam-utils.workspace = true
 derivative.workspace = true
diff --git a/lib/vector-core/Cargo.toml b/lib/vector-core/Cargo.toml
index 76969fbe4..9efe3226a 100644
--- a/lib/vector-core/Cargo.toml
+++ b/lib/vector-core/Cargo.toml
@@ -51,7 +51,7 @@ snafu.workspace = true
 socket2.workspace = true
 tokio = { workspace = true, features = ["net"] }
 tokio-openssl = { version = "0.6.5", default-features = false }
-tokio-stream = { version = "0.1", default-features = false, features = ["time"], optional = true }
+tokio-stream = { workspace = true, features = ["time"], optional = true }
 tokio-util = { version = "0.7.0", default-features = false, features = ["time"] }
 toml.workspace = true
 tonic.workspace = true
@@ -84,7 +84,7 @@ quickcheck = "1"
 quickcheck_macros = "1"
 proptest = "1.8"
 similar-asserts = "1.7.0"
-tokio-test = "0.4.4"
+tokio-test.workspace = true
 toml.workspace = true
 ndarray = "0.16.1"
 ndarray-stats = "0.6.0"
diff --git a/lib/vector-core/src/transform/mod.rs b/lib/vector-core/src/transform/mod.rs
index 335245e1f..5d2829e95 100644
--- a/lib/vector-core/src/transform/mod.rs
+++ b/lib/vector-core/src/transform/mod.rs
@@ -1,31 +1,19 @@
-use std::{collections::HashMap, error, pin::Pin, sync::Arc, time::Instant};
+use std::{collections::HashMap, pin::Pin, sync::Arc};
 
 use futures::{Stream, StreamExt};
-use vector_common::{
-    EventDataEq,
-    byte_size_of::ByteSizeOf,
-    internal_event::{
-        self, CountByteSize, DEFAULT_OUTPUT, EventsSent, InternalEventHandle as _, Registered,
-        register,
-    },
-    json_size::JsonSize,
-};
 
 use crate::{
-    config,
-    config::{ComponentKey, OutputId},
-    event::{
-        EstimatedJsonEncodedSizeOf, Event, EventArray, EventContainer, EventMutRef, EventRef,
-        into_event_stream,
-    },
-    fanout::{self, Fanout},
-    schema,
+    config::OutputId,
+    event::{Event, EventArray, EventContainer, EventMutRef, into_event_stream},
     schema::Definition,
 };
 
+mod outputs;
 #[cfg(feature = "lua")]
 pub mod runtime_transform;
 
+pub use outputs::{OutputBuffer, TransformOutputs, TransformOutputsBuf};
+
 /// Transforms come in two variants. Functions, or tasks.
 ///
 /// While function transforms can be run out of order, or concurrently, task
@@ -182,132 +170,6 @@ impl SyncTransform for Box<dyn FunctionTransform> {
     }
 }
 
-struct TransformOutput {
-    fanout: Fanout,
-    events_sent: Registered<EventsSent>,
-    log_schema_definitions: HashMap<OutputId, Arc<schema::Definition>>,
-    output_id: Arc<OutputId>,
-}
-
-pub struct TransformOutputs {
-    outputs_spec: Vec<config::TransformOutput>,
-    primary_output: Option<TransformOutput>,
-    named_outputs: HashMap<String, TransformOutput>,
-}
-
-impl TransformOutputs {
-    pub fn new(
-        outputs_in: Vec<config::TransformOutput>,
-        component_key: &ComponentKey,
-    ) -> (Self, HashMap<Option<String>, fanout::ControlChannel>) {
-        let outputs_spec = outputs_in.clone();
-        let mut primary_output = None;
-        let mut named_outputs = HashMap::new();
-        let mut controls = HashMap::new();
-
-        for output in outputs_in {
-            let (fanout, control) = Fanout::new();
-
-            let log_schema_definitions = output
-                .log_schema_definitions
-                .into_iter()
-                .map(|(id, definition)| (id, Arc::new(definition)))
-                .collect();
-
-            match output.port {
-                None => {
-                    primary_output = Some(TransformOutput {
-                        fanout,
-                        events_sent: register(EventsSent::from(internal_event::Output(Some(
-                            DEFAULT_OUTPUT.into(),
-                        )))),
-                        log_schema_definitions,
-                        output_id: Arc::new(OutputId {
-                            component: component_key.clone(),
-                            port: None,
-                        }),
-                    });
-                    controls.insert(None, control);
-                }
-                Some(name) => {
-                    named_outputs.insert(
-                        name.clone(),
-                        TransformOutput {
-                            fanout,
-                            events_sent: register(EventsSent::from(internal_event::Output(Some(
-                                name.clone().into(),
-                            )))),
-                            log_schema_definitions,
-                            output_id: Arc::new(OutputId {
-                                component: component_key.clone(),
-                                port: Some(name.clone()),
-                            }),
-                        },
-                    );
-                    controls.insert(Some(name.clone()), control);
-                }
-            }
-        }
-
-        let me = Self {
-            outputs_spec,
-            primary_output,
-            named_outputs,
-        };
-
-        (me, controls)
-    }
-
-    pub fn new_buf_with_capacity(&self, capacity: usize) -> TransformOutputsBuf {
-        TransformOutputsBuf::new_with_capacity(self.outputs_spec.clone(), capacity)
-    }
-
-    /// Sends the events in the buffer to their respective outputs.
-    ///
-    /// # Errors
-    ///
-    /// If an error occurs while sending events to their respective output, an error variant will be
-    /// returned detailing the cause.
-    pub async fn send(
-        &mut self,
-        buf: &mut TransformOutputsBuf,
-    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
-        if let Some(primary) = self.primary_output.as_mut() {
-            let buf = buf
-                .primary_buffer
-                .as_mut()
-                .unwrap_or_else(|| unreachable!("mismatched outputs"));
-            Self::send_single_buffer(buf, primary).await?;
-        }
-        for (key, buf) in &mut buf.named_buffers {
-            let output = self
-                .named_outputs
-                .get_mut(key)
-                .unwrap_or_else(|| unreachable!("unknown output"));
-            Self::send_single_buffer(buf, output).await?;
-        }
-        Ok(())
-    }
-
-    async fn send_single_buffer(
-        buf: &mut OutputBuffer,
-        output: &mut TransformOutput,
-    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
-        for event in buf.events_mut() {
-            update_runtime_schema_definition(
-                event,
-                &output.output_id,
-                &output.log_schema_definitions,
-            );
-        }
-        let count = buf.len();
-        let byte_size = buf.estimated_json_encoded_size_of();
-        buf.send(&mut output.fanout).await?;
-        output.events_sent.emit(CountByteSize(count, byte_size));
-        Ok(())
-    }
-}
-
 #[allow(clippy::implicit_hasher)]
 /// `event`: The event that will be updated
 /// `output_id`: The `output_id` that the current even is being sent to (will be used as the new `parent_id`)
@@ -334,221 +196,6 @@ pub fn update_runtime_schema_definition(
     event.metadata_mut().set_upstream_id(Arc::clone(output_id));
 }
 
-#[derive(Debug, Clone)]
-pub struct TransformOutputsBuf {
-    primary_buffer: Option<OutputBuffer>,
-    named_buffers: HashMap<String, OutputBuffer>,
-}
-
-impl TransformOutputsBuf {
-    pub fn new_with_capacity(outputs_in: Vec<config::TransformOutput>, capacity: usize) -> Self {
-        let mut primary_buffer = None;
-        let mut named_buffers = HashMap::new();
-
-        for output in outputs_in {
-            match output.port {
-                None => {
-                    primary_buffer = Some(OutputBuffer::with_capacity(capacity));
-                }
-                Some(name) => {
-                    named_buffers.insert(name.clone(), OutputBuffer::default());
-                }
-            }
-        }
-
-        Self {
-            primary_buffer,
-            named_buffers,
-        }
-    }
-
-    /// Adds a new event to the named output buffer.
-    ///
-    /// # Panics
-    ///
-    /// Panics if there is no output with the given name.
-    pub fn push(&mut self, name: Option<&str>, event: Event) {
-        match name {
-            Some(name) => self.named_buffers.get_mut(name),
-            None => self.primary_buffer.as_mut(),
-        }
-        .expect("unknown output")
-        .push(event);
-    }
-
-    /// Drains the default output buffer.
-    ///
-    /// # Panics
-    ///
-    /// Panics if there is no default output.
-    pub fn drain(&mut self) -> impl Iterator<Item = Event> + '_ {
-        self.primary_buffer
-            .as_mut()
-            .expect("no default output")
-            .drain()
-    }
-
-    /// Drains the named output buffer.
-    ///
-    /// # Panics
-    ///
-    /// Panics if there is no output with the given name.
-    pub fn drain_named(&mut self, name: &str) -> impl Iterator<Item = Event> + '_ {
-        self.named_buffers
-            .get_mut(name)
-            .expect("unknown output")
-            .drain()
-    }
-
-    /// Takes the default output buffer.
-    ///
-    /// # Panics
-    ///
-    /// Panics if there is no default output.
-    pub fn take_primary(&mut self) -> OutputBuffer {
-        std::mem::take(self.primary_buffer.as_mut().expect("no default output"))
-    }
-
-    pub fn take_all_named(&mut self) -> HashMap<String, OutputBuffer> {
-        std::mem::take(&mut self.named_buffers)
-    }
-}
-
-impl ByteSizeOf for TransformOutputsBuf {
-    fn allocated_bytes(&self) -> usize {
-        self.primary_buffer.size_of()
-            + self
-                .named_buffers
-                .values()
-                .map(ByteSizeOf::size_of)
-                .sum::<usize>()
-    }
-}
-
-#[derive(Debug, Default, Clone)]
-pub struct OutputBuffer(Vec<EventArray>);
-
-impl OutputBuffer {
-    pub fn with_capacity(capacity: usize) -> Self {
-        Self(Vec::with_capacity(capacity))
-    }
-
-    pub fn push(&mut self, event: Event) {
-        // Coalesce multiple pushes of the same type into one array.
-        match (event, self.0.last_mut()) {
-            (Event::Log(log), Some(EventArray::Logs(logs))) => {
-                logs.push(log);
-            }
-            (Event::Metric(metric), Some(EventArray::Metrics(metrics))) => {
-                metrics.push(metric);
-            }
-            (Event::Trace(trace), Some(EventArray::Traces(traces))) => {
-                traces.push(trace);
-            }
-            (event, _) => {
-                self.0.push(event.into());
-            }
-        }
-    }
-
-    pub fn append(&mut self, events: &mut Vec<Event>) {
-        for event in events.drain(..) {
-            self.push(event);
-        }
-    }
-
-    pub fn extend(&mut self, events: impl Iterator<Item = Event>) {
-        for event in events {
-            self.push(event);
-        }
-    }
-
-    pub fn is_empty(&self) -> bool {
-        self.0.is_empty()
-    }
-
-    pub fn len(&self) -> usize {
-        self.0.iter().map(EventArray::len).sum()
-    }
-
-    pub fn capacity(&self) -> usize {
-        self.0.capacity()
-    }
-
-    pub fn first(&self) -> Option<EventRef<'_>> {
-        self.0.first().and_then(|first| match first {
-            EventArray::Logs(l) => l.first().map(Into::into),
-            EventArray::Metrics(m) => m.first().map(Into::into),
-            EventArray::Traces(t) => t.first().map(Into::into),
-        })
-    }
-
-    pub fn drain(&mut self) -> impl Iterator<Item = Event> + '_ {
-        self.0.drain(..).flat_map(EventArray::into_events)
-    }
-
-    async fn send(
-        &mut self,
-        output: &mut Fanout,
-    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
-        let send_start = Some(Instant::now());
-        for array in std::mem::take(&mut self.0) {
-            output.send(array, send_start).await?;
-        }
-
-        Ok(())
-    }
-
-    fn iter_events(&self) -> impl Iterator<Item = EventRef<'_>> {
-        self.0.iter().flat_map(EventArray::iter_events)
-    }
-
-    fn events_mut(&mut self) -> impl Iterator<Item = EventMutRef<'_>> {
-        self.0.iter_mut().flat_map(EventArray::iter_events_mut)
-    }
-
-    pub fn into_events(self) -> impl Iterator<Item = Event> {
-        self.0.into_iter().flat_map(EventArray::into_events)
-    }
-}
-
-impl ByteSizeOf for OutputBuffer {
-    fn allocated_bytes(&self) -> usize {
-        self.0.iter().map(ByteSizeOf::size_of).sum()
-    }
-}
-
-impl EventDataEq<Vec<Event>> for OutputBuffer {
-    fn event_data_eq(&self, other: &Vec<Event>) -> bool {
-        struct Comparator<'a>(EventRef<'a>);
-
-        impl PartialEq<&Event> for Comparator<'_> {
-            fn eq(&self, that: &&Event) -> bool {
-                self.0.event_data_eq(that)
-            }
-        }
-
-        self.iter_events().map(Comparator).eq(other.iter())
-    }
-}
-
-impl EstimatedJsonEncodedSizeOf for OutputBuffer {
-    fn estimated_json_encoded_size_of(&self) -> JsonSize {
-        self.0
-            .iter()
-            .map(EstimatedJsonEncodedSizeOf::estimated_json_encoded_size_of)
-            .sum()
-    }
-}
-
-impl From<Vec<Event>> for OutputBuffer {
-    fn from(events: Vec<Event>) -> Self {
-        let mut result = Self::default();
-        result.extend(events.into_iter());
-        result
-    }
-}
-
 struct WrapEventTask<T>(T);
 
 impl<T: TaskTransform<Event> + Send + 'static> TaskTransform<EventArray> for WrapEventTask<T> {
diff --git a/lib/vector-core/src/transform/outputs.rs b/lib/vector-core/src/transform/outputs.rs
new file mode 100644
index 000000000..e9a4dcf2f
--- /dev/null
+++ b/lib/vector-core/src/transform/outputs.rs
@@ -0,0 +1,358 @@
+use std::{collections::HashMap, error, sync::Arc, time::Instant};
+
+use vector_common::{
+    EventDataEq,
+    byte_size_of::ByteSizeOf,
+    internal_event::{
+        self, CountByteSize, DEFAULT_OUTPUT, EventsSent, InternalEventHandle as _, Registered,
+        register,
+    },
+    json_size::JsonSize,
+};
+
+use crate::{
+    config,
+    config::{ComponentKey, OutputId},
+    event::{EstimatedJsonEncodedSizeOf, Event, EventArray, EventContainer, EventMutRef, EventRef},
+    fanout::{self, Fanout},
+    schema,
+};
+
+struct TransformOutput {
+    fanout: Fanout,
+    events_sent: Registered<EventsSent>,
+    log_schema_definitions: HashMap<OutputId, Arc<schema::Definition>>,
+    output_id: Arc<OutputId>,
+}
+
+pub struct TransformOutputs {
+    outputs_spec: Vec<config::TransformOutput>,
+    primary_output: Option<TransformOutput>,
+    named_outputs: HashMap<String, TransformOutput>,
+}
+
+impl TransformOutputs {
+    pub fn new(
+        outputs_in: Vec<config::TransformOutput>,
+        component_key: &ComponentKey,
+    ) -> (Self, HashMap<Option<String>, fanout::ControlChannel>) {
+        let outputs_spec = outputs_in.clone();
+        let mut primary_output = None;
+        let mut named_outputs = HashMap::new();
+        let mut controls = HashMap::new();
+
+        for output in outputs_in {
+            let (fanout, control) = Fanout::new();
+
+            let log_schema_definitions = output
+                .log_schema_definitions
+                .into_iter()
+                .map(|(id, definition)| (id, Arc::new(definition)))
+                .collect();
+
+            match output.port {
+                None => {
+                    primary_output = Some(TransformOutput {
+                        fanout,
+                        events_sent: register(EventsSent::from(internal_event::Output(Some(
+                            DEFAULT_OUTPUT.into(),
+                        )))),
+                        log_schema_definitions,
+                        output_id: Arc::new(OutputId {
+                            component: component_key.clone(),
+                            port: None,
+                        }),
+                    });
+                    controls.insert(None, control);
+                }
+                Some(name) => {
+                    named_outputs.insert(
+                        name.clone(),
+                        TransformOutput {
+                            fanout,
+                            events_sent: register(EventsSent::from(internal_event::Output(Some(
+                                name.clone().into(),
+                            )))),
+                            log_schema_definitions,
+                            output_id: Arc::new(OutputId {
+                                component: component_key.clone(),
+                                port: Some(name.clone()),
+                            }),
+                        },
+                    );
+                    controls.insert(Some(name.clone()), control);
+                }
+            }
+        }
+
+        let me = Self {
+            outputs_spec,
+            primary_output,
+            named_outputs,
+        };
+
+        (me, controls)
+    }
+
+    pub fn new_buf_with_capacity(&self, capacity: usize) -> TransformOutputsBuf {
+        TransformOutputsBuf::new_with_capacity(self.outputs_spec.clone(), capacity)
+    }
+
+    /// Sends the events in the buffer to their respective outputs.
+    ///
+    /// # Errors
+    ///
+    /// If an error occurs while sending events to their respective output, an error variant will be
+    /// returned detailing the cause.
+    pub async fn send(
+        &mut self,
+        buf: &mut TransformOutputsBuf,
+    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
+        if let Some(primary) = self.primary_output.as_mut() {
+            let Some(buf) = buf.primary_buffer.as_mut() else {
+                unreachable!("mismatched outputs");
+            };
+            Self::send_single_buffer(buf, primary).await?;
+        }
+        for (key, buf) in &mut buf.named_buffers {
+            let Some(output) = self.named_outputs.get_mut(key) else {
+                unreachable!("unknown output");
+            };
+            Self::send_single_buffer(buf, output).await?;
+        }
+        Ok(())
+    }
+
+    async fn send_single_buffer(
+        buf: &mut OutputBuffer,
+        output: &mut TransformOutput,
+    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
+        for event in buf.events_mut() {
+            super::update_runtime_schema_definition(
+                event,
+                &output.output_id,
+                &output.log_schema_definitions,
+            );
+        }
+        let count = buf.len();
+        let byte_size = buf.estimated_json_encoded_size_of();
+        buf.send(&mut output.fanout).await?;
+        output.events_sent.emit(CountByteSize(count, byte_size));
+        Ok(())
+    }
+}
+
+#[derive(Debug, Clone)]
+pub struct TransformOutputsBuf {
+    pub(super) primary_buffer: Option<OutputBuffer>,
+    pub(super) named_buffers: HashMap<String, OutputBuffer>,
+}
+
+impl TransformOutputsBuf {
+    pub fn new_with_capacity(outputs_in: Vec<config::TransformOutput>, capacity: usize) -> Self {
+        let mut primary_buffer = None;
+        let mut named_buffers = HashMap::new();
+
+        for output in outputs_in {
+            match output.port {
+                None => {
+                    primary_buffer = Some(OutputBuffer::with_capacity(capacity));
+                }
+                Some(name) => {
+                    named_buffers.insert(name.clone(), OutputBuffer::default());
+                }
+            }
+        }
+
+        Self {
+            primary_buffer,
+            named_buffers,
+        }
+    }
+
+    /// Adds a new event to the named output buffer.
+    ///
+    /// # Panics
+    ///
+    /// Panics if there is no output with the given name.
+    pub fn push(&mut self, name: Option<&str>, event: Event) {
+        match name {
+            Some(name) => self.named_buffers.get_mut(name),
+            None => self.primary_buffer.as_mut(),
+        }
+        .expect("unknown output")
+        .push(event);
+    }
+
+    /// Drains the default output buffer.
+    ///
+    /// # Panics
+    ///
+    /// Panics if there is no default output.
+    pub fn drain(&mut self) -> impl Iterator<Item = Event> + '_ {
+        self.primary_buffer
+            .as_mut()
+            .expect("no default output")
+            .drain()
+    }
+
+    /// Drains the named output buffer.
+    ///
+    /// # Panics
+    ///
+    /// Panics if there is no output with the given name.
+    pub fn drain_named(&mut self, name: &str) -> impl Iterator<Item = Event> + '_ {
+        self.named_buffers
+            .get_mut(name)
+            .expect("unknown output")
+            .drain()
+    }
+
+    /// Takes the default output buffer.
+    ///
+    /// # Panics
+    ///
+    /// Panics if there is no default output.
+    pub fn take_primary(&mut self) -> OutputBuffer {
+        std::mem::take(self.primary_buffer.as_mut().expect("no default output"))
+    }
+
+    pub fn take_all_named(&mut self) -> HashMap<String, OutputBuffer> {
+        std::mem::take(&mut self.named_buffers)
+    }
+}
+
+impl ByteSizeOf for TransformOutputsBuf {
+    fn allocated_bytes(&self) -> usize {
+        self.primary_buffer.size_of()
+            + self
+                .named_buffers
+                .values()
+                .map(ByteSizeOf::size_of)
+                .sum::<usize>()
+    }
+}
+
+#[derive(Debug, Default, Clone)]
+pub struct OutputBuffer(pub(super) Vec<EventArray>);
+
+impl OutputBuffer {
+    pub fn with_capacity(capacity: usize) -> Self {
+        Self(Vec::with_capacity(capacity))
+    }
+
+    pub fn push(&mut self, event: Event) {
+        // Coalesce multiple pushes of the same type into one array.
+        match (event, self.0.last_mut()) {
+            (Event::Log(log), Some(EventArray::Logs(logs))) => {
+                logs.push(log);
+            }
+            (Event::Metric(metric), Some(EventArray::Metrics(metrics))) => {
+                metrics.push(metric);
+            }
+            (Event::Trace(trace), Some(EventArray::Traces(traces))) => {
+                traces.push(trace);
+            }
+            (event, _) => {
+                self.0.push(event.into());
+            }
+        }
+    }
+
+    pub fn append(&mut self, events: &mut Vec<Event>) {
+        for event in events.drain(..) {
+            self.push(event);
+        }
+    }
+
+    pub fn extend(&mut self, events: impl Iterator<Item = Event>) {
+        for event in events {
+            self.push(event);
+        }
+    }
+
+    pub fn is_empty(&self) -> bool {
+        self.0.is_empty()
+    }
+
+    pub fn len(&self) -> usize {
+        self.0.iter().map(EventArray::len).sum()
+    }
+
+    pub fn capacity(&self) -> usize {
+        self.0.capacity()
+    }
+
+    pub fn first(&self) -> Option<EventRef<'_>> {
+        self.0.first().and_then(|first| match first {
+            EventArray::Logs(l) => l.first().map(Into::into),
+            EventArray::Metrics(m) => m.first().map(Into::into),
+            EventArray::Traces(t) => t.first().map(Into::into),
+        })
+    }
+
+    pub fn drain(&mut self) -> impl Iterator<Item = Event> + '_ {
+        self.0.drain(..).flat_map(EventArray::into_events)
+    }
+
+    async fn send(
+        &mut self,
+        output: &mut Fanout,
+    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
+        let send_start = Some(Instant::now());
+        for array in std::mem::take(&mut self.0) {
+            output.send(array, send_start).await?;
+        }
+
+        Ok(())
+    }
+
+    fn iter_events(&self) -> impl Iterator<Item = EventRef<'_>> {
+        self.0.iter().flat_map(EventArray::iter_events)
+    }
+
+    fn events_mut(&mut self) -> impl Iterator<Item = EventMutRef<'_>> {
+        self.0.iter_mut().flat_map(EventArray::iter_events_mut)
+    }
+
+    pub fn into_events(self) -> impl Iterator<Item = Event> {
+        self.0.into_iter().flat_map(EventArray::into_events)
+    }
+}
+
+impl ByteSizeOf for OutputBuffer {
+    fn allocated_bytes(&self) -> usize {
+        self.0.iter().map(ByteSizeOf::size_of).sum()
+    }
+}
+
+impl EventDataEq<Vec<Event>> for OutputBuffer {
+    fn event_data_eq(&self, other: &Vec<Event>) -> bool {
+        struct Comparator<'a>(EventRef<'a>);
+
+        impl PartialEq<&Event> for Comparator<'_> {
+            fn eq(&self, that: &&Event) -> bool {
+                self.0.event_data_eq(that)
+            }
+        }
+
+        self.iter_events().map(Comparator).eq(other.iter())
+    }
+}
+
+impl EstimatedJsonEncodedSizeOf for OutputBuffer {
+    fn estimated_json_encoded_size_of(&self) -> JsonSize {
+        self.0
+            .iter()
+            .map(EstimatedJsonEncodedSizeOf::estimated_json_encoded_size_of)
+            .sum()
+    }
+}
+
+impl From<Vec<Event>> for OutputBuffer {
+    fn from(events: Vec<Event>) -> Self {
+        let mut result = Self::default();
+        result.extend(events.into_iter());
+        result
+    }
+}
diff --git a/lib/vector-tap/Cargo.toml b/lib/vector-tap/Cargo.toml
index 325520eac..976233f44 100644
--- a/lib/vector-tap/Cargo.toml
+++ b/lib/vector-tap/Cargo.toml
@@ -16,8 +16,8 @@ futures.workspace = true
 glob.workspace = true
 serde_yaml.workspace = true
 tokio = { workspace = true, features = ["time"] }
-tokio-stream = { version = "0.1.17", default-features = false, features = ["sync"] }
-tokio-tungstenite = { version = "0.20.1", default-features = false }
+tokio-stream = { workspace = true, features = ["sync"] }
+tokio-tungstenite.workspace = true
 tracing.workspace = true
 url = { version = "2.5.4", default-features = false }
 uuid.workspace = true
diff --git a/lib/vector-top/Cargo.toml b/lib/vector-top/Cargo.toml
index 4a746f1d4..8e494cc58 100644
--- a/lib/vector-top/Cargo.toml
+++ b/lib/vector-top/Cargo.toml
@@ -14,7 +14,7 @@ futures-util = { workspace = true, features = ["alloc"] }
 glob.workspace = true
 indoc.workspace = true
 tokio = { workspace = true, features = ["full"] }
-tokio-stream = { version = "0.1.17", default-features = false, features = ["net", "sync", "time"] }
+tokio-stream = { workspace = true, features = ["net", "sync", "time"] }
 url.workspace = true
 humantime = { version = "2.2.0", default-features = false }
 crossterm = { version = "0.29.0", default-features = false, features = ["event-stream", "windows"] }
diff --git a/lib/vector-vrl-metrics/Cargo.toml b/lib/vector-vrl-metrics/Cargo.toml
index 6f3bbb7ea..6998448b6 100644
--- a/lib/vector-vrl-metrics/Cargo.toml
+++ b/lib/vector-vrl-metrics/Cargo.toml
@@ -12,5 +12,5 @@ const-str.workspace = true
 vrl.workspace = true
 vector-core = { path = "../vector-core", default-features = false, features = ["vrl"] }
 vector-common = { path = "../vector-common", default-features = false }
-tokio = { version = "1.45.1", default-features = false }
-tokio-stream = { version = "0.1.17", default-features = false }
+tokio.workspace = true
+tokio-stream.workspace = true
diff --git a/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs b/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
index d51c132a9..85c6c1377 100644
--- a/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
+++ b/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
@@ -60,16 +60,19 @@ impl Function for AggregateVectorMetrics {
                 keyword: "function",
                 kind: kind::BYTES,
                 required: true,
+                description: "The metric name to search.",
             },
             Parameter {
                 keyword: "key",
                 kind: kind::BYTES,
                 required: true,
+                description: "The metric name to aggregate.",
             },
             Parameter {
                 keyword: "tags",
                 kind: kind::OBJECT,
                 required: false,
+                description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
             },
         ]
     }
diff --git a/lib/vector-vrl-metrics/src/find_vector_metrics.rs b/lib/vector-vrl-metrics/src/find_vector_metrics.rs
index 5ca5535a0..78a014065 100644
--- a/lib/vector-vrl-metrics/src/find_vector_metrics.rs
+++ b/lib/vector-vrl-metrics/src/find_vector_metrics.rs
@@ -43,11 +43,13 @@ impl Function for FindVectorMetrics {
                 keyword: "key",
                 kind: kind::BYTES,
                 required: true,
+                description: "The metric name to search.",
             },
             Parameter {
                 keyword: "tags",
                 kind: kind::OBJECT,
                 required: false,
+                description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
             },
         ]
     }
diff --git a/lib/vector-vrl-metrics/src/get_vector_metric.rs b/lib/vector-vrl-metrics/src/get_vector_metric.rs
index 5f24705a9..67fa6349e 100644
--- a/lib/vector-vrl-metrics/src/get_vector_metric.rs
+++ b/lib/vector-vrl-metrics/src/get_vector_metric.rs
@@ -40,11 +40,13 @@ impl Function for GetVectorMetric {
                 keyword: "key",
                 kind: kind::BYTES,
                 required: true,
+                description: "The metric name to search.",
             },
             Parameter {
                 keyword: "tags",
                 kind: kind::OBJECT,
                 required: false,
+                description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
             },
         ]
     }
diff --git a/lib/vector-vrl/functions/src/get_secret.rs b/lib/vector-vrl/functions/src/get_secret.rs
index b643d3415..49152f31d 100644
--- a/lib/vector-vrl/functions/src/get_secret.rs
+++ b/lib/vector-vrl/functions/src/get_secret.rs
@@ -26,6 +26,7 @@ impl Function for GetSecret {
             keyword: "key",
             kind: kind::BYTES,
             required: true,
+            description: "The name of the secret.",
         }]
     }
 
diff --git a/lib/vector-vrl/functions/src/remove_secret.rs b/lib/vector-vrl/functions/src/remove_secret.rs
index 5bdc75153..47c75b459 100644
--- a/lib/vector-vrl/functions/src/remove_secret.rs
+++ b/lib/vector-vrl/functions/src/remove_secret.rs
@@ -23,6 +23,7 @@ impl Function for RemoveSecret {
             keyword: "key",
             kind: kind::BYTES,
             required: true,
+            description: "The name of the secret to remove.",
         }]
     }
 
diff --git a/lib/vector-vrl/functions/src/set_secret.rs b/lib/vector-vrl/functions/src/set_secret.rs
index 5a128b448..598822c1f 100644
--- a/lib/vector-vrl/functions/src/set_secret.rs
+++ b/lib/vector-vrl/functions/src/set_secret.rs
@@ -31,11 +31,13 @@ impl Function for SetSecret {
                 keyword: "key",
                 kind: kind::BYTES,
                 required: true,
+                description: "The name of the secret.",
             },
             Parameter {
                 keyword: "secret",
                 kind: kind::BYTES,
                 required: true,
+                description: "The secret value.",
             },
         ]
     }
diff --git a/lib/vector-vrl/functions/src/set_semantic_meaning.rs b/lib/vector-vrl/functions/src/set_semantic_meaning.rs
index 19dc0be34..86bbb6336 100644
--- a/lib/vector-vrl/functions/src/set_semantic_meaning.rs
+++ b/lib/vector-vrl/functions/src/set_semantic_meaning.rs
@@ -52,11 +52,13 @@ impl Function for SetSemanticMeaning {
                 keyword: "target",
                 kind: kind::ANY,
                 required: true,
+                description: "The path of the value that is assigned a meaning.",
             },
             Parameter {
                 keyword: "meaning",
                 kind: kind::BYTES,
                 required: true,
+                description: "The name of the meaning to assign.",
             },
         ]
     }
diff --git a/src/providers/http.rs b/src/providers/http.rs
index c7dfa043e..975db2e17 100644
--- a/src/providers/http.rs
+++ b/src/providers/http.rs
@@ -10,7 +10,7 @@ use vector_lib::configurable::configurable_component;
 
 use super::BuildResult;
 use crate::{
-    config::{self, Format, ProxyConfig, provider::ProviderConfig},
+    config::{self, Format, ProxyConfig, interpolate, provider::ProviderConfig},
     http::HttpClient,
     signal,
     tls::{TlsConfig, TlsSettings},
@@ -57,6 +57,9 @@ pub struct HttpConfig {
     /// Which config format expected to be loaded
     #[configurable(derived)]
     config_format: Format,
+
+    /// Enable environment variable interpolation
+    interpolate_env: bool,
 }
 
 impl Default for HttpConfig {
@@ -68,6 +71,7 @@ impl Default for HttpConfig {
             tls_options: None,
             proxy: Default::default(),
             config_format: Format::default(),
+            interpolate_env: false,
         }
     }
 }
@@ -135,12 +139,31 @@ async fn http_request_to_config_builder(
     headers: &IndexMap<String, String>,
     proxy: &ProxyConfig,
     config_format: &Format,
+    interpolate_env: bool,
 ) -> BuildResult {
     let config_str = http_request(url, tls_options, headers, proxy)
         .await
         .map_err(|e| vec![e.to_owned()])?;
 
-    config::load(config_str.chunk(), *config_format)
+    if !interpolate_env {
+        return config::load(config_str.chunk(), *config_format);
+    }
+
+    let env_vars = std::env::vars_os()
+        .map(|(k, v)| {
+            (
+                k.as_os_str().to_string_lossy().to_string(),
+                v.as_os_str().to_string_lossy().to_string(),
+            )
+        })
+        .collect::<std::collections::HashMap<String, String>>();
+
+    let config_str = interpolate(
+        std::str::from_utf8(&config_str).map_err(|e| vec![e.to_string()])?,
+        &env_vars,
+    )?;
+
+    config::load(config_str.as_bytes().chunk(), *config_format)
 }
 
 /// Polls the HTTP endpoint after/every `poll_interval_secs`, returning a stream of `ConfigBuilder`.
@@ -151,6 +174,7 @@ fn poll_http(
     headers: IndexMap<String, String>,
     proxy: ProxyConfig,
     config_format: Format,
+    interpolate_env: bool,
 ) -> impl Stream<Item = signal::SignalTo> {
     let duration = time::Duration::from_secs(poll_interval_secs);
     let mut interval = time::interval_at(time::Instant::now() + duration, duration);
@@ -159,7 +183,7 @@ fn poll_http(
         loop {
             interval.tick().await;
 
-            match http_request_to_config_builder(&url, tls_options.as_ref(), &headers, &proxy, &config_format).await {
+            match http_request_to_config_builder(&url, tls_options.as_ref(), &headers, &proxy, &config_format, interpolate_env).await {
                 Ok(config_builder) => yield signal::SignalTo::ReloadFromConfigBuilder(config_builder),
                 Err(_) => {},
             };
@@ -191,6 +215,7 @@ impl ProviderConfig for HttpConfig {
             &request.headers,
             &proxy,
             &config_format,
+            self.interpolate_env,
         )
         .await?;
 
@@ -202,6 +227,7 @@ impl ProviderConfig for HttpConfig {
             request.headers.clone(),
             proxy.clone(),
             config_format,
+            self.interpolate_env,
         ));
 
         Ok(config_builder)
diff --git a/src/sinks/clickhouse/arrow/parser.rs b/src/sinks/clickhouse/arrow/parser.rs
index a13bd8234..ddafd0d0d 100644
--- a/src/sinks/clickhouse/arrow/parser.rs
+++ b/src/sinks/clickhouse/arrow/parser.rs
@@ -1,6 +1,18 @@
 //! ClickHouse type parsing and conversion to Arrow types.
 
-use arrow::datatypes::{DataType, TimeUnit};
+use std::str::FromStr;
+
+use arrow::datatypes::{DataType, Field, Fields, TimeUnit};
+use itertools::Itertools;
+use nom::{
+    IResult, Parser,
+    bytes::complete::{tag, take_till, take_while1},
+    character::complete::{char, i8 as parse_i8, u8 as parse_u8, u32 as parse_u32},
+    combinator::{all_consuming, cut, opt},
+    error::{Error, ErrorKind},
+    multi::separated_list0,
+    sequence::{delimited, preceded, separated_pair, terminated},
+};
 
 const DECIMAL32_PRECISION: u8 = 9;
 const DECIMAL64_PRECISION: u8 = 18;
@@ -9,264 +21,270 @@ const DECIMAL256_PRECISION: u8 = 76;
 
 /// Represents a ClickHouse type with its modifiers and nested structure.
 #[derive(Debug, PartialEq, Clone)]
-pub enum ClickHouseType<'a> {
-    /// A primitive type like String, Int64, DateTime, etc.
-    Primitive(&'a str),
-    /// Nullable(T)
-    Nullable(Box<ClickHouseType<'a>>),
-    /// LowCardinality(T)
-    LowCardinality(Box<ClickHouseType<'a>>),
+pub enum ClickHouseType {
+    // Numeric types
+    Int8,
+    Int16,
+    Int32,
+    Int64,
+    UInt8,
+    UInt16,
+    UInt32,
+    UInt64,
+    Float32,
+    Float64,
+    Bool,
+
+    // Decimal with precision and scale
+    Decimal { precision: u8, scale: i8 },
+
+    // String types
+    String,
+    FixedString(u32),
+
+    // Date/time types
+    Date,
+    DateTime,
+    DateTime64 { precision: u8 },
+
+    // Wrapper types
+    Nullable(Box<ClickHouseType>),
+    LowCardinality(Box<ClickHouseType>),
+    Array(Box<ClickHouseType>),
+    Tuple(Vec<(Option<String>, ClickHouseType)>),
+    Map(Box<ClickHouseType>, Box<ClickHouseType>),
 }
 
-impl<'a> ClickHouseType<'a> {
+impl ClickHouseType {
     /// Returns true if this type or any of its nested types is Nullable.
     pub fn is_nullable(&self) -> bool {
         match self {
-            ClickHouseType::Nullable(_) => true,
-            ClickHouseType::LowCardinality(inner) => inner.is_nullable(),
+            Self::Nullable(_) => true,
+            Self::LowCardinality(inner) => inner.is_nullable(),
             _ => false,
         }
     }
 
-    /// Returns the innermost base type, unwrapping all modifiers.
-    /// For example: LowCardinality(Nullable(String)) -> Primitive("String")
-    pub fn base_type(&self) -> &ClickHouseType<'a> {
+    /// Converts this ClickHouse type to an Arrow DataType.
+    /// Recursively handles nested types including Nullable/LowCardinality wrappers.
+    fn to_data_type(&self) -> Result<DataType, String> {
         match self {
-            ClickHouseType::Nullable(inner) | ClickHouseType::LowCardinality(inner) => {
-                inner.base_type()
+            // Wrapper types - recurse to inner type
+            Self::Nullable(inner) | Self::LowCardinality(inner) => inner.to_data_type(),
+
+            // Numeric types
+            Self::Int8 => Ok(DataType::Int8),
+            Self::Int16 => Ok(DataType::Int16),
+            Self::Int32 => Ok(DataType::Int32),
+            Self::Int64 => Ok(DataType::Int64),
+            Self::UInt8 => Ok(DataType::UInt8),
+            Self::UInt16 => Ok(DataType::UInt16),
+            Self::UInt32 => Ok(DataType::UInt32),
+            Self::UInt64 => Ok(DataType::UInt64),
+            Self::Float32 => Ok(DataType::Float32),
+            Self::Float64 => Ok(DataType::Float64),
+            Self::Bool => Ok(DataType::Boolean),
+
+            // Decimal
+            Self::Decimal { precision, scale } => Ok(if *precision <= DECIMAL128_PRECISION {
+                DataType::Decimal128(*precision, *scale)
+            } else {
+                DataType::Decimal256(*precision, *scale)
+            }),
+
+            // String types
+            Self::String | Self::FixedString(_) => Ok(DataType::Utf8),
+
+            // Date/time types
+            Self::Date => Ok(DataType::Date32),
+            Self::DateTime => Ok(DataType::Timestamp(TimeUnit::Second, None)),
+            Self::DateTime64 { precision } => {
+                let unit = match precision {
+                    0 => TimeUnit::Second,
+                    1..=3 => TimeUnit::Millisecond,
+                    4..=6 => TimeUnit::Microsecond,
+                    7..=9 => TimeUnit::Nanosecond,
+                    _ => {
+                        return Err(format!(
+                            "Unsupported DateTime64 precision {}. Must be 0-9",
+                            precision
+                        ));
+                    }
+                };
+                Ok(DataType::Timestamp(unit, None))
+            }
+
+            // Container types
+            Self::Array(inner) => {
+                let (inner_arrow, inner_nullable) = inner.as_ref().try_into()?;
+                Ok(DataType::List(
+                    Field::new("item", inner_arrow, inner_nullable).into(),
+                ))
+            }
+            Self::Tuple(elements) => {
+                let fields: Vec<Field> = elements
+                    .iter()
+                    .enumerate()
+                    .map(|(i, (name_opt, elem))| {
+                        let (dt, nullable) = elem.try_into()?;
+                        let name = name_opt.clone().unwrap_or_else(|| format!("f{i}"));
+                        Ok::<_, String>(Field::new(name, dt, nullable))
+                    })
+                    .try_collect()?;
+                Ok(DataType::Struct(Fields::from(fields)))
+            }
+            Self::Map(key_type, value_type) => {
+                let (key_arrow, _): (DataType, bool) = key_type.as_ref().try_into()?;
+                if !matches!(key_arrow, DataType::Utf8) {
+                    return Err("Map keys must be String type.".to_string());
+                }
+                let (value_arrow, value_nullable) = value_type.as_ref().try_into()?;
+                let entries = DataType::Struct(Fields::from(vec![
+                    Field::new("keys", DataType::Utf8, false),
+                    Field::new("values", value_arrow, value_nullable),
+                ]));
+                Ok(DataType::Map(
+                    Field::new("entries", entries, false).into(),
+                    false,
+                ))
             }
-            _ => self,
         }
     }
 }
 
-/// Parses a ClickHouse type string into a structured representation.
-pub fn parse_ch_type(ty: &str) -> ClickHouseType<'_> {
-    let ty = ty.trim();
+impl TryFrom<&ClickHouseType> for (DataType, bool) {
+    type Error = String;
 
-    // Recursively strip and parse type modifiers
-    if let Some(inner) = strip_wrapper(ty, "Nullable") {
-        return ClickHouseType::Nullable(Box::new(parse_ch_type(inner)));
-    }
-    if let Some(inner) = strip_wrapper(ty, "LowCardinality") {
-        return ClickHouseType::LowCardinality(Box::new(parse_ch_type(inner)));
+    fn try_from(ch_type: &ClickHouseType) -> Result<Self, Self::Error> {
+        Ok((ch_type.to_data_type()?, ch_type.is_nullable()))
     }
-
-    // Base case: return primitive type for anything without modifiers
-    ClickHouseType::Primitive(ty)
 }
 
-/// Helper function to strip a wrapper from a type string.
-/// Returns the inner content if the type matches the wrapper pattern.
-fn strip_wrapper<'a>(ty: &'a str, wrapper_name: &str) -> Option<&'a str> {
-    ty.strip_prefix(wrapper_name)?
-        .trim_start()
-        .strip_prefix('(')?
-        .strip_suffix(')')
+/// Wraps a parser in parentheses with cut (no backtracking after open paren).
+fn parens<'a, O>(
+    inner: impl Parser<&'a str, Output = O, Error = nom::error::Error<&'a str>>,
+) -> impl Parser<&'a str, Output = O, Error = nom::error::Error<&'a str>> {
+    delimited(char('('), cut(inner), char(')'))
 }
 
-/// Unwraps ClickHouse type modifiers like Nullable() and LowCardinality().
-/// Returns a tuple of (base_type, is_nullable).
-/// For example: "LowCardinality(Nullable(String))" -> ("String", true)
-pub fn unwrap_type_modifiers(ch_type: &str) -> (&str, bool) {
-    let parsed = parse_ch_type(ch_type);
-    let is_nullable = parsed.is_nullable();
-
-    match parsed.base_type() {
-        ClickHouseType::Primitive(base) => (base, is_nullable),
-        _ => (ch_type, is_nullable),
-    }
-}
-
-fn unsupported(ch_type: &str, kind: &str) -> String {
-    format!(
-        "{kind} type '{ch_type}' is not supported. \
-         ClickHouse {kind} types cannot be automatically converted to Arrow format."
-    )
+/// Parses an identifier (alphanumeric + underscore, at least one char).
+fn identifier(input: &str) -> IResult<&str, &str> {
+    take_while1(|c: char| c.is_alphanumeric() || c == '_')(input)
 }
 
-/// Converts a ClickHouse type string to an Arrow DataType.
-/// Returns a tuple of (DataType, is_nullable).
-pub fn clickhouse_type_to_arrow(ch_type: &str) -> Result<(DataType, bool), String> {
-    let (base_type, is_nullable) = unwrap_type_modifiers(ch_type);
-    let (type_name, _) = extract_identifier(base_type);
-
-    let data_type = match type_name {
-        // Numeric
-        "Int8" => DataType::Int8,
-        "Int16" => DataType::Int16,
-        "Int32" => DataType::Int32,
-        "Int64" => DataType::Int64,
-        "UInt8" => DataType::UInt8,
-        "UInt16" => DataType::UInt16,
-        "UInt32" => DataType::UInt32,
-        "UInt64" => DataType::UInt64,
-        "Float32" => DataType::Float32,
-        "Float64" => DataType::Float64,
-        "Bool" => DataType::Boolean,
-        "Decimal" | "Decimal32" | "Decimal64" | "Decimal128" | "Decimal256" => {
-            parse_decimal_type(base_type)?
-        }
-
-        // Strings
-        "String" | "FixedString" => DataType::Utf8,
-
-        // Date and time types (timezones not currently handled, defaults to UTC)
-        "Date" | "Date32" => DataType::Date32,
-        "DateTime" => DataType::Timestamp(TimeUnit::Second, None),
-        "DateTime64" => parse_datetime64_precision(base_type)?,
-
-        // Unsupported
-        "Array" => return Err(unsupported(ch_type, "Array")),
-        "Tuple" => return Err(unsupported(ch_type, "Tuple")),
-        "Map" => return Err(unsupported(ch_type, "Map")),
-
-        // Unknown
-        _ => {
-            return Err(format!(
-                "Unknown ClickHouse type '{}'. This type cannot be automatically converted.",
-                type_name
-            ));
+/// Parses a single tuple element (either "Type" or "name Type").
+fn tuple_element(input: &str) -> IResult<&str, (Option<String>, ClickHouseType)> {
+    let (rest, name) = identifier(input)?;
+    match rest.strip_prefix(' ') {
+        Some(after_space) => {
+            let (rest, ty) = ch_type(after_space)?;
+            Ok((rest, (Some(name.to_owned()), ty)))
         }
-    };
-
-    Ok((data_type, is_nullable))
-}
-
-/// Extracts an identifier from the start of a string.
-/// Returns (identifier, remaining_string).
-fn extract_identifier(input: &str) -> (&str, &str) {
-    for (i, c) in input.char_indices() {
-        if c.is_alphabetic() || c == '_' || (i > 0 && c.is_numeric()) {
-            continue;
+        None => {
+            // No space after identifier, so re-parse as a type
+            let (rest, ty) = ch_type(input)?;
+            Ok((rest, (None, ty)))
         }
-        return (&input[..i], &input[i..]);
     }
-    (input, "")
 }
 
-/// Parses comma-separated arguments from a parenthesized string.
-/// Input: "(arg1, arg2, arg3)" -> Output: Ok(vec!["arg1".to_string(), "arg2".to_string(), "arg3".to_string()])
-/// Returns an error if parentheses are malformed.
-fn parse_args(input: &str) -> Result<Vec<String>, String> {
-    let trimmed = input.trim();
-    if !trimmed.starts_with('(') || !trimmed.ends_with(')') {
-        return Err(format!(
-            "Expected parentheses around arguments in '{}'",
-            input
-        ));
-    }
-
-    let inner = trimmed[1..trimmed.len() - 1].trim();
-    if inner.is_empty() {
-        return Ok(vec![]);
-    }
-
-    // Split by comma, handling nested parentheses and quotes
-    let mut args = Vec::new();
-    let mut current_arg = String::new();
-    let mut depth = 0;
-    let mut in_quotes = false;
-
-    for c in inner.chars() {
-        match c {
-            '\'' if !in_quotes => in_quotes = true,
-            '\'' if in_quotes => in_quotes = false,
-            '(' if !in_quotes => depth += 1,
-            ')' if !in_quotes => depth -= 1,
-            ',' if depth == 0 && !in_quotes => {
-                args.push(current_arg.trim().to_string());
-                current_arg = String::new();
-                continue;
-            }
-            _ => {}
+/// Parses a complete ClickHouse type.
+///
+/// Nom parsers return `(rest, output)` where `rest` is the remaining unparsed input.
+/// For example, parsing `"Array(String)"`:
+///   - `identifier` consumes `"Array"`, returns `rest = "(String)"`, `name = "Array"`
+///   - The `"Array"` match arm then parses `rest` with `parens(ch_type)`
+fn ch_type(input: &str) -> IResult<&str, ClickHouseType> {
+    let (rest, name) = identifier(input)?;
+
+    match name {
+        // Wrapper types
+        "Nullable" => parens(ch_type)
+            .map(|t| ClickHouseType::Nullable(Box::new(t)))
+            .parse(rest),
+        "LowCardinality" => parens(ch_type)
+            .map(|t| ClickHouseType::LowCardinality(Box::new(t)))
+            .parse(rest),
+        "Array" => parens(ch_type)
+            .map(|t| ClickHouseType::Array(Box::new(t)))
+            .parse(rest),
+        "Map" => parens(separated_pair(ch_type, tag(", "), ch_type))
+            .map(|(k, v)| ClickHouseType::Map(Box::new(k), Box::new(v)))
+            .parse(rest),
+        "Tuple" => parens(separated_list0(tag(", "), tuple_element))
+            .map(ClickHouseType::Tuple)
+            .parse(rest),
+
+        // Numeric types
+        "Int8" => Ok((rest, ClickHouseType::Int8)),
+        "Int16" => Ok((rest, ClickHouseType::Int16)),
+        "Int32" => Ok((rest, ClickHouseType::Int32)),
+        "Int64" => Ok((rest, ClickHouseType::Int64)),
+        "UInt8" => Ok((rest, ClickHouseType::UInt8)),
+        "UInt16" => Ok((rest, ClickHouseType::UInt16)),
+        "UInt32" => Ok((rest, ClickHouseType::UInt32)),
+        "UInt64" => Ok((rest, ClickHouseType::UInt64)),
+        "Float32" => Ok((rest, ClickHouseType::Float32)),
+        "Float64" => Ok((rest, ClickHouseType::Float64)),
+        "Bool" => Ok((rest, ClickHouseType::Bool)),
+
+        // String types
+        "String" => Ok((rest, ClickHouseType::String)),
+        "FixedString" => parens(parse_u32)
+            .map(ClickHouseType::FixedString)
+            .parse(rest),
+
+        // Date/time types
+        "Date" | "Date32" => Ok((rest, ClickHouseType::Date)),
+        "DateTime" => Ok((rest, ClickHouseType::DateTime)),
+        "DateTime64" => {
+            let tz = delimited(char('\''), take_till(|c| c == '\''), char('\''));
+            parens(terminated(parse_u8, opt(preceded(tag(", "), tz))))
+                .map(|p| ClickHouseType::DateTime64 { precision: p })
+                .parse(rest)
         }
-        current_arg.push(c);
-    }
-
-    if !current_arg.trim().is_empty() {
-        args.push(current_arg.trim().to_string());
-    }
-
-    Ok(args)
-}
 
-/// Parses ClickHouse Decimal types and returns the appropriate Arrow decimal type.
-/// ClickHouse formats:
-/// - Decimal(P, S) -> generic decimal with precision P and scale S
-/// - Decimal32(S) -> precision up to 9, scale S
-/// - Decimal64(S) -> precision up to 18, scale S
-/// - Decimal128(S) -> precision up to 38, scale S
-/// - Decimal256(S) -> precision up to 76, scale S
-///
-/// Uses metadata from ClickHouse's system.columns when available, otherwise falls back to parsing the type string.
-fn parse_decimal_type(ch_type: &str) -> Result<DataType, String> {
-    // Parse from type string
-    let (type_name, args_str) = extract_identifier(ch_type);
-
-    let result = parse_args(args_str).ok().and_then(|args| match type_name {
-        "Decimal" if args.len() == 2 => args[0].parse::<u8>().ok().zip(args[1].parse::<i8>().ok()),
-        "Decimal32" | "Decimal64" | "Decimal128" | "Decimal256" if args.len() == 1 => {
-            args[0].parse::<i8>().ok().map(|scale| {
-                let precision = match type_name {
-                    "Decimal32" => DECIMAL32_PRECISION,
-                    "Decimal64" => DECIMAL64_PRECISION,
-                    "Decimal128" => DECIMAL128_PRECISION,
-                    "Decimal256" => DECIMAL256_PRECISION,
-                    _ => unreachable!(),
-                };
-                (precision, scale)
+        // Decimal types
+        "Decimal" => parens(separated_pair(parse_u8, tag(", "), parse_i8))
+            .map(|(precision, scale)| ClickHouseType::Decimal { precision, scale })
+            .parse(rest),
+        "Decimal32" => parens(parse_i8)
+            .map(|scale| ClickHouseType::Decimal {
+                precision: DECIMAL32_PRECISION,
+                scale,
             })
-        }
-        _ => None,
-    });
+            .parse(rest),
+        "Decimal64" => parens(parse_i8)
+            .map(|scale| ClickHouseType::Decimal {
+                precision: DECIMAL64_PRECISION,
+                scale,
+            })
+            .parse(rest),
+        "Decimal128" => parens(parse_i8)
+            .map(|scale| ClickHouseType::Decimal {
+                precision: DECIMAL128_PRECISION,
+                scale,
+            })
+            .parse(rest),
+        "Decimal256" => parens(parse_i8)
+            .map(|scale| ClickHouseType::Decimal {
+                precision: DECIMAL256_PRECISION,
+                scale,
+            })
+            .parse(rest),
 
-    result
-        .map(|(precision, scale)| {
-            if precision <= DECIMAL128_PRECISION {
-                DataType::Decimal128(precision, scale)
-            } else {
-                DataType::Decimal256(precision, scale)
-            }
-        })
-        .ok_or_else(|| format!("Could not parse Decimal type '{}'.", ch_type))
+        _ => Err(nom::Err::Error(Error::new(input, ErrorKind::Tag))),
+    }
 }
 
-/// Parses DateTime64 precision and returns the appropriate Arrow timestamp type.
-/// DateTime64(0) -> Second
-/// DateTime64(3) -> Millisecond
-/// DateTime64(6) -> Microsecond
-/// DateTime64(9) -> Nanosecond
-///
-fn parse_datetime64_precision(ch_type: &str) -> Result<DataType, String> {
-    // Parse from type string
-    let (_type_name, args_str) = extract_identifier(ch_type);
-
-    let args = parse_args(args_str).map_err(|e| {
-        format!(
-            "Could not parse DateTime64 arguments from '{}': {}. Expected format: DateTime64(0-9) or DateTime64(0-9, 'timezone')",
-            ch_type, e
-        )
-    })?;
-
-    // DateTime64(precision) or DateTime64(precision, 'timezone')
-    if args.is_empty() {
-        return Err(format!(
-            "DateTime64 type '{}' has no precision argument. Expected format: DateTime64(0-9) or DateTime64(0-9, 'timezone')",
-            ch_type
-        ));
-    }
+impl FromStr for ClickHouseType {
+    type Err = String;
 
-    // Parse the precision (first argument)
-    match args[0].parse::<u8>() {
-        Ok(0) => Ok(DataType::Timestamp(TimeUnit::Second, None)),
-        Ok(1..=3) => Ok(DataType::Timestamp(TimeUnit::Millisecond, None)),
-        Ok(4..=6) => Ok(DataType::Timestamp(TimeUnit::Microsecond, None)),
-        Ok(7..=9) => Ok(DataType::Timestamp(TimeUnit::Nanosecond, None)),
-        _ => Err(format!(
-            "Unsupported DateTime64 precision in '{}'. Precision must be 0-9",
-            ch_type
-        )),
+    fn from_str(s: &str) -> Result<Self, Self::Err> {
+        all_consuming(ch_type)
+            .parse(s)
+            .map(|(_, parsed)| parsed)
+            .map_err(|e| format!("Failed to parse ClickHouse type '{s}': {e}"))
     }
 }
 
@@ -274,33 +292,19 @@ fn parse_datetime64_precision(ch_type: &str) -> Result<DataType, String> {
 mod tests {
     use super::*;
 
-    // Helper function for tests that don't need metadata
-    fn convert_type_no_metadata(ch_type: &str) -> Result<(DataType, bool), String> {
-        clickhouse_type_to_arrow(ch_type)
+    // Helper function for tests
+    fn convert_type(s: &str) -> Result<(DataType, bool), String> {
+        (&ClickHouseType::from_str(s)?).try_into()
     }
 
     #[test]
     fn test_clickhouse_type_mapping() {
+        assert_eq!(convert_type("String").unwrap(), (DataType::Utf8, false));
+        assert_eq!(convert_type("Int64").unwrap(), (DataType::Int64, false));
+        assert_eq!(convert_type("Float64").unwrap(), (DataType::Float64, false));
+        assert_eq!(convert_type("Bool").unwrap(), (DataType::Boolean, false));
         assert_eq!(
-            convert_type_no_metadata("String").expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Utf8, false)
-        );
-        assert_eq!(
-            convert_type_no_metadata("Int64").expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Int64, false)
-        );
-        assert_eq!(
-            convert_type_no_metadata("Float64")
-                .expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Float64, false)
-        );
-        assert_eq!(
-            convert_type_no_metadata("Bool").expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Boolean, false)
-        );
-        assert_eq!(
-            convert_type_no_metadata("DateTime")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime").unwrap(),
             (DataType::Timestamp(TimeUnit::Second, None), false)
         );
     }
@@ -308,85 +312,58 @@ mod tests {
     #[test]
     fn test_datetime64_precision_mapping() {
         assert_eq!(
-            convert_type_no_metadata("DateTime64(0)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(0)").unwrap(),
             (DataType::Timestamp(TimeUnit::Second, None), false)
         );
         assert_eq!(
-            convert_type_no_metadata("DateTime64(3)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(3)").unwrap(),
             (DataType::Timestamp(TimeUnit::Millisecond, None), false)
         );
         assert_eq!(
-            convert_type_no_metadata("DateTime64(6)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(6)").unwrap(),
             (DataType::Timestamp(TimeUnit::Microsecond, None), false)
         );
         assert_eq!(
-            convert_type_no_metadata("DateTime64(9)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(9)").unwrap(),
             (DataType::Timestamp(TimeUnit::Nanosecond, None), false)
         );
-        // Test with timezones
+        // Test with timezones (ignored)
         assert_eq!(
-            convert_type_no_metadata("DateTime64(9, 'UTC')")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(9, 'UTC')").unwrap(),
             (DataType::Timestamp(TimeUnit::Nanosecond, None), false)
         );
         assert_eq!(
-            convert_type_no_metadata("DateTime64(6, 'UTC')")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(6, 'America/New_York')").unwrap(),
             (DataType::Timestamp(TimeUnit::Microsecond, None), false)
         );
+        // Edge cases
         assert_eq!(
-            convert_type_no_metadata("DateTime64(9, 'America/New_York')")
-                .expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Timestamp(TimeUnit::Nanosecond, None), false)
-        );
-        // Test edge cases for precision ranges
-        assert_eq!(
-            convert_type_no_metadata("DateTime64(1)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(1)").unwrap(),
             (DataType::Timestamp(TimeUnit::Millisecond, None), false)
         );
         assert_eq!(
-            convert_type_no_metadata("DateTime64(4)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(4)").unwrap(),
             (DataType::Timestamp(TimeUnit::Microsecond, None), false)
         );
         assert_eq!(
-            convert_type_no_metadata("DateTime64(7)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("DateTime64(7)").unwrap(),
             (DataType::Timestamp(TimeUnit::Nanosecond, None), false)
         );
     }
 
     #[test]
     fn test_nullable_type_mapping() {
-        // Non-nullable types
-        assert_eq!(
-            convert_type_no_metadata("String").expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Utf8, false)
-        );
-        assert_eq!(
-            convert_type_no_metadata("Int64").expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Int64, false)
-        );
-
-        // Nullable types
+        assert_eq!(convert_type("String").unwrap(), (DataType::Utf8, false));
         assert_eq!(
-            convert_type_no_metadata("Nullable(String)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Nullable(String)").unwrap(),
             (DataType::Utf8, true)
         );
         assert_eq!(
-            convert_type_no_metadata("Nullable(Int64)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Nullable(Int64)").unwrap(),
             (DataType::Int64, true)
         );
         assert_eq!(
-            convert_type_no_metadata("Nullable(Float64)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Nullable(Float64)").unwrap(),
             (DataType::Float64, true)
         );
     }
@@ -394,254 +371,286 @@ mod tests {
     #[test]
     fn test_lowcardinality_type_mapping() {
         assert_eq!(
-            convert_type_no_metadata("LowCardinality(String)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("LowCardinality(String)").unwrap(),
             (DataType::Utf8, false)
         );
         assert_eq!(
-            convert_type_no_metadata("LowCardinality(FixedString(10))")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("LowCardinality(FixedString(10))").unwrap(),
             (DataType::Utf8, false)
         );
-        // Nullable + LowCardinality
         assert_eq!(
-            convert_type_no_metadata("LowCardinality(Nullable(String))")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("LowCardinality(Nullable(String))").unwrap(),
             (DataType::Utf8, true)
         );
     }
 
     #[test]
     fn test_decimal_type_mapping() {
-        // Generic Decimal(P, S)
+        // Decimal(P, S)
         assert_eq!(
-            convert_type_no_metadata("Decimal(10, 2)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Decimal(10, 2)").unwrap(),
             (DataType::Decimal128(10, 2), false)
         );
         assert_eq!(
-            convert_type_no_metadata("Decimal(38, 6)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Decimal(38, 6)").unwrap(),
             (DataType::Decimal128(38, 6), false)
         );
         assert_eq!(
-            convert_type_no_metadata("Decimal(50, 10)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Decimal(50, 10)").unwrap(),
             (DataType::Decimal256(50, 10), false)
         );
 
-        // Generic Decimal without spaces and with spaces
+        // Decimal32(S) - precision 9
         assert_eq!(
-            convert_type_no_metadata("Decimal(10,2)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Decimal128(10, 2), false)
-        );
-        assert_eq!(
-            convert_type_no_metadata("Decimal( 18 , 6 )")
-                .expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Decimal128(18, 6), false)
-        );
-
-        // Decimal32(S) - precision up to 9
-        assert_eq!(
-            convert_type_no_metadata("Decimal32(2)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Decimal32(2)").unwrap(),
             (DataType::Decimal128(9, 2), false)
         );
-        assert_eq!(
-            convert_type_no_metadata("Decimal32(4)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Decimal128(9, 4), false)
-        );
 
-        // Decimal64(S) - precision up to 18
+        // Decimal64(S) - precision 18
         assert_eq!(
-            convert_type_no_metadata("Decimal64(4)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Decimal64(4)").unwrap(),
             (DataType::Decimal128(18, 4), false)
         );
-        assert_eq!(
-            convert_type_no_metadata("Decimal64(8)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
-            (DataType::Decimal128(18, 8), false)
-        );
 
-        // Decimal128(S) - precision up to 38
+        // Decimal128(S) - precision 38
         assert_eq!(
-            convert_type_no_metadata("Decimal128(10)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Decimal128(10)").unwrap(),
             (DataType::Decimal128(38, 10), false)
         );
 
-        // Decimal256(S) - precision up to 76
+        // Decimal256(S) - precision 76
         assert_eq!(
-            convert_type_no_metadata("Decimal256(20)")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Decimal256(20)").unwrap(),
             (DataType::Decimal256(76, 20), false)
         );
 
-        // With Nullable wrapper
+        // Nullable
         assert_eq!(
-            convert_type_no_metadata("Nullable(Decimal(18, 6))")
-                .expect("Failed to convert ClickHouse type to Arrow"),
+            convert_type("Nullable(Decimal(18, 6))").unwrap(),
             (DataType::Decimal128(18, 6), true)
         );
     }
 
     #[test]
-    fn test_extract_identifier() {
-        assert_eq!(extract_identifier("Decimal(10, 2)"), ("Decimal", "(10, 2)"));
-        assert_eq!(extract_identifier("DateTime64(3)"), ("DateTime64", "(3)"));
-        assert_eq!(extract_identifier("Int32"), ("Int32", ""));
-        assert_eq!(
-            extract_identifier("LowCardinality(String)"),
-            ("LowCardinality", "(String)")
-        );
-        assert_eq!(extract_identifier("Decimal128(10)"), ("Decimal128", "(10)"));
+    fn test_array_type() {
+        let (data_type, is_nullable) = convert_type("Array(Int32)").unwrap();
+        assert!(!is_nullable);
+        match data_type {
+            DataType::List(field) => {
+                assert_eq!(field.data_type(), &DataType::Int32);
+                assert!(!field.is_nullable());
+            }
+            _ => panic!("Expected List type"),
+        }
     }
 
     #[test]
-    fn test_parse_args() {
-        // Simple cases
-        assert_eq!(
-            parse_args("(10, 2)").unwrap(),
-            vec!["10".to_string(), "2".to_string()]
-        );
-        assert_eq!(parse_args("(3)").unwrap(), vec!["3".to_string()]);
-        assert_eq!(parse_args("()").unwrap(), Vec::<String>::new());
+    fn test_tuple_type() {
+        let (data_type, is_nullable) = convert_type("Tuple(String, Int64)").unwrap();
+        assert!(!is_nullable);
+        match data_type {
+            DataType::Struct(fields) => {
+                assert_eq!(fields.len(), 2);
+                assert_eq!(fields[0].data_type(), &DataType::Utf8);
+                assert_eq!(fields[1].data_type(), &DataType::Int64);
+            }
+            _ => panic!("Expected Struct type"),
+        }
+    }
+
+    #[test]
+    fn test_map_type() {
+        let (data_type, is_nullable) = convert_type("Map(String, Int64)").unwrap();
+        assert!(!is_nullable);
+        assert!(matches!(data_type, DataType::Map(_, _)));
+    }
 
-        // With spaces
+    #[test]
+    fn test_unknown_type_fails() {
+        let result = convert_type("UnknownType");
+        assert!(result.is_err());
+    }
+
+    #[test]
+    fn test_parse_primitives() {
+        assert_eq!("String".parse(), Ok(ClickHouseType::String));
+        assert_eq!("Int64".parse(), Ok(ClickHouseType::Int64));
         assert_eq!(
-            parse_args("( 10 , 2 )").unwrap(),
-            vec!["10".to_string(), "2".to_string()]
+            "DateTime64(3)".parse(),
+            Ok(ClickHouseType::DateTime64 { precision: 3 })
         );
+    }
 
-        // With nested parentheses
+    #[test]
+    fn test_parse_nullable() {
         assert_eq!(
-            parse_args("(Nullable(String))").unwrap(),
-            vec!["Nullable(String)".to_string()]
+            "Nullable(String)".parse(),
+            Ok(ClickHouseType::Nullable(Box::new(ClickHouseType::String)))
         );
         assert_eq!(
-            parse_args("(Array(Int32), String)").unwrap(),
-            vec!["Array(Int32)".to_string(), "String".to_string()]
+            "Nullable(Int64)".parse(),
+            Ok(ClickHouseType::Nullable(Box::new(ClickHouseType::Int64)))
         );
+    }
 
-        // With quotes
-        assert_eq!(
-            parse_args("(3, 'UTC')").unwrap(),
-            vec!["3".to_string(), "'UTC'".to_string()]
-        );
+    #[test]
+    fn test_parse_lowcardinality() {
         assert_eq!(
-            parse_args("(9, 'America/New_York')").unwrap(),
-            vec!["9".to_string(), "'America/New_York'".to_string()]
+            "LowCardinality(String)".parse(),
+            Ok(ClickHouseType::LowCardinality(Box::new(
+                ClickHouseType::String
+            )))
         );
-
-        // Complex nested case
         assert_eq!(
-            parse_args("(Tuple(Int32, String), Array(Float64))").unwrap(),
-            vec![
-                "Tuple(Int32, String)".to_string(),
-                "Array(Float64)".to_string()
-            ]
+            "LowCardinality(Nullable(String))".parse(),
+            Ok(ClickHouseType::LowCardinality(Box::new(
+                ClickHouseType::Nullable(Box::new(ClickHouseType::String))
+            )))
         );
-
-        // Error cases
-        assert!(parse_args("10, 2").is_err()); // Missing parentheses
-        assert!(parse_args("(10, 2").is_err()); // Missing closing paren
     }
 
     #[test]
-    fn test_array_type_not_supported() {
-        // Array types should return an error
-        let result = convert_type_no_metadata("Array(Int32)");
-        assert!(result.is_err());
-        let err = result.unwrap_err();
-        assert!(err.contains("Array type"));
-        assert!(err.contains("not supported"));
+    fn test_is_nullable() {
+        assert!(!ClickHouseType::from_str("String").unwrap().is_nullable());
+        assert!(
+            ClickHouseType::from_str("Nullable(String)")
+                .unwrap()
+                .is_nullable()
+        );
+        assert!(
+            ClickHouseType::from_str("LowCardinality(Nullable(String))")
+                .unwrap()
+                .is_nullable()
+        );
+        assert!(
+            !ClickHouseType::from_str("LowCardinality(String)")
+                .unwrap()
+                .is_nullable()
+        );
     }
 
     #[test]
-    fn test_tuple_type_not_supported() {
-        // Tuple types should return an error
-        let result = convert_type_no_metadata("Tuple(String, Int64)");
-        assert!(result.is_err());
-        let err = result.unwrap_err();
-        assert!(err.contains("Tuple type"));
-        assert!(err.contains("not supported"));
-    }
+    fn test_array_type_parsing() {
+        let (dtype, nullable) = convert_type("Array(Int32)").unwrap();
+        assert!(matches!(dtype, DataType::List(_)));
+        assert!(!nullable);
+
+        // Nested array
+        let (dtype, _) = convert_type("Array(Array(String))").unwrap();
+        if let DataType::List(inner) = dtype {
+            assert!(matches!(inner.data_type(), DataType::List(_)));
+        } else {
+            panic!("Expected List type");
+        }
 
-    #[test]
-    fn test_map_type_not_supported() {
-        // Map types should return an error
-        let result = convert_type_no_metadata("Map(String, Int64)");
-        assert!(result.is_err());
-        let err = result.unwrap_err();
-        assert!(err.contains("Map type"));
-        assert!(err.contains("not supported"));
+        // Nullable array
+        let (_, nullable) = convert_type("Nullable(Array(Int64))").unwrap();
+        assert!(nullable);
     }
 
     #[test]
-    fn test_unknown_type_fails() {
-        // Unknown types should return an error
-        let result = convert_type_no_metadata("UnknownType");
-        assert!(result.is_err());
-        let err = result.unwrap_err();
-        assert!(err.contains("Unknown ClickHouse type"));
-    }
+    fn test_tuple_type_parsing() {
+        let (dtype, _) = convert_type("Tuple(String, Int64)").unwrap();
+        if let DataType::Struct(fields) = dtype {
+            assert_eq!(fields.len(), 2);
+            assert_eq!(fields[0].name(), "f0");
+            assert_eq!(fields[1].name(), "f1");
+        } else {
+            panic!("Expected Struct type");
+        }
 
-    #[test]
-    fn test_parse_ch_type_primitives() {
-        assert_eq!(parse_ch_type("String"), ClickHouseType::Primitive("String"));
-        assert_eq!(parse_ch_type("Int64"), ClickHouseType::Primitive("Int64"));
-        assert_eq!(
-            parse_ch_type("DateTime64(3)"),
-            ClickHouseType::Primitive("DateTime64(3)")
-        );
+        // Nested tuple
+        let (dtype, _) = convert_type("Tuple(Int32, Tuple(String, Float64))").unwrap();
+        if let DataType::Struct(fields) = dtype {
+            assert_eq!(fields.len(), 2);
+            assert!(matches!(fields[1].data_type(), DataType::Struct(_)));
+        } else {
+            panic!("Expected Struct type");
+        }
     }
 
     #[test]
-    fn test_parse_ch_type_nullable() {
-        assert_eq!(
-            parse_ch_type("Nullable(String)"),
-            ClickHouseType::Nullable(Box::new(ClickHouseType::Primitive("String")))
-        );
-        assert_eq!(
-            parse_ch_type("Nullable(Int64)"),
-            ClickHouseType::Nullable(Box::new(ClickHouseType::Primitive("Int64")))
-        );
-    }
+    fn test_map_type_parsing() {
+        let (dtype, _) = convert_type("Map(String, Int64)").unwrap();
+        assert!(matches!(dtype, DataType::Map(_, _)));
+
+        // Map with complex value
+        let (dtype, _) = convert_type("Map(String, Array(Int32))").unwrap();
+        if let DataType::Map(entries, _) = dtype
+            && let DataType::Struct(fields) = entries.data_type()
+        {
+            assert!(matches!(fields[1].data_type(), DataType::List(_)));
+        }
 
-    #[test]
-    fn test_parse_ch_type_lowcardinality() {
-        assert_eq!(
-            parse_ch_type("LowCardinality(String)"),
-            ClickHouseType::LowCardinality(Box::new(ClickHouseType::Primitive("String")))
-        );
-        assert_eq!(
-            parse_ch_type("LowCardinality(Nullable(String))"),
-            ClickHouseType::LowCardinality(Box::new(ClickHouseType::Nullable(Box::new(
-                ClickHouseType::Primitive("String")
-            ))))
-        );
+        // Non-string key should error
+        let result = convert_type("Map(Int32, String)");
+        assert!(result.is_err());
+        assert!(result.unwrap_err().contains("Map keys must be String"));
     }
 
     #[test]
-    fn test_parse_ch_type_is_nullable() {
-        assert!(!parse_ch_type("String").is_nullable());
-        assert!(parse_ch_type("Nullable(String)").is_nullable());
-        assert!(parse_ch_type("LowCardinality(Nullable(String))").is_nullable());
-        assert!(!parse_ch_type("LowCardinality(String)").is_nullable());
+    fn test_complex_nested_types() {
+        // Array of tuples
+        let (dtype, _) = convert_type("Array(Tuple(String, Int64))").unwrap();
+        if let DataType::List(inner) = dtype {
+            assert!(matches!(inner.data_type(), DataType::Struct(_)));
+        } else {
+            panic!("Expected List type");
+        }
+
+        // Tuple with array and map
+        let (dtype, _) = convert_type("Tuple(Array(Int32), Map(String, Float64))").unwrap();
+        if let DataType::Struct(fields) = dtype {
+            assert_eq!(fields.len(), 2);
+            assert!(matches!(fields[0].data_type(), DataType::List(_)));
+            assert!(matches!(fields[1].data_type(), DataType::Map(_, _)));
+        } else {
+            panic!("Expected Struct type");
+        }
+
+        // Map with tuple values
+        let (dtype, _) = convert_type("Map(String, Tuple(Int64, String))").unwrap();
+        if let DataType::Map(entries, _) = dtype
+            && let DataType::Struct(fields) = entries.data_type()
+        {
+            assert!(matches!(fields[1].data_type(), DataType::Struct(_)));
+        }
     }
 
     #[test]
-    fn test_parse_ch_type_base_type() {
-        let parsed = parse_ch_type("LowCardinality(Nullable(String))");
-        assert_eq!(parsed.base_type(), &ClickHouseType::Primitive("String"));
+    fn test_named_tuple_fields() {
+        let (dtype, _) = convert_type("Tuple(category String, tag String)").unwrap();
+        if let DataType::Struct(fields) = dtype {
+            assert_eq!(fields.len(), 2);
+            assert_eq!(fields[0].name(), "category");
+            assert_eq!(fields[1].name(), "tag");
+        } else {
+            panic!("Expected Struct type");
+        }
 
-        let parsed = parse_ch_type("Nullable(Int64)");
-        assert_eq!(parsed.base_type(), &ClickHouseType::Primitive("Int64"));
+        // Array of named tuples
+        let (dtype, _) = convert_type("Array(Tuple(category String, tag String))").unwrap();
+        if let DataType::List(inner) = dtype {
+            if let DataType::Struct(fields) = inner.data_type() {
+                assert_eq!(fields[0].name(), "category");
+                assert_eq!(fields[1].name(), "tag");
+            } else {
+                panic!("Expected Struct type inside List");
+            }
+        } else {
+            panic!("Expected List type");
+        }
 
-        let parsed = parse_ch_type("String");
-        assert_eq!(parsed.base_type(), &ClickHouseType::Primitive("String"));
+        // Named tuple with complex types
+        let (dtype, _) =
+            convert_type("Tuple(items Array(Int32), metadata Map(String, String))").unwrap();
+        if let DataType::Struct(fields) = dtype {
+            assert_eq!(fields[0].name(), "items");
+            assert_eq!(fields[1].name(), "metadata");
+            assert!(matches!(fields[0].data_type(), DataType::List(_)));
+            assert!(matches!(fields[1].data_type(), DataType::Map(_, _)));
+        } else {
+            panic!("Expected Struct type");
+        }
     }
 }
diff --git a/src/sinks/clickhouse/arrow/schema.rs b/src/sinks/clickhouse/arrow/schema.rs
index f2359ca5f..52f54d9b1 100644
--- a/src/sinks/clickhouse/arrow/schema.rs
+++ b/src/sinks/clickhouse/arrow/schema.rs
@@ -1,15 +1,19 @@
 //! Schema fetching and Arrow schema construction for ClickHouse tables.
 
+use std::str::FromStr;
+
 use arrow::datatypes::{Field, Schema};
 use async_trait::async_trait;
 use http::{Request, StatusCode};
 use hyper::Body;
+use itertools::Itertools;
 use serde::Deserialize;
+use url::form_urlencoded;
 use vector_lib::codecs::encoding::format::{ArrowEncodingError, SchemaProvider};
 
 use crate::http::{Auth, HttpClient};
 
-use super::parser::clickhouse_type_to_arrow;
+use super::parser::ClickHouseType;
 
 #[derive(Debug, Deserialize)]
 struct ColumnInfo {
@@ -18,9 +22,16 @@ struct ColumnInfo {
     column_type: String,
 }
 
-/// URL-encodes a string for use in HTTP query parameters.
-fn url_encode(s: &str) -> String {
-    percent_encoding::utf8_percent_encode(s, percent_encoding::NON_ALPHANUMERIC).to_string()
+impl TryFrom<ColumnInfo> for Field {
+    type Error = Box<dyn std::error::Error + Send + Sync>;
+
+    fn try_from(column: ColumnInfo) -> Result<Self, Self::Error> {
+        let ch_type = ClickHouseType::from_str(&column.column_type)?;
+        let (dt, nullable) = (&ch_type)
+            .try_into()
+            .map_err(|e| format!("Failed to convert column '{}': {e}", column.name))?;
+        Ok(Field::new(column.name, dt, nullable))
+    }
 }
 
 /// Fetches the schema for a ClickHouse table and converts it to an Arrow schema.
@@ -38,14 +49,15 @@ pub async fn fetch_table_schema(
                  FORMAT JSONEachRow";
 
     // Build URI with query and parameters
-    let uri = format!(
-        "{}?query={}&param_db={}&param_tbl={}",
-        endpoint,
-        url_encode(query),
-        url_encode(database),
-        url_encode(table)
-    );
-    let mut request = Request::get(&uri).body(Body::empty()).unwrap();
+    let query_string = form_urlencoded::Serializer::new(String::new())
+        .append_pair("query", query)
+        .append_pair("param_db", database)
+        .append_pair("param_tbl", table)
+        .finish();
+    let uri = format!("{endpoint}?{query_string}");
+    let mut request = Request::get(&uri)
+        .body(Body::empty())
+        .map_err(|e| format!("Failed to build request: {e}"))?;
 
     if let Some(auth) = auth {
         auth.apply(&mut request);
@@ -59,40 +71,28 @@ pub async fn fetch_table_schema(
                 .await?
                 .to_bytes();
             let body_str = String::from_utf8(body_bytes.into())
-                .map_err(|e| format!("Failed to parse response as UTF-8: {}", e))?;
+                .map_err(|e| format!("Failed to parse response as UTF-8: {e}"))?;
 
             parse_schema_from_response(&body_str)
         }
-        status => Err(format!("Failed to fetch schema from ClickHouse: HTTP {}", status).into()),
+        status => Err(format!("Failed to fetch schema from ClickHouse: HTTP {status}").into()),
     }
 }
 
 /// Parses the JSON response from ClickHouse and builds an Arrow schema.
 fn parse_schema_from_response(response: &str) -> crate::Result<Schema> {
-    let mut columns: Vec<ColumnInfo> = Vec::new();
-
-    for line in response.lines() {
-        if line.trim().is_empty() {
-            continue;
-        }
+    let mut lines = response.lines().filter(|line| !line.is_empty()).peekable();
 
-        let column: ColumnInfo = serde_json::from_str(line)
-            .map_err(|e| format!("Failed to parse column info: {}", e))?;
-        columns.push(column);
+    if lines.peek().is_none() {
+        return Err("Table does not exist or has no columns".into());
     }
 
-    if columns.is_empty() {
-        return Err("No columns found in table schema".into());
-    }
-
-    let mut fields = Vec::new();
-    for column in columns {
-        let (arrow_type, nullable) = clickhouse_type_to_arrow(&column.column_type)
-            .map_err(|e| format!("Failed to convert column '{}': {}", column.name, e))?;
-        fields.push(Field::new(&column.name, arrow_type, nullable));
-    }
-
-    Ok(Schema::new(fields))
+    lines
+        .map(|line| -> crate::Result<Field> {
+            serde_json::from_str::<ColumnInfo>(line)?.try_into()
+        })
+        .try_collect::<_, Vec<Field>, _>()
+        .map(Schema::new)
 }
 
 /// Schema provider implementation for ClickHouse tables.
diff --git a/src/sinks/clickhouse/integration_tests.rs b/src/sinks/clickhouse/integration_tests.rs
index 379859570..a63e4a9e9 100644
--- a/src/sinks/clickhouse/integration_tests.rs
+++ b/src/sinks/clickhouse/integration_tests.rs
@@ -12,11 +12,12 @@ use futures::{
     stream,
 };
 use http::StatusCode;
+use ordered_float::NotNan;
 use serde::Deserialize;
 use serde_json::Value;
 use tokio::time::{Duration, timeout};
 use vector_lib::{
-    codecs::encoding::BatchSerializerConfig,
+    codecs::encoding::{ArrowStreamSerializerConfig, BatchSerializerConfig},
     event::{BatchNotifier, BatchStatus, BatchStatusReceiver, Event, LogEvent},
     lookup::PathPrefix,
 };
@@ -605,3 +606,571 @@ async fn insert_events_arrow_with_schema_fetching() {
         assert!(row.get("active").and_then(|v| v.as_bool()).is_some());
     }
 }
+
+#[tokio::test]
+async fn test_complex_types() {
+    trace_init();
+
+    let table = random_table_name();
+    let host = clickhouse_address();
+
+    let mut batch = BatchConfig::default();
+    batch.max_events = Some(3);
+
+    let arrow_config = ArrowStreamSerializerConfig {
+        allow_nullable_fields: true,
+        ..Default::default()
+    };
+
+    let config = ClickhouseConfig {
+        endpoint: host.parse().unwrap(),
+        table: table.clone().try_into().unwrap(),
+        compression: Compression::None,
+        format: crate::sinks::clickhouse::config::Format::ArrowStream,
+        batch_encoding: Some(BatchSerializerConfig::ArrowStream(arrow_config)),
+        batch,
+        request: TowerRequestConfig {
+            retry_attempts: 1,
+            ..Default::default()
+        },
+        ..Default::default()
+    };
+
+    let client = ClickhouseClient::new(host);
+
+    // Comprehensive schema with all complex types
+    client
+        .create_table(
+            &table,
+            "host String, timestamp DateTime64(3), message String, \
+             nested_int_array Array(Array(Int32)), \
+             nested_string_array Array(Array(String)), \
+             array_map Map(String, Array(String)), \
+             int_array_map Map(String, Array(Int64)), \
+             tuple_with_array Tuple(String, Array(Int32)), \
+             tuple_with_map Tuple(String, Map(String, Float64)), \
+             tuple_with_nested Tuple(String, Array(Int32), Map(String, Float64)), \
+             locations Array(Tuple(String, Float64, Float64)), \
+             tags_history Array(Map(String, String)), \
+             metrics_history Array(Map(String, Int32)), \
+             request_headers Map(String, String), \
+             response_metrics Tuple(Int32, Int64, Float64), \
+             tags Array(String), \
+             user_properties Map(String, Array(String)), \
+             array_with_nulls Array(Nullable(Int32)), \
+             array_with_named_tuple Array(Tuple(category String, tag String))",
+        )
+        .await;
+
+    let (sink, _hc) = config.build(SinkContext::default()).await.unwrap();
+
+    let mut events: Vec<Event> = Vec::new();
+
+    // Event 1: Comprehensive test with all complex types
+    let mut event1 = LogEvent::from("Comprehensive complex types test");
+    event1.insert("host", "host1.example.com");
+
+    // Nested arrays
+    event1.insert(
+        "nested_int_array",
+        vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Array(vec![
+                vector_lib::event::Value::Integer(1),
+                vector_lib::event::Value::Integer(2),
+            ]),
+            vector_lib::event::Value::Array(vec![
+                vector_lib::event::Value::Integer(3),
+                vector_lib::event::Value::Integer(4),
+            ]),
+        ]),
+    );
+    event1.insert(
+        "nested_string_array",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Bytes("a".into()),
+            vector_lib::event::Value::Bytes("b".into()),
+        ])]),
+    );
+
+    // Maps with arrays
+    let mut array_map = vector_lib::event::ObjectMap::new();
+    array_map.insert(
+        "fruits".into(),
+        vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Bytes("apple".into()),
+            vector_lib::event::Value::Bytes("banana".into()),
+        ]),
+    );
+    event1.insert("array_map", vector_lib::event::Value::Object(array_map));
+
+    let mut int_array_map = vector_lib::event::ObjectMap::new();
+    int_array_map.insert(
+        "scores".into(),
+        vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Integer(95),
+            vector_lib::event::Value::Integer(87),
+        ]),
+    );
+    event1.insert(
+        "int_array_map",
+        vector_lib::event::Value::Object(int_array_map),
+    );
+
+    // Tuples with complex types
+    let mut tuple_with_array = vector_lib::event::ObjectMap::new();
+    tuple_with_array.insert(
+        "f0".into(),
+        vector_lib::event::Value::Bytes("numbers".into()),
+    );
+    tuple_with_array.insert(
+        "f1".into(),
+        vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Integer(10),
+            vector_lib::event::Value::Integer(20),
+        ]),
+    );
+    event1.insert(
+        "tuple_with_array",
+        vector_lib::event::Value::Object(tuple_with_array),
+    );
+
+    let mut inner_map = vector_lib::event::ObjectMap::new();
+    inner_map.insert(
+        "temp".into(),
+        vector_lib::event::Value::Float(NotNan::new(22.5).unwrap()),
+    );
+    let mut tuple_with_map = vector_lib::event::ObjectMap::new();
+    tuple_with_map.insert(
+        "f0".into(),
+        vector_lib::event::Value::Bytes("metrics".into()),
+    );
+    tuple_with_map.insert("f1".into(), vector_lib::event::Value::Object(inner_map));
+    event1.insert(
+        "tuple_with_map",
+        vector_lib::event::Value::Object(tuple_with_map),
+    );
+
+    let mut inner_map2 = vector_lib::event::ObjectMap::new();
+    inner_map2.insert(
+        "avg".into(),
+        vector_lib::event::Value::Float(NotNan::new(95.5).unwrap()),
+    );
+    let mut tuple_complex = vector_lib::event::ObjectMap::new();
+    tuple_complex.insert(
+        "f0".into(),
+        vector_lib::event::Value::Bytes("results".into()),
+    );
+    tuple_complex.insert(
+        "f1".into(),
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(95)]),
+    );
+    tuple_complex.insert("f2".into(), vector_lib::event::Value::Object(inner_map2));
+    event1.insert(
+        "tuple_with_nested",
+        vector_lib::event::Value::Object(tuple_complex),
+    );
+
+    // Array of tuples
+    let mut loc1 = vector_lib::event::ObjectMap::new();
+    loc1.insert(
+        "f0".into(),
+        vector_lib::event::Value::Bytes("San Francisco".into()),
+    );
+    loc1.insert(
+        "f1".into(),
+        vector_lib::event::Value::Float(NotNan::new(37.7749).unwrap()),
+    );
+    loc1.insert(
+        "f2".into(),
+        vector_lib::event::Value::Float(NotNan::new(-122.4194).unwrap()),
+    );
+    event1.insert(
+        "locations",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(loc1)]),
+    );
+
+    // Array of maps
+    let mut tags1 = vector_lib::event::ObjectMap::new();
+    tags1.insert("env".into(), vector_lib::event::Value::Bytes("prod".into()));
+    event1.insert(
+        "tags_history",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(tags1)]),
+    );
+
+    let mut metrics1 = vector_lib::event::ObjectMap::new();
+    metrics1.insert("cpu".into(), vector_lib::event::Value::Integer(45));
+    event1.insert(
+        "metrics_history",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(metrics1)]),
+    );
+
+    // Structured log data
+    let mut headers = vector_lib::event::ObjectMap::new();
+    headers.insert(
+        "user-agent".into(),
+        vector_lib::event::Value::Bytes("Mozilla/5.0".into()),
+    );
+    event1.insert("request_headers", vector_lib::event::Value::Object(headers));
+
+    let mut metrics = vector_lib::event::ObjectMap::new();
+    metrics.insert("f0".into(), vector_lib::event::Value::Integer(200));
+    metrics.insert("f1".into(), vector_lib::event::Value::Integer(1234));
+    metrics.insert(
+        "f2".into(),
+        vector_lib::event::Value::Float(NotNan::new(0.145).unwrap()),
+    );
+    event1.insert(
+        "response_metrics",
+        vector_lib::event::Value::Object(metrics),
+    );
+
+    event1.insert(
+        "tags",
+        vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Bytes("api".into()),
+            vector_lib::event::Value::Bytes("v2".into()),
+        ]),
+    );
+
+    let mut user_props = vector_lib::event::ObjectMap::new();
+    user_props.insert(
+        "roles".into(),
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Bytes("admin".into())]),
+    );
+    event1.insert(
+        "user_properties",
+        vector_lib::event::Value::Object(user_props),
+    );
+
+    // Nullable array
+    event1.insert(
+        "array_with_nulls",
+        vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Integer(100),
+            vector_lib::event::Value::Integer(200),
+        ]),
+    );
+
+    // Named tuple array - tests that named fields work correctly
+    let mut named_tuple1 = vector_lib::event::ObjectMap::new();
+    named_tuple1.insert(
+        "category".into(),
+        vector_lib::event::Value::Bytes("priority".into()),
+    );
+    named_tuple1.insert("tag".into(), vector_lib::event::Value::Bytes("high".into()));
+
+    let mut named_tuple2 = vector_lib::event::ObjectMap::new();
+    named_tuple2.insert(
+        "category".into(),
+        vector_lib::event::Value::Bytes("environment".into()),
+    );
+    named_tuple2.insert(
+        "tag".into(),
+        vector_lib::event::Value::Bytes("production".into()),
+    );
+
+    event1.insert(
+        "array_with_named_tuple",
+        vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Object(named_tuple1),
+            vector_lib::event::Value::Object(named_tuple2),
+        ]),
+    );
+
+    events.push(event1.into());
+
+    // Event 2: Empty and edge cases
+    let mut event2 = LogEvent::from("Test empty collections");
+    event2.insert("host", "host2.example.com");
+    event2.insert("nested_int_array", vector_lib::event::Value::Array(vec![]));
+    event2.insert(
+        "nested_string_array",
+        vector_lib::event::Value::Array(vec![]),
+    );
+
+    let empty_map = vector_lib::event::ObjectMap::new();
+    event2.insert(
+        "array_map",
+        vector_lib::event::Value::Object(empty_map.clone()),
+    );
+    event2.insert(
+        "int_array_map",
+        vector_lib::event::Value::Object(empty_map.clone()),
+    );
+
+    let mut empty_tuple = vector_lib::event::ObjectMap::new();
+    empty_tuple.insert("f0".into(), vector_lib::event::Value::Bytes("empty".into()));
+    empty_tuple.insert("f1".into(), vector_lib::event::Value::Array(vec![]));
+    event2.insert(
+        "tuple_with_array",
+        vector_lib::event::Value::Object(empty_tuple),
+    );
+
+    let mut empty_tuple_map = vector_lib::event::ObjectMap::new();
+    empty_tuple_map.insert("f0".into(), vector_lib::event::Value::Bytes("empty".into()));
+    empty_tuple_map.insert(
+        "f1".into(),
+        vector_lib::event::Value::Object(empty_map.clone()),
+    );
+    event2.insert(
+        "tuple_with_map",
+        vector_lib::event::Value::Object(empty_tuple_map),
+    );
+
+    let mut empty_tuple_complex = vector_lib::event::ObjectMap::new();
+    empty_tuple_complex.insert("f0".into(), vector_lib::event::Value::Bytes("empty".into()));
+    empty_tuple_complex.insert("f1".into(), vector_lib::event::Value::Array(vec![]));
+    empty_tuple_complex.insert(
+        "f2".into(),
+        vector_lib::event::Value::Object(empty_map.clone()),
+    );
+    event2.insert(
+        "tuple_with_nested",
+        vector_lib::event::Value::Object(empty_tuple_complex),
+    );
+
+    event2.insert("locations", vector_lib::event::Value::Array(vec![]));
+    event2.insert("tags_history", vector_lib::event::Value::Array(vec![]));
+    event2.insert("metrics_history", vector_lib::event::Value::Array(vec![]));
+    event2.insert(
+        "request_headers",
+        vector_lib::event::Value::Object(empty_map.clone()),
+    );
+
+    let mut empty_metrics = vector_lib::event::ObjectMap::new();
+    empty_metrics.insert("f0".into(), vector_lib::event::Value::Integer(0));
+    empty_metrics.insert("f1".into(), vector_lib::event::Value::Integer(0));
+    empty_metrics.insert(
+        "f2".into(),
+        vector_lib::event::Value::Float(NotNan::new(0.0).unwrap()),
+    );
+    event2.insert(
+        "response_metrics",
+        vector_lib::event::Value::Object(empty_metrics),
+    );
+
+    event2.insert("tags", vector_lib::event::Value::Array(vec![]));
+    event2.insert(
+        "user_properties",
+        vector_lib::event::Value::Object(empty_map),
+    );
+    event2.insert("array_with_nulls", vector_lib::event::Value::Array(vec![]));
+    event2.insert(
+        "array_with_named_tuple",
+        vector_lib::event::Value::Array(vec![]),
+    );
+
+    events.push(event2.into());
+
+    // Event 3: More varied data
+    let mut event3 = LogEvent::from("Test varied data");
+    event3.insert("host", "host3.example.com");
+
+    event3.insert(
+        "nested_int_array",
+        vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Array(vec![]),
+            vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(99)]),
+        ]),
+    );
+    event3.insert(
+        "nested_string_array",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Array(vec![
+            vector_lib::event::Value::Bytes("test".into()),
+        ])]),
+    );
+
+    let mut map3 = vector_lib::event::ObjectMap::new();
+    map3.insert(
+        "colors".into(),
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Bytes("red".into())]),
+    );
+    event3.insert("array_map", vector_lib::event::Value::Object(map3));
+
+    let mut int_map3 = vector_lib::event::ObjectMap::new();
+    int_map3.insert(
+        "values".into(),
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(42)]),
+    );
+    event3.insert("int_array_map", vector_lib::event::Value::Object(int_map3));
+
+    let mut tuple3 = vector_lib::event::ObjectMap::new();
+    tuple3.insert("f0".into(), vector_lib::event::Value::Bytes("data".into()));
+    tuple3.insert(
+        "f1".into(),
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(5)]),
+    );
+    event3.insert("tuple_with_array", vector_lib::event::Value::Object(tuple3));
+
+    let mut map_inner = vector_lib::event::ObjectMap::new();
+    map_inner.insert(
+        "val".into(),
+        vector_lib::event::Value::Float(NotNan::new(1.0).unwrap()),
+    );
+    let mut tuple_map3 = vector_lib::event::ObjectMap::new();
+    tuple_map3.insert("f0".into(), vector_lib::event::Value::Bytes("test".into()));
+    tuple_map3.insert("f1".into(), vector_lib::event::Value::Object(map_inner));
+    event3.insert(
+        "tuple_with_map",
+        vector_lib::event::Value::Object(tuple_map3),
+    );
+
+    let mut map_inner2 = vector_lib::event::ObjectMap::new();
+    map_inner2.insert(
+        "x".into(),
+        vector_lib::event::Value::Float(NotNan::new(2.0).unwrap()),
+    );
+    let mut tuple_nested3 = vector_lib::event::ObjectMap::new();
+    tuple_nested3.insert("f0".into(), vector_lib::event::Value::Bytes("nest".into()));
+    tuple_nested3.insert(
+        "f1".into(),
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(1)]),
+    );
+    tuple_nested3.insert("f2".into(), vector_lib::event::Value::Object(map_inner2));
+    event3.insert(
+        "tuple_with_nested",
+        vector_lib::event::Value::Object(tuple_nested3),
+    );
+
+    let mut loc3 = vector_lib::event::ObjectMap::new();
+    loc3.insert("f0".into(), vector_lib::event::Value::Bytes("NYC".into()));
+    loc3.insert(
+        "f1".into(),
+        vector_lib::event::Value::Float(NotNan::new(40.7128).unwrap()),
+    );
+    loc3.insert(
+        "f2".into(),
+        vector_lib::event::Value::Float(NotNan::new(-74.0060).unwrap()),
+    );
+    event3.insert(
+        "locations",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(loc3)]),
+    );
+
+    let mut tags3 = vector_lib::event::ObjectMap::new();
+    tags3.insert("env".into(), vector_lib::event::Value::Bytes("dev".into()));
+    event3.insert(
+        "tags_history",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(tags3)]),
+    );
+
+    let mut metrics3 = vector_lib::event::ObjectMap::new();
+    metrics3.insert("cpu".into(), vector_lib::event::Value::Integer(60));
+    event3.insert(
+        "metrics_history",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(metrics3)]),
+    );
+
+    let mut headers3 = vector_lib::event::ObjectMap::new();
+    headers3.insert(
+        "content-type".into(),
+        vector_lib::event::Value::Bytes("application/json".into()),
+    );
+    event3.insert(
+        "request_headers",
+        vector_lib::event::Value::Object(headers3),
+    );
+
+    let mut metrics3_resp = vector_lib::event::ObjectMap::new();
+    metrics3_resp.insert("f0".into(), vector_lib::event::Value::Integer(404));
+    metrics3_resp.insert("f1".into(), vector_lib::event::Value::Integer(0));
+    metrics3_resp.insert(
+        "f2".into(),
+        vector_lib::event::Value::Float(NotNan::new(0.001).unwrap()),
+    );
+    event3.insert(
+        "response_metrics",
+        vector_lib::event::Value::Object(metrics3_resp),
+    );
+
+    event3.insert(
+        "tags",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Bytes("test".into())]),
+    );
+
+    let mut user_props3 = vector_lib::event::ObjectMap::new();
+    user_props3.insert(
+        "permissions".into(),
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Bytes("read".into())]),
+    );
+    event3.insert(
+        "user_properties",
+        vector_lib::event::Value::Object(user_props3),
+    );
+
+    event3.insert(
+        "array_with_nulls",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(42)]),
+    );
+
+    // Named tuple with single element
+    let mut named_tuple3 = vector_lib::event::ObjectMap::new();
+    named_tuple3.insert(
+        "category".into(),
+        vector_lib::event::Value::Bytes("status".into()),
+    );
+    named_tuple3.insert(
+        "tag".into(),
+        vector_lib::event::Value::Bytes("active".into()),
+    );
+    event3.insert(
+        "array_with_named_tuple",
+        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(named_tuple3)]),
+    );
+
+    events.push(event3.into());
+
+    run_and_assert_sink_compliance(sink, stream::iter(events), &SINK_TAGS).await;
+
+    let output = client.select_all(&table).await;
+    assert_eq!(3, output.rows);
+
+    // Verify event 1 - comprehensive data
+    let row1 = &output.data[0];
+    assert!(
+        row1.get("nested_int_array")
+            .and_then(|v| v.as_array())
+            .is_some()
+    );
+    assert!(row1.get("array_map").and_then(|v| v.as_object()).is_some());
+    // Tuples are returned as arrays from ClickHouse
+    assert!(
+        row1.get("tuple_with_array")
+            .and_then(|v| v.as_array())
+            .is_some()
+    );
+    assert!(row1.get("locations").and_then(|v| v.as_array()).is_some());
+    assert!(
+        row1.get("tags_history")
+            .and_then(|v| v.as_array())
+            .is_some()
+    );
+    assert!(
+        row1.get("request_headers")
+            .and_then(|v| v.as_object())
+            .is_some()
+    );
+    assert!(
+        row1.get("array_with_nulls")
+            .and_then(|v| v.as_array())
+            .is_some()
+    );
+
+    // Verify event 2 - empty collections
+    let row2 = &output.data[1];
+    let empty_nested = row2
+        .get("nested_int_array")
+        .and_then(|v| v.as_array())
+        .unwrap();
+    assert_eq!(0, empty_nested.len());
+    let empty_tags = row2.get("tags").and_then(|v| v.as_array()).unwrap();
+    assert_eq!(0, empty_tags.len());
+
+    // Verify event 3 - varied data
+    let row3 = &output.data[2];
+    let nested3 = row3
+        .get("nested_int_array")
+        .and_then(|v| v.as_array())
+        .unwrap();
+    assert_eq!(2, nested3.len());
+}
diff --git a/src/sinks/util/encoding.rs b/src/sinks/util/encoding.rs
index 9d0c383d8..1dda331b3 100644
--- a/src/sinks/util/encoding.rs
+++ b/src/sinks/util/encoding.rs
@@ -125,6 +125,7 @@ impl Encoder<Vec<Event>> for (Transformer, vector_lib::codecs::BatchEncoder) {
         encoder
             .encode(transformed_events, &mut bytes)
             .map_err(|error| {
+                #[cfg(feature = "codecs-arrow")]
                 if let vector_lib::codecs::encoding::Error::SchemaConstraintViolation(
                     ref constraint_error,
                 ) = error
diff --git a/src/sources/websocket/source.rs b/src/sources/websocket/source.rs
index cf3c75d40..a195f2b48 100644
--- a/src/sources/websocket/source.rs
+++ b/src/sources/websocket/source.rs
@@ -1,6 +1,6 @@
 use std::pin::Pin;
 
-use chrono::Utc;
+use chrono::{DateTime, Utc};
 use futures::{Sink, Stream, StreamExt, pin_mut, sink::SinkExt};
 use snafu::Snafu;
 use tokio::time;
@@ -235,9 +235,10 @@ impl WebSocketSource {
                         kind,
                     });
 
+                    let now = Utc::now();
                     let events_with_meta = events.into_iter().map(|mut event| {
                         if let Event::Log(event) = &mut event {
-                            self.add_metadata(event);
+                            self.add_metadata(event, now);
                         }
                         event
                     });
@@ -255,10 +256,10 @@ impl WebSocketSource {
         }
     }
 
-    fn add_metadata(&self, event: &mut LogEvent) {
+    fn add_metadata(&self, event: &mut LogEvent, now: DateTime<Utc>) {
         self.params
             .log_namespace
-            .insert_standard_vector_source_metadata(event, WebSocketConfig::NAME, Utc::now());
+            .insert_standard_vector_source_metadata(event, WebSocketConfig::NAME, now);
     }
 
     async fn reconnect(
diff --git a/src/transforms/sample/transform.rs b/src/transforms/sample/transform.rs
index 20013c5e7..74786a43f 100644
--- a/src/transforms/sample/transform.rs
+++ b/src/transforms/sample/transform.rs
@@ -1,4 +1,4 @@
-use std::{borrow::Cow, collections::HashMap, fmt};
+use std::{collections::HashMap, fmt};
 
 use vector_lib::{
     config::LegacyKey,
@@ -7,7 +7,7 @@ use vector_lib::{
 
 use crate::{
     conditions::Condition,
-    event::Event,
+    event::{Event, Value},
     internal_events::SampleEventDiscarded,
     sinks::prelude::TemplateRenderingError,
     template::Template,
@@ -53,16 +53,16 @@ impl SampleMode {
         }
     }
 
-    fn increment(&mut self, group_by_key: &Option<String>, value: &Option<Cow<'_, str>>) -> bool {
+    fn increment(&mut self, group_by_key: Option<String>, value: Option<&Value>) -> bool {
         let threshold_exceeded = match self {
             Self::Rate { rate, counters } => {
-                let counter_value = counters.entry(group_by_key.clone()).or_default();
+                let counter_value = counters.entry(group_by_key).or_default();
                 let old_counter_value = *counter_value;
                 *counter_value += 1;
                 old_counter_value % *rate == 0
             }
             Self::Ratio { ratio, values, .. } => {
-                let value = values.entry(group_by_key.clone()).or_insert(1.0 - *ratio);
+                let value = values.entry(group_by_key).or_insert(1.0 - *ratio);
                 let increment: f64 = *value + *ratio;
                 *value = if increment >= 1.0 {
                     increment - 1.0
@@ -73,7 +73,7 @@ impl SampleMode {
             }
         };
         if let Some(value) = value {
-            self.hash_within_ratio(value.as_bytes())
+            self.hash_within_ratio(value.to_string_lossy().as_bytes())
         } else {
             threshold_exceeded
         }
@@ -159,49 +159,36 @@ impl FunctionTransform for Sample {
             }
         };
 
-        let value = self
-            .key_field
-            .as_ref()
-            .and_then(|key_field| match &event {
-                Event::Log(event) => event
-                    .parse_path_and_get_value(key_field.as_str())
-                    .ok()
-                    .flatten(),
-                Event::Trace(event) => event
-                    .parse_path_and_get_value(key_field.as_str())
-                    .ok()
-                    .flatten(),
-                Event::Metric(_) => panic!("component can never receive metric events"),
-            })
-            .map(|v| v.to_string_lossy());
+        let value = self.key_field.as_ref().and_then(|key_field| match &event {
+            Event::Log(event) => event
+                .parse_path_and_get_value(key_field.as_str())
+                .ok()
+                .flatten(),
+            Event::Trace(event) => event
+                .parse_path_and_get_value(key_field.as_str())
+                .ok()
+                .flatten(),
+            Event::Metric(_) => panic!("component can never receive metric events"),
+        });
 
         // Fetch actual field value if group_by option is set.
-        let group_by_key = self.group_by.as_ref().and_then(|group_by| match &event {
-            Event::Log(event) => group_by
-                .render_string(event)
-                .map_err(|error| {
-                    emit!(TemplateRenderingError {
-                        error,
-                        field: Some("group_by"),
-                        drop_event: false,
-                    })
-                })
-                .ok(),
-            Event::Trace(event) => group_by
-                .render_string(event)
-                .map_err(|error| {
-                    emit!(TemplateRenderingError {
-                        error,
-                        field: Some("group_by"),
-                        drop_event: false,
-                    })
+        let group_by_key = self.group_by.as_ref().and_then(|group_by| {
+            match &event {
+                Event::Log(event) => group_by.render_string(event),
+                Event::Trace(event) => group_by.render_string(event),
+                Event::Metric(_) => panic!("component can never receive metric events"),
+            }
+            .map_err(|error| {
+                emit!(TemplateRenderingError {
+                    error,
+                    field: Some("group_by"),
+                    drop_event: false,
                 })
-                .ok(),
-            Event::Metric(_) => panic!("component can never receive metric events"),
+            })
+            .ok()
         });
 
-        let should_sample = self.rate.increment(&group_by_key, &value);
-        if should_sample {
+        if self.rate.increment(group_by_key, value) {
             if let Some(path) = &self.sample_rate_key.path {
                 match event {
                     Event::Log(ref mut event) => {
diff --git a/vdev/Cargo.toml b/vdev/Cargo.toml
index ea2e01533..ab7a68d0d 100644
--- a/vdev/Cargo.toml
+++ b/vdev/Cargo.toml
@@ -43,7 +43,7 @@ toml.workspace = true
 toml_edit = "0.22"
 semver.workspace = true
 indoc.workspace = true
-git2 = { version = "0.20.2" }
+git2 = { version = "0.20.4" }
 cfg-if.workspace = true
 
 [package.metadata.binstall]
diff --git a/website/cue/reference/components/sinks/clickhouse.cue b/website/cue/reference/components/sinks/clickhouse.cue
index 1049cf521..c0d80d144 100644
--- a/website/cue/reference/components/sinks/clickhouse.cue
+++ b/website/cue/reference/components/sinks/clickhouse.cue
@@ -142,9 +142,6 @@ components: sinks: clickhouse: {
 
 				The following ClickHouse column types are **not yet supported** by Vector's
 				ArrowStream implementation:
-				- `Array`
-				- `Tuple`
-				- `Map`
 				- `IPv4`
 				- `IPv6`
 
