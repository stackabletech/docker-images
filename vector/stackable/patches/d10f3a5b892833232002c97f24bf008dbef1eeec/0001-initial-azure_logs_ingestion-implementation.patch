From 35ef996ec1ebbf2cdf42a26e773763a9e27b2687 Mon Sep 17 00:00:00 2001
From: Jed Laundry <jlaundry@jlaundry.com>
Date: Sat, 2 Nov 2024 22:04:08 +0000
Subject: initial azure_logs_ingestion implementation

Signed-off-by: Jed Laundry <jlaundry@jlaundry.com>
---
 .github/ISSUE_TEMPLATE/minor-release.md       |    3 +-
 .github/actions/install-vdev/action.yml       |    4 +-
 .github/actions/setup/action.yml              |    6 +-
 .github/actions/spelling/allow.txt            |    1 -
 .github/audit.yml                             |   18 +
 .github/workflows/build-test-runner.yml       |    4 +-
 .github/workflows/changelog.yaml              |    4 +-
 .github/workflows/changes.yml                 |    6 +-
 .github/workflows/ci-integration-review.yml   |    4 +-
 .github/workflows/cli.yml                     |    2 +-
 .github/workflows/compilation-timings.yml     |   10 +-
 .github/workflows/component_features.yml      |    2 +-
 .github/workflows/cross.yml                   |    4 +-
 .github/workflows/deny.yml                    |    2 +-
 .github/workflows/environment.yml             |    4 +-
 .../gardener_remove_waiting_author.yml        |    2 +-
 .github/workflows/install-sh.yml              |    2 +-
 .github/workflows/integration-test.yml        |    4 +-
 .github/workflows/integration.yml             |    4 +-
 .github/workflows/k8s_e2e.yml                 |    4 +-
 .github/workflows/msrv.yml                    |    2 +-
 .github/workflows/protobuf.yml                |    2 +-
 .github/workflows/publish-homebrew.yml        |    2 +-
 .github/workflows/publish.yml                 |   40 +-
 .github/workflows/regression.yml              |   22 +-
 .github/workflows/scorecard.yml               |    4 +-
 .github/workflows/semantic.yml                |    1 +
 .github/workflows/test-make-command.yml       |    2 +-
 .github/workflows/test.yml                    |   24 +-
 .github/workflows/unit_mac.yml                |    2 +-
 .github/workflows/unit_windows.yml            |    2 +-
 .github/workflows/vdev_publish.yml            |    2 +-
 Cargo.lock                                    |  437 ++--
 Cargo.toml                                    |   24 +-
 LICENSE-3rdparty.csv                          |   15 +-
 ...ckhouse_arrow_complex_types.enhancement.md |    3 -
 .../24355_vector_top_controls.feature.md      |    3 -
 ...emote_write_healthcheck_uri.enhancement.md |    3 -
 .../7538_filesystem_inode_metrics.feature.md  |    3 -
 .../component_latency-metrics.enhancement.md  |    5 -
 .../optimize-websocket-source.enhancement.md  |    3 -
 lib/codecs/Cargo.toml                         |    5 +-
 lib/codecs/src/encoding/format/arrow.rs       | 2008 +++++++++++------
 .../src/vrl_functions/parse_dnstap.rs         |   60 +-
 lib/enrichment/Cargo.toml                     |    1 -
 .../src/find_enrichment_table_records.rs      |   81 +-
 .../src/get_enrichment_table_record.rs        |   86 +-
 lib/enrichment/src/vrl_util.rs                |   26 +-
 lib/file-source-common/Cargo.toml             |    2 +-
 lib/vector-api-client/Cargo.toml              |    4 +-
 lib/vector-buffers/Cargo.toml                 |    2 +-
 lib/vector-common/Cargo.toml                  |    2 +-
 lib/vector-core/Cargo.toml                    |    4 +-
 lib/vector-core/src/config/global_options.rs  |   22 +-
 lib/vector-core/src/event/array.rs            |   58 +-
 lib/vector-core/src/event/metadata.rs         |   88 +-
 lib/vector-core/src/event/proto.rs            |   28 +-
 lib/vector-core/src/latency.rs                |   59 -
 lib/vector-core/src/lib.rs                    |    1 -
 lib/vector-core/src/transform/mod.rs          |  365 ++-
 lib/vector-core/src/transform/outputs.rs      |  378 ----
 lib/vector-lib/src/lib.rs                     |    4 +-
 lib/vector-tap/Cargo.toml                     |    4 +-
 lib/vector-top/Cargo.toml                     |    3 +-
 lib/vector-top/src/dashboard.rs               |  309 +--
 lib/vector-top/src/events.rs                  |    6 +-
 lib/vector-top/src/input.rs                   |  229 --
 lib/vector-top/src/lib.rs                     |    1 -
 lib/vector-top/src/state.rs                   |  367 +--
 lib/vector-vrl-metrics/Cargo.toml             |    5 +-
 .../src/aggregate_vector_metrics.rs           |   55 +-
 .../src/find_vector_metrics.rs                |   45 +-
 .../src/get_vector_metric.rs                  |   45 +-
 lib/vector-vrl/category/Cargo.toml            |   10 -
 lib/vector-vrl/category/src/lib.rs            |   16 -
 lib/vector-vrl/functions/Cargo.toml           |    1 -
 lib/vector-vrl/functions/src/get_secret.rs    |   11 -
 lib/vector-vrl/functions/src/remove_secret.rs |   11 -
 lib/vector-vrl/functions/src/set_secret.rs    |   13 -
 .../functions/src/set_semantic_meaning.rs     |   13 -
 src/config/compiler.rs                        |    2 +-
 src/config/validation.rs                      |   44 +-
 src/providers/http.rs                         |   32 +-
 src/sinks/azure_logs_ingestion/config.rs      |  163 ++
 src/sinks/azure_logs_ingestion/mod.rs         |   13 +
 src/sinks/azure_logs_ingestion/service.rs     |  192 ++
 src/sinks/azure_logs_ingestion/sink.rs        |  151 ++
 src/sinks/clickhouse/arrow/parser.rs          |  917 ++++----
 src/sinks/clickhouse/arrow/schema.rs          |   70 +-
 src/sinks/clickhouse/integration_tests.rs     |  571 +----
 src/sinks/mod.rs                              |    2 +
 src/sinks/prometheus/remote_write/config.rs   |    7 +-
 src/sinks/util/encoding.rs                    |    1 -
 src/sources/host_metrics/filesystem.rs        |   61 +-
 src/sources/websocket/source.rs               |    9 +-
 src/test_util/mock/sinks/completion.rs        |   95 -
 src/test_util/mock/sinks/mod.rs               |    3 -
 src/test_util/mock/transforms/noop.rs         |   50 +-
 src/top/cmd.rs                                |   18 +-
 src/top/mod.rs                                |   19 -
 src/topology/builder.rs                       |  381 ++--
 src/topology/test/compliance.rs               |   31 +-
 src/topology/test/latency_metrics.rs          |  147 --
 src/topology/test/mod.rs                      |   85 +-
 src/transforms/dedupe/config.rs               |  118 +-
 src/transforms/filter.rs                      |   18 +-
 src/transforms/log_to_metric.rs               |   70 +-
 src/transforms/sample/transform.rs            |   75 +-
 vdev/Cargo.toml                               |    2 +-
 .../reference/components/sinks/clickhouse.cue |    3 +
 .../sinks/prometheus_remote_write.cue         |    5 +-
 .../components/sources/internal_metrics.cue   |   22 -
 .../cue/reference/components/transforms.cue   |    6 +-
 .../cue/reference/generated/configuration.cue |   22 +-
 website/package.json                          |   10 +-
 website/scripts/typesense-index.ts            |    6 +-
 website/scripts/typesense-sync.ts             |   32 +-
 website/typesense.config.json                 |   40 +-
 website/yarn.lock                             |  340 +--
 119 files changed, 3947 insertions(+), 5005 deletions(-)
 create mode 100644 .github/audit.yml
 delete mode 100644 changelog.d/24074_clickhouse_arrow_complex_types.enhancement.md
 delete mode 100644 changelog.d/24355_vector_top_controls.feature.md
 delete mode 100644 changelog.d/24603_prometheus_remote_write_healthcheck_uri.enhancement.md
 delete mode 100644 changelog.d/7538_filesystem_inode_metrics.feature.md
 delete mode 100644 changelog.d/component_latency-metrics.enhancement.md
 delete mode 100644 changelog.d/optimize-websocket-source.enhancement.md
 delete mode 100644 lib/vector-core/src/latency.rs
 delete mode 100644 lib/vector-core/src/transform/outputs.rs
 delete mode 100644 lib/vector-top/src/input.rs
 delete mode 100644 lib/vector-vrl/category/Cargo.toml
 delete mode 100644 lib/vector-vrl/category/src/lib.rs
 create mode 100644 src/sinks/azure_logs_ingestion/config.rs
 create mode 100644 src/sinks/azure_logs_ingestion/mod.rs
 create mode 100644 src/sinks/azure_logs_ingestion/service.rs
 create mode 100644 src/sinks/azure_logs_ingestion/sink.rs
 delete mode 100644 src/test_util/mock/sinks/completion.rs
 delete mode 100644 src/topology/test/latency_metrics.rs

diff --git a/.github/ISSUE_TEMPLATE/minor-release.md b/.github/ISSUE_TEMPLATE/minor-release.md
index bcecf7508..bdf7150d1 100644
--- a/.github/ISSUE_TEMPLATE/minor-release.md
+++ b/.github/ISSUE_TEMPLATE/minor-release.md
@@ -92,12 +92,11 @@ Automated steps include:
     - Refer to the internal releasing doc to monitor the deployment.
 - [ ] Release Linux packages. Refer to the internal releasing doc.
 - [ ] Release updated Helm chart. See [releasing Helm chart](https://github.com/vectordotdev/helm-charts/blob/develop/RELEASING.md).
-- [ ] Release Homebrew. Refer to the internal releasing doc.
 - [ ] Create internal Docker images. Refer to the internal releasing doc.
-- [ ] Update the latest [release tag](https://github.com/vectordotdev/vector/release) description with the release announcement.
 - [ ] Create a new PR with title starting as `chore(releasing):`
   - [ ] Cherry-pick any release commits from the release branch that are not on `master`, to `master`.
   - [ ] Run `cargo vdev build manifests` and commit changes.
   - [ ] Bump the release number in the `Cargo.toml` on master to the next minor release.
   - [ ] Also, update `Cargo.lock` with: `cargo update -p vector`.
   - [ ] If there is a VRL version update, revert it and make it track the git `main` branch and then run `cargo update -p vrl`.
+- [ ] Kick-off post-mortems for any regressions resolved by the release.
diff --git a/.github/actions/install-vdev/action.yml b/.github/actions/install-vdev/action.yml
index 7b35d99d5..8c5947c07 100644
--- a/.github/actions/install-vdev/action.yml
+++ b/.github/actions/install-vdev/action.yml
@@ -17,7 +17,7 @@ runs:
     - name: Cache vdev binary
       id: cache-vdev
       if: ${{ inputs.skip-cache != 'true' }}
-      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
+      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
       with:
         path: ~/.cargo/bin/vdev
         key: ${{ runner.os }}-vdev-${{ hashFiles('vdev/**', 'Cargo.toml', 'Cargo.lock') }}
@@ -33,7 +33,7 @@ runs:
 
     - name: Save vdev to cache
       if: ${{ inputs.skip-cache == 'true' }}
-      uses: actions/cache/save@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
+      uses: actions/cache/save@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
       with:
         path: ~/.cargo/bin/vdev
         key: ${{ runner.os }}-vdev-${{ hashFiles('vdev/**', 'Cargo.toml', 'Cargo.lock') }}
diff --git a/.github/actions/setup/action.yml b/.github/actions/setup/action.yml
index d8c5e8c02..06b5bcfe6 100644
--- a/.github/actions/setup/action.yml
+++ b/.github/actions/setup/action.yml
@@ -83,7 +83,7 @@ runs:
     - name: Check vdev cache status
       id: check-vdev-cache
       if: ${{ inputs.vdev == 'true' }}
-      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
+      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
       with:
         path: ~/.cargo/bin/vdev
         key: ${{ runner.os }}-vdev-${{ hashFiles('vdev/**', 'Cargo.toml', 'Cargo.lock') }}
@@ -148,7 +148,7 @@ runs:
 
     - name: Cache Cargo registry, index, and git DB
       if: ${{ inputs.cargo-cache == 'true' || env.NEEDS_RUST == 'true' || env.VDEV_NEEDS_COMPILE == 'true' }}
-      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
+      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
       with:
         path: |
           ~/.cargo/registry/index/
@@ -244,7 +244,7 @@ runs:
 
     - name: Cache prepare.sh binaries
       id: cache-prepare-binaries
-      uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
+      uses: actions/cache@5a3ec84eff668545956fd18022155c47e93e2684 # v4.2.3
       with:
         path: |
           ~/.cargo/bin/cargo-deb
diff --git a/.github/actions/spelling/allow.txt b/.github/actions/spelling/allow.txt
index 7581a21c6..9b14c6dbd 100644
--- a/.github/actions/spelling/allow.txt
+++ b/.github/actions/spelling/allow.txt
@@ -276,7 +276,6 @@ KDL
 keepappkey
 keephq
 kenton
-keybinds
 Kingcom
 Kolkata
 konqueror
diff --git a/.github/audit.yml b/.github/audit.yml
new file mode 100644
index 000000000..2544a1bad
--- /dev/null
+++ b/.github/audit.yml
@@ -0,0 +1,18 @@
+name: Security audit
+
+on:
+  schedule:
+    - cron: '0 * * * *'
+  push:
+    branches:
+      - master
+
+jobs:
+  security_audit:
+    runs-on: ubuntu-24.04
+    steps:
+      - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
+      # TODO: replace this action - abandoned since 2020
+      - uses: actions-rs/audit-check@35b7b53b1e25b55642157ac01b4adceb5b9ebef3 # v1.2.0
+        with:
+          token: ${{ secrets.GITHUB_TOKEN }}
diff --git a/.github/workflows/build-test-runner.yml b/.github/workflows/build-test-runner.yml
index 82e247c6f..f8fc383e4 100644
--- a/.github/workflows/build-test-runner.yml
+++ b/.github/workflows/build-test-runner.yml
@@ -24,7 +24,7 @@ jobs:
   build:
     runs-on: ubuntu-24.04
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.checkout_ref || inputs.commit_sha }}
 
@@ -33,7 +33,7 @@ jobs:
           vdev: true
 
       - name: Login to GitHub Container Registry
-        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
+        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
         with:
           registry: ghcr.io
           username: ${{ github.actor }}
diff --git a/.github/workflows/changelog.yaml b/.github/workflows/changelog.yaml
index e1d6beffd..d7b94723f 100644
--- a/.github/workflows/changelog.yaml
+++ b/.github/workflows/changelog.yaml
@@ -45,7 +45,7 @@ jobs:
           exit 0
 
       # Checkout changelog script and changelog.d/ from master
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         if: env.SHOULD_RUN == 'true'
         with:
           ref: master
@@ -55,7 +55,7 @@ jobs:
           sparse-checkout-cone-mode: false
 
       # Checkout PR's changelog.d/ into tmp/
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         if: env.SHOULD_RUN == 'true'
         with:
           repository: ${{ github.event.pull_request.head.repo.full_name }}
diff --git a/.github/workflows/changes.yml b/.github/workflows/changes.yml
index 49e9fd19e..31dd60615 100644
--- a/.github/workflows/changes.yml
+++ b/.github/workflows/changes.yml
@@ -182,7 +182,7 @@ jobs:
       unit_mac-yml: ${{ steps.filter.outputs.unit_mac-yml }}
       unit_windows-yml: ${{ steps.filter.outputs.unit_windows-yml }}
     steps:
-    - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+    - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
     - uses: dorny/paths-filter@de90cc6fb38fc0963ad72b210f1f284cd68cea36 # v3.0.2
       id: filter
@@ -326,7 +326,7 @@ jobs:
       webhdfs: ${{ steps.filter.outputs.webhdfs }}
       any: ${{ steps.detect-changes.outputs.any }}
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
       - uses: ./.github/actions/setup
         with:
@@ -418,7 +418,7 @@ jobs:
       opentelemetry-metrics: ${{ steps.filter.outputs.opentelemetry-metrics }}
       any: ${{ steps.detect-changes.outputs.any }}
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
       - uses: ./.github/actions/setup
         with:
diff --git a/.github/workflows/ci-integration-review.yml b/.github/workflows/ci-integration-review.yml
index d634a9839..79958b5a9 100644
--- a/.github/workflows/ci-integration-review.yml
+++ b/.github/workflows/ci-integration-review.yml
@@ -105,7 +105,7 @@ jobs:
           "redis", "splunk", "webhdfs"
         ]
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           submodules: "recursive"
           ref: ${{ github.event.review.commit_id }}
@@ -141,7 +141,7 @@ jobs:
           "datadog-logs", "datadog-metrics", "opentelemetry-logs", "opentelemetry-metrics"
         ]
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           submodules: "recursive"
           ref: ${{ github.event.review.commit_id }}
diff --git a/.github/workflows/cli.yml b/.github/workflows/cli.yml
index 9c6728fb2..19b63afa4 100644
--- a/.github/workflows/cli.yml
+++ b/.github/workflows/cli.yml
@@ -16,7 +16,7 @@ jobs:
     runs-on: ubuntu-24.04
     timeout-minutes: 30
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
       - uses: ./.github/actions/setup
diff --git a/.github/workflows/compilation-timings.yml b/.github/workflows/compilation-timings.yml
index dec323e81..4f147fdc1 100644
--- a/.github/workflows/compilation-timings.yml
+++ b/.github/workflows/compilation-timings.yml
@@ -16,7 +16,7 @@ jobs:
     runs-on: ubuntu-24.04-8core
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
@@ -32,7 +32,7 @@ jobs:
       PROFILE: debug
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
@@ -43,7 +43,7 @@ jobs:
     runs-on: ubuntu-24.04-8core
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
@@ -54,7 +54,7 @@ jobs:
     runs-on: ubuntu-24.04-8core
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
@@ -67,7 +67,7 @@ jobs:
     runs-on: ubuntu-24.04-8core
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - run: sudo -E bash scripts/environment/bootstrap-ubuntu-24.04.sh
       - run: bash scripts/environment/prepare.sh --modules=rustup
       - run: cargo clean
diff --git a/.github/workflows/component_features.yml b/.github/workflows/component_features.yml
index 78abccaa0..04492c62c 100644
--- a/.github/workflows/component_features.yml
+++ b/.github/workflows/component_features.yml
@@ -35,7 +35,7 @@ jobs:
     timeout-minutes: 180 # Usually takes 2h but this prevents it from hanging for an indefinite time
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/cross.yml b/.github/workflows/cross.yml
index 64da74d3f..0ea0b45cb 100644
--- a/.github/workflows/cross.yml
+++ b/.github/workflows/cross.yml
@@ -31,11 +31,11 @@ jobs:
           - arm-unknown-linux-musleabi
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
-      - uses: actions/cache@cdf6c1fa76f9f475f3d7449005a359c84ca0f306 # v5.0.3
+      - uses: actions/cache@9255dc7a253b0ccc959486e2bca901246202afeb # v5.0.1
         name: Cache Cargo registry + index
         with:
           path: |
diff --git a/.github/workflows/deny.yml b/.github/workflows/deny.yml
index 2a9696cc3..b55c0bc65 100644
--- a/.github/workflows/deny.yml
+++ b/.github/workflows/deny.yml
@@ -52,7 +52,7 @@ jobs:
       CARGO_INCREMENTAL: 0
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/environment.yml b/.github/workflows/environment.yml
index 129e4c661..2efb01513 100644
--- a/.github/workflows/environment.yml
+++ b/.github/workflows/environment.yml
@@ -36,7 +36,7 @@ jobs:
     timeout-minutes: 30
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
@@ -45,7 +45,7 @@ jobs:
       - name: Set up Docker Buildx
         uses: docker/setup-buildx-action@8d2750c68a42422c14e847fe6c8ac0403b4cbd6f # v3.12.0
       - name: Login to DockerHub
-        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
+        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
         if: env.SHOULD_PUBLISH == 'true'
         with:
           username: ${{ secrets.CI_DOCKER_USERNAME }}
diff --git a/.github/workflows/gardener_remove_waiting_author.yml b/.github/workflows/gardener_remove_waiting_author.yml
index b431395b7..6366cc7b0 100644
--- a/.github/workflows/gardener_remove_waiting_author.yml
+++ b/.github/workflows/gardener_remove_waiting_author.yml
@@ -16,7 +16,7 @@ jobs:
     runs-on: ubuntu-24.04
     timeout-minutes: 5
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: actions-ecosystem/action-remove-labels@2ce5d41b4b6aa8503e285553f75ed56e0a40bae0 # v1.3.0
         with:
           labels: "meta: awaiting author"
diff --git a/.github/workflows/install-sh.yml b/.github/workflows/install-sh.yml
index 69777ead1..873a3a3d6 100644
--- a/.github/workflows/install-sh.yml
+++ b/.github/workflows/install-sh.yml
@@ -23,7 +23,7 @@ jobs:
     timeout-minutes: 5
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/integration-test.yml b/.github/workflows/integration-test.yml
index 838c922e6..8b62da0a0 100644
--- a/.github/workflows/integration-test.yml
+++ b/.github/workflows/integration-test.yml
@@ -39,13 +39,13 @@ jobs:
 
       - name: (PR comment) Checkout PR branch
         if: ${{ github.event_name == 'issue_comment' }}
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ steps.comment-branch.outputs.head_ref }}
 
       - name: Checkout branch
         if: ${{ github.event_name != 'issue_comment' }}
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
       - run: bash scripts/environment/prepare.sh --modules=rustup,datadog-ci
 
diff --git a/.github/workflows/integration.yml b/.github/workflows/integration.yml
index 94e8a9466..de2cc01c2 100644
--- a/.github/workflows/integration.yml
+++ b/.github/workflows/integration.yml
@@ -79,7 +79,7 @@ jobs:
         ]
     timeout-minutes: 90
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           submodules: "recursive"
 
@@ -140,7 +140,7 @@ jobs:
     timeout-minutes: 90
 
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           submodules: "recursive"
 
diff --git a/.github/workflows/k8s_e2e.yml b/.github/workflows/k8s_e2e.yml
index fc36a0ed8..9b019590d 100644
--- a/.github/workflows/k8s_e2e.yml
+++ b/.github/workflows/k8s_e2e.yml
@@ -78,7 +78,7 @@ jobs:
       DISABLE_MOLD: true
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
       - run: sudo -E bash scripts/ci-free-disk-space.sh
@@ -171,7 +171,7 @@ jobs:
       fail-fast: false
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/msrv.yml b/.github/workflows/msrv.yml
index 5bc98ced1..4d08fdb20 100644
--- a/.github/workflows/msrv.yml
+++ b/.github/workflows/msrv.yml
@@ -22,7 +22,7 @@ jobs:
     runs-on: ubuntu-24.04
     timeout-minutes: 45
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.checkout_ref }}
       - uses: ./.github/actions/setup
diff --git a/.github/workflows/protobuf.yml b/.github/workflows/protobuf.yml
index 06e8411a8..6e56ea14c 100644
--- a/.github/workflows/protobuf.yml
+++ b/.github/workflows/protobuf.yml
@@ -22,7 +22,7 @@ jobs:
     timeout-minutes: 5
     steps:
       # Run `git checkout`
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       # Install the `buf` CLI
       - uses: bufbuild/buf-setup-action@a47c93e0b1648d5651a065437926377d060baa99 # v1.50.0
         with:
diff --git a/.github/workflows/publish-homebrew.yml b/.github/workflows/publish-homebrew.yml
index 117fe087c..4d17069c3 100644
--- a/.github/workflows/publish-homebrew.yml
+++ b/.github/workflows/publish-homebrew.yml
@@ -28,7 +28,7 @@ jobs:
       GITHUB_TOKEN: ${{ secrets.HOMEBREW_PAT }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref || github.ref_name }}
 
diff --git a/.github/workflows/publish.yml b/.github/workflows/publish.yml
index 3e33c5811..fc9dae047 100644
--- a/.github/workflows/publish.yml
+++ b/.github/workflows/publish.yml
@@ -37,7 +37,7 @@ jobs:
       vector_release_channel: ${{ steps.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Generate publish metadata
@@ -55,7 +55,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -81,7 +81,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -107,7 +107,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -135,7 +135,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -163,7 +163,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -191,7 +191,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -219,7 +219,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -247,7 +247,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Ubuntu-specific)
@@ -290,7 +290,7 @@ jobs:
             exit 1
           fi
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (macOS-specific)
@@ -324,7 +324,7 @@ jobs:
       RELEASE_BUILDER: "true"
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Bootstrap runner environment (Windows-specific)
@@ -394,7 +394,7 @@ jobs:
       - name: Fix Git safe directories issue when in containers (actions/checkout#760)
         run: git config --global --add safe.directory /__w/vector/vector
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (x86_64-unknown-linux-gnu)
@@ -445,7 +445,7 @@ jobs:
       - name: Fix Git safe directories issue when in containers (actions/checkout#760)
         run: git config --global --add safe.directory /__w/vector/vector
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (x86_64-unknown-linux-gnu)
@@ -474,7 +474,7 @@ jobs:
             runner: macos-14
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (${{ matrix.target }})
@@ -508,16 +508,16 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Login to DockerHub
-        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
+        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
         with:
           username: ${{ secrets.CI_DOCKER_USERNAME }}
           password: ${{ secrets.CI_DOCKER_PASSWORD }}
       - name: Login to GitHub Container Registry
-        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
+        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
         with:
           registry: ghcr.io
           username: ${{ github.actor }}
@@ -607,7 +607,7 @@ jobs:
       CHANNEL: ${{ needs.generate-publish-metadata.outputs.vector_release_channel }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (aarch64-unknown-linux-gnu)
@@ -692,7 +692,7 @@ jobs:
       VECTOR_VERSION: ${{ needs.generate-publish-metadata.outputs.vector_version }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (aarch64-unknown-linux-gnu)
@@ -775,7 +775,7 @@ jobs:
       VECTOR_VERSION: ${{ needs.generate-publish-metadata.outputs.vector_version }}
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.git_ref }}
       - name: Download staged package artifacts (aarch64-unknown-linux-gnu)
diff --git a/.github/workflows/regression.yml b/.github/workflows/regression.yml
index 0c462b27d..14c45a827 100644
--- a/.github/workflows/regression.yml
+++ b/.github/workflows/regression.yml
@@ -51,7 +51,7 @@ jobs:
       smp-version: ${{ steps.experimental-meta.outputs.SMP_CRATE_VERSION }}
     steps:
       - name: Checkout repository
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           fetch-depth: 0  # need to pull repository history to find merge bases
 
@@ -117,7 +117,7 @@ jobs:
     outputs:
       source_changed: ${{ steps.filter.outputs.SOURCE_CHANGED }}
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
       - name: Collect file changes
         id: changes
@@ -192,9 +192,9 @@ jobs:
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
 
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ needs.resolve-inputs.outputs.baseline-sha }}
           path: baseline-vector
@@ -231,9 +231,9 @@ jobs:
     steps:
       - uses: colpal/actions-clean@36e6ca1abd35efe61cb60f912bd7837f67887c8a # v1.1.1
 
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ needs.resolve-inputs.outputs.comparison-sha }}
           path: comparison-vector
@@ -314,7 +314,7 @@ jobs:
         uses: aws-actions/amazon-ecr-login@062b18b96a7aff071d4dc91bc00c4c1a7945b076 # v2.0.1
 
       - name: Docker Login to ECR
-        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
+        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
         with:
           registry: ${{ steps.login-ecr.outputs.registry }}
 
@@ -354,7 +354,7 @@ jobs:
         uses: aws-actions/amazon-ecr-login@062b18b96a7aff071d4dc91bc00c4c1a7945b076 # v2.0.1
 
       - name: Docker Login to ECR
-        uses: docker/login-action@c94ce9fb468520275223c153574b00df6fe4bcc9 # v3.7.0
+        uses: docker/login-action@5e57cd118135c172c3672efd75eb46360885c0ef # v3.6.0
         with:
           registry: ${{ steps.login-ecr.outputs.registry }}
 
@@ -373,7 +373,7 @@ jobs:
       - upload-baseline-image-to-ecr
       - upload-comparison-image-to-ecr
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ needs.resolve-inputs.outputs.comparison-sha }}
 
@@ -448,7 +448,7 @@ jobs:
       - should-run-gate
       - resolve-inputs
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
       - name: Configure AWS Credentials
         uses: aws-actions/configure-aws-credentials@61815dcd50bd041e203e49132bacad1fd04d2708 # v5.1.1
@@ -485,7 +485,7 @@ jobs:
       - submit-job
       - resolve-inputs
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ needs.resolve-inputs.outputs.comparison-sha }}
 
diff --git a/.github/workflows/scorecard.yml b/.github/workflows/scorecard.yml
index a6482fb83..1ad210543 100644
--- a/.github/workflows/scorecard.yml
+++ b/.github/workflows/scorecard.yml
@@ -32,7 +32,7 @@ jobs:
 
     steps:
       - name: "Checkout code"
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           persist-credentials: false
 
@@ -68,6 +68,6 @@ jobs:
       # Upload the results to GitHub's code scanning dashboard (optional).
       # Commenting out will disable upload of results to your repo's Code Scanning dashboard
       - name: "Upload to code-scanning"
-        uses: github/codeql-action/upload-sarif@b20883b0cd1f46c72ae0ba6d1090936928f9fa30 # v4.32.0
+        uses: github/codeql-action/upload-sarif@5d4e8d1aca955e8d8589aabd499c5cae939e33c7 # v4.31.9
         with:
           sarif_file: results.sarif
diff --git a/.github/workflows/semantic.yml b/.github/workflows/semantic.yml
index cd5c6f933..39bf02856 100644
--- a/.github/workflows/semantic.yml
+++ b/.github/workflows/semantic.yml
@@ -212,6 +212,7 @@ jobs:
             aws_sqs sink
             axiom sink
             azure_blob sink
+            azure_logs_ingestion sink
             azure_monitor_logs sink
             blackhole sink
             clickhouse sink
diff --git a/.github/workflows/test-make-command.yml b/.github/workflows/test-make-command.yml
index 3f2d483ed..fe565bc48 100644
--- a/.github/workflows/test-make-command.yml
+++ b/.github/workflows/test-make-command.yml
@@ -28,7 +28,7 @@ jobs:
       CARGO_INCREMENTAL: 0
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/test.yml b/.github/workflows/test.yml
index 114321ae2..ed5c8f764 100644
--- a/.github/workflows/test.yml
+++ b/.github/workflows/test.yml
@@ -30,7 +30,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -42,7 +42,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -56,7 +56,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -83,7 +83,7 @@ jobs:
     if: ${{ needs.changes.outputs.scripts == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           cargo-cache: false
@@ -97,7 +97,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           cargo-cache: false
@@ -111,7 +111,7 @@ jobs:
     if: ${{ needs.changes.outputs.dependencies == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           cargo-cache: false
@@ -126,7 +126,7 @@ jobs:
     if: ${{ needs.changes.outputs.cue == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -139,7 +139,7 @@ jobs:
     if: ${{ needs.changes.outputs.markdown == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -152,7 +152,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.component_docs == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -167,7 +167,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -179,7 +179,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.cue == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
@@ -193,7 +193,7 @@ jobs:
     if: ${{ needs.changes.outputs.source == 'true' || needs.changes.outputs.dependencies == 'true' || needs.changes.outputs.test-yml == 'true' }}
     needs: changes
     steps:
-      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+      - uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
       - uses: ./.github/actions/setup
         with:
           rust: true
diff --git a/.github/workflows/unit_mac.yml b/.github/workflows/unit_mac.yml
index b177b018d..e69390f2b 100644
--- a/.github/workflows/unit_mac.yml
+++ b/.github/workflows/unit_mac.yml
@@ -20,7 +20,7 @@ jobs:
       CARGO_INCREMENTAL: 0
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/unit_windows.yml b/.github/workflows/unit_windows.yml
index 831eec042..0c2eeae07 100644
--- a/.github/workflows/unit_windows.yml
+++ b/.github/workflows/unit_windows.yml
@@ -18,7 +18,7 @@ jobs:
     timeout-minutes: 60
     steps:
       - name: Checkout branch
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
         with:
           ref: ${{ inputs.ref }}
 
diff --git a/.github/workflows/vdev_publish.yml b/.github/workflows/vdev_publish.yml
index 155db01ec..5ef36fb36 100644
--- a/.github/workflows/vdev_publish.yml
+++ b/.github/workflows/vdev_publish.yml
@@ -19,7 +19,7 @@ jobs:
 
     steps:
       - name: Checkout Vector
-        uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
+        uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v6.0.1
 
       - name: Bootstrap runner environment (Ubuntu)
         if: startsWith(matrix.os, 'ubuntu')
diff --git a/Cargo.lock b/Cargo.lock
index 493020d75..953cffb65 100644
--- a/Cargo.lock
+++ b/Cargo.lock
@@ -12,6 +12,15 @@ dependencies = [
  "regex",
 ]
 
+[[package]]
+name = "addr2line"
+version = "0.24.2"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "dfbe277e56a376000877090da837660b4427aad530e3028d44e0bffe4f89a1c1"
+dependencies = [
+ "gimli",
+]
+
 [[package]]
 name = "adler2"
 version = "2.0.0"
@@ -403,7 +412,7 @@ version = "56.2.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e003216336f70446457e280807a73899dd822feaf02087d31febca1363e2fccc"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "half",
  "num",
 ]
@@ -670,7 +679,7 @@ dependencies = [
  "async-stream",
  "async-trait",
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "fnv",
  "futures-timer",
@@ -724,7 +733,7 @@ version = "7.0.17"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "34ecdaff7c9cffa3614a9f9999bf9ee4c3078fe3ce4d6a6e161736b56febf2de"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "indexmap 2.12.0",
  "serde",
  "serde_json",
@@ -808,7 +817,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "08f6da6d49a956424ca4e28fe93656f790d748b469eaccbc7488fec545315180"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures 0.3.31",
  "memchr",
  "nkeys",
@@ -984,7 +993,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "hex",
  "http 1.3.1",
@@ -1023,7 +1032,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "http-body 0.4.6",
@@ -1076,7 +1085,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1099,7 +1108,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1122,7 +1131,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1146,7 +1155,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1169,7 +1178,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "regex-lite",
@@ -1195,7 +1204,7 @@ dependencies = [
  "aws-smithy-types",
  "aws-smithy-xml",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "hex",
  "hmac",
@@ -1226,7 +1235,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "regex-lite",
@@ -1271,7 +1280,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1294,7 +1303,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1317,7 +1326,7 @@ dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
  "aws-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "once_cell",
@@ -1359,7 +1368,7 @@ dependencies = [
  "aws-smithy-http",
  "aws-smithy-runtime-api",
  "aws-smithy-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "form_urlencoded",
  "hex",
  "hmac",
@@ -1390,7 +1399,7 @@ checksum = "b65d21e1ba6f2cdec92044f904356a19f5ad86961acf015741106cdfafd747c0"
 dependencies = [
  "aws-smithy-http",
  "aws-smithy-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "crc32c",
  "crc32fast",
  "crc64fast-nvme",
@@ -1412,7 +1421,7 @@ checksum = "c41172a5393f54e26d6b1bfbfce5d0abaa5c46870a1641c1c1899b527f8b6388"
 dependencies = [
  "aws-smithy-runtime-api",
  "aws-smithy-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "flate2",
  "futures-util",
  "http 0.2.9",
@@ -1428,7 +1437,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "604c7aec361252b8f1c871a7641d5e0ba3a7f5a586e51b66bc9510a5519594d9"
 dependencies = [
  "aws-smithy-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "crc32fast",
 ]
 
@@ -1441,7 +1450,7 @@ dependencies = [
  "aws-smithy-eventstream",
  "aws-smithy-runtime-api",
  "aws-smithy-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "bytes-utils",
  "futures-core",
  "http 0.2.9",
@@ -1514,7 +1523,7 @@ dependencies = [
  "aws-smithy-observability",
  "aws-smithy-runtime-api",
  "aws-smithy-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fastrand 2.3.0",
  "http 0.2.9",
  "http 1.3.1",
@@ -1534,7 +1543,7 @@ checksum = "07f5e0fc8a6b3f2303f331b94504bbf754d85488f402d6f1dd7a6080f99afe56"
 dependencies = [
  "aws-smithy-async",
  "aws-smithy-types",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "http 0.2.9",
  "http 1.3.1",
  "pin-project-lite",
@@ -1550,7 +1559,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d498595448e43de7f4296b7b7a18a8a02c61ec9349128c80a368f7c3b4ab11a8"
 dependencies = [
  "base64-simd",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "bytes-utils",
  "futures-core",
  "http 0.2.9",
@@ -1601,7 +1610,7 @@ dependencies = [
  "async-trait",
  "axum-core 0.3.4",
  "bitflags 1.3.2",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-util",
  "http 0.2.9",
  "http-body 0.4.6",
@@ -1629,7 +1638,7 @@ checksum = "3a6c9af12842a67734c9a2e355436e5d03b22383ed60cf13cd0c18fbfe3dcbcf"
 dependencies = [
  "async-trait",
  "axum-core 0.4.5",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-util",
  "http 1.3.1",
  "http-body 1.0.0",
@@ -1655,7 +1664,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "759fa577a247914fd3f7f76d62972792636412fbfd634cd452f6a385a74d2d2c"
 dependencies = [
  "async-trait",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-util",
  "http 0.2.9",
  "http-body 0.4.6",
@@ -1672,7 +1681,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "09f2bd6146b97ae3359fa0cc6d6b376d9539582c7b4220f041a33ec24c226199"
 dependencies = [
  "async-trait",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-util",
  "http 1.3.1",
  "http-body 1.0.0",
@@ -1694,7 +1703,7 @@ dependencies = [
  "async-lock 3.4.0",
  "async-trait",
  "azure_core_macros",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures 0.3.31",
  "openssl",
  "pin-project",
@@ -1718,6 +1727,23 @@ dependencies = [
  "tracing 0.1.41",
 ]
 
+[[package]]
+name = "azure_identity"
+version = "0.30.0"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "0f07bb0ee212021e75c3645e82d078e436b4b4184bde1295e9e81fcbcef923af"
+dependencies = [
+ "async-lock 3.4.0",
+ "async-trait",
+ "azure_core",
+ "futures 0.3.31",
+ "pin-project",
+ "serde",
+ "time",
+ "tracing 0.1.41",
+ "url",
+]
+
 [[package]]
 name = "azure_storage_blob"
 version = "0.7.0"
@@ -1755,6 +1781,21 @@ dependencies = [
  "tokio",
 ]
 
+[[package]]
+name = "backtrace"
+version = "0.3.75"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "6806a6321ec58106fea15becdad98371e28d92ccbc7c8f1b3b6dd724fe8f1002"
+dependencies = [
+ "addr2line",
+ "cfg-if",
+ "libc",
+ "miniz_oxide",
+ "object",
+ "rustc-demangle",
+ "windows-targets 0.52.6",
+]
+
 [[package]]
 name = "base16"
 version = "0.2.1"
@@ -1945,7 +1986,7 @@ checksum = "8796b390a5b4c86f9f2e8173a68c2791f4fa6b038b84e96dbc01c016d1e6722c"
 dependencies = [
  "base64 0.22.1",
  "bollard-stubs",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "futures-core",
  "futures-util",
@@ -2149,20 +2190,6 @@ name = "bytemuck"
 version = "1.21.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ef657dfab802224e671f5818e9a4935f9b1957ed18e58292690cc39e7a4092a3"
-dependencies = [
- "bytemuck_derive",
-]
-
-[[package]]
-name = "bytemuck_derive"
-version = "1.10.2"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "f9abbd1bc6865053c427f7198e6af43bfdedc55ab791faed4fbd361d789575ff"
-dependencies = [
- "proc-macro2 1.0.101",
- "quote 1.0.40",
- "syn 2.0.106",
-]
 
 [[package]]
 name = "byteorder"
@@ -2182,9 +2209,9 @@ dependencies = [
 
 [[package]]
 name = "bytes"
-version = "1.11.1"
+version = "1.10.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "1e748733b7cbc798e1434b6ac524f0c1ff2ab456fe201501e6497c8417a4fc33"
+checksum = "d71b6127be86fdcfddb610f7182ac57211d4b18a3e9c82eb2d17662f2227ad6a"
 dependencies = [
  "serde",
 ]
@@ -2195,7 +2222,7 @@ version = "0.1.3"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e47d3a8076e283f3acd27400535992edb3ba4b5bb72f8891ad8fbe7932a7d4b9"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "either",
 ]
 
@@ -2405,9 +2432,9 @@ dependencies = [
 
 [[package]]
 name = "clap"
-version = "4.5.56"
+version = "4.5.53"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a75ca66430e33a14957acc24c5077b503e7d374151b2b4b3a10c83b4ceb4be0e"
+checksum = "c9e340e012a1bf4935f5282ed1436d1489548e8f72308207ea5df0e23d2d03f8"
 dependencies = [
  "clap_builder",
  "clap_derive",
@@ -2425,9 +2452,9 @@ dependencies = [
 
 [[package]]
 name = "clap_builder"
-version = "4.5.56"
+version = "4.5.53"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "793207c7fa6300a0608d1080b858e5fdbe713cdc1c8db9fb17777d8a13e63df0"
+checksum = "d76b5d13eaa18c901fd2f7fca939fefe3a0727a953561fefdf3b2922b8569d00"
 dependencies = [
  "anstream",
  "anstyle",
@@ -2447,9 +2474,9 @@ dependencies = [
 
 [[package]]
 name = "clap_derive"
-version = "4.5.55"
+version = "4.5.49"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "a92793da1a46a5f2a02a6f4c46c6496b28c43638adea8306fcb0caa1634f24e5"
+checksum = "2a0b5487afeab2deb2ff4e03a807ad1a03ac532ff5a2cee5d86884440c7f7671"
 dependencies = [
  "heck 0.5.0",
  "proc-macro2 1.0.101",
@@ -2499,7 +2526,7 @@ dependencies = [
  "apache-avro 0.20.0",
  "arrow",
  "async-trait",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "csv-core",
  "derivative",
@@ -2521,7 +2548,6 @@ dependencies = [
  "rust_decimal",
  "serde",
  "serde-aux",
- "serde_arrow",
  "serde_json",
  "serde_with",
  "similar-asserts",
@@ -2590,7 +2616,7 @@ version = "4.6.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "35ed6e9d84f0b51a7f52daf1c7d71dd136fd7a3f41a8462b8cdb8c78d920fad4"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-core",
  "memchr",
  "pin-project-lite",
@@ -3487,14 +3513,14 @@ dependencies = [
 
 [[package]]
 name = "dns-lookup"
-version = "3.0.1"
+version = "2.0.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "6e39034cee21a2f5bbb66ba0e3689819c4bb5d00382a282006e802a7ffa6c41d"
+checksum = "e5766087c2235fec47fafa4cfecc81e494ee679d0fd4a59887ea0919bfb0e4fc"
 dependencies = [
  "cfg-if",
  "libc",
- "socket2 0.6.0",
- "windows-sys 0.60.2",
+ "socket2 0.5.10",
+ "windows-sys 0.48.0",
 ]
 
 [[package]]
@@ -3513,7 +3539,7 @@ version = "0.1.0"
 dependencies = [
  "anyhow",
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "chrono-tz",
  "dnsmsg-parser",
@@ -3565,7 +3591,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a11dd7f04a6a6d2aea0153c6e31f5ea7af8b2efdf52cdaeea7a9a592c7fefef9"
 dependencies = [
  "bumpalo",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "domain-macros",
  "futures-util",
  "hashbrown 0.14.5",
@@ -3738,7 +3764,6 @@ dependencies = [
  "const-str",
  "dyn-clone",
  "indoc",
- "vector-vrl-category",
  "vrl",
 ]
 
@@ -4025,7 +4050,7 @@ name = "file-source"
 version = "0.1.0"
 dependencies = [
  "async-compression",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "file-source-common",
  "futures 0.3.31",
@@ -4047,7 +4072,7 @@ version = "0.1.0"
 dependencies = [
  "async-compression",
  "bstr 1.12.0",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "crc",
  "dashmap",
@@ -4398,11 +4423,17 @@ dependencies = [
  "wasm-bindgen",
 ]
 
+[[package]]
+name = "gimli"
+version = "0.31.1"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "07e28edb80900c19c28f1072f2e8aeca7fa06b23cd4169cefe1af5aa3260783f"
+
 [[package]]
 name = "git2"
-version = "0.20.4"
+version = "0.20.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7b88256088d75a56f8ecfa070513a775dd9107f6530ef14919dac831af9cfe2b"
+checksum = "2deb07a133b1520dc1a5690e9bd08950108873d7ed5de38dcc74d3b5ebffa110"
 dependencies = [
  "bitflags 2.10.0",
  "libc",
@@ -4606,7 +4637,7 @@ version = "0.3.26"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "81fe527a889e1532da5c525686d96d4c2e74cdd345badf8dfef9f6b39dd5f5e8"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fnv",
  "futures-core",
  "futures-sink",
@@ -4626,7 +4657,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f3c0b69cfcb4e1b9f1bf2f53f95f766e4661169728ec61cd3fe5a0166f2d1386"
 dependencies = [
  "atomic-waker",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fnv",
  "futures-core",
  "futures-sink",
@@ -4644,7 +4675,6 @@ version = "2.4.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6dd08c532ae367adf81c312a4580bc67f1d0fe8bc9c460520283f4c0ff277888"
 dependencies = [
- "bytemuck",
  "cfg-if",
  "crunchy",
  "num-traits",
@@ -4733,7 +4763,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "06683b93020a07e3dbcf5f8c0f6d40080d725bea7936fc01ad345c01b97dc270"
 dependencies = [
  "base64 0.21.7",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "headers-core",
  "http 0.2.9",
  "httpdate",
@@ -5031,7 +5061,7 @@ version = "0.2.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "bd6effc99afb63425aff9b05836f029929e345a6148a14b7ecd5ab67af944482"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fnv",
  "itoa",
 ]
@@ -5042,7 +5072,7 @@ version = "1.3.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f4a85d31aea989eead29a3aaf9e1115a180df8282431156e533de47660892565"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fnv",
  "itoa",
 ]
@@ -5053,7 +5083,7 @@ version = "0.4.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "7ceab25649e9960c0311ea418d17bee82c0dcec1bd053b5f9a66e265a693bed2"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "http 0.2.9",
  "pin-project-lite",
 ]
@@ -5064,7 +5094,7 @@ version = "1.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "1cac85db508abc24a2e48553ba12a996e87244a0395ce011e62b37158745d643"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "http 1.3.1",
 ]
 
@@ -5074,7 +5104,7 @@ version = "0.1.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "793429d76616a256bcb62c2a2ec2bed781c8307e797e2598c50010f2bee2544f"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-util",
  "http 1.3.1",
  "http-body 1.0.0",
@@ -5121,7 +5151,7 @@ version = "0.14.32"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "41dfc780fdec9373c01bae43289ea34c972e40ee3c9f6b3c8801a35f35586ce7"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-channel",
  "futures-core",
  "futures-util",
@@ -5146,7 +5176,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "eb3aa54a13a0dfe7fbe3a59e0c76093041720fdc77b110cc0fc260fafb4dc51e"
 dependencies = [
  "atomic-waker",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-channel",
  "futures-core",
  "h2 0.4.12",
@@ -5220,7 +5250,7 @@ version = "0.9.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ca815a891b24fdfb243fa3239c86154392b0953ee584aa1a2a1f66d20cbe75cc"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures 0.3.31",
  "headers",
  "http 0.2.9",
@@ -5297,7 +5327,7 @@ version = "0.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d6183ddfa99b85da61a140bea0efc93fdf56ceaa041b37d553518030827f9905"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "hyper 0.14.32",
  "native-tls",
  "tokio",
@@ -5310,7 +5340,7 @@ version = "0.6.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "70206fc6890eaca9fde8a0bf71caa2ddfc9fe045ac9e5c70df101a7dbde866e0"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "http-body-util",
  "hyper 1.7.0",
  "hyper-util",
@@ -5327,7 +5357,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "3c6995591a8f1380fcb4ba966a252a4b29188d51d2b89e3a252f5305be65aea8"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-channel",
  "futures-core",
  "futures-util",
@@ -5578,7 +5608,7 @@ version = "2.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "22fa7ee6be451ea0b1912b962c91c8380835e97cf1584a77e18264e908448dcb"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "log",
  "nom 7.1.3",
  "smallvec",
@@ -5657,6 +5687,17 @@ dependencies = [
  "windows-sys 0.48.0",
 ]
 
+[[package]]
+name = "io-uring"
+version = "0.7.9"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "d93587f37623a1a17d94ef2bc9ada592f5465fe7732084ab7beefabe5c77c0c4"
+dependencies = [
+ "bitflags 2.10.0",
+ "cfg-if",
+ "libc",
+]
+
 [[package]]
 name = "iovec"
 version = "0.1.4"
@@ -5910,7 +5951,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "6d9455388f4977de4d0934efa9f7d36296295537d774574113a20f6082de03da"
 dependencies = [
  "base64 0.13.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "serde",
  "serde-value",
@@ -6009,7 +6050,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "d81336eb3a5b10a40c97a5a97ad66622e92bad942ce05ee789edd730aa4f8603"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "either",
  "futures 0.3.31",
@@ -6233,9 +6274,9 @@ dependencies = [
 
 [[package]]
 name = "libgit2-sys"
-version = "0.18.3+1.9.2"
+version = "0.18.2+1.9.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c9b3acc4b91781bb0b3386669d325163746af5f6e4f73e6d2d630e09a35f3487"
+checksum = "1c42fe03df2bd3c53a3a9c7317ad91d80c81cd1fb0caec8d7cc4cd2bfa10c222"
 dependencies = [
  "cc",
  "libc",
@@ -6559,21 +6600,6 @@ dependencies = [
  "libc",
 ]
 
-[[package]]
-name = "marrow"
-version = "0.2.5"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "ea734fcb7619dfcc47a396f7bf0c72571ccc8c18ae7236ae028d485b27424b74"
-dependencies = [
- "arrow-array",
- "arrow-buffer",
- "arrow-data",
- "arrow-schema",
- "bytemuck",
- "half",
- "serde",
-]
-
 [[package]]
 name = "match_cfg"
 version = "0.1.0"
@@ -6920,7 +6946,7 @@ version = "3.0.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a15d522be0a9c3e46fd2632e272d178f56387bdb5c9fbb3a36c649062e9b5219"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "encoding_rs",
  "futures-util",
  "http 1.3.1",
@@ -7035,7 +7061,7 @@ version = "0.8.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "16c903aa70590cb93691bf97a767c8d1d6122d2cc9070433deb3bbf36ce8bd23"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures 0.3.31",
  "libc",
  "log",
@@ -7295,9 +7321,9 @@ dependencies = [
 
 [[package]]
 name = "num-conv"
-version = "0.2.0"
+version = "0.1.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "cf97ec579c3c42f953ef76dbf8d55ac91fb219dde70e49aa4a6b7d74e9919050"
+checksum = "51d515d32fb182ee37cda2ccdcb92950d6a3c2893aa280e540671c2cd0f3b1d9"
 
 [[package]]
 name = "num-format"
@@ -7471,13 +7497,22 @@ dependencies = [
  "objc2-core-foundation",
 ]
 
+[[package]]
+name = "object"
+version = "0.36.7"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "62948e14d923ea95ea2c7c86c71013138b66525b86bdc08d2dcc262bdb497b87"
+dependencies = [
+ "memchr",
+]
+
 [[package]]
 name = "octseq"
 version = "0.5.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "126c3ca37c9c44cec575247f43a3e4374d8927684f129d2beeb0d2cef262fe12"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "serde",
  "smallvec",
 ]
@@ -7544,7 +7579,7 @@ dependencies = [
  "anyhow",
  "backon",
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "futures 0.3.31",
  "getrandom 0.2.15",
@@ -7651,7 +7686,7 @@ dependencies = [
 name = "opentelemetry-proto"
 version = "0.1.0"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "glob",
  "hex",
@@ -7917,21 +7952,20 @@ dependencies = [
 
 [[package]]
 name = "phf"
-version = "0.12.1"
+version = "0.11.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "913273894cec178f401a31ec4b656318d95473527be05c0752cc41cdc32be8b7"
+checksum = "ade2d8b8f33c7333b51bcf0428d37e217e9f32192ae4772156f65063b8ce03dc"
 dependencies = [
- "phf_shared 0.12.1",
+ "phf_shared 0.11.2",
 ]
 
 [[package]]
 name = "phf"
-version = "0.13.1"
+version = "0.12.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "c1562dc717473dbaa4c1f85a36410e03c047b2e7df7f45ee938fbef64ae7fadf"
+checksum = "913273894cec178f401a31ec4b656318d95473527be05c0752cc41cdc32be8b7"
 dependencies = [
- "phf_shared 0.13.1",
- "serde",
+ "phf_shared 0.12.1",
 ]
 
 [[package]]
@@ -7945,18 +7979,18 @@ dependencies = [
 
 [[package]]
 name = "phf_shared"
-version = "0.12.1"
+version = "0.11.2"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "06005508882fb681fd97892ecff4b7fd0fee13ef1aa569f8695dae7ab9099981"
+checksum = "90fcb95eef784c2ac79119d1dd819e162b5da872ce6f3c3abe1e8ca1c082f72b"
 dependencies = [
- "siphasher 1.0.1",
+ "siphasher 0.3.11",
 ]
 
 [[package]]
 name = "phf_shared"
-version = "0.13.1"
+version = "0.12.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "e57fef6bc5981e38c2ce2d63bfa546861309f875b8a75f092d1d54ae2d64f266"
+checksum = "06005508882fb681fd97892ecff4b7fd0fee13ef1aa569f8695dae7ab9099981"
 dependencies = [
  "siphasher 1.0.1",
 ]
@@ -8148,13 +8182,13 @@ dependencies = [
 
 [[package]]
 name = "postgres-protocol"
-version = "0.6.10"
+version = "0.6.8"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3ee9dd5fe15055d2b6806f4736aa0c9637217074e224bbec46d4041b91bb9491"
+checksum = "76ff0abab4a9b844b93ef7b81f1efc0a366062aaef2cd702c76256b5dc075c54"
 dependencies = [
  "base64 0.22.1",
  "byteorder",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fallible-iterator",
  "hmac",
  "md-5",
@@ -8166,11 +8200,11 @@ dependencies = [
 
 [[package]]
 name = "postgres-types"
-version = "0.2.12"
+version = "0.2.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "54b858f82211e84682fecd373f68e1ceae642d8d751a1ebd13f33de6257b3e20"
+checksum = "613283563cd90e1dfc3518d548caee47e0e725455ed619881f5cf21f36de4b48"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "fallible-iterator",
  "postgres-protocol",
@@ -8395,7 +8429,7 @@ version = "0.11.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "0b82eaa1d779e9a4bc1c3217db8ffbeabaae1dca241bf70183242128d48681cd"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "prost-derive 0.11.9",
 ]
 
@@ -8405,7 +8439,7 @@ version = "0.12.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "deb1435c188b76130da55f17a466d252ff7b1418b2ad3e037d127b94e3411f29"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "prost-derive 0.12.6",
 ]
 
@@ -8415,7 +8449,7 @@ version = "0.13.5"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "2796faa41db3ec313a31f7624d9286acf277b52de526150b7e69f3debf891ee5"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "prost-derive 0.13.5",
 ]
 
@@ -8425,7 +8459,7 @@ version = "0.11.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "119533552c9a7ffacc21e099c24a0ac8bb19c2a2a3f363de84cd9b844feab270"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "heck 0.4.1",
  "itertools 0.10.5",
  "lazy_static",
@@ -8447,7 +8481,7 @@ version = "0.12.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "22505a5c94da8e3b7c2996394d1c933236c4d743e81a410bcca4e6989fc066a4"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "heck 0.5.0",
  "itertools 0.12.1",
  "log",
@@ -8615,7 +8649,7 @@ checksum = "6cee616af00383c461f9ceb0067d15dee68e7d313ae47dbd7f8543236aed7ee9"
 dependencies = [
  "async-channel 2.3.1",
  "async-trait",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "crc",
  "data-url",
@@ -8734,7 +8768,7 @@ version = "0.11.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "62e96808277ec6f97351a2380e6c25114bc9e67037775464979f3037c92d05ef"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "pin-project-lite",
  "quinn-proto",
  "quinn-udp",
@@ -8752,7 +8786,7 @@ version = "0.11.9"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "a2fe5ef3495d7d2e377ff17b1a8ce2ee2ec2a18cde8b6ad6619d65d0701c135d"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "getrandom 0.2.15",
  "rand 0.8.5",
  "ring",
@@ -9063,7 +9097,7 @@ checksum = "7cd3650deebc68526b304898b192fa4102a4ef0b9ada24da096559cb60e0eef8"
 dependencies = [
  "arc-swap",
  "backon",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "cfg-if",
  "combine 4.6.6",
  "futures-channel",
@@ -9243,7 +9277,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "78bf93c4af7a8bb7d879d51cebe797356ff10ae8516ace542b5182d9dcac10b2"
 dependencies = [
  "base64 0.21.7",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "encoding_rs",
  "futures-core",
  "futures-util",
@@ -9287,7 +9321,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "eddd3ca559203180a307f12d114c268abf583f59b03cb906fd0b3ff8646c1147"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "cookie",
  "cookie_store",
  "encoding_rs",
@@ -9415,7 +9449,7 @@ checksum = "2297bf9c81a3f0dc96bc9521370b88f054168c29826a75e89c55ff196e7ed6a1"
 dependencies = [
  "bitvec",
  "bytecheck",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "hashbrown 0.12.3",
  "ptr_meta",
  "rend",
@@ -9550,7 +9584,7 @@ version = "0.24.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "e1568e15fab2d546f940ed3a21f48bbbd1c494c90c99c4481339364a497f94a9"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "flume 0.11.0",
  "futures-util",
  "log",
@@ -9570,7 +9604,7 @@ checksum = "35affe401787a9bd846712274d97654355d21b2a2c092a3139aabe31e9022282"
 dependencies = [
  "arrayvec",
  "borsh",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "num-traits",
  "rand 0.8.5",
  "rkyv",
@@ -9578,6 +9612,12 @@ dependencies = [
  "serde_json",
 ]
 
+[[package]]
+name = "rustc-demangle"
+version = "0.1.26"
+source = "registry+https://github.com/rust-lang/crates.io-index"
+checksum = "56f7d92ca342cea22a06f2121d944b4fd82af56988c270852495420f961d4ace"
+
 [[package]]
 name = "rustc-hash"
 version = "2.1.1"
@@ -10030,21 +10070,6 @@ dependencies = [
  "serde",
 ]
 
-[[package]]
-name = "serde_arrow"
-version = "0.13.7"
-source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "038967a6dda16f5c6ca5b6e1afec9cd2361d39f0db681ca338ac5f0ccece6469"
-dependencies = [
- "arrow-array",
- "arrow-schema",
- "bytemuck",
- "chrono",
- "half",
- "marrow",
- "serde",
-]
-
 [[package]]
 name = "serde_bytes"
 version = "0.11.19"
@@ -10595,7 +10620,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "ee6798b1838b6a0f69c007c133b8df5866302197e404e8b6ee8ed3e3a5e68dc6"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "crc",
  "crossbeam-queue",
@@ -10673,7 +10698,7 @@ dependencies = [
  "base64 0.22.1",
  "bitflags 2.10.0",
  "byteorder",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "crc",
  "digest",
@@ -11249,9 +11274,9 @@ dependencies = [
 
 [[package]]
 name = "time"
-version = "0.3.47"
+version = "0.3.44"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "743bd48c283afc0388f9b8827b976905fb217ad9e647fae3a379a9283c4def2c"
+checksum = "91e7d9e3bb61134e77bde20dd4825b97c010155709965fedf0f49bb138e52a9d"
 dependencies = [
  "deranged",
  "itoa",
@@ -11260,22 +11285,22 @@ dependencies = [
  "num-conv",
  "num_threads",
  "powerfmt",
- "serde_core",
+ "serde",
  "time-core",
  "time-macros",
 ]
 
 [[package]]
 name = "time-core"
-version = "0.1.8"
+version = "0.1.6"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "7694e1cfe791f8d31026952abf09c69ca6f6fa4e1a1229e18988f06a04a12dca"
+checksum = "40868e7c1d2f0b8d73e4a8c7f0ff63af4f6d19be117e90bd73eb1d62cf831c6b"
 
 [[package]]
 name = "time-macros"
-version = "0.2.27"
+version = "0.2.24"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2e70e4c5a0e0a8a4823ad65dfe1a6930e4f4d756dcd9dd7939022b5e8c501215"
+checksum = "30cfb0125f12d9c277f35663a0a33f8c30190f4e4574868a330595412d34ebf3"
 dependencies = [
  "num-conv",
  "time-core",
@@ -11327,20 +11352,23 @@ checksum = "1f3ccbac311fea05f86f61904b462b55fb3df8837a366dfc601a0161d0532f20"
 
 [[package]]
 name = "tokio"
-version = "1.49.0"
+version = "1.47.1"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "72a2903cd7736441aac9df9d7688bd0ce48edccaadf181c3b90be801e81d3d86"
+checksum = "89e49afdadebb872d3145a5638b59eb0691ea23e46ca484037cfab3b76b95038"
 dependencies = [
- "bytes 1.11.1",
+ "backtrace",
+ "bytes 1.10.1",
+ "io-uring",
  "libc",
  "mio",
  "parking_lot 0.12.4",
  "pin-project-lite",
  "signal-hook-registry",
+ "slab",
  "socket2 0.6.0",
  "tokio-macros",
  "tracing 0.1.41",
- "windows-sys 0.61.0",
+ "windows-sys 0.59.0",
 ]
 
 [[package]]
@@ -11366,9 +11394,9 @@ dependencies = [
 
 [[package]]
 name = "tokio-macros"
-version = "2.6.0"
+version = "2.5.0"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "af407857209536a95c8e56f8231ef2c2e2aff839b22e07a1ffcbc617e9db9fa5"
+checksum = "6e06d43f1345a3bcd39f6a56dbb7dcab2ba47e68e8ac134855e7e2bdbaf8cab8"
 dependencies = [
  "proc-macro2 1.0.101",
  "quote 1.0.40",
@@ -11398,25 +11426,25 @@ dependencies = [
 
 [[package]]
 name = "tokio-postgres"
-version = "0.7.15"
+version = "0.7.13"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "2b40d66d9b2cfe04b628173409368e58247e8eddbbd3b0e6c6ba1d09f20f6c9e"
+checksum = "6c95d533c83082bb6490e0189acaa0bbeef9084e60471b696ca6988cd0541fb0"
 dependencies = [
  "async-trait",
  "byteorder",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "fallible-iterator",
  "futures-channel",
  "futures-util",
  "log",
  "parking_lot 0.12.4",
  "percent-encoding",
- "phf 0.13.1",
+ "phf 0.11.2",
  "pin-project-lite",
  "postgres-protocol",
  "postgres-types",
  "rand 0.9.2",
- "socket2 0.6.0",
+ "socket2 0.5.10",
  "tokio",
  "tokio-util",
  "whoami",
@@ -11466,9 +11494,9 @@ dependencies = [
 
 [[package]]
 name = "tokio-stream"
-version = "0.1.18"
+version = "0.1.17"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "32da49809aab5c3bc678af03902d4ccddea2a87d028d86392a4b1560c6906c70"
+checksum = "eca58d7bba4a75707817a2c44174253f9236b2d5fbd055602e9d5c07c139a047"
 dependencies = [
  "futures-core",
  "pin-project-lite",
@@ -11478,10 +11506,12 @@ dependencies = [
 
 [[package]]
 name = "tokio-test"
-version = "0.4.5"
+version = "0.4.4"
 source = "registry+https://github.com/rust-lang/crates.io-index"
-checksum = "3f6d24790a10a7af737693a3e8f1d03faef7e6ca0cc99aae5066f533766de545"
+checksum = "2468baabc3311435b55dd935f702f42cd1b8abb7e754fb7dfb16bd36aa88f9f7"
 dependencies = [
+ "async-stream",
+ "bytes 1.10.1",
  "futures-core",
  "tokio",
  "tokio-stream",
@@ -11517,7 +11547,7 @@ name = "tokio-util"
 version = "0.7.13"
 source = "git+https://github.com/vectordotdev/tokio?branch=tokio-util-0.7.13-framed-read-continue-on-error#b4bdfda8fe8aa24eba36de0d60063b14f30c7fe7"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-core",
  "futures-io",
  "futures-sink",
@@ -11533,7 +11563,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "f591660438b3038dd04d16c938271c79e7e06260ad2ea2885a4861bfb238605d"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-core",
  "futures-sink",
  "http 1.3.1",
@@ -11648,7 +11678,7 @@ dependencies = [
  "async-trait",
  "axum 0.6.20",
  "base64 0.21.7",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "flate2",
  "h2 0.3.26",
  "http 0.2.9",
@@ -11681,7 +11711,7 @@ dependencies = [
  "async-trait",
  "axum 0.7.5",
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "h2 0.4.12",
  "http 1.3.1",
  "http-body 1.0.0",
@@ -11774,7 +11804,7 @@ checksum = "61c5bb1d698276a2443e5ecfabc1008bf15a36c12e6a7176e7bf089ea9131140"
 dependencies = [
  "async-compression",
  "bitflags 2.10.0",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-core",
  "futures-util",
  "http 0.2.9",
@@ -11796,7 +11826,7 @@ checksum = "1e9cd434a998747dd2c4276bc96ee2e0c7a2eadf3cae88e52be55a05fa9053f5"
 dependencies = [
  "base64 0.21.7",
  "bitflags 2.10.0",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "http 1.3.1",
  "http-body 1.0.0",
  "http-body-util",
@@ -11815,7 +11845,7 @@ checksum = "d4e6559d53cc268e5031cd8429d05415bc4cb4aefc4aa5d6cc35fbf5b924a1f8"
 dependencies = [
  "async-compression",
  "bitflags 2.10.0",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-core",
  "futures-util",
  "http 1.3.1",
@@ -12055,7 +12085,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9e3dac10fd62eaf6617d3a904ae222845979aec67c615d1c842b4002c7666fb9"
 dependencies = [
  "byteorder",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "data-encoding",
  "http 0.2.9",
  "httparse",
@@ -12074,7 +12104,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "9ef1a641ea34f399a848dea702823bbecfb4c486f911735368f1f137cb8257e1"
 dependencies = [
  "byteorder",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "data-encoding",
  "http 1.3.1",
  "httparse",
@@ -12145,7 +12175,7 @@ source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "44f91ea93fdd5fd4985fcc0a197ed8e8da18705912bef63c9b9b3148d6f35510"
 dependencies = [
  "base64 0.22.1",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures 0.3.31",
  "quick-xml 0.38.4",
  "serde",
@@ -12518,12 +12548,13 @@ dependencies = [
  "aws-types",
  "axum 0.6.20",
  "azure_core",
+ "azure_identity",
  "azure_storage_blob",
  "base64 0.22.1",
  "bloomy",
  "bollard",
  "byteorder",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "bytesize",
  "cfg-if",
  "chrono",
@@ -12715,7 +12746,7 @@ dependencies = [
  "async-stream",
  "async-trait",
  "bytecheck",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "clap",
  "crc32fast",
  "criterion",
@@ -12756,7 +12787,7 @@ name = "vector-common"
 version = "0.1.0"
 dependencies = [
  "async-stream",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "chrono",
  "crossbeam-utils",
  "derivative",
@@ -12846,7 +12877,7 @@ dependencies = [
  "async-trait",
  "base64 0.22.1",
  "bitmask-enum",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "cfg-if",
  "chrono",
  "chrono-tz",
@@ -13008,7 +13039,6 @@ dependencies = [
  "indoc",
  "num-format",
  "ratatui",
- "regex",
  "tokio",
  "tokio-stream",
  "unit-prefix",
@@ -13017,13 +13047,6 @@ dependencies = [
  "vector-common",
 ]
 
-[[package]]
-name = "vector-vrl-category"
-version = "0.1.0"
-dependencies = [
- "strum 0.27.2",
-]
-
 [[package]]
 name = "vector-vrl-cli"
 version = "0.1.0"
@@ -13040,7 +13063,6 @@ dependencies = [
  "dnstap-parser",
  "enrichment",
  "indoc",
- "vector-vrl-category",
  "vector-vrl-metrics",
  "vrl",
 ]
@@ -13055,7 +13077,6 @@ dependencies = [
  "tokio-stream",
  "vector-common",
  "vector-core",
- "vector-vrl-category",
  "vrl",
 ]
 
@@ -13107,7 +13128,7 @@ checksum = "6a02e4885ed3bc0f2de90ea6dd45ebcbb66dacffe03547fadbb0eeae2770887d"
 [[package]]
 name = "vrl"
 version = "0.30.0"
-source = "git+https://github.com/vectordotdev/vrl.git?branch=main#7830c051501a08a328c53033364af55fd898aa86"
+source = "git+https://github.com/vectordotdev/vrl.git?branch=main#ee45ff6caf724ff91ff7fac825baa7267fd7aab7"
 dependencies = [
  "aes",
  "aes-siv",
@@ -13116,7 +13137,7 @@ dependencies = [
  "base16",
  "base62",
  "base64-simd",
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "cbc",
  "cfb-mode",
  "cfg-if",
@@ -13199,8 +13220,6 @@ dependencies = [
  "snafu 0.8.9",
  "snap",
  "strip-ansi-escapes",
- "strum 0.26.3",
- "strum_macros 0.26.4",
  "syslog_loose 0.22.0",
  "termcolor",
  "thiserror 2.0.17",
@@ -13272,7 +13291,7 @@ version = "0.3.7"
 source = "registry+https://github.com/rust-lang/crates.io-index"
 checksum = "4378d202ff965b011c64817db11d5829506d3404edeadb61f190d111da3f231c"
 dependencies = [
- "bytes 1.11.1",
+ "bytes 1.10.1",
  "futures-channel",
  "futures-util",
  "headers",
diff --git a/Cargo.toml b/Cargo.toml
index ee8a3d55b..698ce21b8 100644
--- a/Cargo.toml
+++ b/Cargo.toml
@@ -128,7 +128,6 @@ members = [
   "lib/vector-stream",
   "lib/vector-tap",
   "lib/vector-top",
-  "lib/vector-vrl/category",
   "lib/vector-vrl/cli",
   "lib/vector-vrl/functions",
   "lib/vector-vrl/tests",
@@ -143,11 +142,11 @@ arc-swap = { version = "1.7.1", default-features = false }
 async-stream = { version = "0.3.6", default-features = false }
 async-trait = { version = "0.1.89", default-features = false }
 base64 = { version = "0.22.1", default-features = false }
-bytes = { version = "1.11.1", default-features = false, features = ["serde"] }
+bytes = { version = "1.10.1", default-features = false, features = ["serde"] }
 cfg-if = { version = "1.0.3", default-features = false }
 chrono = { version = "0.4.41", default-features = false, features = ["clock", "serde"] }
 chrono-tz = { version = "0.10.4", default-features = false, features = ["serde"] }
-clap = { version = "4.5.56", default-features = false, features = ["derive", "error-context", "env", "help", "std", "string", "usage", "wrap_help"] }
+clap = { version = "4.5.53", default-features = false, features = ["derive", "error-context", "env", "help", "std", "string", "usage", "wrap_help"] }
 clap_complete = "4.5.65"
 colored = { version = "3.0.0", default-features = false }
 const-str = { version = "1.0.0", default-features = false }
@@ -192,10 +191,7 @@ serde_yaml = { version = "0.9.34", default-features = false }
 snafu = { version = "0.8.9", default-features = false, features = ["futures", "std"] }
 socket2 = { version = "0.5.10", default-features = false }
 tempfile = "3.23.0"
-tokio = { version = "1.49.0", default-features = false }
-tokio-stream = { version = "0.1.18", default-features = false }
-tokio-test = "0.4.5"
-tokio-tungstenite = { version = "0.20.1", default-features = false }
+tokio = { version = "1.45.1", default-features = false }
 toml = { version = "0.9.8", default-features = false, features = ["serde", "display", "parse"] }
 tonic = { version = "0.11", default-features = false, features = ["transport", "codegen", "prost", "tls", "tls-roots", "gzip"] }
 tonic-build = { version = "0.11", default-features = false, features = ["transport", "prost"] }
@@ -208,7 +204,6 @@ vector-config-common = { path = "lib/vector-config-common" }
 vector-config-macros = { path = "lib/vector-config-macros" }
 vector-common-macros = { path = "lib/vector-common-macros" }
 vector-lib = { path = "lib/vector-lib", default-features = false, features = ["vrl"] }
-vector-vrl-category = { path = "lib/vector-vrl/category" }
 vector-vrl-functions = { path = "lib/vector-vrl/functions" }
 vrl = { git = "https://github.com/vectordotdev/vrl.git", branch = "main", features = ["arbitrary", "cli", "test", "test_framework"] }
 mock_instant = { version = "0.6" }
@@ -250,7 +245,7 @@ async-trait.workspace = true
 futures.workspace = true
 tokio = { workspace = true, features = ["full"] }
 tokio-openssl = { version = "0.6.5", default-features = false }
-tokio-stream = { workspace = true, features = ["net", "sync", "time"] }
+tokio-stream = { version = "0.1.17", default-features = false, features = ["net", "sync", "time"] }
 tokio-util = { version = "0.7", default-features = false, features = ["io", "time"] }
 console-subscriber = { version = "0.4.1", default-features = false, optional = true }
 
@@ -298,6 +293,7 @@ aws-smithy-types = { version = "1.2.11", default-features = false, features = ["
 
 # Azure
 azure_core = { version = "0.30", features = ["reqwest", "hmac_openssl"], optional = true }
+azure_identity = { version = "0.30", optional = true }
 
 # Azure Storage
 azure_storage_blob = { version = "0.7", optional = true }
@@ -426,7 +422,7 @@ strip-ansi-escapes = { version = "0.2.1", default-features = false }
 syslog = { version = "6.1.1", default-features = false, optional = true }
 tikv-jemallocator = { version = "0.6.0", default-features = false, features = ["unprefixed_malloc_on_supported_platforms"], optional = true }
 tokio-postgres = { version = "0.7.13", default-features = false, features = ["runtime", "with-chrono-0_4"], optional = true }
-tokio-tungstenite = { workspace = true, features = ["connect"], optional = true }
+tokio-tungstenite = { version = "0.20.1", default-features = false, features = ["connect"], optional = true }
 toml.workspace = true
 hickory-proto = { workspace = true, optional = true }
 tonic = { workspace = true, optional = true }
@@ -450,7 +446,7 @@ byteorder = "1.5.0"
 windows-service = "0.8.0"
 
 [target.'cfg(unix)'.dependencies]
-nix = { version = "0.26.2", default-features = false, features = ["socket", "signal", "fs"] }
+nix = { version = "0.26.2", default-features = false, features = ["socket", "signal"] }
 
 [target.'cfg(target_os = "linux")'.dependencies]
 netlink-packet-utils = "0.5.2"
@@ -482,7 +478,7 @@ similar-asserts = "1.7.0"
 tempfile.workspace = true
 test-generator = "0.3.1"
 tokio = { workspace = true, features = ["test-util"] }
-tokio-test.workspace = true
+tokio-test = "0.4.4"
 tower-test = "0.4.0"
 vector-lib = { workspace = true, features = ["test"] }
 vrl.workspace = true
@@ -787,6 +783,7 @@ sinks-logs = [
   "sinks-aws_sqs",
   "sinks-axiom",
   "sinks-azure_blob",
+  "sinks-azure_logs_ingestion",
   "sinks-azure_monitor_logs",
   "sinks-blackhole",
   "sinks-chronicle",
@@ -854,10 +851,11 @@ sinks-aws_sqs = ["aws-core", "dep:aws-sdk-sqs"]
 sinks-aws_sns = ["aws-core", "dep:aws-sdk-sns"]
 sinks-axiom = ["sinks-http"]
 sinks-azure_blob = ["dep:azure_core", "dep:azure_storage_blob"]
+sinks-azure_logs_ingestion = ["dep:azure_core", "dep:azure_identity"]
 sinks-azure_monitor_logs = []
 sinks-blackhole = []
 sinks-chronicle = []
-sinks-clickhouse = ["dep:nom", "dep:rust_decimal", "codecs-arrow"]
+sinks-clickhouse = ["dep:rust_decimal", "codecs-arrow"]
 sinks-console = []
 sinks-databend = ["dep:databend-client"]
 sinks-datadog_events = []
diff --git a/LICENSE-3rdparty.csv b/LICENSE-3rdparty.csv
index 81eb19036..7e2d08bbf 100644
--- a/LICENSE-3rdparty.csv
+++ b/LICENSE-3rdparty.csv
@@ -1,5 +1,6 @@
 Component,Origin,License,Copyright
 Inflector,https://github.com/whatisinternet/inflector,BSD-2-Clause,Josh Teeter<joshteeter@gmail.com>
+addr2line,https://github.com/gimli-rs/addr2line,Apache-2.0 OR MIT,The addr2line Authors
 adler2,https://github.com/oyvindln/adler2,0BSD OR MIT OR Apache-2.0,"Jonas Schievink <jonasschievink@gmail.com>, oyvindln <oyvindln@users.noreply.github.com>"
 adler32,https://github.com/remram44/adler32-rs,Zlib,Remi Rampin <remirampin@gmail.com>
 aead,https://github.com/RustCrypto/traits,MIT OR Apache-2.0,RustCrypto Developers
@@ -106,6 +107,7 @@ azure_core_macros,https://github.com/azure/azure-sdk-for-rust,MIT,Microsoft
 azure_storage_blob,https://github.com/azure/azure-sdk-for-rust,MIT,Microsoft
 backoff,https://github.com/ihrwein/backoff,MIT OR Apache-2.0,Tibor Benke <ihrwein@gmail.com>
 backon,https://github.com/Xuanwo/backon,Apache-2.0,The backon Authors
+backtrace,https://github.com/rust-lang/backtrace-rs,MIT OR Apache-2.0,The Rust Project Developers
 base16,https://github.com/thomcc/rust-base16,CC0-1.0,Thom Chiovoloni <tchiovoloni@mozilla.com>
 base16ct,https://github.com/RustCrypto/formats/tree/master/base16ct,Apache-2.0 OR MIT,RustCrypto Developers
 base62,https://github.com/fbernier/base62,MIT,"Franois Bernier <frankbernier@gmail.com>, Chai T. Rex <ChaiTRex@users.noreply.github.com>"
@@ -139,7 +141,6 @@ bytecheck,https://github.com/djkoloski/bytecheck,MIT,David Koloski <djkoloski@gm
 bytecheck_derive,https://github.com/djkoloski/bytecheck,MIT,David Koloski <djkoloski@gmail.com>
 bytecount,https://github.com/llogiq/bytecount,Apache-2.0 OR MIT,"Andre Bogus <bogusandre@gmail.de>, Joshua Landau <joshua@landau.ws>"
 bytemuck,https://github.com/Lokathor/bytemuck,Zlib OR Apache-2.0 OR MIT,Lokathor <zefria@gmail.com>
-bytemuck_derive,https://github.com/Lokathor/bytemuck,Zlib OR Apache-2.0 OR MIT,Lokathor <zefria@gmail.com>
 byteorder,https://github.com/BurntSushi/byteorder,Unlicense OR MIT,Andrew Gallant <jamslam@gmail.com>
 bytes,https://github.com/carllerche/bytes,MIT,Carl Lerche <me@carllerche.com>
 bytes,https://github.com/tokio-rs/bytes,MIT,"Carl Lerche <me@carllerche.com>, Sean McArthur <sean@seanmonstar.com>"
@@ -308,6 +309,7 @@ futures-timer,https://github.com/async-rs/futures-timer,MIT OR Apache-2.0,Alex C
 futures-util,https://github.com/rust-lang/futures-rs,MIT OR Apache-2.0,The futures-util Authors
 generic-array,https://github.com/fizyk20/generic-array,MIT,"Bartomiej Kamiski <fizyk20@gmail.com>, Aaron Trent <novacrazy@gmail.com>"
 getrandom,https://github.com/rust-random/getrandom,MIT OR Apache-2.0,The Rand Project Developers
+gimli,https://github.com/gimli-rs/gimli,MIT OR Apache-2.0,The gimli Authors
 glob,https://github.com/rust-lang/glob,MIT OR Apache-2.0,The Rust Project Developers
 gloo-timers,https://github.com/rustwasm/gloo/tree/master/crates/timers,MIT OR Apache-2.0,Rust and WebAssembly Working Group
 goauth,https://github.com/durch/rust-goauth,MIT,Drazen Urch <github@drazenur.ch>
@@ -390,6 +392,7 @@ instability,https://github.com/ratatui-org/instability,MIT,"Stephen M. Coakley <
 instant,https://github.com/sebcrozet/instant,BSD-3-Clause,sebcrozet <developer@crozet.re>
 inventory,https://github.com/dtolnay/inventory,MIT OR Apache-2.0,David Tolnay <dtolnay@gmail.com>
 io-lifetimes,https://github.com/sunfishcode/io-lifetimes,Apache-2.0 WITH LLVM-exception OR Apache-2.0 OR MIT,Dan Gohman <dev@sunfishcode.online>
+io-uring,https://github.com/tokio-rs/io-uring,MIT OR Apache-2.0,quininer <quininer@live.com>
 iovec,https://github.com/carllerche/iovec,MIT OR Apache-2.0,Carl Lerche <me@carllerche.com>
 ipconfig,https://github.com/liranringel/ipconfig,MIT OR Apache-2.0,Liran Ringel <liranringel@gmail.com>
 ipcrypt-rs,https://github.com/jedisct1/rust-ipcrypt2,ISC,Frank Denis <github@pureftpd.org>
@@ -455,7 +458,6 @@ macro_magic_core,https://github.com/sam0x17/macro_magic,MIT,The macro_magic_core
 macro_magic_core_macros,https://github.com/sam0x17/macro_magic,MIT,The macro_magic_core_macros Authors
 macro_magic_macros,https://github.com/sam0x17/macro_magic,MIT,The macro_magic_macros Authors
 malloc_buf,https://github.com/SSheldon/malloc_buf,MIT,Steven Sheldon
-marrow,https://github.com/chmp/marrow,MIT,Christopher Prohm <mail@cprohm.de>
 match_cfg,https://github.com/gnzlbg/match_cfg,MIT OR Apache-2.0,gnzlbg <gonzalobg88@gmail.com>
 matchers,https://github.com/hawkw/matchers,MIT,Eliza Weisman <eliza@buoyant.io>
 matchit,https://github.com/ibraheemdev/matchit,MIT AND BSD-3-Clause,Ibraheem Ahmed <ibraheem@ibraheem.ca>
@@ -521,6 +523,7 @@ oauth2,https://github.com/ramosbugs/oauth2-rs,MIT OR Apache-2.0,"Alex Crichton <
 objc,http://github.com/SSheldon/rust-objc,MIT,Steven Sheldon
 objc2-core-foundation,https://github.com/madsmtm/objc2,Zlib OR Apache-2.0 OR MIT,The objc2-core-foundation Authors
 objc2-io-kit,https://github.com/madsmtm/objc2,Zlib OR Apache-2.0 OR MIT,The objc2-io-kit Authors
+object,https://github.com/gimli-rs/object,Apache-2.0 OR MIT,The object Authors
 octseq,https://github.com/NLnetLabs/octets/,BSD-3-Clause,NLnet Labs <rust-team@nlnetlabs.nl>
 ofb,https://github.com/RustCrypto/block-modes,MIT OR Apache-2.0,RustCrypto Developers
 once_cell,https://github.com/matklad/once_cell,MIT OR Apache-2.0,Aleksey Kladov <aleksey.kladov@gmail.com>
@@ -572,8 +575,8 @@ polling,https://github.com/smol-rs/polling,Apache-2.0 OR MIT,"Stjepan Glavina <s
 poly1305,https://github.com/RustCrypto/universal-hashes,Apache-2.0 OR MIT,RustCrypto Developers
 portable-atomic,https://github.com/taiki-e/portable-atomic,Apache-2.0 OR MIT,The portable-atomic Authors
 postgres-openssl,https://github.com/sfackler/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
-postgres-protocol,https://github.com/rust-postgres/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
-postgres-types,https://github.com/rust-postgres/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
+postgres-protocol,https://github.com/sfackler/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
+postgres-types,https://github.com/sfackler/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
 powerfmt,https://github.com/jhpratt/powerfmt,MIT OR Apache-2.0,Jacob Pratt <jacob@jhpratt.dev>
 ppv-lite86,https://github.com/cryptocorrosion/cryptocorrosion,MIT OR Apache-2.0,The CryptoCorrosion Contributors
 prettydiff,https://github.com/romankoblov/prettydiff,MIT,Roman Koblov <penpen938@me.com>
@@ -655,6 +658,7 @@ roxmltree,https://github.com/RazrFalcon/roxmltree,MIT OR Apache-2.0,Yevhenii Rei
 rsa,https://github.com/RustCrypto/RSA,MIT OR Apache-2.0,"RustCrypto Developers, dignifiedquire <dignifiedquire@gmail.com>"
 rumqttc,https://github.com/bytebeamio/rumqtt,Apache-2.0,tekjar <raviteja@bytebeam.io>
 rust_decimal,https://github.com/paupino/rust-decimal,MIT,Paul Mason <paul@form1.co.nz>
+rustc-demangle,https://github.com/rust-lang/rustc-demangle,MIT OR Apache-2.0,Alex Crichton <alex@alexcrichton.com>
 rustc-hash,https://github.com/rust-lang/rustc-hash,Apache-2.0 OR MIT,The Rust Project Developers
 rustc_version,https://github.com/djc/rustc-version-rs,MIT OR Apache-2.0,The rustc_version Authors
 rustc_version_runtime,https://github.com/seppo0010/rustc-version-runtime-rs,MIT,Sebastian Waisbrot <seppo0010@gmail.com>
@@ -687,7 +691,6 @@ serde,https://github.com/serde-rs/serde,MIT OR Apache-2.0,"Erick Tryzelaar <eric
 serde-aux,https://github.com/iddm/serde-aux,MIT,Victor Polevoy <maintainer@vpolevoy.com>
 serde-toml-merge,https://github.com/jdrouet/serde-toml-merge,MIT,Jeremie Drouet <jeremie.drouet@gmail.com>
 serde-value,https://github.com/arcnmx/serde-value,MIT,arcnmx
-serde_arrow,https://github.com/chmp/serde_arrow,MIT,Christopher Prohm <mail@cprohm.de>
 serde_bytes,https://github.com/serde-rs/bytes,MIT OR Apache-2.0,David Tolnay <dtolnay@gmail.com>
 serde_core,https://github.com/serde-rs/serde,MIT OR Apache-2.0,"Erick Tryzelaar <erick.tryzelaar@gmail.com>, David Tolnay <dtolnay@gmail.com>"
 serde_derive,https://github.com/serde-rs/serde,MIT OR Apache-2.0,"Erick Tryzelaar <erick.tryzelaar@gmail.com>, David Tolnay <dtolnay@gmail.com>"
@@ -780,7 +783,7 @@ tokio-io-timeout,https://github.com/sfackler/tokio-io-timeout,MIT OR Apache-2.0,
 tokio-macros,https://github.com/tokio-rs/tokio,MIT,Tokio Contributors <team@tokio.rs>
 tokio-native-tls,https://github.com/tokio-rs/tls,MIT,Tokio Contributors <team@tokio.rs>
 tokio-openssl,https://github.com/tokio-rs/tokio-openssl,MIT OR Apache-2.0,Alex Crichton <alex@alexcrichton.com>
-tokio-postgres,https://github.com/rust-postgres/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
+tokio-postgres,https://github.com/sfackler/rust-postgres,MIT OR Apache-2.0,Steven Fackler <sfackler@gmail.com>
 tokio-retry,https://github.com/srijs/rust-tokio-retry,MIT,Sam Rijs <srijs@airpost.net>
 tokio-rustls,https://github.com/rustls/tokio-rustls,MIT OR Apache-2.0,The tokio-rustls Authors
 tokio-stream,https://github.com/tokio-rs/tokio,MIT,Tokio Contributors <team@tokio.rs>
diff --git a/changelog.d/24074_clickhouse_arrow_complex_types.enhancement.md b/changelog.d/24074_clickhouse_arrow_complex_types.enhancement.md
deleted file mode 100644
index 5a5d6c65e..000000000
--- a/changelog.d/24074_clickhouse_arrow_complex_types.enhancement.md
+++ /dev/null
@@ -1,3 +0,0 @@
-The `clickhouse` sink now supports complex data types (Array, Map, and Tuple) when using the `arrow_stream` format.
-
-authors: benjamin-awd
diff --git a/changelog.d/24355_vector_top_controls.feature.md b/changelog.d/24355_vector_top_controls.feature.md
deleted file mode 100644
index 386fde3ab..000000000
--- a/changelog.d/24355_vector_top_controls.feature.md
+++ /dev/null
@@ -1,3 +0,0 @@
-Added new keybinds to `vector top` for scrolling, sorting and filtering. You can now press `?` when using `vector top` to see all available keybinds.
-
-authors: esensar Quad9DNS
diff --git a/changelog.d/24603_prometheus_remote_write_healthcheck_uri.enhancement.md b/changelog.d/24603_prometheus_remote_write_healthcheck_uri.enhancement.md
deleted file mode 100644
index d49a0c461..000000000
--- a/changelog.d/24603_prometheus_remote_write_healthcheck_uri.enhancement.md
+++ /dev/null
@@ -1,3 +0,0 @@
-The `prometheus_remote_write` sink now supports the `healthcheck.uri` field to customize the healthcheck endpoint.
-
-authors: simonhammes
diff --git a/changelog.d/7538_filesystem_inode_metrics.feature.md b/changelog.d/7538_filesystem_inode_metrics.feature.md
deleted file mode 100644
index c43ae8d68..000000000
--- a/changelog.d/7538_filesystem_inode_metrics.feature.md
+++ /dev/null
@@ -1,3 +0,0 @@
-Added inode metrics to the `host_metrics` source filesystem collector on unix systems. The `filesystem_inodes_total`, `filesystem_inodes_free`, `filesystem_inodes_used`, and `filesystem_inodes_used_ratio` metrics are now available.
-
-authors: mushrowan
diff --git a/changelog.d/component_latency-metrics.enhancement.md b/changelog.d/component_latency-metrics.enhancement.md
deleted file mode 100644
index 844a8867a..000000000
--- a/changelog.d/component_latency-metrics.enhancement.md
+++ /dev/null
@@ -1,5 +0,0 @@
-Added the `component_latency_seconds` histogram and
-`component_latency_mean_seconds` gauge internal metrics, exposing the time an
-event spends in a single transform including the transform buffer.
-
-authors: bruceg
diff --git a/changelog.d/optimize-websocket-source.enhancement.md b/changelog.d/optimize-websocket-source.enhancement.md
deleted file mode 100644
index 9745ed1eb..000000000
--- a/changelog.d/optimize-websocket-source.enhancement.md
+++ /dev/null
@@ -1,3 +0,0 @@
-Small optimization to the `websocket` source performance by avoiding getting a new time for every event in an array.
-
-authors: bruceg
diff --git a/lib/codecs/Cargo.toml b/lib/codecs/Cargo.toml
index 8b3cbac1b..d8ef9e105 100644
--- a/lib/codecs/Cargo.toml
+++ b/lib/codecs/Cargo.toml
@@ -14,7 +14,7 @@ path = "tests/bin/generate-avro-fixtures.rs"
 
 [dependencies]
 apache-avro = { version = "0.20.0", default-features = false }
-arrow = { version = "56.2.0", default-features = false, features = ["ipc"], optional = true }
+arrow = { version = "56.2.0", default-features = false, features = ["ipc"] }
 async-trait.workspace = true
 bytes.workspace = true
 chrono.workspace = true
@@ -38,7 +38,6 @@ regex.workspace = true
 serde.workspace = true
 serde_with = { version = "3.14.0", default-features = false, features = ["std", "macros", "chrono_0_4"] }
 serde_json.workspace = true
-serde_arrow = { version = "0.13", features = ["arrow-56"], optional = true }
 serde-aux = { version = "4.5", optional = true }
 smallvec = { version = "1", default-features = false, features = ["union"] }
 snafu.workspace = true
@@ -68,7 +67,7 @@ uuid.workspace = true
 vrl.workspace = true
 
 [features]
-arrow = ["dep:arrow", "dep:serde_arrow"]
+arrow = []
 opentelemetry = ["dep:opentelemetry-proto"]
 syslog = ["dep:syslog_loose", "dep:strum", "dep:derive_more", "dep:serde-aux", "dep:toml"]
 test = []
diff --git a/lib/codecs/src/encoding/format/arrow.rs b/lib/codecs/src/encoding/format/arrow.rs
index 4812b4f57..3c2d3863f 100644
--- a/lib/codecs/src/encoding/format/arrow.rs
+++ b/lib/codecs/src/encoding/format/arrow.rs
@@ -5,19 +5,26 @@
 //! a continuous stream of record batches without a file footer.
 
 use arrow::{
-    array::ArrayRef,
-    compute::{CastOptions, cast_with_options},
-    datatypes::{DataType, Field, Fields, Schema, SchemaRef, TimeUnit},
+    array::{
+        ArrayRef, BinaryBuilder, BooleanBuilder, Decimal128Builder, Decimal256Builder,
+        Float32Builder, Float64Builder, Int8Builder, Int16Builder, Int32Builder, Int64Builder,
+        StringBuilder, TimestampMicrosecondBuilder, TimestampMillisecondBuilder,
+        TimestampNanosecondBuilder, TimestampSecondBuilder, UInt8Builder, UInt16Builder,
+        UInt32Builder, UInt64Builder,
+    },
+    datatypes::{DataType, Schema, TimeUnit, i256},
     ipc::writer::StreamWriter,
     record_batch::RecordBatch,
 };
 use async_trait::async_trait;
 use bytes::{BufMut, Bytes, BytesMut};
 use chrono::{DateTime, Utc};
+use rust_decimal::Decimal;
 use snafu::Snafu;
 use std::sync::Arc;
 use vector_config::configurable_component;
-use vector_core::event::{Event, LogEvent, Value};
+
+use vector_core::event::{Event, Value};
 
 /// Provides Arrow schema for encoding.
 ///
@@ -91,7 +98,7 @@ impl ArrowStreamSerializerConfig {
 /// Arrow IPC stream batch serializer that holds the schema
 #[derive(Clone, Debug)]
 pub struct ArrowStreamSerializer {
-    schema: SchemaRef,
+    schema: Arc<Schema>,
 }
 
 impl ArrowStreamSerializer {
@@ -104,20 +111,20 @@ impl ArrowStreamSerializer {
         // If allow_nullable_fields is enabled, transform the schema once here
         // instead of on every batch encoding
         let schema = if config.allow_nullable_fields {
-            let nullable_fields: Fields = schema
-                .fields()
-                .iter()
-                .map(|f| make_field_nullable(f))
-                .collect::<Result<Vec<_>, _>>()
-                .map_err(|e| vector_common::Error::from(e.to_string()))?
-                .into();
-            Schema::new_with_metadata(nullable_fields, schema.metadata().clone())
+            Schema::new_with_metadata(
+                schema
+                    .fields()
+                    .iter()
+                    .map(|f| Arc::new(make_field_nullable(f)))
+                    .collect::<Vec<_>>(),
+                schema.metadata().clone(),
+            )
         } else {
             schema
         };
 
         Ok(Self {
-            schema: SchemaRef::new(schema),
+            schema: Arc::new(schema),
         })
     }
 }
@@ -169,6 +176,19 @@ pub enum ArrowEncodingError {
         message: String,
     },
 
+    /// Unsupported Arrow data type for field
+    #[snafu(display(
+        "Unsupported Arrow data type for field '{}': {:?}",
+        field_name,
+        data_type
+    ))]
+    UnsupportedType {
+        /// The field name
+        field_name: String,
+        /// The unsupported data type
+        data_type: DataType,
+    },
+
     /// Null value encountered for non-nullable field
     #[snafu(display("Null value for non-nullable field '{}'", field_name))]
     NullConstraint {
@@ -182,35 +202,6 @@ pub enum ArrowEncodingError {
         /// The underlying IO error
         source: std::io::Error,
     },
-
-    /// Serde Arrow serialization error
-    #[snafu(display("Serde Arrow error: {}", source))]
-    SerdeArrow {
-        /// The underlying serde_arrow error
-        source: serde_arrow::Error,
-    },
-
-    /// Timestamp value overflows the representable range
-    #[snafu(display(
-        "Timestamp overflow for field '{}': value '{}' cannot be represented as i64 nanoseconds",
-        field_name,
-        timestamp
-    ))]
-    TimestampOverflow {
-        /// The field name
-        field_name: String,
-        /// The timestamp value that overflowed
-        timestamp: String,
-    },
-
-    /// Invalid Map schema structure
-    #[snafu(display("Invalid Map schema for field '{}': {}", field_name, reason))]
-    InvalidMapSchema {
-        /// The field name
-        field_name: String,
-        /// Description of the schema violation
-        reason: String,
-    },
 }
 
 impl From<std::io::Error> for ArrowEncodingError {
@@ -222,7 +213,7 @@ impl From<std::io::Error> for ArrowEncodingError {
 /// Encodes a batch of events into Arrow IPC streaming format
 pub fn encode_events_to_arrow_ipc_stream(
     events: &[Event],
-    schema: Option<SchemaRef>,
+    schema: Option<Arc<Schema>>,
 ) -> Result<Bytes, ArrowEncodingError> {
     if events.is_empty() {
         return Err(ArrowEncodingError::NoEvents);
@@ -244,156 +235,417 @@ pub fn encode_events_to_arrow_ipc_stream(
 }
 
 /// Recursively makes a Field and all its nested fields nullable
-fn make_field_nullable(field: &Field) -> Result<Field, ArrowEncodingError> {
+fn make_field_nullable(field: &arrow::datatypes::Field) -> arrow::datatypes::Field {
     let new_data_type = match field.data_type() {
-        DataType::List(inner_field) => DataType::List(make_field_nullable(inner_field)?.into()),
-        DataType::Struct(fields) => DataType::Struct(
-            fields
-                .iter()
-                .map(|f| make_field_nullable(f))
-                .collect::<Result<Vec<_>, _>>()?
-                .into(),
-        ),
-        DataType::Map(inner, sorted) => {
-            // A Map's inner field is a "entries" Struct<Key, Value>
-            let DataType::Struct(fields) = inner.data_type() else {
-                return Err(ArrowEncodingError::InvalidMapSchema {
-                    field_name: field.name().to_string(),
-                    reason: format!("inner type must be Struct, found {:?}", inner.data_type()),
-                });
-            };
-
-            if fields.len() != 2 {
-                return Err(ArrowEncodingError::InvalidMapSchema {
-                    field_name: field.name().to_string(),
-                    reason: format!("expected 2 fields (key, value), found {}", fields.len()),
-                });
-            }
-            let key_field = &fields[0];
-            let value_field = &fields[1];
-
-            let new_struct_fields: Fields =
-                [key_field.clone(), make_field_nullable(value_field)?.into()].into();
-
-            // Reconstruct the inner "entries" field
-            // The inner field itself must be non-nullable (only the Map wrapper is nullable)
-            let new_inner_field = inner
-                .as_ref()
-                .clone()
-                .with_data_type(DataType::Struct(new_struct_fields))
-                .with_nullable(false);
-
-            DataType::Map(new_inner_field.into(), *sorted)
+        DataType::List(inner_field) => DataType::List(Arc::new(make_field_nullable(inner_field))),
+        DataType::Struct(fields) => {
+            DataType::Struct(fields.iter().map(|f| make_field_nullable(f)).collect())
+        }
+        DataType::Map(inner_field, sorted) => {
+            DataType::Map(Arc::new(make_field_nullable(inner_field)), *sorted)
         }
         other => other.clone(),
     };
 
-    Ok(field
+    field
         .clone()
         .with_data_type(new_data_type)
-        .with_nullable(true))
+        .with_nullable(true)
 }
 
-/// Build an Arrow RecordBatch from a slice of events using the provided schema.
+/// Builds an Arrow RecordBatch from events
 fn build_record_batch(
-    schema: SchemaRef,
+    schema: Arc<Schema>,
     events: &[Event],
 ) -> Result<RecordBatch, ArrowEncodingError> {
-    let log_events: Vec<LogEvent> = events
-        .iter()
-        .filter_map(Event::maybe_as_log)
-        .map(|log| convert_timestamps(log, &schema))
-        .collect::<Result<Vec<_>, _>>()?;
-
-    let batch = serde_arrow::to_record_batch(schema.fields(), &log_events).map_err(|source| {
-        // serde_arrow doesn't expose structured error variants (see
-        // https://docs.rs/serde_arrow/latest/serde_arrow/enum.Error.html), so we string-match on
-        // the message to detect null constraint violations, then find the actual field ourselves.
-        if source.message().contains("non-nullable")
-            && let Some(field_name) = find_null_field(&log_events, &schema)
-        {
-            return ArrowEncodingError::NullConstraint { field_name };
-        }
-        ArrowEncodingError::SerdeArrow { source }
-    })?;
-
-    // Post-process: use Arrow's cast for any remaining type mismatches.
-    // serde_arrow serializes Vector's Value types using fixed Arrow types (e.g., Int64
-    // for all integers, Float64 for floats, LargeUtf8 for strings), but the target schema
-    // may specify narrower types. Arrow's cast handles these conversions safely.
-    let columns: Result<Vec<ArrayRef>, _> = batch
-        .columns()
-        .iter()
-        .zip(schema.fields())
-        .map(|(col, field)| {
-            if col.data_type() == field.data_type() {
-                Ok(col.clone())
-            } else {
-                cast_with_options(col, field.data_type(), &CastOptions::default())
-                    .map_err(|source| ArrowEncodingError::RecordBatchCreation { source })
+    let num_fields = schema.fields().len();
+    let mut columns: Vec<ArrayRef> = Vec::with_capacity(num_fields);
+
+    for field in schema.fields() {
+        let field_name = field.name();
+        let nullable = field.is_nullable();
+        let array: ArrayRef = match field.data_type() {
+            DataType::Timestamp(time_unit, _) => {
+                build_timestamp_array(events, field_name, *time_unit, nullable)?
             }
-        })
-        .collect();
+            DataType::Utf8 => build_string_array(events, field_name, nullable)?,
+            DataType::Int8 => build_int8_array(events, field_name, nullable)?,
+            DataType::Int16 => build_int16_array(events, field_name, nullable)?,
+            DataType::Int32 => build_int32_array(events, field_name, nullable)?,
+            DataType::Int64 => build_int64_array(events, field_name, nullable)?,
+            DataType::UInt8 => build_uint8_array(events, field_name, nullable)?,
+            DataType::UInt16 => build_uint16_array(events, field_name, nullable)?,
+            DataType::UInt32 => build_uint32_array(events, field_name, nullable)?,
+            DataType::UInt64 => build_uint64_array(events, field_name, nullable)?,
+            DataType::Float32 => build_float32_array(events, field_name, nullable)?,
+            DataType::Float64 => build_float64_array(events, field_name, nullable)?,
+            DataType::Boolean => build_boolean_array(events, field_name, nullable)?,
+            DataType::Binary => build_binary_array(events, field_name, nullable)?,
+            DataType::Decimal128(precision, scale) => {
+                build_decimal128_array(events, field_name, *precision, *scale, nullable)?
+            }
+            DataType::Decimal256(precision, scale) => {
+                build_decimal256_array(events, field_name, *precision, *scale, nullable)?
+            }
+            other_type => {
+                return Err(ArrowEncodingError::UnsupportedType {
+                    field_name: field_name.into(),
+                    data_type: other_type.clone(),
+                });
+            }
+        };
+
+        columns.push(array);
+    }
 
-    RecordBatch::try_new(schema, columns?)
+    RecordBatch::try_new(schema, columns)
         .map_err(|source| ArrowEncodingError::RecordBatchCreation { source })
 }
 
-/// Find which non-nullable field has a missing value (called only on error).
-fn find_null_field(events: &[LogEvent], schema: &SchemaRef) -> Option<String> {
-    for field in schema.fields() {
-        if !field.is_nullable() {
-            let name = field.name();
-            if events
-                .iter()
-                .any(|e| e.get(lookup::event_path!(name)).is_none())
-            {
-                return Some(name.to_string());
+/// Macro to handle appending null or returning an error for non-nullable fields.
+macro_rules! handle_null_constraints {
+    ($builder:expr, $nullable:expr, $field_name:expr) => {{
+        if !$nullable {
+            return Err(ArrowEncodingError::NullConstraint {
+                field_name: $field_name.into(),
+            });
+        }
+        $builder.append_null();
+    }};
+}
+
+/// Macro to generate a `build_*_array` function for primitive types.
+macro_rules! define_build_primitive_array_fn {
+    (
+        $fn_name:ident, // The function name (e.g., build_int8_array)
+        $builder_ty:ty, // The builder type (e.g., Int8Builder)
+        // One or more match arms for valid Value types
+        $( $value_pat:pat $(if $guard:expr)? => $append_expr:expr ),+
+    ) => {
+        fn $fn_name(
+            events: &[Event],
+            field_name: &str,
+            nullable: bool,
+        ) -> Result<ArrayRef, ArrowEncodingError> {
+            let mut builder = <$builder_ty>::with_capacity(events.len());
+
+            for event in events {
+                if let Event::Log(log) = event {
+                    match log.get(field_name) {
+                        $(
+                            $value_pat $(if $guard)? => builder.append_value($append_expr),
+                        )+
+                        // All other patterns are treated as null/invalid
+                        _ => handle_null_constraints!(builder, nullable, field_name),
+                    }
+                }
             }
+            Ok(Arc::new(builder.finish()))
         }
+    };
+}
+
+fn extract_timestamp(value: &Value) -> Option<DateTime<Utc>> {
+    match value {
+        Value::Timestamp(ts) => Some(*ts),
+        Value::Bytes(bytes) => std::str::from_utf8(bytes)
+            .ok()
+            .and_then(|s| chrono::DateTime::parse_from_rfc3339(s).ok())
+            .map(|dt| dt.with_timezone(&Utc)),
+        _ => None,
     }
-    None
 }
 
-/// Convert Value::Timestamp to Value::Integer for timestamp columns.
-///
-/// This is necessary because serde_arrow's string parsing expects specific formats
-/// based on the timezone setting, but Vector's timestamps always serialize as RFC 3339
-/// with 'Z' suffix. Converting to i64 directly avoids this format mismatch.
-fn convert_timestamps(
-    event: &LogEvent,
-    schema: &SchemaRef,
-) -> Result<LogEvent, ArrowEncodingError> {
-    let mut result = event.clone();
+fn build_timestamp_array(
+    events: &[Event],
+    field_name: &str,
+    time_unit: TimeUnit,
+    nullable: bool,
+) -> Result<ArrayRef, ArrowEncodingError> {
+    macro_rules! build_array {
+        ($builder:ty, $converter:expr) => {{
+            let mut builder = <$builder>::with_capacity(events.len());
+            for event in events {
+                if let Event::Log(log) = event {
+                    let value_to_append = log.get(field_name).and_then(|value| {
+                        // First, try to extract it as a native or string timestamp
+                        if let Some(ts) = extract_timestamp(value) {
+                            $converter(&ts)
+                        }
+                        // Else, fall back to a raw integer
+                        else if let Value::Integer(i) = value {
+                            Some(*i)
+                        }
+                        // Else, it's an unsupported type (e.g., Bool, Float)
+                        else {
+                            None
+                        }
+                    });
+
+                    if value_to_append.is_none() && !nullable {
+                        return Err(ArrowEncodingError::NullConstraint {
+                            field_name: field_name.into(),
+                        });
+                    }
 
-    for field in schema.fields() {
-        if let DataType::Timestamp(unit, _) = field.data_type() {
-            let field_name = field.name().as_str();
-
-            if let Some(Value::Timestamp(ts)) = event.get(lookup::event_path!(field_name)) {
-                let val = timestamp_to_unit(ts, unit).ok_or_else(|| {
-                    ArrowEncodingError::TimestampOverflow {
-                        field_name: field_name.to_string(),
-                        timestamp: ts.to_rfc3339(),
+                    builder.append_option(value_to_append);
+                }
+            }
+            Ok(Arc::new(builder.finish()))
+        }};
+    }
+
+    match time_unit {
+        TimeUnit::Second => {
+            build_array!(TimestampSecondBuilder, |ts: &DateTime<Utc>| Some(
+                ts.timestamp()
+            ))
+        }
+        TimeUnit::Millisecond => {
+            build_array!(TimestampMillisecondBuilder, |ts: &DateTime<Utc>| Some(
+                ts.timestamp_millis()
+            ))
+        }
+        TimeUnit::Microsecond => {
+            build_array!(TimestampMicrosecondBuilder, |ts: &DateTime<Utc>| Some(
+                ts.timestamp_micros()
+            ))
+        }
+        TimeUnit::Nanosecond => {
+            build_array!(TimestampNanosecondBuilder, |ts: &DateTime<Utc>| ts
+                .timestamp_nanos_opt())
+        }
+    }
+}
+
+fn build_string_array(
+    events: &[Event],
+    field_name: &str,
+    nullable: bool,
+) -> Result<ArrayRef, ArrowEncodingError> {
+    let mut builder = StringBuilder::with_capacity(events.len(), 0);
+
+    for event in events {
+        if let Event::Log(log) = event {
+            let mut appended = false;
+            if let Some(value) = log.get(field_name) {
+                match value {
+                    Value::Bytes(bytes) => {
+                        // Attempt direct UTF-8 conversion first, fallback to lossy
+                        match std::str::from_utf8(bytes) {
+                            Ok(s) => builder.append_value(s),
+                            Err(_) => builder.append_value(&String::from_utf8_lossy(bytes)),
+                        }
+                        appended = true;
+                    }
+                    Value::Object(obj) => {
+                        if let Ok(s) = serde_json::to_string(&obj) {
+                            builder.append_value(s);
+                            appended = true;
+                        }
+                    }
+                    Value::Array(arr) => {
+                        if let Ok(s) = serde_json::to_string(&arr) {
+                            builder.append_value(s);
+                            appended = true;
+                        }
                     }
-                })?;
-                result.insert(field_name, Value::Integer(val));
+                    _ => {
+                        builder.append_value(&value.to_string_lossy());
+                        appended = true;
+                    }
+                }
+            }
+
+            if !appended {
+                handle_null_constraints!(builder, nullable, field_name);
             }
         }
     }
 
-    Ok(result)
+    Ok(Arc::new(builder.finish()))
 }
 
-/// Convert a DateTime<Utc> to i64 in the specified Arrow TimeUnit.
-/// Returns None if the value would overflow (only possible for nanoseconds).
-fn timestamp_to_unit(ts: &DateTime<Utc>, unit: &TimeUnit) -> Option<i64> {
-    match unit {
-        TimeUnit::Second => Some(ts.timestamp()),
-        TimeUnit::Millisecond => Some(ts.timestamp_millis()),
-        TimeUnit::Microsecond => Some(ts.timestamp_micros()),
-        TimeUnit::Nanosecond => ts.timestamp_nanos_opt(),
+define_build_primitive_array_fn!(
+    build_int8_array,
+    Int8Builder,
+    Some(Value::Integer(i)) if *i >= i8::MIN as i64 && *i <= i8::MAX as i64 => *i as i8
+);
+
+define_build_primitive_array_fn!(
+    build_int16_array,
+    Int16Builder,
+    Some(Value::Integer(i)) if *i >= i16::MIN as i64 && *i <= i16::MAX as i64 => *i as i16
+);
+
+define_build_primitive_array_fn!(
+    build_int32_array,
+    Int32Builder,
+    Some(Value::Integer(i)) if *i >= i32::MIN as i64 && *i <= i32::MAX as i64 => *i as i32
+);
+
+define_build_primitive_array_fn!(
+    build_int64_array,
+    Int64Builder,
+    Some(Value::Integer(i)) => *i
+);
+
+define_build_primitive_array_fn!(
+    build_uint8_array,
+    UInt8Builder,
+    Some(Value::Integer(i)) if *i >= 0 && *i <= u8::MAX as i64 => *i as u8
+);
+
+define_build_primitive_array_fn!(
+    build_uint16_array,
+    UInt16Builder,
+    Some(Value::Integer(i)) if *i >= 0 && *i <= u16::MAX as i64 => *i as u16
+);
+
+define_build_primitive_array_fn!(
+    build_uint32_array,
+    UInt32Builder,
+    Some(Value::Integer(i)) if *i >= 0 && *i <= u32::MAX as i64 => *i as u32
+);
+
+define_build_primitive_array_fn!(
+    build_uint64_array,
+    UInt64Builder,
+    Some(Value::Integer(i)) if *i >= 0 => *i as u64
+);
+
+define_build_primitive_array_fn!(
+    build_float32_array,
+    Float32Builder,
+    Some(Value::Float(f)) => f.into_inner() as f32,
+    Some(Value::Integer(i)) => *i as f32
+);
+
+define_build_primitive_array_fn!(
+    build_float64_array,
+    Float64Builder,
+    Some(Value::Float(f)) => f.into_inner(),
+    Some(Value::Integer(i)) => *i as f64
+);
+
+define_build_primitive_array_fn!(
+    build_boolean_array,
+    BooleanBuilder,
+    Some(Value::Boolean(b)) => *b
+);
+
+fn build_binary_array(
+    events: &[Event],
+    field_name: &str,
+    nullable: bool,
+) -> Result<ArrayRef, ArrowEncodingError> {
+    let mut builder = BinaryBuilder::with_capacity(events.len(), 0);
+
+    for event in events {
+        if let Event::Log(log) = event {
+            match log.get(field_name) {
+                Some(Value::Bytes(bytes)) => builder.append_value(bytes),
+                _ => handle_null_constraints!(builder, nullable, field_name),
+            }
+        }
+    }
+
+    Ok(Arc::new(builder.finish()))
+}
+
+fn build_decimal128_array(
+    events: &[Event],
+    field_name: &str,
+    precision: u8,
+    scale: i8,
+    nullable: bool,
+) -> Result<ArrayRef, ArrowEncodingError> {
+    let mut builder = Decimal128Builder::with_capacity(events.len())
+        .with_precision_and_scale(precision, scale)
+        .map_err(|_| ArrowEncodingError::UnsupportedType {
+            field_name: field_name.into(),
+            data_type: DataType::Decimal128(precision, scale),
+        })?;
+
+    let target_scale = scale.unsigned_abs() as u32;
+
+    for event in events {
+        if let Event::Log(log) = event {
+            let mut appended = false;
+            match log.get(field_name) {
+                Some(Value::Float(f)) => {
+                    if let Ok(mut decimal) = Decimal::try_from(f.into_inner()) {
+                        decimal.rescale(target_scale);
+                        let mantissa = decimal.mantissa();
+                        builder.append_value(mantissa);
+                        appended = true;
+                    }
+                }
+                Some(Value::Integer(i)) => {
+                    let mut decimal = Decimal::from(*i);
+                    decimal.rescale(target_scale);
+                    let mantissa = decimal.mantissa();
+                    builder.append_value(mantissa);
+                    appended = true;
+                }
+                _ => {}
+            }
+
+            if !appended {
+                handle_null_constraints!(builder, nullable, field_name);
+            }
+        }
+    }
+
+    Ok(Arc::new(builder.finish()))
+}
+
+fn build_decimal256_array(
+    events: &[Event],
+    field_name: &str,
+    precision: u8,
+    scale: i8,
+    nullable: bool,
+) -> Result<ArrayRef, ArrowEncodingError> {
+    let mut builder = Decimal256Builder::with_capacity(events.len())
+        .with_precision_and_scale(precision, scale)
+        .map_err(|_| ArrowEncodingError::UnsupportedType {
+            field_name: field_name.into(),
+            data_type: DataType::Decimal256(precision, scale),
+        })?;
+
+    let target_scale = scale.unsigned_abs() as u32;
+
+    for event in events {
+        if let Event::Log(log) = event {
+            let mut appended = false;
+            match log.get(field_name) {
+                Some(Value::Float(f)) => {
+                    if let Ok(mut decimal) = Decimal::try_from(f.into_inner()) {
+                        decimal.rescale(target_scale);
+                        let mantissa = decimal.mantissa();
+                        // rust_decimal does not support i256 natively so we upcast here
+                        builder.append_value(i256::from_i128(mantissa));
+                        appended = true;
+                    }
+                }
+                Some(Value::Integer(i)) => {
+                    let mut decimal = Decimal::from(*i);
+                    decimal.rescale(target_scale);
+                    let mantissa = decimal.mantissa();
+                    builder.append_value(i256::from_i128(mantissa));
+                    appended = true;
+                }
+                _ => {}
+            }
+
+            if !appended {
+                handle_null_constraints!(builder, nullable, field_name);
+            }
+        }
     }
+
+    Ok(Arc::new(builder.finish()))
 }
 
 #[cfg(test)]
@@ -405,575 +657,1015 @@ mod tests {
             TimestampMicrosecondArray, TimestampMillisecondArray, TimestampNanosecondArray,
             TimestampSecondArray,
         },
+        datatypes::Field,
         ipc::reader::StreamReader,
     };
+    use chrono::Utc;
     use std::io::Cursor;
+    use vector_core::event::LogEvent;
+
+    #[test]
+    fn test_encode_all_types() {
+        let mut log = LogEvent::default();
+        log.insert("string_field", "test");
+        log.insert("int8_field", 127);
+        log.insert("int16_field", 32000);
+        log.insert("int32_field", 1000000);
+        log.insert("int64_field", 42);
+        log.insert("float32_field", 3.15);
+        log.insert("float64_field", 3.15);
+        log.insert("bool_field", true);
+        log.insert("bytes_field", bytes::Bytes::from("binary"));
+        log.insert("timestamp_field", Utc::now());
+
+        let events = vec![Event::Log(log)];
+
+        let schema = Arc::new(Schema::new(vec![
+            Field::new("string_field", DataType::Utf8, true),
+            Field::new("int8_field", DataType::Int8, true),
+            Field::new("int16_field", DataType::Int16, true),
+            Field::new("int32_field", DataType::Int32, true),
+            Field::new("int64_field", DataType::Int64, true),
+            Field::new("float32_field", DataType::Float32, true),
+            Field::new("float64_field", DataType::Float64, true),
+            Field::new("bool_field", DataType::Boolean, true),
+            Field::new("bytes_field", DataType::Binary, true),
+            Field::new(
+                "timestamp_field",
+                DataType::Timestamp(TimeUnit::Millisecond, None),
+                true,
+            ),
+        ]));
 
-    /// Helper to encode events and return the decoded RecordBatch
-    fn encode_and_decode(
-        events: Vec<Event>,
-        schema: SchemaRef,
-    ) -> Result<RecordBatch, Box<dyn std::error::Error>> {
-        let bytes = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)))?;
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
         let cursor = Cursor::new(bytes);
-        let mut reader = StreamReader::try_new(cursor, None)?;
-        Ok(reader.next().unwrap()?)
-    }
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
 
-    /// Create a simple event from key-value pairs
-    fn create_event<V>(fields: Vec<(&str, V)>) -> Event
-    where
-        V: Into<Value>,
-    {
-        let mut log = LogEvent::default();
-        for (key, value) in fields {
-            log.insert(key, value.into());
-        }
-        Event::Log(log)
-    }
-
-    /// Assert a primitive value at a specific column and row
-    macro_rules! assert_primitive_value {
-        ($batch:expr, $col:expr, $row:expr, $array_type:ty, $expected:expr) => {
-            assert_eq!(
-                $batch
-                    .column($col)
-                    .as_any()
-                    .downcast_ref::<$array_type>()
-                    .unwrap()
-                    .value($row),
-                $expected
-            )
-        };
-    }
+        assert_eq!(batch.num_rows(), 1);
+        assert_eq!(batch.num_columns(), 10);
 
-    mod comprehensive {
-        use super::*;
-
-        #[test]
-        fn test_encode_all_types() {
-            use arrow::array::{
-                Decimal128Array, ListArray, MapArray, UInt8Array, UInt16Array, UInt32Array,
-                UInt64Array,
-            };
-            use vrl::value::ObjectMap;
-
-            let now = Utc::now();
-
-            // Create a struct (tuple) value with unnamed fields
-            let mut tuple_value = ObjectMap::new();
-            tuple_value.insert("f0".into(), Value::Bytes("nested_str".into()));
-            tuple_value.insert("f1".into(), Value::Integer(999));
-
-            // Create a named struct (named tuple) value
-            let mut named_tuple_value = ObjectMap::new();
-            named_tuple_value.insert("category".into(), Value::Bytes("test_category".into()));
-            named_tuple_value.insert("tag".into(), Value::Bytes("test_tag".into()));
-
-            // Create a list value
-            let list_value = Value::Array(vec![
-                Value::Integer(1),
-                Value::Integer(2),
-                Value::Integer(3),
-            ]);
-
-            // Create a map value
-            let mut map_value = ObjectMap::new();
-            map_value.insert("key1".into(), Value::Integer(100));
-            map_value.insert("key2".into(), Value::Integer(200));
-
-            let mut log = LogEvent::default();
-            // Primitive types
-            log.insert("string_field", "test");
-            log.insert("int8_field", 127);
-            log.insert("int16_field", 32000);
-            log.insert("int32_field", 1000000);
-            log.insert("int64_field", 42);
-            log.insert("uint8_field", 255);
-            log.insert("uint16_field", 65535);
-            log.insert("uint32_field", 4000000);
-            log.insert("uint64_field", 9000000000_i64);
-            log.insert("float32_field", 3.15);
-            log.insert("float64_field", 3.15);
-            log.insert("bool_field", true);
-            log.insert("bytes_field", bytes::Bytes::from("binary"));
-            log.insert("timestamp_field", now);
-            log.insert("decimal_field", 99.99);
-            // Complex types
-            log.insert("list_field", list_value);
-            log.insert("struct_field", Value::Object(tuple_value));
-            log.insert("named_struct_field", Value::Object(named_tuple_value));
-            log.insert("map_field", Value::Object(map_value));
-
-            let events = vec![Event::Log(log)];
-
-            // Build schema with all supported types
-            let struct_fields = arrow::datatypes::Fields::from(vec![
-                Field::new("f0", DataType::Utf8, true),
-                Field::new("f1", DataType::Int64, true),
-            ]);
-
-            let named_struct_fields = arrow::datatypes::Fields::from(vec![
-                Field::new("category", DataType::Utf8, true),
-                Field::new("tag", DataType::Utf8, true),
-            ]);
-
-            let map_entries = Field::new(
-                "entries",
-                DataType::Struct(arrow::datatypes::Fields::from(vec![
-                    Field::new("keys", DataType::Utf8, false),
-                    Field::new("values", DataType::Int64, true),
-                ])),
-                false,
-            );
+        // Verify string field
+        assert_eq!(
+            batch
+                .column(0)
+                .as_any()
+                .downcast_ref::<StringArray>()
+                .unwrap()
+                .value(0),
+            "test"
+        );
 
-            let schema = SchemaRef::new(Schema::new(vec![
-                Field::new("string_field", DataType::Utf8, true),
-                Field::new("int8_field", DataType::Int8, true),
-                Field::new("int16_field", DataType::Int16, true),
-                Field::new("int32_field", DataType::Int32, true),
-                Field::new("int64_field", DataType::Int64, true),
-                Field::new("uint8_field", DataType::UInt8, true),
-                Field::new("uint16_field", DataType::UInt16, true),
-                Field::new("uint32_field", DataType::UInt32, true),
-                Field::new("uint64_field", DataType::UInt64, true),
-                Field::new("float32_field", DataType::Float32, true),
-                Field::new("float64_field", DataType::Float64, true),
-                Field::new("bool_field", DataType::Boolean, true),
-                Field::new("bytes_field", DataType::Binary, true),
-                Field::new(
-                    "timestamp_field",
-                    DataType::Timestamp(TimeUnit::Millisecond, None),
-                    true,
-                ),
-                Field::new("decimal_field", DataType::Decimal128(10, 2), true),
-                Field::new(
-                    "list_field",
-                    DataType::List(Field::new("item", DataType::Int64, true).into()),
-                    true,
-                ),
-                Field::new("struct_field", DataType::Struct(struct_fields), true),
-                Field::new(
-                    "named_struct_field",
-                    DataType::Struct(named_struct_fields),
-                    true,
-                ),
-                Field::new("map_field", DataType::Map(map_entries.into(), false), true),
-            ]));
-
-            let batch = encode_and_decode(events, schema).expect("Failed to encode");
-
-            assert_eq!(batch.num_rows(), 1);
-            assert_eq!(batch.num_columns(), 19);
-
-            // Verify all primitive types
-            assert_eq!(
-                batch
-                    .column(0)
-                    .as_any()
-                    .downcast_ref::<StringArray>()
-                    .unwrap()
-                    .value(0),
-                "test"
-            );
-            assert_primitive_value!(batch, 1, 0, arrow::array::Int8Array, 127);
-            assert_primitive_value!(batch, 2, 0, arrow::array::Int16Array, 32000);
-            assert_primitive_value!(batch, 3, 0, arrow::array::Int32Array, 1000000);
-            assert_primitive_value!(batch, 4, 0, Int64Array, 42);
-            assert_primitive_value!(batch, 5, 0, UInt8Array, 255);
-            assert_primitive_value!(batch, 6, 0, UInt16Array, 65535);
-            assert_primitive_value!(batch, 7, 0, UInt32Array, 4000000);
-            assert_primitive_value!(batch, 8, 0, UInt64Array, 9000000000);
-            assert!(
-                (batch
-                    .column(9)
-                    .as_any()
-                    .downcast_ref::<arrow::array::Float32Array>()
-                    .unwrap()
-                    .value(0)
-                    - 3.15)
-                    .abs()
-                    < 0.001
-            );
-            assert!(
-                (batch
-                    .column(10)
-                    .as_any()
-                    .downcast_ref::<Float64Array>()
-                    .unwrap()
-                    .value(0)
-                    - 3.15)
-                    .abs()
-                    < 0.001
-            );
-            assert!(
-                batch
-                    .column(11)
-                    .as_any()
-                    .downcast_ref::<BooleanArray>()
-                    .unwrap()
-                    .value(0)
-            );
-            assert_primitive_value!(batch, 12, 0, BinaryArray, b"binary");
-            assert_primitive_value!(
-                batch,
-                13,
-                0,
-                TimestampMillisecondArray,
-                now.timestamp_millis()
-            );
-            assert_primitive_value!(batch, 14, 0, Decimal128Array, 9999);
+        // Verify int8 field
+        assert_eq!(
+            batch
+                .column(1)
+                .as_any()
+                .downcast_ref::<arrow::array::Int8Array>()
+                .unwrap()
+                .value(0),
+            127
+        );
 
-            let list_array = batch
-                .column(15)
+        // Verify int16 field
+        assert_eq!(
+            batch
+                .column(2)
                 .as_any()
-                .downcast_ref::<ListArray>()
-                .unwrap();
-            assert!(!list_array.is_null(0));
-            let list_value = list_array.value(0);
-            assert_eq!(list_value.len(), 3);
-            let int_array = list_value.as_any().downcast_ref::<Int64Array>().unwrap();
-            assert_eq!(int_array.value(0), 1);
-            assert_eq!(int_array.value(1), 2);
-            assert_eq!(int_array.value(2), 3);
-
-            // Verify struct field (unnamed)
-            let struct_array = batch
-                .column(16)
+                .downcast_ref::<arrow::array::Int16Array>()
+                .unwrap()
+                .value(0),
+            32000
+        );
+
+        // Verify int32 field
+        assert_eq!(
+            batch
+                .column(3)
                 .as_any()
-                .downcast_ref::<arrow::array::StructArray>()
-                .unwrap();
-            assert!(!struct_array.is_null(0));
-            assert_primitive_value!(struct_array, 0, 0, StringArray, "nested_str");
-            assert_primitive_value!(struct_array, 1, 0, Int64Array, 999);
-
-            // Verify named struct field (named tuple)
-            let named_struct_array = batch
-                .column(17)
+                .downcast_ref::<arrow::array::Int32Array>()
+                .unwrap()
+                .value(0),
+            1000000
+        );
+
+        // Verify int64 field
+        assert_eq!(
+            batch
+                .column(4)
                 .as_any()
-                .downcast_ref::<arrow::array::StructArray>()
-                .unwrap();
-            assert!(!named_struct_array.is_null(0));
-            assert_primitive_value!(named_struct_array, 0, 0, StringArray, "test_category");
-            assert_primitive_value!(named_struct_array, 1, 0, StringArray, "test_tag");
-
-            // Verify map field
-            let map_array = batch
-                .column(18)
+                .downcast_ref::<Int64Array>()
+                .unwrap()
+                .value(0),
+            42
+        );
+
+        // Verify float32 field
+        assert!(
+            (batch
+                .column(5)
                 .as_any()
-                .downcast_ref::<MapArray>()
-                .unwrap();
-            assert!(!map_array.is_null(0));
-            let map_value = map_array.value(0);
-            assert_eq!(map_value.len(), 2);
-        }
+                .downcast_ref::<arrow::array::Float32Array>()
+                .unwrap()
+                .value(0)
+                - 3.15)
+                .abs()
+                < 0.001
+        );
+
+        // Verify float64 field
+        assert!(
+            (batch
+                .column(6)
+                .as_any()
+                .downcast_ref::<Float64Array>()
+                .unwrap()
+                .value(0)
+                - 3.15)
+                .abs()
+                < 0.001
+        );
+
+        // Verify boolean field
+        assert!(
+            batch
+                .column(7)
+                .as_any()
+                .downcast_ref::<BooleanArray>()
+                .unwrap()
+                .value(0),
+            "{}",
+            true
+        );
+
+        // Verify binary field
+        assert_eq!(
+            batch
+                .column(8)
+                .as_any()
+                .downcast_ref::<BinaryArray>()
+                .unwrap()
+                .value(0),
+            b"binary"
+        );
+
+        // Verify timestamp field
+        assert!(
+            !batch
+                .column(9)
+                .as_any()
+                .downcast_ref::<TimestampMillisecondArray>()
+                .unwrap()
+                .is_null(0)
+        );
     }
 
-    mod error_handling {
-        use super::*;
+    #[test]
+    fn test_encode_null_values() {
+        let mut log1 = LogEvent::default();
+        log1.insert("field_a", 1);
+        // field_b is missing
 
-        #[test]
-        fn test_encode_without_schema_fails() {
-            let events = vec![create_event(vec![("message", "hello")])];
+        let mut log2 = LogEvent::default();
+        log2.insert("field_b", 2);
+        // field_a is missing
 
-            let result = encode_events_to_arrow_ipc_stream(&events, None);
-            assert!(result.is_err());
-            assert!(matches!(
-                result.unwrap_err(),
-                ArrowEncodingError::NoSchemaProvided
-            ));
-        }
+        let events = vec![Event::Log(log1), Event::Log(log2)];
 
-        #[test]
-        fn test_encode_empty_events() {
-            let events: Vec<Event> = vec![];
-            let result = encode_events_to_arrow_ipc_stream(&events, None);
-            assert!(result.is_err());
-            assert!(matches!(result.unwrap_err(), ArrowEncodingError::NoEvents));
-        }
+        let schema = Arc::new(Schema::new(vec![
+            Field::new("field_a", DataType::Int64, true),
+            Field::new("field_b", DataType::Int64, true),
+        ]));
 
-        #[test]
-        fn test_null_constraint_error() {
-            let events = vec![create_event(vec![("other_field", "value")])];
-
-            let schema = SchemaRef::new(Schema::new(vec![Field::new(
-                "required_field",
-                DataType::Utf8,
-                false, // non-nullable
-            )]));
-
-            let result = encode_events_to_arrow_ipc_stream(&events, Some(schema));
-            assert!(result.is_err());
-            match result.unwrap_err() {
-                ArrowEncodingError::NullConstraint { field_name } => {
-                    assert_eq!(field_name, "required_field");
-                }
-                other => panic!("Expected NullConstraint error, got: {:?}", other),
-            }
-        }
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 2);
+
+        let field_a = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<Int64Array>()
+            .unwrap();
+        assert_eq!(field_a.value(0), 1);
+        assert!(field_a.is_null(1));
+
+        let field_b = batch
+            .column(1)
+            .as_any()
+            .downcast_ref::<Int64Array>()
+            .unwrap();
+        assert!(field_b.is_null(0));
+        assert_eq!(field_b.value(1), 2);
     }
 
-    mod temporal_types {
-        use super::*;
-
-        #[test]
-        fn test_encode_timestamp_precisions() {
-            let now = Utc::now();
-            let mut log = LogEvent::default();
-            log.insert("ts_second", now);
-            log.insert("ts_milli", now);
-            log.insert("ts_micro", now);
-            log.insert("ts_nano", now);
-
-            let events = vec![Event::Log(log)];
-
-            let schema = SchemaRef::new(Schema::new(vec![
-                Field::new(
-                    "ts_second",
-                    DataType::Timestamp(TimeUnit::Second, None),
-                    true,
-                ),
-                Field::new(
-                    "ts_milli",
-                    DataType::Timestamp(TimeUnit::Millisecond, None),
-                    true,
-                ),
-                Field::new(
-                    "ts_micro",
-                    DataType::Timestamp(TimeUnit::Microsecond, None),
-                    true,
-                ),
-                Field::new(
-                    "ts_nano",
-                    DataType::Timestamp(TimeUnit::Nanosecond, None),
-                    true,
-                ),
-            ]));
-
-            let batch = encode_and_decode(events, schema).unwrap();
-
-            assert_eq!(batch.num_rows(), 1);
-            assert_eq!(batch.num_columns(), 4);
-
-            let ts_second = batch
-                .column(0)
-                .as_any()
-                .downcast_ref::<TimestampSecondArray>()
-                .unwrap();
-            assert!(!ts_second.is_null(0));
-            assert_eq!(ts_second.value(0), now.timestamp());
+    #[test]
+    fn test_encode_type_mismatches() {
+        let mut log1 = LogEvent::default();
+        log1.insert("field", 42); // Integer
 
-            let ts_milli = batch
-                .column(1)
-                .as_any()
-                .downcast_ref::<TimestampMillisecondArray>()
-                .unwrap();
-            assert!(!ts_milli.is_null(0));
-            assert_eq!(ts_milli.value(0), now.timestamp_millis());
+        let mut log2 = LogEvent::default();
+        log2.insert("field", 3.15); // Float - type mismatch!
 
-            let ts_micro = batch
-                .column(2)
-                .as_any()
-                .downcast_ref::<TimestampMicrosecondArray>()
-                .unwrap();
-            assert!(!ts_micro.is_null(0));
-            assert_eq!(ts_micro.value(0), now.timestamp_micros());
+        let events = vec![Event::Log(log1), Event::Log(log2)];
 
-            let ts_nano = batch
-                .column(3)
-                .as_any()
-                .downcast_ref::<TimestampNanosecondArray>()
-                .unwrap();
-            assert!(!ts_nano.is_null(0));
-            assert_eq!(ts_nano.value(0), now.timestamp_nanos_opt().unwrap());
-        }
+        // Schema expects Int64
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "field",
+            DataType::Int64,
+            true,
+        )]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 2);
+
+        let field_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<Int64Array>()
+            .unwrap();
+        assert_eq!(field_array.value(0), 42);
+        assert!(field_array.is_null(1)); // Type mismatch becomes null
+    }
+
+    #[test]
+    fn test_encode_complex_json_values() {
+        use serde_json::json;
+
+        let mut log = LogEvent::default();
+        log.insert(
+            "object_field",
+            json!({"key": "value", "nested": {"count": 42}}),
+        );
+        log.insert("array_field", json!([1, 2, 3]));
+
+        let events = vec![Event::Log(log)];
 
-        #[test]
-        fn test_encode_mixed_timestamp_string_native_and_integer() {
-            // Test mixing RFC3339 string timestamps, native Timestamp values, and integers.
-            // Note: String timestamps require the schema to have Some("UTC") timezone for
-            // serde_arrow to parse them correctly. Native Value::Timestamp values are
-            // converted to integers internally, so they work with any timezone setting.
-            let now = Utc::now();
+        let schema = Arc::new(Schema::new(vec![
+            Field::new("object_field", DataType::Utf8, true),
+            Field::new("array_field", DataType::Utf8, true),
+        ]));
 
-            let mut log1 = LogEvent::default();
-            log1.insert("ts", "2025-10-22T10:18:44.256Z"); // RFC3339 String
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
 
-            let mut log2 = LogEvent::default();
-            log2.insert("ts", now); // Native Timestamp
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 1);
+
+        let object_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<StringArray>()
+            .unwrap();
+        let object_str = object_array.value(0);
+        assert!(object_str.contains("key"));
+        assert!(object_str.contains("value"));
+
+        let array_array = batch
+            .column(1)
+            .as_any()
+            .downcast_ref::<StringArray>()
+            .unwrap();
+        let array_str = array_array.value(0);
+        assert_eq!(array_str, "[1,2,3]");
+    }
+
+    #[test]
+    fn test_encode_unsupported_type() {
+        let mut log = LogEvent::default();
+        log.insert("field", "value");
+
+        let events = vec![Event::Log(log)];
+
+        // Use an unsupported type
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "field",
+            DataType::Duration(TimeUnit::Millisecond),
+            true,
+        )]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(schema));
+        assert!(result.is_err());
+        assert!(matches!(
+            result.unwrap_err(),
+            ArrowEncodingError::UnsupportedType { .. }
+        ));
+    }
 
-            let mut log3 = LogEvent::default();
-            log3.insert("ts", 1729594724256000000_i64); // Integer (nanoseconds)
+    #[test]
+    fn test_encode_without_schema_fails() {
+        let mut log1 = LogEvent::default();
+        log1.insert("message", "hello");
 
-            let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
+        let events = vec![Event::Log(log1)];
 
-            // Use Some("UTC") to enable serde_arrow's RFC3339 string parsing
-            let schema = SchemaRef::new(Schema::new(vec![Field::new(
-                "ts",
-                DataType::Timestamp(TimeUnit::Nanosecond, Some("UTC".into())),
+        let result = encode_events_to_arrow_ipc_stream(&events, None);
+        assert!(result.is_err());
+        assert!(matches!(
+            result.unwrap_err(),
+            ArrowEncodingError::NoSchemaProvided
+        ));
+    }
+
+    #[test]
+    fn test_encode_empty_events() {
+        let events: Vec<Event> = vec![];
+        let result = encode_events_to_arrow_ipc_stream(&events, None);
+        assert!(result.is_err());
+        assert!(matches!(result.unwrap_err(), ArrowEncodingError::NoEvents));
+    }
+
+    #[test]
+    fn test_encode_timestamp_precisions() {
+        let now = Utc::now();
+        let mut log = LogEvent::default();
+        log.insert("ts_second", now);
+        log.insert("ts_milli", now);
+        log.insert("ts_micro", now);
+        log.insert("ts_nano", now);
+
+        let events = vec![Event::Log(log)];
+
+        let schema = Arc::new(Schema::new(vec![
+            Field::new(
+                "ts_second",
+                DataType::Timestamp(TimeUnit::Second, None),
+                true,
+            ),
+            Field::new(
+                "ts_milli",
+                DataType::Timestamp(TimeUnit::Millisecond, None),
+                true,
+            ),
+            Field::new(
+                "ts_micro",
+                DataType::Timestamp(TimeUnit::Microsecond, None),
+                true,
+            ),
+            Field::new(
+                "ts_nano",
+                DataType::Timestamp(TimeUnit::Nanosecond, None),
                 true,
-            )]));
+            ),
+        ]));
 
-            let batch = encode_and_decode(events, schema).unwrap();
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
 
-            assert_eq!(batch.num_rows(), 3);
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 1);
+        assert_eq!(batch.num_columns(), 4);
+
+        let ts_second = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<TimestampSecondArray>()
+            .unwrap();
+        assert!(!ts_second.is_null(0));
+        assert_eq!(ts_second.value(0), now.timestamp());
+
+        let ts_milli = batch
+            .column(1)
+            .as_any()
+            .downcast_ref::<TimestampMillisecondArray>()
+            .unwrap();
+        assert!(!ts_milli.is_null(0));
+        assert_eq!(ts_milli.value(0), now.timestamp_millis());
+
+        let ts_micro = batch
+            .column(2)
+            .as_any()
+            .downcast_ref::<TimestampMicrosecondArray>()
+            .unwrap();
+        assert!(!ts_micro.is_null(0));
+        assert_eq!(ts_micro.value(0), now.timestamp_micros());
+
+        let ts_nano = batch
+            .column(3)
+            .as_any()
+            .downcast_ref::<TimestampNanosecondArray>()
+            .unwrap();
+        assert!(!ts_nano.is_null(0));
+        assert_eq!(ts_nano.value(0), now.timestamp_nanos_opt().unwrap());
+    }
 
-            let ts_array = batch
-                .column(0)
-                .as_any()
-                .downcast_ref::<TimestampNanosecondArray>()
-                .unwrap();
+    #[test]
+    fn test_encode_mixed_timestamp_string_and_native() {
+        // Test mixing string timestamps with native Timestamp values
+        let mut log1 = LogEvent::default();
+        log1.insert("ts", "2025-10-22T10:18:44.256Z"); // String
 
-            // All three should be non-null
-            assert!(!ts_array.is_null(0));
-            assert!(!ts_array.is_null(1));
-            assert!(!ts_array.is_null(2));
+        let mut log2 = LogEvent::default();
+        log2.insert("ts", Utc::now()); // Native Timestamp
 
-            // First one should match the parsed RFC3339 string
-            let expected = chrono::DateTime::parse_from_rfc3339("2025-10-22T10:18:44.256Z")
-                .unwrap()
-                .timestamp_nanos_opt()
-                .unwrap();
-            assert_eq!(ts_array.value(0), expected);
+        let mut log3 = LogEvent::default();
+        log3.insert("ts", 1729594724256000000_i64); // Integer (nanoseconds)
 
-            // Second one should match the native timestamp
-            assert_eq!(ts_array.value(1), now.timestamp_nanos_opt().unwrap());
+        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
 
-            // Third one should match the integer
-            assert_eq!(ts_array.value(2), 1729594724256000000_i64);
-        }
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "ts",
+            DataType::Timestamp(TimeUnit::Nanosecond, None),
+            true,
+        )]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 3);
+
+        let ts_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<TimestampNanosecondArray>()
+            .unwrap();
+
+        // All three should be non-null
+        assert!(!ts_array.is_null(0));
+        assert!(!ts_array.is_null(1));
+        assert!(!ts_array.is_null(2));
+
+        // First one should match the parsed string
+        let expected = chrono::DateTime::parse_from_rfc3339("2025-10-22T10:18:44.256Z")
+            .unwrap()
+            .timestamp_nanos_opt()
+            .unwrap();
+        assert_eq!(ts_array.value(0), expected);
+
+        // Third one should match the integer
+        assert_eq!(ts_array.value(2), 1729594724256000000_i64);
     }
 
-    mod config_tests {
-        use super::*;
-        use tokio_util::codec::Encoder;
+    #[test]
+    fn test_encode_invalid_string_timestamp() {
+        // Test that invalid timestamp strings become null
+        let mut log1 = LogEvent::default();
+        log1.insert("timestamp", "not-a-timestamp");
 
-        #[test]
-        fn test_config_allow_nullable_fields_overrides_schema() {
-            let mut log1 = LogEvent::default();
-            log1.insert("strict_field", 42);
-            let log2 = LogEvent::default();
-            let events = vec![Event::Log(log1), Event::Log(log2)];
+        let mut log2 = LogEvent::default();
+        log2.insert("timestamp", "2025-10-22T10:18:44.256Z"); // Valid
 
-            let schema = Schema::new(vec![Field::new("strict_field", DataType::Int64, false)]);
+        let mut log3 = LogEvent::default();
+        log3.insert("timestamp", "2025-99-99T99:99:99Z"); // Invalid
 
-            let mut config = ArrowStreamSerializerConfig::new(schema);
-            config.allow_nullable_fields = true;
+        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
 
-            let mut serializer =
-                ArrowStreamSerializer::new(config).expect("Failed to create serializer");
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "timestamp",
+            DataType::Timestamp(TimeUnit::Nanosecond, None),
+            true,
+        )]));
 
-            let mut buffer = BytesMut::new();
-            serializer
-                .encode(events, &mut buffer)
-                .expect("Encoding should succeed when allow_nullable_fields is true");
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
 
-            let cursor = Cursor::new(buffer);
-            let mut reader = StreamReader::try_new(cursor, None).expect("Failed to create reader");
-            let batch = reader.next().unwrap().expect("Failed to read batch");
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
 
-            assert_eq!(batch.num_rows(), 2);
+        assert_eq!(batch.num_rows(), 3);
 
-            let binding = batch.schema();
-            let output_field = binding.field(0);
-            assert!(
-                output_field.is_nullable(),
-                "The output schema field should have been transformed to nullable=true"
-            );
+        let ts_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<TimestampNanosecondArray>()
+            .unwrap();
 
-            let array = batch
-                .column(0)
-                .as_any()
-                .downcast_ref::<Int64Array>()
-                .unwrap();
+        // Invalid timestamps should be null
+        assert!(ts_array.is_null(0));
+        assert!(!ts_array.is_null(1)); // Valid one
+        assert!(ts_array.is_null(2));
+    }
 
-            assert_eq!(array.value(0), 42);
-            assert!(!array.is_null(0));
-            assert!(
-                array.is_null(1),
-                "The missing value should be encoded as null"
-            );
+    #[test]
+    fn test_encode_decimal128_from_integer() {
+        use arrow::array::Decimal128Array;
+
+        let mut log = LogEvent::default();
+        // Store quantity as integer: 1000
+        log.insert("quantity", 1000_i64);
+
+        let events = vec![Event::Log(log)];
+
+        // Decimal(10, 3) - will represent 1000 as 1000.000
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "quantity",
+            DataType::Decimal128(10, 3),
+            true,
+        )]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 1);
+
+        let decimal_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<Decimal128Array>()
+            .unwrap();
+
+        assert!(!decimal_array.is_null(0));
+        // 1000 with scale 3 = 1000 * 10^3 = 1000000
+        assert_eq!(decimal_array.value(0), 1000000_i128);
+    }
+
+    #[test]
+    fn test_encode_decimal256() {
+        use arrow::array::Decimal256Array;
+
+        let mut log = LogEvent::default();
+        // Very large precision number
+        log.insert("big_value", 123456789.123456_f64);
+
+        let events = vec![Event::Log(log)];
+
+        // Decimal256(50, 6) - high precision decimal
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "big_value",
+            DataType::Decimal256(50, 6),
+            true,
+        )]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 1);
+
+        let decimal_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<Decimal256Array>()
+            .unwrap();
+
+        assert!(!decimal_array.is_null(0));
+        // Value should be non-null and encoded
+        let value = decimal_array.value(0);
+        assert!(value.to_i128().is_some());
+    }
+
+    #[test]
+    fn test_encode_decimal_null_values() {
+        use arrow::array::Decimal128Array;
+
+        let mut log1 = LogEvent::default();
+        log1.insert("price", 99.99_f64);
+
+        let log2 = LogEvent::default();
+        // No price field - should be null
+
+        let mut log3 = LogEvent::default();
+        log3.insert("price", 50.00_f64);
+
+        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
+
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "price",
+            DataType::Decimal128(10, 2),
+            true,
+        )]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 3);
+
+        let decimal_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<Decimal128Array>()
+            .unwrap();
+
+        // First row: 99.99
+        assert!(!decimal_array.is_null(0));
+        assert_eq!(decimal_array.value(0), 9999_i128);
+
+        // Second row: null
+        assert!(decimal_array.is_null(1));
+
+        // Third row: 50.00
+        assert!(!decimal_array.is_null(2));
+        assert_eq!(decimal_array.value(2), 5000_i128);
+    }
+
+    #[test]
+    fn test_encode_unsigned_integer_types() {
+        use arrow::array::{UInt8Array, UInt16Array, UInt32Array, UInt64Array};
+
+        let mut log = LogEvent::default();
+        log.insert("uint8_field", 255_i64);
+        log.insert("uint16_field", 65535_i64);
+        log.insert("uint32_field", 4294967295_i64);
+        log.insert("uint64_field", 9223372036854775807_i64);
+
+        let events = vec![Event::Log(log)];
+
+        let schema = Arc::new(Schema::new(vec![
+            Field::new("uint8_field", DataType::UInt8, true),
+            Field::new("uint16_field", DataType::UInt16, true),
+            Field::new("uint32_field", DataType::UInt32, true),
+            Field::new("uint64_field", DataType::UInt64, true),
+        ]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 1);
+        assert_eq!(batch.num_columns(), 4);
+
+        // Verify uint8
+        let uint8_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<UInt8Array>()
+            .unwrap();
+        assert_eq!(uint8_array.value(0), 255_u8);
+
+        // Verify uint16
+        let uint16_array = batch
+            .column(1)
+            .as_any()
+            .downcast_ref::<UInt16Array>()
+            .unwrap();
+        assert_eq!(uint16_array.value(0), 65535_u16);
+
+        // Verify uint32
+        let uint32_array = batch
+            .column(2)
+            .as_any()
+            .downcast_ref::<UInt32Array>()
+            .unwrap();
+        assert_eq!(uint32_array.value(0), 4294967295_u32);
+
+        // Verify uint64
+        let uint64_array = batch
+            .column(3)
+            .as_any()
+            .downcast_ref::<UInt64Array>()
+            .unwrap();
+        assert_eq!(uint64_array.value(0), 9223372036854775807_u64);
+    }
+
+    #[test]
+    fn test_encode_unsigned_integers_with_null_and_overflow() {
+        use arrow::array::{UInt8Array, UInt32Array};
+
+        let mut log1 = LogEvent::default();
+        log1.insert("uint8_field", 100_i64);
+        log1.insert("uint32_field", 1000_i64);
+
+        let mut log2 = LogEvent::default();
+        log2.insert("uint8_field", 300_i64); // Overflow - should be null
+        log2.insert("uint32_field", -1_i64); // Negative - should be null
+
+        let log3 = LogEvent::default();
+        // Missing fields - should be null
+
+        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
+
+        let schema = Arc::new(Schema::new(vec![
+            Field::new("uint8_field", DataType::UInt8, true),
+            Field::new("uint32_field", DataType::UInt32, true),
+        ]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 3);
+
+        // Check uint8 column
+        let uint8_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<UInt8Array>()
+            .unwrap();
+        assert_eq!(uint8_array.value(0), 100_u8); // Valid
+        assert!(uint8_array.is_null(1)); // Overflow
+        assert!(uint8_array.is_null(2)); // Missing
+
+        // Check uint32 column
+        let uint32_array = batch
+            .column(1)
+            .as_any()
+            .downcast_ref::<UInt32Array>()
+            .unwrap();
+        assert_eq!(uint32_array.value(0), 1000_u32); // Valid
+        assert!(uint32_array.is_null(1)); // Negative
+        assert!(uint32_array.is_null(2)); // Missing
+    }
+
+    #[test]
+    fn test_encode_non_nullable_field_with_null_value() {
+        // Test that encoding fails when a non-nullable field encounters a null value
+        let mut log1 = LogEvent::default();
+        log1.insert("required_field", 42);
+
+        let log2 = LogEvent::default();
+        // log2 is missing required_field - should cause an error
+
+        let events = vec![Event::Log(log1), Event::Log(log2)];
+
+        // Create schema with non-nullable field
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "required_field",
+            DataType::Int64,
+            false, // Not nullable
+        )]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(schema));
+        assert!(result.is_err());
+
+        match result.unwrap_err() {
+            ArrowEncodingError::NullConstraint { field_name } => {
+                assert_eq!(field_name, "required_field");
+            }
+            other => panic!("Expected NullConstraint error, got: {:?}", other),
         }
+    }
 
-        #[test]
-        fn test_make_field_nullable_with_nested_types() {
-            let inner_struct_field = Field::new("nested_field", DataType::Int64, false);
-            let inner_struct =
-                DataType::Struct(arrow::datatypes::Fields::from(vec![inner_struct_field]));
-            let list_field = Field::new("item", inner_struct, false);
-            let list_type = DataType::List(list_field.into());
-            let outer_field = Field::new("inner_list", list_type, false);
-            let outer_struct = DataType::Struct(arrow::datatypes::Fields::from(vec![outer_field]));
+    #[test]
+    fn test_encode_non_nullable_string_field_with_missing_value() {
+        // Test that encoding fails for non-nullable string field
+        let mut log1 = LogEvent::default();
+        log1.insert("name", "Alice");
 
-            let original_field = Field::new("root", outer_struct, false);
-            let nullable_field = make_field_nullable(&original_field).unwrap();
+        let mut log2 = LogEvent::default();
+        log2.insert("name", "Bob");
 
-            assert!(
-                nullable_field.is_nullable(),
-                "Root field should be nullable"
-            );
+        let log3 = LogEvent::default();
+        // log3 is missing name field
 
-            if let DataType::Struct(root_fields) = nullable_field.data_type() {
-                let inner_list_field = &root_fields[0];
-                assert!(inner_list_field.is_nullable());
+        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
 
-                if let DataType::List(list_item_field) = inner_list_field.data_type() {
-                    assert!(list_item_field.is_nullable());
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "name",
+            DataType::Utf8,
+            false, // Not nullable
+        )]));
 
-                    if let DataType::Struct(inner_struct_fields) = list_item_field.data_type() {
-                        let nested_field = &inner_struct_fields[0];
-                        assert!(nested_field.is_nullable());
-                    } else {
-                        panic!("Expected Struct type for list items");
-                    }
-                } else {
-                    panic!("Expected List type for inner_list");
-                }
-            } else {
-                panic!("Expected Struct type for root field");
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(schema));
+        assert!(result.is_err());
+
+        match result.unwrap_err() {
+            ArrowEncodingError::NullConstraint { field_name } => {
+                assert_eq!(field_name, "name");
             }
+            other => panic!("Expected NullConstraint error, got: {:?}", other),
         }
+    }
+
+    #[test]
+    fn test_encode_non_nullable_field_all_values_present() {
+        // Test that encoding succeeds when all values are present for non-nullable field
+        let mut log1 = LogEvent::default();
+        log1.insert("id", 1);
+
+        let mut log2 = LogEvent::default();
+        log2.insert("id", 2);
+
+        let mut log3 = LogEvent::default();
+        log3.insert("id", 3);
 
-        #[test]
-        fn test_make_field_nullable_with_map_type() {
-            let key_field = Field::new("key", DataType::Utf8, false);
-            let value_field = Field::new("value", DataType::Int64, false);
-            let entries_struct =
-                DataType::Struct(arrow::datatypes::Fields::from(vec![key_field, value_field]));
-            let entries_field = Field::new("entries", entries_struct, false);
-            let map_type = DataType::Map(entries_field.into(), false);
+        let events = vec![Event::Log(log1), Event::Log(log2), Event::Log(log3)];
 
-            let original_field = Field::new("my_map", map_type, false);
-            let nullable_field = make_field_nullable(&original_field).unwrap();
+        let schema = Arc::new(Schema::new(vec![Field::new(
+            "id",
+            DataType::Int64,
+            false, // Not nullable
+        )]));
+
+        let result = encode_events_to_arrow_ipc_stream(&events, Some(Arc::clone(&schema)));
+        assert!(result.is_ok());
+
+        let bytes = result.unwrap();
+        let cursor = Cursor::new(bytes);
+        let mut reader = StreamReader::try_new(cursor, None).unwrap();
+        let batch = reader.next().unwrap().unwrap();
+
+        assert_eq!(batch.num_rows(), 3);
+
+        let id_array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<Int64Array>()
+            .unwrap();
+
+        assert_eq!(id_array.value(0), 1);
+        assert_eq!(id_array.value(1), 2);
+        assert_eq!(id_array.value(2), 3);
+        assert!(!id_array.is_null(0));
+        assert!(!id_array.is_null(1));
+        assert!(!id_array.is_null(2));
+    }
 
+    #[test]
+    fn test_config_allow_nullable_fields_overrides_schema() {
+        use tokio_util::codec::Encoder;
+
+        // Create events: One valid, one missing the "required" field
+        let mut log1 = LogEvent::default();
+        log1.insert("strict_field", 42);
+        let log2 = LogEvent::default();
+        let events = vec![Event::Log(log1), Event::Log(log2)];
+
+        let schema = Schema::new(vec![Field::new("strict_field", DataType::Int64, false)]);
+
+        let mut config = ArrowStreamSerializerConfig::new(schema);
+        config.allow_nullable_fields = true;
+
+        let mut serializer =
+            ArrowStreamSerializer::new(config).expect("Failed to create serializer");
+
+        let mut buffer = BytesMut::new();
+        serializer
+            .encode(events, &mut buffer)
+            .expect("Encoding should succeed when allow_nullable_fields is true");
+
+        let cursor = Cursor::new(buffer);
+        let mut reader = StreamReader::try_new(cursor, None).expect("Failed to create reader");
+        let batch = reader.next().unwrap().expect("Failed to read batch");
+
+        assert_eq!(batch.num_rows(), 2);
+
+        let binding = batch.schema();
+        let output_field = binding.field(0);
+        assert!(
+            output_field.is_nullable(),
+            "The output schema field should have been transformed to nullable=true"
+        );
+
+        let array = batch
+            .column(0)
+            .as_any()
+            .downcast_ref::<Int64Array>()
+            .unwrap();
+
+        assert_eq!(array.value(0), 42);
+        assert!(!array.is_null(0));
+        assert!(
+            array.is_null(1),
+            "The missing value should be encoded as null"
+        );
+    }
+
+    #[test]
+    fn test_make_field_nullable_with_nested_types() {
+        // Test that make_field_nullable recursively handles List and Struct types
+
+        // Create a nested structure: Struct containing a List of Structs
+        // struct { inner_list: [{ nested_field: Int64 }] }
+        let inner_struct_field = Field::new("nested_field", DataType::Int64, false);
+        let inner_struct =
+            DataType::Struct(arrow::datatypes::Fields::from(vec![inner_struct_field]));
+        let list_field = Field::new("item", inner_struct, false);
+        let list_type = DataType::List(Arc::new(list_field));
+        let outer_field = Field::new("inner_list", list_type, false);
+        let outer_struct = DataType::Struct(arrow::datatypes::Fields::from(vec![outer_field]));
+
+        let original_field = Field::new("root", outer_struct, false);
+
+        // Apply make_field_nullable
+        let nullable_field = make_field_nullable(&original_field);
+
+        // Verify root field is nullable
+        assert!(
+            nullable_field.is_nullable(),
+            "Root field should be nullable"
+        );
+
+        // Verify nested struct is nullable
+        if let DataType::Struct(root_fields) = nullable_field.data_type() {
+            let inner_list_field = &root_fields[0];
             assert!(
-                nullable_field.is_nullable(),
-                "Root map field should be nullable"
+                inner_list_field.is_nullable(),
+                "inner_list field should be nullable"
             );
 
-            if let DataType::Map(entries_field, _sorted) = nullable_field.data_type() {
+            // Verify list element is nullable
+            if let DataType::List(list_item_field) = inner_list_field.data_type() {
                 assert!(
-                    !entries_field.is_nullable(),
-                    "Map entries field should be non-nullable"
+                    list_item_field.is_nullable(),
+                    "List item field should be nullable"
                 );
 
-                if let DataType::Struct(struct_fields) = entries_field.data_type() {
-                    let key_field = &struct_fields[0];
-                    let value_field = &struct_fields[1];
+                // Verify inner struct fields are nullable
+                if let DataType::Struct(inner_struct_fields) = list_item_field.data_type() {
+                    let nested_field = &inner_struct_fields[0];
                     assert!(
-                        !key_field.is_nullable(),
-                        "Map key field should be non-nullable"
-                    );
-                    assert!(
-                        value_field.is_nullable(),
-                        "Map value field should be nullable"
+                        nested_field.is_nullable(),
+                        "nested_field should be nullable"
                     );
                 } else {
-                    panic!("Expected Struct type for map entries");
+                    panic!("Expected Struct type for list items");
                 }
             } else {
-                panic!("Expected Map type for my_map field");
+                panic!("Expected List type for inner_list");
+            }
+        } else {
+            panic!("Expected Struct type for root field");
+        }
+    }
+
+    #[test]
+    fn test_make_field_nullable_with_map_type() {
+        // Test that make_field_nullable handles Map types
+        // Map is internally represented as List<Struct<key, value>>
+
+        // Create a map: Map<Utf8, Int64>
+        // Internally: List<Struct<entries: {key: Utf8, value: Int64}>>
+        let key_field = Field::new("key", DataType::Utf8, false);
+        let value_field = Field::new("value", DataType::Int64, false);
+        let entries_struct =
+            DataType::Struct(arrow::datatypes::Fields::from(vec![key_field, value_field]));
+        let entries_field = Field::new("entries", entries_struct, false);
+        let map_type = DataType::Map(Arc::new(entries_field), false);
+
+        let original_field = Field::new("my_map", map_type, false);
+
+        // Apply make_field_nullable
+        let nullable_field = make_field_nullable(&original_field);
+
+        // Verify root field is nullable
+        assert!(
+            nullable_field.is_nullable(),
+            "Root map field should be nullable"
+        );
+
+        // Verify map entries are nullable
+        if let DataType::Map(entries_field, _sorted) = nullable_field.data_type() {
+            assert!(
+                entries_field.is_nullable(),
+                "Map entries field should be nullable"
+            );
+
+            // Verify the struct inside the map is nullable
+            if let DataType::Struct(struct_fields) = entries_field.data_type() {
+                let key_field = &struct_fields[0];
+                let value_field = &struct_fields[1];
+                assert!(key_field.is_nullable(), "Map key field should be nullable");
+                assert!(
+                    value_field.is_nullable(),
+                    "Map value field should be nullable"
+                );
+            } else {
+                panic!("Expected Struct type for map entries");
             }
+        } else {
+            panic!("Expected Map type for my_map field");
         }
     }
 }
diff --git a/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs b/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
index 0d92e5835..f61ef30c8 100644
--- a/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
+++ b/lib/dnstap-parser/src/vrl_functions/parse_dnstap.rs
@@ -1,5 +1,3 @@
-use std::sync::LazyLock;
-
 use base64::prelude::{BASE64_STANDARD, Engine as _};
 use dnsmsg_parser::dns_message_parser::DnsParserOptions;
 use vector_core::event::LogEvent;
@@ -7,27 +5,6 @@ use vrl::prelude::*;
 
 use crate::{parser::DnstapParser, schema::DnstapEventSchema};
 
-static DEFAULT_LOWERCASE_HOSTNAMES: LazyLock<Value> = LazyLock::new(|| Value::Boolean(false));
-
-static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
-    vec![
-        Parameter {
-            keyword: "value",
-            kind: kind::BYTES,
-            required: true,
-            description: "The base64 encoded representation of the DNSTAP data to parse.",
-            default: None,
-        },
-        Parameter {
-            keyword: "lowercase_hostnames",
-            kind: kind::BOOLEAN,
-            required: false,
-            description: "Whether to turn all hostnames found in resulting data lowercase, for consistency.",
-            default: Some(&DEFAULT_LOWERCASE_HOSTNAMES),
-        },
-    ]
-});
-
 #[derive(Clone, Copy, Debug)]
 pub struct ParseDnstap;
 
@@ -40,25 +17,21 @@ impl Function for ParseDnstap {
         "Parses the `value` as base64 encoded DNSTAP data."
     }
 
-    fn internal_failure_reasons(&self) -> &'static [&'static str] {
+    fn parameters(&self) -> &'static [Parameter] {
         &[
-            "`value` is not a valid base64 encoded string.",
-            "dnstap parsing failed for `value`",
+            Parameter {
+                keyword: "value",
+                kind: kind::BYTES,
+                required: true,
+            },
+            Parameter {
+                keyword: "lowercase_hostnames",
+                kind: kind::BOOLEAN,
+                required: false,
+            },
         ]
     }
 
-    fn category(&self) -> &'static str {
-        Category::Parse.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::OBJECT
-    }
-
-    fn parameters(&self) -> &'static [Parameter] {
-        &PARAMETERS
-    }
-
     fn examples(&self) -> &'static [Example] {
         &[example!(
             title: "Parse dnstap query message",
@@ -176,7 +149,9 @@ impl Function for ParseDnstap {
         arguments: ArgumentList,
     ) -> Compiled {
         let value = arguments.required("value");
-        let lowercase_hostnames = arguments.optional("lowercase_hostnames");
+        let lowercase_hostnames = arguments
+            .optional("lowercase_hostnames")
+            .unwrap_or_else(|| expr!(false));
         Ok(ParseDnstapFn {
             value,
             lowercase_hostnames,
@@ -188,7 +163,7 @@ impl Function for ParseDnstap {
 #[derive(Debug, Clone)]
 struct ParseDnstapFn {
     value: Box<dyn Expression>,
-    lowercase_hostnames: Option<Box<dyn Expression>>,
+    lowercase_hostnames: Box<dyn Expression>,
 }
 
 impl FunctionExpression for ParseDnstapFn {
@@ -205,10 +180,7 @@ impl FunctionExpression for ParseDnstapFn {
                 .map_err(|_| format!("{input} is not a valid base64 encoded string"))?
                 .into(),
             DnsParserOptions {
-                lowercase_hostnames: self
-                    .lowercase_hostnames
-                    .map_resolve_with_default(ctx, || DEFAULT_LOWERCASE_HOSTNAMES.clone())?
-                    .try_boolean()?,
+                lowercase_hostnames: self.lowercase_hostnames.resolve(ctx)?.try_boolean()?,
             },
         )
         .map_err(|e| format!("dnstap parsing failed for {input}: {e}"))?;
diff --git a/lib/enrichment/Cargo.toml b/lib/enrichment/Cargo.toml
index 151d329d1..1d2a67a2c 100644
--- a/lib/enrichment/Cargo.toml
+++ b/lib/enrichment/Cargo.toml
@@ -12,4 +12,3 @@ const-str.workspace = true
 dyn-clone = { version = "1.0.20", default-features = false }
 indoc.workspace = true
 vrl.workspace = true
-vector-vrl-category.workspace = true
diff --git a/lib/enrichment/src/find_enrichment_table_records.rs b/lib/enrichment/src/find_enrichment_table_records.rs
index 32a3cf949..1b016aa90 100644
--- a/lib/enrichment/src/find_enrichment_table_records.rs
+++ b/lib/enrichment/src/find_enrichment_table_records.rs
@@ -1,53 +1,12 @@
-use std::{collections::BTreeMap, sync::LazyLock};
+use std::collections::BTreeMap;
 
-use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 use crate::{
     Case, Condition, IndexHandle, TableRegistry, TableSearch,
-    vrl_util::{self, DEFAULT_CASE_SENSITIVE, add_index, evaluate_condition, is_case_sensitive},
+    vrl_util::{self, add_index, evaluate_condition, is_case_sensitive},
 };
 
-static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
-    vec![
-        Parameter {
-            keyword: "table",
-            kind: kind::BYTES,
-            required: true,
-            description: "The [enrichment table](/docs/reference/glossary/#enrichment-tables) to search.",
-            default: None,
-        },
-        Parameter {
-            keyword: "condition",
-            kind: kind::OBJECT,
-            required: true,
-            description: "The condition to search on. Since the condition is used at boot time to create indices into the data, these conditions must be statically defined.",
-            default: None,
-        },
-        Parameter {
-            keyword: "select",
-            kind: kind::ARRAY,
-            required: false,
-            description: "A subset of fields from the enrichment table to return. If not specified, all fields are returned.",
-            default: None,
-        },
-        Parameter {
-            keyword: "case_sensitive",
-            kind: kind::BOOLEAN,
-            required: false,
-            description: "Whether text fields need to match cases exactly.",
-            default: Some(&DEFAULT_CASE_SENSITIVE),
-        },
-        Parameter {
-            keyword: "wildcard",
-            kind: kind::BYTES,
-            required: false,
-            description: "Value to use for wildcard matching in the search.",
-            default: None,
-        },
-    ]
-});
-
 fn find_enrichment_table_records(
     select: Option<Value>,
     enrichment_tables: &TableSearch,
@@ -99,16 +58,34 @@ impl Function for FindEnrichmentTableRecords {
         )
     }
 
-    fn category(&self) -> &'static str {
-        Category::Enrichment.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::ARRAY
-    }
-
     fn parameters(&self) -> &'static [Parameter] {
-        &PARAMETERS
+        &[
+            Parameter {
+                keyword: "table",
+                kind: kind::BYTES,
+                required: true,
+            },
+            Parameter {
+                keyword: "condition",
+                kind: kind::OBJECT,
+                required: true,
+            },
+            Parameter {
+                keyword: "select",
+                kind: kind::ARRAY,
+                required: false,
+            },
+            Parameter {
+                keyword: "case_sensitive",
+                kind: kind::BOOLEAN,
+                required: false,
+            },
+            Parameter {
+                keyword: "wildcard",
+                kind: kind::BYTES,
+                required: false,
+            },
+        ]
     }
 
     fn examples(&self) -> &'static [Example] {
diff --git a/lib/enrichment/src/get_enrichment_table_record.rs b/lib/enrichment/src/get_enrichment_table_record.rs
index c8e92c3e7..ef2103702 100644
--- a/lib/enrichment/src/get_enrichment_table_record.rs
+++ b/lib/enrichment/src/get_enrichment_table_record.rs
@@ -1,53 +1,12 @@
-use std::{collections::BTreeMap, sync::LazyLock};
+use std::collections::BTreeMap;
 
-use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 use crate::{
     Case, Condition, IndexHandle, TableRegistry, TableSearch,
-    vrl_util::{self, DEFAULT_CASE_SENSITIVE, add_index, evaluate_condition, is_case_sensitive},
+    vrl_util::{self, add_index, evaluate_condition, is_case_sensitive},
 };
 
-static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
-    vec![
-        Parameter {
-            keyword: "table",
-            kind: kind::BYTES,
-            required: true,
-            description: "The [enrichment table](/docs/reference/glossary/#enrichment-tables) to search.",
-            default: None,
-        },
-        Parameter {
-            keyword: "condition",
-            kind: kind::OBJECT,
-            required: true,
-            description: "The condition to search on. Since the condition is used at boot time to create indices into the data, these conditions must be statically defined.",
-            default: None,
-        },
-        Parameter {
-            keyword: "select",
-            kind: kind::ARRAY,
-            required: false,
-            description: "A subset of fields from the enrichment table to return. If not specified, all fields are returned.",
-            default: None,
-        },
-        Parameter {
-            keyword: "case_sensitive",
-            kind: kind::BOOLEAN,
-            required: false,
-            description: "Whether the text fields match the case exactly.",
-            default: Some(&DEFAULT_CASE_SENSITIVE),
-        },
-        Parameter {
-            keyword: "wildcard",
-            kind: kind::BYTES,
-            required: false,
-            description: "Value to use for wildcard matching in the search.",
-            default: None,
-        },
-    ]
-});
-
 fn get_enrichment_table_record(
     select: Option<Value>,
     enrichment_tables: &TableSearch,
@@ -97,25 +56,36 @@ impl Function for GetEnrichmentTableRecord {
         USAGE
     }
 
-    fn internal_failure_reasons(&self) -> &'static [&'static str] {
+    fn parameters(&self) -> &'static [Parameter] {
         &[
-            "The row is not found.",
-            "Multiple rows are found that match the condition.",
+            Parameter {
+                keyword: "table",
+                kind: kind::BYTES,
+                required: true,
+            },
+            Parameter {
+                keyword: "condition",
+                kind: kind::OBJECT,
+                required: true,
+            },
+            Parameter {
+                keyword: "select",
+                kind: kind::ARRAY,
+                required: false,
+            },
+            Parameter {
+                keyword: "case_sensitive",
+                kind: kind::BOOLEAN,
+                required: false,
+            },
+            Parameter {
+                keyword: "wildcard",
+                kind: kind::BYTES,
+                required: false,
+            },
         ]
     }
 
-    fn category(&self) -> &'static str {
-        Category::Enrichment.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::OBJECT
-    }
-
-    fn parameters(&self) -> &'static [Parameter] {
-        &PARAMETERS
-    }
-
     fn examples(&self) -> &'static [Example] {
         &[example!(
             title: "find records",
diff --git a/lib/enrichment/src/vrl_util.rs b/lib/enrichment/src/vrl_util.rs
index 58909e5da..255cbeb3a 100644
--- a/lib/enrichment/src/vrl_util.rs
+++ b/lib/enrichment/src/vrl_util.rs
@@ -1,5 +1,5 @@
 //! Utilities shared between both VRL functions.
-use std::{collections::BTreeMap, sync::LazyLock};
+use std::collections::BTreeMap;
 
 use vrl::{
     diagnostic::{Label, Span},
@@ -103,24 +103,26 @@ pub(crate) fn add_index(
 
     Ok(index)
 }
-pub(crate) static DEFAULT_CASE_SENSITIVE: LazyLock<Value> = LazyLock::new(|| Value::Boolean(true));
 
 #[allow(clippy::result_large_err)]
 pub(crate) fn is_case_sensitive(
     arguments: &ArgumentList,
     state: &TypeState,
 ) -> Result<Case, function::Error> {
-    let case_sensitive = arguments
+    Ok(arguments
         .optional_literal("case_sensitive", state)?
-        .unwrap_or_else(|| DEFAULT_CASE_SENSITIVE.clone())
-        .as_boolean()
-        .expect("case_sensitive should be boolean"); // This will have been caught by the type checker.
-
-    Ok(if case_sensitive {
-        Case::Sensitive
-    } else {
-        Case::Insensitive
-    })
+        .map(|value| {
+            let case_sensitive = value
+                .as_boolean()
+                .expect("case_sensitive should be boolean"); // This will have been caught by the type checker.
+
+            if case_sensitive {
+                Case::Sensitive
+            } else {
+                Case::Insensitive
+            }
+        })
+        .unwrap_or(Case::Sensitive))
 }
 
 #[cfg(test)]
diff --git a/lib/file-source-common/Cargo.toml b/lib/file-source-common/Cargo.toml
index 31730d15e..3b912196c 100644
--- a/lib/file-source-common/Cargo.toml
+++ b/lib/file-source-common/Cargo.toml
@@ -18,7 +18,7 @@ crc = "3.3.0"
 serde = { version = "1.0", default-features = false, features = ["derive"] }
 serde_json = { version = "1.0.143", default-features = false }
 bstr = { version = "1.12", default-features = false }
-bytes = { version = "1.11.1", default-features = false, features = ["serde"] }
+bytes = { version = "1.10.1", default-features = false, features = ["serde"] }
 dashmap = { version = "6.1", default-features = false }
 async-compression = { version = "0.4.27", features = ["tokio", "gzip"] }
 vector-common = { path = "../vector-common", default-features = false }
diff --git a/lib/vector-api-client/Cargo.toml b/lib/vector-api-client/Cargo.toml
index 3fbc200af..5b3b32908 100644
--- a/lib/vector-api-client/Cargo.toml
+++ b/lib/vector-api-client/Cargo.toml
@@ -18,14 +18,14 @@ anyhow.workspace = true
 # Tokio / Futures
 futures.workspace = true
 tokio = { workspace = true, features = ["macros", "rt", "sync"] }
-tokio-stream = { workspace = true, features = ["sync"] }
+tokio-stream = { version = "0.1.17", default-features = false, features = ["sync"] }
 
 # GraphQL
 graphql_client = { version = "0.14.0", default-features = false, features = ["graphql_query_derive"] }
 
 # HTTP / WebSockets
 reqwest = { version = "0.11.26", default-features = false, features = ["json"] }
-tokio-tungstenite = { workspace = true, features = ["connect", "rustls"] }
+tokio-tungstenite = { version = "0.20.1", default-features = false, features = ["connect", "rustls"] }
 
 # External libs
 chrono.workspace = true
diff --git a/lib/vector-buffers/Cargo.toml b/lib/vector-buffers/Cargo.toml
index c89500fad..bb99e14b8 100644
--- a/lib/vector-buffers/Cargo.toml
+++ b/lib/vector-buffers/Cargo.toml
@@ -47,7 +47,7 @@ quickcheck = "1.0"
 rand.workspace = true
 serde_yaml.workspace = true
 temp-dir = "0.1.16"
-tokio-test.workspace = true
+tokio-test = "0.4.4"
 tracing-fluent-assertions = { version = "0.3" }
 tracing-subscriber = { workspace = true, features = ["env-filter", "fmt", "registry", "std", "ansi"] }
 
diff --git a/lib/vector-common/Cargo.toml b/lib/vector-common/Cargo.toml
index 06f3a2f87..3604dfa94 100644
--- a/lib/vector-common/Cargo.toml
+++ b/lib/vector-common/Cargo.toml
@@ -35,7 +35,7 @@ tokenize = []
 
 [dependencies]
 async-stream = "0.3.6"
-bytes = { version = "1.11.1", default-features = false, optional = true }
+bytes = { version = "1.10.1", default-features = false, optional = true }
 chrono.workspace = true
 crossbeam-utils.workspace = true
 derivative.workspace = true
diff --git a/lib/vector-core/Cargo.toml b/lib/vector-core/Cargo.toml
index 9efe3226a..76969fbe4 100644
--- a/lib/vector-core/Cargo.toml
+++ b/lib/vector-core/Cargo.toml
@@ -51,7 +51,7 @@ snafu.workspace = true
 socket2.workspace = true
 tokio = { workspace = true, features = ["net"] }
 tokio-openssl = { version = "0.6.5", default-features = false }
-tokio-stream = { workspace = true, features = ["time"], optional = true }
+tokio-stream = { version = "0.1", default-features = false, features = ["time"], optional = true }
 tokio-util = { version = "0.7.0", default-features = false, features = ["time"] }
 toml.workspace = true
 tonic.workspace = true
@@ -84,7 +84,7 @@ quickcheck = "1"
 quickcheck_macros = "1"
 proptest = "1.8"
 similar-asserts = "1.7.0"
-tokio-test.workspace = true
+tokio-test = "0.4.4"
 toml.workspace = true
 ndarray = "0.16.1"
 ndarray-stats = "0.6.0"
diff --git a/lib/vector-core/src/config/global_options.rs b/lib/vector-core/src/config/global_options.rs
index 864505519..93ad1d80f 100644
--- a/lib/vector-core/src/config/global_options.rs
+++ b/lib/vector-core/src/config/global_options.rs
@@ -143,29 +143,16 @@ pub struct GlobalOptions {
     /// The alpha value for the exponential weighted moving average (EWMA) of source and transform
     /// buffer utilization metrics.
     ///
-    /// This controls how quickly the `*_buffer_utilization_mean` gauges respond to new
-    /// observations. Values closer to 1.0 retain more of the previous value, leading to slower
-    /// adjustments. The default value of 0.9 is equivalent to a "half life" of 6-7 measurements.
+    /// This value specifies how much of the existing value is retained when each update is made.
+    /// Values closer to 1.0 result in the value adjusting slower to changes. The default value of
+    /// 0.9 is equivalent to a "half life" of 6-7 measurements.
     ///
-    /// Must be between 0 and 1 exclusively (0 < alpha < 1).
+    /// Must be between 0 and 1 exclusive (0 < alpha < 1).
     #[serde(default, skip_serializing_if = "crate::serde::is_default")]
     #[configurable(validation(range(min = 0.0, max = 1.0)))]
     #[configurable(metadata(docs::advanced))]
     pub buffer_utilization_ewma_alpha: Option<f64>,
 
-    /// The alpha value for the exponential weighted moving average (EWMA) of transform latency
-    /// metrics.
-    ///
-    /// This controls how quickly the `component_latency_mean_seconds` gauge responds to new
-    /// observations. Values closer to 1.0 retain more of the previous value, leading to slower
-    /// adjustments. The default value of 0.9 is equivalent to a "half life" of 6-7 measurements.
-    ///
-    /// Must be between 0 and 1 exclusively (0 < alpha < 1).
-    #[serde(default, skip_serializing_if = "crate::serde::is_default")]
-    #[configurable(validation(range(min = 0.0, max = 1.0)))]
-    #[configurable(metadata(docs::advanced))]
-    pub latency_ewma_alpha: Option<f64>,
-
     /// The interval, in seconds, at which the internal metrics cache for VRL is refreshed.
     /// This must be set to be able to access metrics in VRL functions.
     ///
@@ -324,7 +311,6 @@ impl GlobalOptions {
                 buffer_utilization_ewma_alpha: self
                     .buffer_utilization_ewma_alpha
                     .or(with.buffer_utilization_ewma_alpha),
-                latency_ewma_alpha: self.latency_ewma_alpha.or(with.latency_ewma_alpha),
                 metrics_storage_refresh_period: self
                     .metrics_storage_refresh_period
                     .or(with.metrics_storage_refresh_period),
diff --git a/lib/vector-core/src/event/array.rs b/lib/vector-core/src/event/array.rs
index 2711b5fcf..00c634991 100644
--- a/lib/vector-core/src/event/array.rs
+++ b/lib/vector-core/src/event/array.rs
@@ -2,7 +2,7 @@
 //! This module contains the definitions and wrapper types for handling
 //! arrays of type `Event`, in the various forms they may appear.
 
-use std::{iter, slice, vec};
+use std::{iter, slice, sync::Arc, vec};
 
 use futures::{Stream, stream};
 #[cfg(test)]
@@ -10,13 +10,14 @@ use quickcheck::{Arbitrary, Gen};
 use vector_buffers::EventCount;
 use vector_common::{
     byte_size_of::ByteSizeOf,
+    config::ComponentKey,
     finalization::{AddBatchNotifier, BatchNotifier, EventFinalizers, Finalizable},
     json_size::JsonSize,
 };
 
 use super::{
-    EstimatedJsonEncodedSizeOf, Event, EventDataEq, EventFinalizer, EventMetadata, EventMutRef,
-    EventRef, LogEvent, Metric, TraceEvent,
+    EstimatedJsonEncodedSizeOf, Event, EventDataEq, EventFinalizer, EventMutRef, EventRef,
+    LogEvent, Metric, TraceEvent,
 };
 
 /// The type alias for an array of `LogEvent` elements.
@@ -141,6 +142,36 @@ pub enum EventArray {
 }
 
 impl EventArray {
+    /// Sets the `OutputId` in the metadata for all the events in this array.
+    pub fn set_output_id(&mut self, output_id: &Arc<ComponentKey>) {
+        match self {
+            EventArray::Logs(logs) => {
+                for log in logs {
+                    log.metadata_mut().set_source_id(Arc::clone(output_id));
+                }
+            }
+            EventArray::Metrics(metrics) => {
+                for metric in metrics {
+                    metric.metadata_mut().set_source_id(Arc::clone(output_id));
+                }
+            }
+            EventArray::Traces(traces) => {
+                for trace in traces {
+                    trace.metadata_mut().set_source_id(Arc::clone(output_id));
+                }
+            }
+        }
+    }
+
+    /// Sets the `source_type` in the metadata for all metric events in this array.
+    pub fn set_source_type(&mut self, source_type: &'static str) {
+        if let EventArray::Metrics(metrics) = self {
+            for metric in metrics {
+                metric.metadata_mut().set_source_type(source_type);
+            }
+        }
+    }
+
     /// Iterate over references to this array's events.
     pub fn iter_events(&self) -> impl Iterator<Item = EventRef<'_>> {
         match self {
@@ -166,27 +197,6 @@ impl EventArray {
             _ => TypedArrayIterMut(None),
         }
     }
-
-    /// Applies a closure to each event's metadata in this array.
-    pub fn for_each_metadata_mut(&mut self, mut f: impl FnMut(&mut EventMetadata)) {
-        match self {
-            Self::Logs(logs) => {
-                for log in logs {
-                    f(log.metadata_mut());
-                }
-            }
-            Self::Metrics(metrics) => {
-                for metric in metrics {
-                    f(metric.metadata_mut());
-                }
-            }
-            Self::Traces(traces) => {
-                for trace in traces {
-                    f(trace.metadata_mut());
-                }
-            }
-        }
-    }
 }
 
 impl From<Event> for EventArray {
diff --git a/lib/vector-core/src/event/metadata.rs b/lib/vector-core/src/event/metadata.rs
index 09eaf60c2..f860b03bb 100644
--- a/lib/vector-core/src/event/metadata.rs
+++ b/lib/vector-core/src/event/metadata.rs
@@ -1,6 +1,6 @@
 #![deny(missing_docs)]
 
-use std::{borrow::Cow, collections::BTreeMap, fmt, sync::Arc, time::Instant};
+use std::{borrow::Cow, collections::BTreeMap, fmt, sync::Arc};
 
 use derivative::Derivative;
 use lookup::OwnedTargetPath;
@@ -23,17 +23,8 @@ const SPLUNK_HEC_TOKEN: &str = "splunk_hec_token";
 
 /// The event metadata structure is a `Arc` wrapper around the actual metadata to avoid cloning the
 /// underlying data until it becomes necessary to provide a `mut` copy.
-#[derive(Clone, Debug, Derivative, Deserialize, Serialize)]
-#[derivative(PartialEq)]
-pub struct EventMetadata {
-    #[serde(flatten)]
-    pub(super) inner: Arc<Inner>,
-
-    /// The timestamp when the event last entered a transform buffer.
-    #[derivative(PartialEq = "ignore")]
-    #[serde(default, skip)]
-    pub(crate) last_transform_timestamp: Option<Instant>,
-}
+#[derive(Clone, Debug, Deserialize, PartialEq, Serialize)]
+pub struct EventMetadata(pub(super) Arc<Inner>);
 
 /// The actual metadata structure contained by both `struct Metric`
 /// and `struct LogEvent` types.
@@ -138,26 +129,23 @@ fn default_metadata_value() -> Value {
 impl EventMetadata {
     /// Creates `EventMetadata` with the given `Value`, and the rest of the fields with default values
     pub fn default_with_value(value: Value) -> Self {
-        Self {
-            inner: Arc::new(Inner {
-                value,
-                ..Default::default()
-            }),
-            last_transform_timestamp: None,
-        }
+        Self(Arc::new(Inner {
+            value,
+            ..Default::default()
+        }))
     }
 
     fn get_mut(&mut self) -> &mut Inner {
-        Arc::make_mut(&mut self.inner)
+        Arc::make_mut(&mut self.0)
     }
 
     pub(super) fn into_owned(self) -> Inner {
-        Arc::unwrap_or_clone(self.inner)
+        Arc::unwrap_or_clone(self.0)
     }
 
     /// Returns a reference to the metadata value
     pub fn value(&self) -> &Value {
-        &self.inner.value
+        &self.0.value
     }
 
     /// Returns a mutable reference to the metadata value
@@ -167,7 +155,7 @@ impl EventMetadata {
 
     /// Returns a reference to the secrets
     pub fn secrets(&self) -> &Secrets {
-        &self.inner.secrets
+        &self.0.secrets
     }
 
     /// Returns a mutable reference to the secrets
@@ -178,20 +166,20 @@ impl EventMetadata {
     /// Returns a reference to the metadata source id.
     #[must_use]
     pub fn source_id(&self) -> Option<&Arc<ComponentKey>> {
-        self.inner.source_id.as_ref()
+        self.0.source_id.as_ref()
     }
 
     /// Returns a reference to the metadata source type.
     #[must_use]
     pub fn source_type(&self) -> Option<&str> {
-        self.inner.source_type.as_deref()
+        self.0.source_type.as_deref()
     }
 
     /// Returns a reference to the metadata parent id. This is the `OutputId`
     /// of the previous component the event was sent through (if any).
     #[must_use]
     pub fn upstream_id(&self) -> Option<&OutputId> {
-        self.inner.upstream_id.as_deref()
+        self.0.upstream_id.as_deref()
     }
 
     /// Sets the `source_id` in the metadata to the provided value.
@@ -211,7 +199,7 @@ impl EventMetadata {
 
     /// Return the datadog API key, if it exists
     pub fn datadog_api_key(&self) -> Option<Arc<str>> {
-        self.inner.secrets.get(DATADOG_API_KEY).cloned()
+        self.0.secrets.get(DATADOG_API_KEY).cloned()
     }
 
     /// Set the datadog API key to passed value
@@ -221,7 +209,7 @@ impl EventMetadata {
 
     /// Return the splunk hec token, if it exists
     pub fn splunk_hec_token(&self) -> Option<Arc<str>> {
-        self.inner.secrets.get(SPLUNK_HEC_TOKEN).cloned()
+        self.0.secrets.get(SPLUNK_HEC_TOKEN).cloned()
     }
 
     /// Set the splunk hec token to passed value
@@ -239,28 +227,17 @@ impl EventMetadata {
 
     /// Fetches the dropped field by meaning.
     pub fn dropped_field(&self, meaning: impl AsRef<str>) -> Option<&Value> {
-        self.inner.dropped_fields.get(meaning.as_ref())
+        self.0.dropped_fields.get(meaning.as_ref())
     }
 
     /// Returns a reference to the `DatadogMetricOriginMetadata`.
     pub fn datadog_origin_metadata(&self) -> Option<&DatadogMetricOriginMetadata> {
-        self.inner.datadog_origin_metadata.as_ref()
+        self.0.datadog_origin_metadata.as_ref()
     }
 
     /// Returns a reference to the event id.
     pub fn source_event_id(&self) -> Option<Uuid> {
-        self.inner.source_event_id
-    }
-
-    /// Returns the timestamp of the last transform buffer enqueue operation, if it exists.
-    #[must_use]
-    pub fn last_transform_timestamp(&self) -> Option<Instant> {
-        self.last_transform_timestamp
-    }
-
-    /// Sets the transform enqueue timestamp to the provided value.
-    pub fn set_last_transform_timestamp(&mut self, timestamp: Instant) {
-        self.last_transform_timestamp = Some(timestamp);
+        self.0.source_event_id
     }
 }
 
@@ -283,10 +260,7 @@ impl Default for Inner {
 
 impl Default for EventMetadata {
     fn default() -> Self {
-        Self {
-            inner: Arc::new(Inner::default()),
-            last_transform_timestamp: None,
-        }
+        Self(Arc::new(Inner::default()))
     }
 }
 
@@ -302,7 +276,7 @@ impl ByteSizeOf for EventMetadata {
         // NOTE we don't count the `str` here because it's allocated somewhere
         // else. We're just moving around the pointer, which is already captured
         // by `ByteSizeOf::size_of`.
-        self.inner.finalizers.allocated_bytes()
+        self.0.finalizers.allocated_bytes()
     }
 }
 
@@ -368,7 +342,6 @@ impl EventMetadata {
     /// If a Datadog API key is not set in `self`, the one from `other` will be used.
     /// If a Splunk HEC token is not set in `self`, the one from `other` will be used.
     pub fn merge(&mut self, other: Self) {
-        let other_timestamp = other.last_transform_timestamp;
         let inner = self.get_mut();
         let other = other.into_owned();
         inner.finalizers.merge(other.finalizers);
@@ -378,24 +351,11 @@ impl EventMetadata {
         if inner.source_event_id.is_none() {
             inner.source_event_id = other.source_event_id;
         }
-
-        // Keep the earliest `last_transform_timestamp` for accurate latency measurement.
-        match (self.last_transform_timestamp, other_timestamp) {
-            (Some(self_ts), Some(other_ts)) => {
-                if other_ts < self_ts {
-                    self.last_transform_timestamp = Some(other_ts);
-                }
-            }
-            (None, Some(other_ts)) => {
-                self.last_transform_timestamp = Some(other_ts);
-            }
-            _ => {}
-        }
     }
 
     /// Update the finalizer(s) status.
     pub fn update_status(&self, status: EventStatus) {
-        self.inner.finalizers.update_status(status);
+        self.0.finalizers.update_status(status);
     }
 
     /// Update the finalizers' sources.
@@ -405,7 +365,7 @@ impl EventMetadata {
 
     /// Gets a reference to the event finalizers.
     pub fn finalizers(&self) -> &EventFinalizers {
-        &self.inner.finalizers
+        &self.0.finalizers
     }
 
     /// Add a new event finalizer to the existing list of event finalizers.
@@ -425,7 +385,7 @@ impl EventMetadata {
 
     /// Get the schema definition.
     pub fn schema_definition(&self) -> &Arc<schema::Definition> {
-        &self.inner.schema_definition
+        &self.0.schema_definition
     }
 
     /// Set the schema definition.
diff --git a/lib/vector-core/src/event/proto.rs b/lib/vector-core/src/event/proto.rs
index 21311267e..6a4796906 100644
--- a/lib/vector-core/src/event/proto.rs
+++ b/lib/vector-core/src/event/proto.rs
@@ -677,22 +677,18 @@ impl From<Metadata> for EventMetadata {
             }
         };
 
-        EventMetadata {
-            inner: Arc::new(Inner {
-                value: metadata_value
-                    .unwrap_or_else(|| vrl::value::Value::Object(ObjectMap::new())),
-                secrets: secrets.unwrap_or_default(),
-                finalizers: EventFinalizers::default(),
-                source_id,
-                source_type: source_type.map(Into::into),
-                upstream_id,
-                schema_definition: default_schema_definition(),
-                dropped_fields: ObjectMap::new(),
-                datadog_origin_metadata,
-                source_event_id,
-            }),
-            last_transform_timestamp: None,
-        }
+        EventMetadata(Arc::new(Inner {
+            value: metadata_value.unwrap_or_else(|| vrl::value::Value::Object(ObjectMap::new())),
+            secrets: secrets.unwrap_or_default(),
+            finalizers: EventFinalizers::default(),
+            source_id,
+            source_type: source_type.map(Into::into),
+            upstream_id,
+            schema_definition: default_schema_definition(),
+            dropped_fields: ObjectMap::new(),
+            datadog_origin_metadata,
+            source_event_id,
+        }))
     }
 }
 
diff --git a/lib/vector-core/src/latency.rs b/lib/vector-core/src/latency.rs
deleted file mode 100644
index 536a8ef53..000000000
--- a/lib/vector-core/src/latency.rs
+++ /dev/null
@@ -1,59 +0,0 @@
-use std::time::Instant;
-
-use metrics::{Histogram, gauge, histogram};
-use vector_common::stats::EwmaGauge;
-
-use crate::event::EventArray;
-
-const COMPONENT_LATENCY: &str = "component_latency_seconds";
-const COMPONENT_LATENCY_MEAN: &str = "component_latency_mean_seconds";
-const DEFAULT_LATENCY_EWMA_ALPHA: f64 = 0.9;
-
-#[derive(Debug)]
-pub struct LatencyRecorder {
-    histogram: Histogram,
-    gauge: EwmaGauge,
-}
-
-impl LatencyRecorder {
-    pub fn new(ewma_alpha: Option<f64>) -> Self {
-        Self {
-            histogram: histogram!(COMPONENT_LATENCY),
-            gauge: EwmaGauge::new(
-                gauge!(COMPONENT_LATENCY_MEAN),
-                ewma_alpha.or(Some(DEFAULT_LATENCY_EWMA_ALPHA)),
-            ),
-        }
-    }
-
-    pub fn on_send(&self, events: &mut EventArray, now: Instant) {
-        let mut sum = 0.0;
-        let mut count = 0usize;
-
-        // Since all of the events in the array will most likely have entered and exited the
-        // component at close to the same time, we average all the latencies over the entire array
-        // and record it just once in the EWMA-backed gauge. If we were to record each latency
-        // individually, the gauge would effectively just reflect the latest array's latency,
-        // eliminating the utility of the EWMA averaging. However, we record the individual
-        // latencies in the histogram to get a more granular view of the latency distribution.
-        for mut event in events.iter_events_mut() {
-            let metadata = event.metadata_mut();
-            if let Some(previous) = metadata.last_transform_timestamp() {
-                let latency = now.saturating_duration_since(previous).as_secs_f64();
-                sum += latency;
-                count += 1;
-                self.histogram.record(latency);
-            }
-
-            metadata.set_last_transform_timestamp(now);
-        }
-        if count > 0 {
-            #[expect(
-                clippy::cast_precision_loss,
-                reason = "losing precision is acceptable here"
-            )]
-            let mean = sum / count as f64;
-            self.gauge.record(mean);
-        }
-    }
-}
diff --git a/lib/vector-core/src/lib.rs b/lib/vector-core/src/lib.rs
index ad1b36d1d..e12b4dc04 100644
--- a/lib/vector-core/src/lib.rs
+++ b/lib/vector-core/src/lib.rs
@@ -31,7 +31,6 @@ pub mod config;
 pub mod event;
 pub mod fanout;
 pub mod ipallowlist;
-pub mod latency;
 pub mod metrics;
 pub mod partition;
 pub mod schema;
diff --git a/lib/vector-core/src/transform/mod.rs b/lib/vector-core/src/transform/mod.rs
index 5d2829e95..335245e1f 100644
--- a/lib/vector-core/src/transform/mod.rs
+++ b/lib/vector-core/src/transform/mod.rs
@@ -1,19 +1,31 @@
-use std::{collections::HashMap, pin::Pin, sync::Arc};
+use std::{collections::HashMap, error, pin::Pin, sync::Arc, time::Instant};
 
 use futures::{Stream, StreamExt};
+use vector_common::{
+    EventDataEq,
+    byte_size_of::ByteSizeOf,
+    internal_event::{
+        self, CountByteSize, DEFAULT_OUTPUT, EventsSent, InternalEventHandle as _, Registered,
+        register,
+    },
+    json_size::JsonSize,
+};
 
 use crate::{
-    config::OutputId,
-    event::{Event, EventArray, EventContainer, EventMutRef, into_event_stream},
+    config,
+    config::{ComponentKey, OutputId},
+    event::{
+        EstimatedJsonEncodedSizeOf, Event, EventArray, EventContainer, EventMutRef, EventRef,
+        into_event_stream,
+    },
+    fanout::{self, Fanout},
+    schema,
     schema::Definition,
 };
 
-mod outputs;
 #[cfg(feature = "lua")]
 pub mod runtime_transform;
 
-pub use outputs::{OutputBuffer, TransformOutputs, TransformOutputsBuf};
-
 /// Transforms come in two variants. Functions, or tasks.
 ///
 /// While function transforms can be run out of order, or concurrently, task
@@ -170,6 +182,132 @@ impl SyncTransform for Box<dyn FunctionTransform> {
     }
 }
 
+struct TransformOutput {
+    fanout: Fanout,
+    events_sent: Registered<EventsSent>,
+    log_schema_definitions: HashMap<OutputId, Arc<schema::Definition>>,
+    output_id: Arc<OutputId>,
+}
+
+pub struct TransformOutputs {
+    outputs_spec: Vec<config::TransformOutput>,
+    primary_output: Option<TransformOutput>,
+    named_outputs: HashMap<String, TransformOutput>,
+}
+
+impl TransformOutputs {
+    pub fn new(
+        outputs_in: Vec<config::TransformOutput>,
+        component_key: &ComponentKey,
+    ) -> (Self, HashMap<Option<String>, fanout::ControlChannel>) {
+        let outputs_spec = outputs_in.clone();
+        let mut primary_output = None;
+        let mut named_outputs = HashMap::new();
+        let mut controls = HashMap::new();
+
+        for output in outputs_in {
+            let (fanout, control) = Fanout::new();
+
+            let log_schema_definitions = output
+                .log_schema_definitions
+                .into_iter()
+                .map(|(id, definition)| (id, Arc::new(definition)))
+                .collect();
+
+            match output.port {
+                None => {
+                    primary_output = Some(TransformOutput {
+                        fanout,
+                        events_sent: register(EventsSent::from(internal_event::Output(Some(
+                            DEFAULT_OUTPUT.into(),
+                        )))),
+                        log_schema_definitions,
+                        output_id: Arc::new(OutputId {
+                            component: component_key.clone(),
+                            port: None,
+                        }),
+                    });
+                    controls.insert(None, control);
+                }
+                Some(name) => {
+                    named_outputs.insert(
+                        name.clone(),
+                        TransformOutput {
+                            fanout,
+                            events_sent: register(EventsSent::from(internal_event::Output(Some(
+                                name.clone().into(),
+                            )))),
+                            log_schema_definitions,
+                            output_id: Arc::new(OutputId {
+                                component: component_key.clone(),
+                                port: Some(name.clone()),
+                            }),
+                        },
+                    );
+                    controls.insert(Some(name.clone()), control);
+                }
+            }
+        }
+
+        let me = Self {
+            outputs_spec,
+            primary_output,
+            named_outputs,
+        };
+
+        (me, controls)
+    }
+
+    pub fn new_buf_with_capacity(&self, capacity: usize) -> TransformOutputsBuf {
+        TransformOutputsBuf::new_with_capacity(self.outputs_spec.clone(), capacity)
+    }
+
+    /// Sends the events in the buffer to their respective outputs.
+    ///
+    /// # Errors
+    ///
+    /// If an error occurs while sending events to their respective output, an error variant will be
+    /// returned detailing the cause.
+    pub async fn send(
+        &mut self,
+        buf: &mut TransformOutputsBuf,
+    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
+        if let Some(primary) = self.primary_output.as_mut() {
+            let buf = buf
+                .primary_buffer
+                .as_mut()
+                .unwrap_or_else(|| unreachable!("mismatched outputs"));
+            Self::send_single_buffer(buf, primary).await?;
+        }
+        for (key, buf) in &mut buf.named_buffers {
+            let output = self
+                .named_outputs
+                .get_mut(key)
+                .unwrap_or_else(|| unreachable!("unknown output"));
+            Self::send_single_buffer(buf, output).await?;
+        }
+        Ok(())
+    }
+
+    async fn send_single_buffer(
+        buf: &mut OutputBuffer,
+        output: &mut TransformOutput,
+    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
+        for event in buf.events_mut() {
+            update_runtime_schema_definition(
+                event,
+                &output.output_id,
+                &output.log_schema_definitions,
+            );
+        }
+        let count = buf.len();
+        let byte_size = buf.estimated_json_encoded_size_of();
+        buf.send(&mut output.fanout).await?;
+        output.events_sent.emit(CountByteSize(count, byte_size));
+        Ok(())
+    }
+}
+
 #[allow(clippy::implicit_hasher)]
 /// `event`: The event that will be updated
 /// `output_id`: The `output_id` that the current even is being sent to (will be used as the new `parent_id`)
@@ -196,6 +334,221 @@ pub fn update_runtime_schema_definition(
     event.metadata_mut().set_upstream_id(Arc::clone(output_id));
 }
 
+#[derive(Debug, Clone)]
+pub struct TransformOutputsBuf {
+    primary_buffer: Option<OutputBuffer>,
+    named_buffers: HashMap<String, OutputBuffer>,
+}
+
+impl TransformOutputsBuf {
+    pub fn new_with_capacity(outputs_in: Vec<config::TransformOutput>, capacity: usize) -> Self {
+        let mut primary_buffer = None;
+        let mut named_buffers = HashMap::new();
+
+        for output in outputs_in {
+            match output.port {
+                None => {
+                    primary_buffer = Some(OutputBuffer::with_capacity(capacity));
+                }
+                Some(name) => {
+                    named_buffers.insert(name.clone(), OutputBuffer::default());
+                }
+            }
+        }
+
+        Self {
+            primary_buffer,
+            named_buffers,
+        }
+    }
+
+    /// Adds a new event to the named output buffer.
+    ///
+    /// # Panics
+    ///
+    /// Panics if there is no output with the given name.
+    pub fn push(&mut self, name: Option<&str>, event: Event) {
+        match name {
+            Some(name) => self.named_buffers.get_mut(name),
+            None => self.primary_buffer.as_mut(),
+        }
+        .expect("unknown output")
+        .push(event);
+    }
+
+    /// Drains the default output buffer.
+    ///
+    /// # Panics
+    ///
+    /// Panics if there is no default output.
+    pub fn drain(&mut self) -> impl Iterator<Item = Event> + '_ {
+        self.primary_buffer
+            .as_mut()
+            .expect("no default output")
+            .drain()
+    }
+
+    /// Drains the named output buffer.
+    ///
+    /// # Panics
+    ///
+    /// Panics if there is no output with the given name.
+    pub fn drain_named(&mut self, name: &str) -> impl Iterator<Item = Event> + '_ {
+        self.named_buffers
+            .get_mut(name)
+            .expect("unknown output")
+            .drain()
+    }
+
+    /// Takes the default output buffer.
+    ///
+    /// # Panics
+    ///
+    /// Panics if there is no default output.
+    pub fn take_primary(&mut self) -> OutputBuffer {
+        std::mem::take(self.primary_buffer.as_mut().expect("no default output"))
+    }
+
+    pub fn take_all_named(&mut self) -> HashMap<String, OutputBuffer> {
+        std::mem::take(&mut self.named_buffers)
+    }
+}
+
+impl ByteSizeOf for TransformOutputsBuf {
+    fn allocated_bytes(&self) -> usize {
+        self.primary_buffer.size_of()
+            + self
+                .named_buffers
+                .values()
+                .map(ByteSizeOf::size_of)
+                .sum::<usize>()
+    }
+}
+
+#[derive(Debug, Default, Clone)]
+pub struct OutputBuffer(Vec<EventArray>);
+
+impl OutputBuffer {
+    pub fn with_capacity(capacity: usize) -> Self {
+        Self(Vec::with_capacity(capacity))
+    }
+
+    pub fn push(&mut self, event: Event) {
+        // Coalesce multiple pushes of the same type into one array.
+        match (event, self.0.last_mut()) {
+            (Event::Log(log), Some(EventArray::Logs(logs))) => {
+                logs.push(log);
+            }
+            (Event::Metric(metric), Some(EventArray::Metrics(metrics))) => {
+                metrics.push(metric);
+            }
+            (Event::Trace(trace), Some(EventArray::Traces(traces))) => {
+                traces.push(trace);
+            }
+            (event, _) => {
+                self.0.push(event.into());
+            }
+        }
+    }
+
+    pub fn append(&mut self, events: &mut Vec<Event>) {
+        for event in events.drain(..) {
+            self.push(event);
+        }
+    }
+
+    pub fn extend(&mut self, events: impl Iterator<Item = Event>) {
+        for event in events {
+            self.push(event);
+        }
+    }
+
+    pub fn is_empty(&self) -> bool {
+        self.0.is_empty()
+    }
+
+    pub fn len(&self) -> usize {
+        self.0.iter().map(EventArray::len).sum()
+    }
+
+    pub fn capacity(&self) -> usize {
+        self.0.capacity()
+    }
+
+    pub fn first(&self) -> Option<EventRef<'_>> {
+        self.0.first().and_then(|first| match first {
+            EventArray::Logs(l) => l.first().map(Into::into),
+            EventArray::Metrics(m) => m.first().map(Into::into),
+            EventArray::Traces(t) => t.first().map(Into::into),
+        })
+    }
+
+    pub fn drain(&mut self) -> impl Iterator<Item = Event> + '_ {
+        self.0.drain(..).flat_map(EventArray::into_events)
+    }
+
+    async fn send(
+        &mut self,
+        output: &mut Fanout,
+    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
+        let send_start = Some(Instant::now());
+        for array in std::mem::take(&mut self.0) {
+            output.send(array, send_start).await?;
+        }
+
+        Ok(())
+    }
+
+    fn iter_events(&self) -> impl Iterator<Item = EventRef<'_>> {
+        self.0.iter().flat_map(EventArray::iter_events)
+    }
+
+    fn events_mut(&mut self) -> impl Iterator<Item = EventMutRef<'_>> {
+        self.0.iter_mut().flat_map(EventArray::iter_events_mut)
+    }
+
+    pub fn into_events(self) -> impl Iterator<Item = Event> {
+        self.0.into_iter().flat_map(EventArray::into_events)
+    }
+}
+
+impl ByteSizeOf for OutputBuffer {
+    fn allocated_bytes(&self) -> usize {
+        self.0.iter().map(ByteSizeOf::size_of).sum()
+    }
+}
+
+impl EventDataEq<Vec<Event>> for OutputBuffer {
+    fn event_data_eq(&self, other: &Vec<Event>) -> bool {
+        struct Comparator<'a>(EventRef<'a>);
+
+        impl PartialEq<&Event> for Comparator<'_> {
+            fn eq(&self, that: &&Event) -> bool {
+                self.0.event_data_eq(that)
+            }
+        }
+
+        self.iter_events().map(Comparator).eq(other.iter())
+    }
+}
+
+impl EstimatedJsonEncodedSizeOf for OutputBuffer {
+    fn estimated_json_encoded_size_of(&self) -> JsonSize {
+        self.0
+            .iter()
+            .map(EstimatedJsonEncodedSizeOf::estimated_json_encoded_size_of)
+            .sum()
+    }
+}
+
+impl From<Vec<Event>> for OutputBuffer {
+    fn from(events: Vec<Event>) -> Self {
+        let mut result = Self::default();
+        result.extend(events.into_iter());
+        result
+    }
+}
+
 struct WrapEventTask<T>(T);
 
 impl<T: TaskTransform<Event> + Send + 'static> TaskTransform<EventArray> for WrapEventTask<T> {
diff --git a/lib/vector-core/src/transform/outputs.rs b/lib/vector-core/src/transform/outputs.rs
deleted file mode 100644
index 7918fb008..000000000
--- a/lib/vector-core/src/transform/outputs.rs
+++ /dev/null
@@ -1,378 +0,0 @@
-use std::{collections::HashMap, error, sync::Arc, time::Instant};
-
-use vector_common::{
-    EventDataEq,
-    byte_size_of::ByteSizeOf,
-    internal_event::{
-        self, CountByteSize, DEFAULT_OUTPUT, EventsSent, InternalEventHandle as _, Registered,
-        register,
-    },
-    json_size::JsonSize,
-};
-
-use crate::{
-    config,
-    config::{ComponentKey, OutputId},
-    event::{EstimatedJsonEncodedSizeOf, Event, EventArray, EventContainer, EventMutRef, EventRef},
-    fanout::{self, Fanout},
-    schema,
-};
-
-struct TransformOutput {
-    fanout: Fanout,
-    events_sent: Registered<EventsSent>,
-    log_schema_definitions: HashMap<OutputId, Arc<schema::Definition>>,
-    output_id: Arc<OutputId>,
-}
-
-pub struct TransformOutputs {
-    outputs_spec: Vec<config::TransformOutput>,
-    primary_output: Option<TransformOutput>,
-    named_outputs: HashMap<String, TransformOutput>,
-}
-
-impl TransformOutputs {
-    pub fn new(
-        outputs_in: Vec<config::TransformOutput>,
-        component_key: &ComponentKey,
-    ) -> (Self, HashMap<Option<String>, fanout::ControlChannel>) {
-        let outputs_spec = outputs_in.clone();
-        let mut primary_output = None;
-        let mut named_outputs = HashMap::new();
-        let mut controls = HashMap::new();
-
-        for output in outputs_in {
-            let (fanout, control) = Fanout::new();
-
-            let log_schema_definitions = output
-                .log_schema_definitions
-                .into_iter()
-                .map(|(id, definition)| (id, Arc::new(definition)))
-                .collect();
-
-            match output.port {
-                None => {
-                    primary_output = Some(TransformOutput {
-                        fanout,
-                        events_sent: register(EventsSent::from(internal_event::Output(Some(
-                            DEFAULT_OUTPUT.into(),
-                        )))),
-                        log_schema_definitions,
-                        output_id: Arc::new(OutputId {
-                            component: component_key.clone(),
-                            port: None,
-                        }),
-                    });
-                    controls.insert(None, control);
-                }
-                Some(name) => {
-                    named_outputs.insert(
-                        name.clone(),
-                        TransformOutput {
-                            fanout,
-                            events_sent: register(EventsSent::from(internal_event::Output(Some(
-                                name.clone().into(),
-                            )))),
-                            log_schema_definitions,
-                            output_id: Arc::new(OutputId {
-                                component: component_key.clone(),
-                                port: Some(name.clone()),
-                            }),
-                        },
-                    );
-                    controls.insert(Some(name.clone()), control);
-                }
-            }
-        }
-
-        let me = Self {
-            outputs_spec,
-            primary_output,
-            named_outputs,
-        };
-
-        (me, controls)
-    }
-
-    pub fn new_buf_with_capacity(&self, capacity: usize) -> TransformOutputsBuf {
-        TransformOutputsBuf::new_with_capacity(self.outputs_spec.clone(), capacity)
-    }
-
-    /// Sends the events in the buffer to their respective outputs.
-    ///
-    /// # Errors
-    ///
-    /// If an error occurs while sending events to their respective output, an error variant will be
-    /// returned detailing the cause.
-    pub async fn send(
-        &mut self,
-        buf: &mut TransformOutputsBuf,
-    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
-        if let Some(primary) = self.primary_output.as_mut() {
-            let Some(buf) = buf.primary_buffer.as_mut() else {
-                unreachable!("mismatched outputs");
-            };
-            Self::send_single_buffer(buf, primary).await?;
-        }
-        for (key, buf) in &mut buf.named_buffers {
-            let Some(output) = self.named_outputs.get_mut(key) else {
-                unreachable!("unknown output");
-            };
-            Self::send_single_buffer(buf, output).await?;
-        }
-        Ok(())
-    }
-
-    async fn send_single_buffer(
-        buf: &mut OutputBuffer,
-        output: &mut TransformOutput,
-    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
-        for event in buf.events_mut() {
-            super::update_runtime_schema_definition(
-                event,
-                &output.output_id,
-                &output.log_schema_definitions,
-            );
-        }
-        let count = buf.len();
-        let byte_size = buf.estimated_json_encoded_size_of();
-        buf.send(&mut output.fanout).await?;
-        output.events_sent.emit(CountByteSize(count, byte_size));
-        Ok(())
-    }
-}
-
-#[derive(Debug, Clone)]
-pub struct TransformOutputsBuf {
-    pub(super) primary_buffer: Option<OutputBuffer>,
-    pub(super) named_buffers: HashMap<String, OutputBuffer>,
-}
-
-impl TransformOutputsBuf {
-    pub fn new_with_capacity(outputs_in: Vec<config::TransformOutput>, capacity: usize) -> Self {
-        let mut primary_buffer = None;
-        let mut named_buffers = HashMap::new();
-
-        for output in outputs_in {
-            match output.port {
-                None => {
-                    primary_buffer = Some(OutputBuffer::with_capacity(capacity));
-                }
-                Some(name) => {
-                    named_buffers.insert(name.clone(), OutputBuffer::default());
-                }
-            }
-        }
-
-        Self {
-            primary_buffer,
-            named_buffers,
-        }
-    }
-
-    /// Adds a new event to the named output buffer.
-    ///
-    /// # Panics
-    ///
-    /// Panics if there is no output with the given name.
-    pub fn push(&mut self, name: Option<&str>, event: Event) {
-        match name {
-            Some(name) => self.named_buffers.get_mut(name),
-            None => self.primary_buffer.as_mut(),
-        }
-        .expect("unknown output")
-        .push(event);
-    }
-
-    /// Drains the default output buffer.
-    ///
-    /// # Panics
-    ///
-    /// Panics if there is no default output.
-    pub fn drain(&mut self) -> impl Iterator<Item = Event> + '_ {
-        self.primary_buffer
-            .as_mut()
-            .expect("no default output")
-            .drain()
-    }
-
-    /// Drains the named output buffer.
-    ///
-    /// # Panics
-    ///
-    /// Panics if there is no output with the given name.
-    pub fn drain_named(&mut self, name: &str) -> impl Iterator<Item = Event> + '_ {
-        self.named_buffers
-            .get_mut(name)
-            .expect("unknown output")
-            .drain()
-    }
-
-    /// Takes the default output buffer.
-    ///
-    /// # Panics
-    ///
-    /// Panics if there is no default output.
-    pub fn take_primary(&mut self) -> OutputBuffer {
-        std::mem::take(self.primary_buffer.as_mut().expect("no default output"))
-    }
-
-    pub fn take_all_named(&mut self) -> HashMap<String, OutputBuffer> {
-        std::mem::take(&mut self.named_buffers)
-    }
-
-    /// Applies `f` to each [`EventArray`] currently buffered in this outputs buffer.
-    ///
-    /// This is useful for cross-cutting instrumentation (e.g. latency timestamp propagation)
-    /// that needs mutable access to the buffered arrays before they are sent.
-    pub fn for_each_array_mut(&mut self, mut f: impl FnMut(&mut EventArray)) {
-        if let Some(primary) = self.primary_buffer.as_mut() {
-            primary.for_each_array_mut(&mut f);
-        }
-        for buf in self.named_buffers.values_mut() {
-            buf.for_each_array_mut(&mut f);
-        }
-    }
-}
-
-impl ByteSizeOf for TransformOutputsBuf {
-    fn allocated_bytes(&self) -> usize {
-        self.primary_buffer.size_of()
-            + self
-                .named_buffers
-                .values()
-                .map(ByteSizeOf::size_of)
-                .sum::<usize>()
-    }
-}
-
-#[derive(Debug, Default, Clone)]
-pub struct OutputBuffer(pub(super) Vec<EventArray>);
-
-impl OutputBuffer {
-    pub fn with_capacity(capacity: usize) -> Self {
-        Self(Vec::with_capacity(capacity))
-    }
-
-    pub fn push(&mut self, event: Event) {
-        // Coalesce multiple pushes of the same type into one array.
-        match (event, self.0.last_mut()) {
-            (Event::Log(log), Some(EventArray::Logs(logs))) => {
-                logs.push(log);
-            }
-            (Event::Metric(metric), Some(EventArray::Metrics(metrics))) => {
-                metrics.push(metric);
-            }
-            (Event::Trace(trace), Some(EventArray::Traces(traces))) => {
-                traces.push(trace);
-            }
-            (event, _) => {
-                self.0.push(event.into());
-            }
-        }
-    }
-
-    pub fn append(&mut self, events: &mut Vec<Event>) {
-        for event in events.drain(..) {
-            self.push(event);
-        }
-    }
-
-    pub fn extend(&mut self, events: impl Iterator<Item = Event>) {
-        for event in events {
-            self.push(event);
-        }
-    }
-
-    pub fn is_empty(&self) -> bool {
-        self.0.is_empty()
-    }
-
-    pub fn len(&self) -> usize {
-        self.0.iter().map(EventArray::len).sum()
-    }
-
-    pub fn capacity(&self) -> usize {
-        self.0.capacity()
-    }
-
-    pub fn first(&self) -> Option<EventRef<'_>> {
-        self.0.first().and_then(|first| match first {
-            EventArray::Logs(l) => l.first().map(Into::into),
-            EventArray::Metrics(m) => m.first().map(Into::into),
-            EventArray::Traces(t) => t.first().map(Into::into),
-        })
-    }
-
-    pub fn drain(&mut self) -> impl Iterator<Item = Event> + '_ {
-        self.0.drain(..).flat_map(EventArray::into_events)
-    }
-
-    /// Applies `f` to each [`EventArray`] currently held by this buffer.
-    pub fn for_each_array_mut(&mut self, mut f: impl FnMut(&mut EventArray)) {
-        for array in &mut self.0 {
-            f(array);
-        }
-    }
-
-    async fn send(
-        &mut self,
-        output: &mut Fanout,
-    ) -> Result<(), Box<dyn error::Error + Send + Sync>> {
-        let send_start = Some(Instant::now());
-        for array in std::mem::take(&mut self.0) {
-            output.send(array, send_start).await?;
-        }
-
-        Ok(())
-    }
-
-    fn iter_events(&self) -> impl Iterator<Item = EventRef<'_>> {
-        self.0.iter().flat_map(EventArray::iter_events)
-    }
-
-    fn events_mut(&mut self) -> impl Iterator<Item = EventMutRef<'_>> {
-        self.0.iter_mut().flat_map(EventArray::iter_events_mut)
-    }
-
-    pub fn into_events(self) -> impl Iterator<Item = Event> {
-        self.0.into_iter().flat_map(EventArray::into_events)
-    }
-}
-
-impl ByteSizeOf for OutputBuffer {
-    fn allocated_bytes(&self) -> usize {
-        self.0.iter().map(ByteSizeOf::size_of).sum()
-    }
-}
-
-impl EventDataEq<Vec<Event>> for OutputBuffer {
-    fn event_data_eq(&self, other: &Vec<Event>) -> bool {
-        struct Comparator<'a>(EventRef<'a>);
-
-        impl PartialEq<&Event> for Comparator<'_> {
-            fn eq(&self, that: &&Event) -> bool {
-                self.0.event_data_eq(that)
-            }
-        }
-
-        self.iter_events().map(Comparator).eq(other.iter())
-    }
-}
-
-impl EstimatedJsonEncodedSizeOf for OutputBuffer {
-    fn estimated_json_encoded_size_of(&self) -> JsonSize {
-        self.0
-            .iter()
-            .map(EstimatedJsonEncodedSizeOf::estimated_json_encoded_size_of)
-            .sum()
-    }
-}
-
-impl From<Vec<Event>> for OutputBuffer {
-    fn from(events: Vec<Event>) -> Self {
-        let mut result = Self::default();
-        result.extend(events.into_iter());
-        result
-    }
-}
diff --git a/lib/vector-lib/src/lib.rs b/lib/vector-lib/src/lib.rs
index 82bb67cf9..3400a03c2 100644
--- a/lib/vector-lib/src/lib.rs
+++ b/lib/vector-lib/src/lib.rs
@@ -21,8 +21,8 @@ pub use vector_config::impl_generate_config_from_default;
 pub use vector_core::compile_vrl;
 pub use vector_core::{
     EstimatedJsonEncodedSizeOf, buckets, default_data_dir, emit, event, fanout, ipallowlist,
-    latency, metric_tags, metrics, partition, quantiles, register, samples, schema, serde, sink,
-    source, source_sender, tcp, tls, transform,
+    metric_tags, metrics, partition, quantiles, register, samples, schema, serde, sink, source,
+    source_sender, tcp, tls, transform,
 };
 pub use vector_lookup as lookup;
 pub use vector_stream as stream;
diff --git a/lib/vector-tap/Cargo.toml b/lib/vector-tap/Cargo.toml
index 976233f44..325520eac 100644
--- a/lib/vector-tap/Cargo.toml
+++ b/lib/vector-tap/Cargo.toml
@@ -16,8 +16,8 @@ futures.workspace = true
 glob.workspace = true
 serde_yaml.workspace = true
 tokio = { workspace = true, features = ["time"] }
-tokio-stream = { workspace = true, features = ["sync"] }
-tokio-tungstenite.workspace = true
+tokio-stream = { version = "0.1.17", default-features = false, features = ["sync"] }
+tokio-tungstenite = { version = "0.20.1", default-features = false }
 tracing.workspace = true
 url = { version = "2.5.4", default-features = false }
 uuid.workspace = true
diff --git a/lib/vector-top/Cargo.toml b/lib/vector-top/Cargo.toml
index c0da764b4..4a746f1d4 100644
--- a/lib/vector-top/Cargo.toml
+++ b/lib/vector-top/Cargo.toml
@@ -14,14 +14,13 @@ futures-util = { workspace = true, features = ["alloc"] }
 glob.workspace = true
 indoc.workspace = true
 tokio = { workspace = true, features = ["full"] }
-tokio-stream = { workspace = true, features = ["net", "sync", "time"] }
+tokio-stream = { version = "0.1.17", default-features = false, features = ["net", "sync", "time"] }
 url.workspace = true
 humantime = { version = "2.2.0", default-features = false }
 crossterm = { version = "0.29.0", default-features = false, features = ["event-stream", "windows"] }
 unit-prefix = { version = "0.5.2", default-features = false, features = ["std"] }
 num-format = { version = "0.4.4", default-features = false, features = ["with-num-bigint"] }
 ratatui = { version = "0.30.0", default-features = false, features = ["crossterm", "layout-cache"] }
-regex.workspace = true
 vector-common = { path = "../vector-common" }
 vector-api-client = { path = "../vector-api-client" }
 
diff --git a/lib/vector-top/src/dashboard.rs b/lib/vector-top/src/dashboard.rs
index c4078ebde..418ee0831 100644
--- a/lib/vector-top/src/dashboard.rs
+++ b/lib/vector-top/src/dashboard.rs
@@ -3,7 +3,7 @@ use std::{io::stdout, time::Duration};
 use crossterm::{
     ExecutableCommand,
     cursor::Show,
-    event::{DisableMouseCapture, EnableMouseCapture},
+    event::{DisableMouseCapture, EnableMouseCapture, KeyCode},
     execute,
     terminal::{EnterAlternateScreen, LeaveAlternateScreen, disable_raw_mode, enable_raw_mode},
     tty::IsTty,
@@ -12,22 +12,14 @@ use num_format::{Locale, ToFormattedString};
 use ratatui::{
     Frame, Terminal,
     backend::CrosstermBackend,
-    layout::{Alignment, Constraint, Flex, Layout, Position, Rect},
-    style::{Color, Modifier, Style, Stylize},
+    layout::{Alignment, Constraint, Layout, Rect},
+    style::{Color, Modifier, Style},
     text::{Line, Span},
-    widgets::{
-        Block, Borders, Cell, Clear, List, ListItem, ListState, Padding, Paragraph, Row, Scrollbar,
-        ScrollbarOrientation, ScrollbarState, Table, TableState, Wrap,
-    },
+    widgets::{Block, Borders, Cell, Paragraph, Row, Table, Wrap},
 };
 use tokio::sync::oneshot;
 use unit_prefix::NumberPrefix;
 
-use crate::{
-    input::{InputMode, handle_input},
-    state::{ComponentRow, FilterColumn, FilterMenuState, SortColumn},
-};
-
 use super::{
     events::capture_key_press,
     state::{self, ConnectionStatus},
@@ -37,12 +29,6 @@ pub const fn is_allocation_tracing_enabled() -> bool {
     cfg!(feature = "allocation-tracing")
 }
 
-macro_rules! row_comparator {
-    ($field:ident) => {
-        |l: &ComponentRow, r: &ComponentRow| l.$field.cmp(&r.$field)
-    };
-}
-
 /// Format metrics, with thousands separation
 trait ThousandsFormatter {
     fn thousands_format(&self) -> String;
@@ -143,36 +129,18 @@ const NUM_COLUMNS: usize = if is_allocation_tracing_enabled() {
     9
 };
 
-pub mod columns {
-    pub const ID: &str = "ID";
-    pub const OUTPUT: &str = "Output";
-    pub const KIND: &str = "Kind";
-    pub const TYPE: &str = "Type";
-    pub const EVENTS_IN: &str = "Events In";
-    pub const EVENTS_IN_TOTAL: &str = "Events In Total";
-    pub const BYTES_IN: &str = "Bytes In";
-    pub const BYTES_IN_TOTAL: &str = "Bytes In Total";
-    pub const EVENTS_OUT: &str = "Events Out";
-    pub const EVENTS_OUT_TOTAL: &str = "Events Out Total";
-    pub const BYTES_OUT: &str = "Bytes Out";
-    pub const BYTES_OUT_TOTAL: &str = "Bytes Out Total";
-    pub const ERRORS: &str = "Errors";
-    #[cfg(feature = "allocation-tracing")]
-    pub const MEMORY_USED: &str = "Memory Used";
-}
-
 static HEADER: [&str; NUM_COLUMNS] = [
-    columns::ID,
-    columns::OUTPUT,
-    columns::KIND,
-    columns::TYPE,
-    columns::EVENTS_IN,
-    columns::BYTES_IN,
-    columns::EVENTS_OUT,
-    columns::BYTES_OUT,
-    columns::ERRORS,
+    "ID",
+    "Output",
+    "Kind",
+    "Type",
+    "Events In",
+    "Bytes In",
+    "Events Out",
+    "Bytes Out",
+    "Errors",
     #[cfg(feature = "allocation-tracing")]
-    columns::MEMORY_USED,
+    "Memory Used",
 ];
 
 struct Widgets<'a> {
@@ -242,79 +210,12 @@ impl<'a> Widgets<'a> {
         // Header columns
         let header = HEADER
             .iter()
-            .map(|s| {
-                let mut content_line = Line::from(*s);
-                let c = Cell::default().style(Style::default().add_modifier(Modifier::BOLD));
-                if state.filter_state.column.matches_header(s)
-                    && let Some(pattern) = state.filter_state.pattern.as_ref().map(|p| p.as_str())
-                {
-                    content_line.push_span(" ");
-                    let filter_span =
-                        Span::styled(format!("/{pattern}/"), Style::default().fg(Color::Yellow));
-                    content_line.push_span(filter_span);
-                };
-                if state
-                    .sort_state
-                    .column
-                    .map(|c| c.matches_header(s))
-                    .unwrap_or_default()
-                {
-                    content_line.push_span(if state.sort_state.reverse {
-                        " "
-                    } else {
-                        " "
-                    });
-                };
-                c.content(content_line)
-            })
+            .map(|s| Cell::from(*s).style(Style::default().add_modifier(Modifier::BOLD)))
             .collect::<Vec<_>>();
 
         // Data columns
         let mut items = Vec::new();
-        let mut sorted = state.components.iter().collect::<Vec<_>>();
-        if let Some(column) = state.sort_state.column {
-            let sort_fn = match column {
-                SortColumn::Id => row_comparator!(key),
-                SortColumn::Kind => row_comparator!(kind),
-                SortColumn::Type => row_comparator!(component_type),
-                SortColumn::EventsIn => row_comparator!(received_events_throughput_sec),
-                SortColumn::EventsInTotal => row_comparator!(received_events_total),
-                SortColumn::BytesIn => row_comparator!(received_bytes_throughput_sec),
-                SortColumn::BytesInTotal => row_comparator!(received_bytes_total),
-                SortColumn::EventsOut => row_comparator!(sent_events_throughput_sec),
-                SortColumn::EventsOutTotal => row_comparator!(sent_events_total),
-                SortColumn::BytesOut => row_comparator!(sent_bytes_throughput_sec),
-                SortColumn::BytesOutTotal => row_comparator!(sent_bytes_total),
-                SortColumn::Errors => row_comparator!(errors),
-                #[cfg(feature = "allocation-tracing")]
-                SortColumn::MemoryUsed => row_comparator!(allocated_bytes),
-            };
-            if state.sort_state.reverse {
-                sorted.sort_by(|a, b| sort_fn(a.1, b.1).reverse())
-            } else {
-                sorted.sort_by(|a, b| sort_fn(a.1, b.1));
-            }
-        }
-
-        for (_, r) in sorted.into_iter().filter(|(_, r)| {
-            let column = state.filter_state.column;
-            if let Some(regex) = &state.filter_state.pattern {
-                match column {
-                    FilterColumn::Id => {
-                        regex.is_match(r.key.id()) || r.key.id().contains(regex.as_str())
-                    }
-                    FilterColumn::Kind => {
-                        regex.is_match(&r.kind) || r.kind.contains(regex.as_str())
-                    }
-                    FilterColumn::Type => {
-                        regex.is_match(&r.component_type)
-                            || r.component_type.contains(regex.as_str())
-                    }
-                }
-            } else {
-                true
-            }
-        }) {
+        for (_, r) in state.components.iter() {
             let mut data = vec![
                 r.key.id().to_string(),
                 if !r.has_displayable_outputs() {
@@ -409,34 +310,7 @@ impl<'a> Widgets<'a> {
             .header(Row::new(header).bottom_margin(1))
             .block(Block::default().borders(Borders::ALL).title("Components"))
             .column_spacing(2);
-        f.render_stateful_widget(
-            w,
-            area,
-            // We don't need selection, so just create a table state for the scroll
-            &mut TableState::new().with_offset(state.ui.scroll),
-        );
-        // Skip the border + header row + 1 row of padding as well as the bottom border
-        let scrollbar_area = Rect::new(area.x, area.y + 3, area.width, area.height - 4);
-        f.render_stateful_widget(
-            Scrollbar::new(ScrollbarOrientation::VerticalRight)
-                .begin_symbol(Some(""))
-                .end_symbol(Some("")),
-            scrollbar_area,
-            &mut ScrollbarState::new(
-                // Maximum allowed scroll value
-                // We calculate it like this, because scrollbar usually accounts for full
-                // overscroll, but we want scrolling to stop when last available item is visible and
-                // at the bottom of the table.
-                state
-                    .components
-                    .len()
-                    .saturating_sub(scrollbar_area.height.into())
-                    // 1 is also added, because ScrollBar removes 1, to ensure last item is visible
-                    // when overscrolling - we avoid overscroll, so this is useless to us.
-                    .saturating_add(1),
-            )
-            .position(state.ui.scroll),
-        );
+        f.render_widget(w, area);
     }
 
     /// Alerts the user to resize the window to view columns
@@ -449,109 +323,9 @@ impl<'a> Widgets<'a> {
         f.render_widget(w, area);
     }
 
-    /// Renders a box showing instructions on how to use `vector top`.
-    fn help_box(&self, f: &mut Frame, area: Rect) {
-        let text = vec![
-            Line::from("General").bold(),
-            Line::from("ESC, q => quit (or close window)"),
-            Line::from(", j => scroll down by 1 row"),
-            Line::from(", k => scroll up by 1 row"),
-            Line::from(", PageDown, CTRL+f => scroll down by 1 page"),
-            Line::from(", PageUp, CTRL+b => scroll up by 1 page"),
-            Line::from("End, G => scroll to bottom"),
-            Line::from("Home, g => scroll to top"),
-            Line::from("F1, ? => toggle this help window"),
-            Line::from("1-9 => sort by column"),
-            Line::from("F6, s => toggle sort menu"),
-            Line::from("F7, r => toggle ascending/descending sort"),
-            Line::from("F4, f, / => toggle filter menu"),
-            Line::default(),
-            Line::from("Sort menu").bold(),
-            Line::from(", Shift+Tab, k => move sort column selection up"),
-            Line::from(", Tab, j => move sort column selection down"),
-            Line::from("Enter => confirm sort selection"),
-            Line::from("F6, s => toggle sort menu"),
-            Line::default(),
-            Line::from("Filter menu").bold(),
-            Line::from(", Shift+Tab => move filter column selection up"),
-            Line::from(", Tab => move filter column selection down"),
-            Line::from("Enter => confirm filter selection"),
-            Line::from("F4 => toggle sort menu"),
-        ];
-
-        let block = Block::default()
-            .borders(Borders::ALL)
-            .border_style(Style::default())
-            .padding(Padding::proportional(2))
-            .title("Help");
-        let w = Paragraph::new(text)
-            .block(block)
-            .style(Style::default().fg(Color::Gray))
-            .alignment(Alignment::Left);
-
-        f.render_widget(Clear, area);
-        f.render_widget(w, area);
-    }
-
-    /// Renders a box with sorting options.
-    fn sort_box(&self, f: &mut Frame, area: Rect, mut list_state: ListState) {
-        f.render_widget(Clear, area);
-        let w = List::new(
-            SortColumn::items()
-                .into_iter()
-                .map(|h| ListItem::new(Line::from(h))),
-        )
-        .block(
-            Block::default()
-                .padding(Padding::proportional(2))
-                .borders(Borders::ALL)
-                .title("Sort by"),
-        )
-        .highlight_style(Style::new().reversed());
-        f.render_stateful_widget(w, area, &mut list_state);
-    }
-
-    /// Renders a box with filtering options.
-    fn filter_box(&self, f: &mut Frame, area: Rect, filter_menu_state: &FilterMenuState) {
-        f.render_widget(Clear, area);
-        let w = List::new(
-            FilterColumn::items()
-                .into_iter()
-                .map(|h| ListItem::new(Line::from(h))),
-        )
-        .block(Block::default().borders(Borders::ALL).title("Filter by"))
-        .highlight_style(Style::new().reversed());
-        let (top, bottom) = {
-            (
-                Rect::new(area.x, area.y, area.width, area.height / 2),
-                Rect::new(
-                    area.x,
-                    area.y + area.height / 2,
-                    area.width,
-                    area.height / 2,
-                ),
-            )
-        };
-        f.render_stateful_widget(w, top, &mut filter_menu_state.column_selection.clone());
-        f.render_widget(
-            Paragraph::new(filter_menu_state.input.clone()).block(
-                Block::default()
-                    .borders(Borders::ALL)
-                    .title("Filter pattern"),
-            ),
-            bottom,
-        );
-        f.set_cursor_position(Position::new(
-            bottom.x + 1 + filter_menu_state.input.len() as u16,
-            bottom.y + 1,
-        ));
-    }
-
     /// Renders a box showing instructions on how to exit from `vector top`.
     fn quit_box(&self, f: &mut Frame, area: Rect) {
-        let text = vec![Line::from(
-            "To quit, press ESC or 'q'; Press F1 or '?' for help",
-        )];
+        let text = vec![Line::from("To quit, press ESC or 'q'")];
 
         let block = Block::default()
             .borders(Borders::ALL)
@@ -581,37 +355,6 @@ impl<'a> Widgets<'a> {
         }
 
         self.quit_box(f, rects[2]);
-
-        // Render help, sort and filter over other items
-        if state.ui.help_visible {
-            let [area] = Layout::horizontal([Constraint::Length(64)])
-                .flex(Flex::Center)
-                .areas(size);
-            let [area] = Layout::vertical([Constraint::Length(32)])
-                .flex(Flex::Center)
-                .areas(area);
-            self.help_box(f, area);
-        }
-
-        if state.ui.sort_visible {
-            let [area] = Layout::horizontal([Constraint::Length(64)])
-                .flex(Flex::Center)
-                .areas(size);
-            let [area] = Layout::vertical([Constraint::Length(32)])
-                .flex(Flex::Center)
-                .areas(area);
-            self.sort_box(f, area, state.ui.sort_menu_state);
-        }
-
-        if state.ui.filter_visible {
-            let [area] = Layout::horizontal([Constraint::Length(64)])
-                .flex(Flex::Center)
-                .areas(size);
-            let [area] = Layout::vertical([Constraint::Length(12)])
-                .flex(Flex::Center)
-                .areas(area);
-            self.filter_box(f, area, &state.ui.filter_menu_state);
-        }
     }
 }
 
@@ -629,7 +372,6 @@ pub async fn init_dashboard<'a>(
     url: &'a str,
     interval: u32,
     human_metrics: bool,
-    event_tx: state::EventTx,
     mut state_rx: state::StateRx,
     mut shutdown_rx: oneshot::Receiver<()>,
 ) -> Result<(), Box<dyn std::error::Error>> {
@@ -653,27 +395,16 @@ pub async fn init_dashboard<'a>(
     terminal.clear()?;
 
     let widgets = Widgets::new(title, url, interval, human_metrics);
-    let mut input_mode = InputMode::Top;
 
     loop {
         tokio::select! {
             Some(state) = state_rx.recv() => {
-                if state.ui.filter_visible {
-                    input_mode = InputMode::FilterInput;
-                } else if state.ui.sort_visible {
-                    input_mode = InputMode::SortMenu;
-                } else if state.ui.help_visible {
-                    input_mode = InputMode::HelpMenu;
-                } else {
-                    input_mode = InputMode::Top;
-                }
                 terminal.draw(|f| widgets.draw(f, state))?;
             },
             k = key_press_rx.recv() => {
-                let k = k.unwrap();
-                if handle_input(input_mode, k, &event_tx, &terminal).await {
+                if let KeyCode::Esc | KeyCode::Char('q') = k.unwrap() {
                     _ = key_press_kill_tx.send(());
-                    break;
+                    break
                 }
             }
             _ = &mut shutdown_rx => {
diff --git a/lib/vector-top/src/events.rs b/lib/vector-top/src/events.rs
index 6864966f6..5696e696c 100644
--- a/lib/vector-top/src/events.rs
+++ b/lib/vector-top/src/events.rs
@@ -1,10 +1,10 @@
-use crossterm::event::{Event, EventStream, KeyEvent};
+use crossterm::event::{Event, EventStream, KeyCode};
 use futures::StreamExt;
 use tokio::sync::{mpsc, oneshot};
 
 /// Capture keyboard input, and send it upstream via a channel. This is used for interaction
 /// with the dashboard, and exiting from `vector top`.
-pub fn capture_key_press() -> (mpsc::UnboundedReceiver<KeyEvent>, oneshot::Sender<()>) {
+pub fn capture_key_press() -> (mpsc::UnboundedReceiver<KeyCode>, oneshot::Sender<()>) {
     let (tx, rx) = mpsc::unbounded_channel();
     let (kill_tx, mut kill_rx) = oneshot::channel();
 
@@ -17,7 +17,7 @@ pub fn capture_key_press() -> (mpsc::UnboundedReceiver<KeyEvent>, oneshot::Sende
                 _ = &mut kill_rx => return,
                 Some(Ok(event)) = events.next() => {
                      if let Event::Key(k) = event {
-                        _ = tx.clone().send(k);
+                        _ = tx.clone().send(k.code);
                     };
                 }
             }
diff --git a/lib/vector-top/src/input.rs b/lib/vector-top/src/input.rs
deleted file mode 100644
index 874a0f5be..000000000
--- a/lib/vector-top/src/input.rs
+++ /dev/null
@@ -1,229 +0,0 @@
-use crossterm::event::{KeyCode, KeyEvent, KeyModifiers};
-use ratatui::{Terminal, prelude::Backend};
-
-use crate::state::{self, EventType, SortColumn, UiEventType};
-
-#[derive(Debug, Clone, Copy)]
-pub(crate) enum InputMode {
-    Top,
-    HelpMenu,
-    FilterInput,
-    SortMenu,
-}
-
-/// Handles keyboard input for top
-///
-/// Returns true if input handling is done (quit is requested)
-pub(crate) async fn handle_input<B: Backend>(
-    mode: InputMode,
-    key_event: KeyEvent,
-    event_tx: &state::EventTx,
-    terminal: &Terminal<B>,
-) -> bool {
-    match mode {
-        InputMode::Top => handle_top_input(key_event, event_tx, terminal).await,
-        InputMode::HelpMenu => handle_help_input(key_event, event_tx, terminal).await,
-        InputMode::FilterInput => handle_filter_input(key_event, event_tx, terminal).await,
-        InputMode::SortMenu => handle_sort_input(key_event, event_tx, terminal).await,
-    }
-}
-
-async fn handle_top_input<B: Backend>(
-    key_event: KeyEvent,
-    event_tx: &state::EventTx,
-    terminal: &Terminal<B>,
-) -> bool {
-    match key_event.code {
-        KeyCode::Esc | KeyCode::Char('q') => {
-            return true;
-        }
-        KeyCode::Up | KeyCode::Char('k') => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::Scroll(
-                    -1,
-                    terminal.size().unwrap_or_default(),
-                )))
-                .await;
-        }
-        KeyCode::Down | KeyCode::Char('j') => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::Scroll(
-                    1,
-                    terminal.size().unwrap_or_default(),
-                )))
-                .await;
-        }
-        KeyCode::End | KeyCode::Char('G') => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::Scroll(
-                    isize::MAX,
-                    terminal.size().unwrap_or_default(),
-                )))
-                .await;
-        }
-        KeyCode::Home | KeyCode::Char('g') => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::Scroll(
-                    isize::MIN,
-                    terminal.size().unwrap_or_default(),
-                )))
-                .await;
-        }
-        KeyCode::Left | KeyCode::PageUp => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ScrollPage(
-                    -1,
-                    terminal.size().unwrap_or_default(),
-                )))
-                .await;
-        }
-        KeyCode::Char('b') if key_event.modifiers.intersects(KeyModifiers::CONTROL) => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ScrollPage(
-                    -1,
-                    terminal.size().unwrap_or_default(),
-                )))
-                .await;
-        }
-        KeyCode::Right | KeyCode::PageDown => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ScrollPage(
-                    1,
-                    terminal.size().unwrap_or_default(),
-                )))
-                .await;
-        }
-        KeyCode::Char('f') if key_event.modifiers.intersects(KeyModifiers::CONTROL) => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ScrollPage(
-                    1,
-                    terminal.size().unwrap_or_default(),
-                )))
-                .await;
-        }
-        KeyCode::Char('?') | KeyCode::F(1) => {
-            let _ = event_tx.send(EventType::Ui(UiEventType::ToggleHelp)).await;
-        }
-        KeyCode::Char('s') | KeyCode::F(6) => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ToggleSortMenu))
-                .await;
-        }
-        KeyCode::Char('r') | KeyCode::F(7) => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ToggleSortDirection))
-                .await;
-        }
-        KeyCode::Char(d) if d.is_ascii_digit() => {
-            let col = match d {
-                '1' => SortColumn::Id,
-                '3' => SortColumn::Kind,
-                '4' => SortColumn::Type,
-                '5' => SortColumn::EventsInTotal,
-                '6' => SortColumn::BytesInTotal,
-                '7' => SortColumn::EventsOutTotal,
-                '8' => SortColumn::BytesOutTotal,
-                '9' => SortColumn::Errors,
-                #[cfg(feature = "allocation-tracing")]
-                '0' => SortColumn::MemoryUsed,
-                _ => return false,
-            };
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::SortByColumn(col)))
-                .await;
-        }
-        KeyCode::F(4) | KeyCode::Char('f') | KeyCode::Char('/') => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ToggleFilterMenu))
-                .await;
-        }
-        _ => (),
-    }
-    false
-}
-
-async fn handle_help_input<B: Backend>(
-    key_event: KeyEvent,
-    event_tx: &state::EventTx,
-    terminal: &Terminal<B>,
-) -> bool {
-    match key_event.code {
-        KeyCode::Esc => {
-            let _ = event_tx.send(EventType::Ui(UiEventType::ToggleHelp)).await;
-        }
-        _ => return handle_top_input(key_event, event_tx, terminal).await,
-    }
-    false
-}
-
-async fn handle_sort_input<B: Backend>(
-    key_event: KeyEvent,
-    event_tx: &state::EventTx,
-    terminal: &Terminal<B>,
-) -> bool {
-    match key_event.code {
-        KeyCode::Esc => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ToggleSortMenu))
-                .await;
-        }
-        KeyCode::Up | KeyCode::BackTab | KeyCode::Char('k') => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::SortSelection(-1)))
-                .await;
-        }
-        KeyCode::Down | KeyCode::Tab | KeyCode::Char('j') => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::SortSelection(1)))
-                .await;
-        }
-        KeyCode::Enter => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::SortConfirmation))
-                .await;
-        }
-        _ => return handle_top_input(key_event, event_tx, terminal).await,
-    }
-    false
-}
-
-async fn handle_filter_input<B: Backend>(
-    key_event: KeyEvent,
-    event_tx: &state::EventTx,
-    terminal: &Terminal<B>,
-) -> bool {
-    match key_event.code {
-        KeyCode::Esc => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::ToggleFilterMenu))
-                .await;
-        }
-        KeyCode::BackTab | KeyCode::Up => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::FilterColumnSelection(-1)))
-                .await;
-        }
-        KeyCode::Tab | KeyCode::Down => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::FilterColumnSelection(1)))
-                .await;
-        }
-        KeyCode::Backspace => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::FilterBackspace))
-                .await;
-        }
-        KeyCode::Enter => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::FilterConfirmation))
-                .await;
-        }
-        KeyCode::Char(any) => {
-            let _ = event_tx
-                .send(EventType::Ui(UiEventType::FilterInput(any)))
-                .await;
-        }
-        _ => return handle_top_input(key_event, event_tx, terminal).await,
-    }
-    false
-}
diff --git a/lib/vector-top/src/lib.rs b/lib/vector-top/src/lib.rs
index 7f4f9b8a8..c3580a464 100644
--- a/lib/vector-top/src/lib.rs
+++ b/lib/vector-top/src/lib.rs
@@ -1,6 +1,5 @@
 //! Top subcommand
 pub mod dashboard;
 pub mod events;
-mod input;
 pub mod metrics;
 pub mod state;
diff --git a/lib/vector-top/src/state.rs b/lib/vector-top/src/state.rs
index e4c535f78..4bf3ffb8e 100644
--- a/lib/vector-top/src/state.rs
+++ b/lib/vector-top/src/state.rs
@@ -1,24 +1,18 @@
 use std::{
     collections::{BTreeMap, HashMap},
-    str::FromStr,
     time::Duration,
 };
 
 use chrono::{DateTime, Local};
 use ratatui::{
-    layout::Size,
     style::{Color, Style},
     text::Span,
-    widgets::ListState,
 };
-use regex::Regex;
 use tokio::sync::mpsc;
 use vector_common::internal_event::DEFAULT_OUTPUT;
 
 use vector_common::config::ComponentKey;
 
-use crate::dashboard::columns;
-
 type IdentifiedMetric = (ComponentKey, i64);
 
 #[derive(Debug)]
@@ -51,37 +45,6 @@ pub enum EventType {
     ComponentAdded(ComponentRow),
     ComponentRemoved(ComponentKey),
     ConnectionUpdated(ConnectionStatus),
-    Ui(UiEventType),
-}
-
-#[derive(Debug)]
-pub enum UiEventType {
-    // Scroll up (-) or down (+). Also passes the window size for correct max scroll calculation.
-    Scroll(isize, Size),
-    // Scroll up (-) or down (+) by a whole page. Also passes the window size for page size and max scroll calculation.
-    ScrollPage(isize, Size),
-    // Toggles help window. Also closes other windows.
-    ToggleHelp,
-    // Toggles sort menu. Also closes other windows.
-    ToggleSortMenu,
-    // Toggles sort direction.
-    ToggleSortDirection,
-    // Change sort selection up (-) or down (+).
-    SortSelection(isize),
-    // Change sort selection to a specific column.
-    SortByColumn(SortColumn),
-    // Confirms current sort selection.
-    SortConfirmation,
-    // Toggles filter menu. Also closes other windows.
-    ToggleFilterMenu,
-    // Change filter column selection left (-) or right (+).
-    FilterColumnSelection(isize),
-    // Adds input to filter string.
-    FilterInput(char),
-    // Removes a character from the end of the filter string.
-    FilterBackspace,
-    // Confirms current filter selection.
-    FilterConfirmation,
 }
 
 #[derive(Debug, Copy, Clone)]
@@ -119,253 +82,17 @@ pub struct State {
     pub connection_status: ConnectionStatus,
     pub uptime: Duration,
     pub components: BTreeMap<ComponentKey, ComponentRow>,
-    pub sort_state: SortState,
-    pub filter_state: FilterState,
-    pub ui: UiState,
-}
-
-#[derive(Debug, Clone, Copy)]
-pub enum SortColumn {
-    Id = 0,
-    Kind = 1,
-    Type = 2,
-    EventsIn = 3,
-    EventsInTotal = 4,
-    BytesIn = 5,
-    BytesInTotal = 6,
-    EventsOut = 7,
-    EventsOutTotal = 8,
-    BytesOut = 9,
-    BytesOutTotal = 10,
-    Errors = 11,
-    #[cfg(feature = "allocation-tracing")]
-    MemoryUsed = 12,
-}
-
-#[derive(Debug, Default, Clone, Copy)]
-pub enum FilterColumn {
-    #[default]
-    Id = 0,
-    Kind = 1,
-    Type = 2,
-}
-
-impl SortColumn {
-    pub fn matches_header(&self, header: &str) -> bool {
-        match self {
-            SortColumn::Id => header == columns::ID,
-            SortColumn::Kind => header == columns::KIND,
-            SortColumn::Type => header == columns::TYPE,
-            SortColumn::EventsIn | SortColumn::EventsInTotal => header == columns::EVENTS_IN,
-            SortColumn::BytesIn | SortColumn::BytesInTotal => header == columns::BYTES_IN,
-            SortColumn::EventsOut | SortColumn::EventsOutTotal => header == columns::EVENTS_OUT,
-            SortColumn::BytesOut | SortColumn::BytesOutTotal => header == columns::BYTES_OUT,
-            SortColumn::Errors => header == columns::ERRORS,
-            #[cfg(feature = "allocation-tracing")]
-            SortColumn::MemoryUsed => header == columns::MEMORY_USED,
-        }
-    }
-
-    pub fn items() -> Vec<&'static str> {
-        vec![
-            columns::ID,
-            columns::KIND,
-            columns::TYPE,
-            columns::EVENTS_IN,
-            columns::EVENTS_IN_TOTAL,
-            columns::BYTES_IN,
-            columns::BYTES_IN_TOTAL,
-            columns::EVENTS_OUT,
-            columns::EVENTS_OUT_TOTAL,
-            columns::BYTES_OUT,
-            columns::BYTES_OUT_TOTAL,
-            columns::ERRORS,
-            #[cfg(feature = "allocation-tracing")]
-            columns::MEMORY_USED,
-        ]
-    }
-}
-
-impl FilterColumn {
-    pub fn matches_header(&self, header: &str) -> bool {
-        match self {
-            FilterColumn::Id => header == columns::ID,
-            FilterColumn::Kind => header == columns::KIND,
-            FilterColumn::Type => header == columns::TYPE,
-        }
-    }
-
-    pub fn items() -> Vec<&'static str> {
-        vec![columns::ID, columns::KIND, columns::TYPE]
-    }
-}
-
-impl From<usize> for SortColumn {
-    fn from(value: usize) -> Self {
-        match value {
-            1 => SortColumn::Kind,
-            2 => SortColumn::Type,
-            3 => SortColumn::EventsIn,
-            4 => SortColumn::EventsInTotal,
-            5 => SortColumn::BytesIn,
-            6 => SortColumn::BytesInTotal,
-            7 => SortColumn::EventsOut,
-            8 => SortColumn::EventsOutTotal,
-            9 => SortColumn::BytesOut,
-            10 => SortColumn::BytesOutTotal,
-            11 => SortColumn::Errors,
-            #[cfg(feature = "allocation-tracing")]
-            12 => SortColumn::MemoryUsed,
-            _ => SortColumn::Id,
-        }
-    }
-}
-
-impl From<usize> for FilterColumn {
-    fn from(value: usize) -> Self {
-        match value {
-            1 => FilterColumn::Kind,
-            2 => FilterColumn::Type,
-            _ => FilterColumn::Id,
-        }
-    }
-}
-
-impl FromStr for SortColumn {
-    type Err = String;
-
-    fn from_str(s: &str) -> Result<Self, Self::Err> {
-        if let Some((index, _)) = Self::items()
-            .iter()
-            .enumerate()
-            .find(|(_, item)| item.eq_ignore_ascii_case(s))
-        {
-            Ok(index.into())
-        } else {
-            Err("Unknown sort field".to_string())
-        }
-    }
-}
-
-impl FromStr for FilterColumn {
-    type Err = String;
-
-    fn from_str(s: &str) -> Result<Self, Self::Err> {
-        if let Some((index, _)) = Self::items()
-            .iter()
-            .enumerate()
-            .find(|(_, item)| item.eq_ignore_ascii_case(s))
-        {
-            Ok(index.into())
-        } else {
-            Err("Unknown filter field".to_string())
-        }
-    }
-}
-
-#[derive(Debug, Default, Clone)]
-pub struct SortState {
-    pub column: Option<SortColumn>,
-    pub reverse: bool,
-}
-
-#[derive(Debug, Default, Clone)]
-pub struct FilterState {
-    pub column: FilterColumn,
-    pub pattern: Option<Regex>,
-}
-
-#[derive(Debug, Default, Clone)]
-pub struct UiState {
-    pub scroll: usize,
-    pub help_visible: bool,
-    pub sort_visible: bool,
-    pub sort_menu_state: ListState,
-    pub filter_visible: bool,
-    pub filter_menu_state: FilterMenuState,
-}
-
-#[derive(Debug, Clone)]
-pub struct FilterMenuState {
-    pub input: String,
-    pub column_selection: ListState,
-}
-
-impl Default for FilterMenuState {
-    fn default() -> Self {
-        Self {
-            input: Default::default(),
-            column_selection: ListState::default().with_selected(Some(0)),
-        }
-    }
-}
-
-impl UiState {
-    /// Returns the height of components display box in rows, based on provided [`Size`].
-    /// Calculates by deducting rows used for header and footer.
-    pub fn components_box_height(area: Size) -> u16 {
-        // Currently hardcoded (10 is the number of rows the header and footer take up)
-        area.height.saturating_sub(10)
-    }
-
-    /// Returns the maximum scroll value
-    pub fn max_scroll(area: Size, components_count: usize) -> usize {
-        components_count.saturating_sub(Self::components_box_height(area).into())
-    }
-
-    /// Changes current scroll by provided diff in rows. Uses [`Size`] to limit scroll,
-    /// so that scrolling down is possible until the last component is visible.
-    pub fn scroll(&mut self, diff: isize, area: Size, components_count: usize) {
-        let max_scroll = Self::max_scroll(area, components_count);
-        self.scroll = self.scroll.saturating_add_signed(diff);
-        if self.scroll > max_scroll {
-            self.scroll = max_scroll;
-        }
-    }
-
-    /// Changes current scroll by provided diff in pages. Uses [`Size`] to limit scroll,
-    /// and to calculate number of rows a page contains.
-    pub fn scroll_page(&mut self, diff: isize, area: Size, components_count: usize) {
-        self.scroll(
-            diff * (Self::components_box_height(area) as isize),
-            area,
-            components_count,
-        );
-    }
 }
 
 impl State {
-    pub fn new(components: BTreeMap<ComponentKey, ComponentRow>) -> Self {
+    pub const fn new(components: BTreeMap<ComponentKey, ComponentRow>) -> Self {
         Self {
             connection_status: ConnectionStatus::Pending,
             uptime: Duration::from_secs(0),
             components,
-            ui: UiState::default(),
-            sort_state: SortState::default(),
-            filter_state: FilterState::default(),
         }
     }
-
-    pub fn apply_sort_state_to_ui(&mut self) {
-        self.ui
-            .sort_menu_state
-            .select(self.sort_state.column.map(|c| c as usize));
-    }
-
-    pub fn apply_filter_state_to_ui(&mut self) {
-        self.ui
-            .filter_menu_state
-            .column_selection
-            .select(Some(self.filter_state.column as usize));
-        self.ui.filter_menu_state.input = self
-            .filter_state
-            .pattern
-            .as_ref()
-            .map(|r| r.as_str().to_string())
-            .unwrap_or("".to_string());
-    }
 }
-
 pub type EventTx = mpsc::Sender<EventType>;
 pub type EventRx = mpsc::Receiver<EventType>;
 pub type StateRx = mpsc::Receiver<State>;
@@ -416,19 +143,15 @@ impl ComponentRow {
 /// Takes the receiver `EventRx` channel, and returns a `StateRx` state receiver. This
 /// represents the single destination for handling subscriptions and returning 'immutable' state
 /// for re-rendering the dashboard. This approach uses channels vs. mutexes.
-pub async fn updater(mut event_rx: EventRx, mut state: State) -> StateRx {
+pub async fn updater(mut event_rx: EventRx) -> StateRx {
     let (tx, rx) = mpsc::channel(20);
 
+    let mut state = State::new(BTreeMap::new());
     tokio::spawn(async move {
         while let Some(event_type) = event_rx.recv().await {
             match event_type {
                 EventType::InitializeState(new_state) => {
-                    let old_state = state;
                     state = new_state;
-                    // Keep filters, sort and UI states
-                    state.filter_state = old_state.filter_state;
-                    state.sort_state = old_state.sort_state;
-                    state.ui = old_state.ui;
                 }
                 EventType::ReceivedBytesTotals(rows) => {
                     for (key, v) in rows {
@@ -530,7 +253,6 @@ pub async fn updater(mut event_rx: EventRx, mut state: State) -> StateRx {
                 EventType::UptimeChanged(uptime) => {
                     state.uptime = Duration::from_secs_f64(uptime);
                 }
-                EventType::Ui(ui_event_type) => handle_ui_event(ui_event_type, &mut state),
             }
 
             // Send updated map to listeners
@@ -540,86 +262,3 @@ pub async fn updater(mut event_rx: EventRx, mut state: State) -> StateRx {
 
     rx
 }
-
-fn handle_ui_event(event: UiEventType, state: &mut State) {
-    match event {
-        UiEventType::Scroll(diff, area) => {
-            state.ui.scroll(diff, area, state.components.len());
-        }
-        UiEventType::ScrollPage(diff, area) => {
-            state.ui.scroll_page(diff, area, state.components.len());
-        }
-        UiEventType::ToggleHelp => {
-            state.ui.help_visible = !state.ui.help_visible;
-            if state.ui.help_visible {
-                state.ui.sort_visible = false;
-                state.ui.filter_visible = false;
-            }
-        }
-        UiEventType::ToggleSortMenu => {
-            state.ui.sort_visible = !state.ui.sort_visible;
-            if state.ui.sort_visible {
-                state.apply_sort_state_to_ui();
-                state.ui.help_visible = false;
-                state.ui.filter_visible = false;
-            }
-        }
-        UiEventType::ToggleSortDirection => state.sort_state.reverse = !state.sort_state.reverse,
-        UiEventType::SortSelection(diff) => {
-            let next = state.ui.sort_menu_state.selected().map_or(0, |s| {
-                s.saturating_add_signed(diff)
-                    .min(SortColumn::items().len() - 1)
-            });
-            state.ui.sort_menu_state.select(Some(next));
-        }
-        UiEventType::SortByColumn(col) => state.sort_state.column = Some(col),
-        UiEventType::SortConfirmation => {
-            if let Some(selected) = state.ui.sort_menu_state.selected() {
-                state.sort_state.column = Some(selected.into())
-            }
-            state.ui.sort_visible = false;
-        }
-        UiEventType::ToggleFilterMenu => {
-            state.ui.filter_visible = !state.ui.filter_visible;
-            if state.ui.filter_visible {
-                state.apply_filter_state_to_ui();
-                state.ui.help_visible = false;
-                state.ui.sort_visible = false;
-            }
-        }
-        UiEventType::FilterColumnSelection(diff) => {
-            let next = state
-                .ui
-                .filter_menu_state
-                .column_selection
-                .selected()
-                .map_or(0, |s| {
-                    s.saturating_add_signed(diff)
-                        .min(FilterColumn::items().len() - 1)
-                });
-            state
-                .ui
-                .filter_menu_state
-                .column_selection
-                .select(Some(next));
-        }
-        UiEventType::FilterInput(c) => {
-            state.ui.filter_menu_state.input.push(c);
-        }
-        UiEventType::FilterBackspace => {
-            let _ = state.ui.filter_menu_state.input.pop();
-        }
-        UiEventType::FilterConfirmation => {
-            if state.ui.filter_menu_state.input.is_empty() {
-                state.filter_state.pattern = None;
-            } else {
-                // display errors (https://github.com/vectordotdev/vector/issues/24620)?
-                state.filter_state.pattern = Regex::new(&state.ui.filter_menu_state.input).ok();
-            }
-            if let Some(selected) = state.ui.filter_menu_state.column_selection.selected() {
-                state.filter_state.column = selected.into()
-            }
-            state.ui.filter_visible = false;
-        }
-    }
-}
diff --git a/lib/vector-vrl-metrics/Cargo.toml b/lib/vector-vrl-metrics/Cargo.toml
index 6f6a30b66..6f3bbb7ea 100644
--- a/lib/vector-vrl-metrics/Cargo.toml
+++ b/lib/vector-vrl-metrics/Cargo.toml
@@ -12,6 +12,5 @@ const-str.workspace = true
 vrl.workspace = true
 vector-core = { path = "../vector-core", default-features = false, features = ["vrl"] }
 vector-common = { path = "../vector-common", default-features = false }
-tokio.workspace = true
-tokio-stream.workspace = true
-vector-vrl-category.workspace = true
+tokio = { version = "1.45.1", default-features = false }
+tokio-stream = { version = "0.1.17", default-features = false }
diff --git a/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs b/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
index 949b2c831..d51c132a9 100644
--- a/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
+++ b/lib/vector-vrl-metrics/src/aggregate_vector_metrics.rs
@@ -1,6 +1,4 @@
 use std::collections::BTreeMap;
-use std::sync::LazyLock;
-use vector_vrl_category::Category;
 use vrl::prelude::expression::Expr;
 use vrl::value;
 
@@ -10,33 +8,6 @@ use crate::common::resolve_tags;
 use crate::common::validate_tags;
 use crate::common::{Error, MetricsStorage};
 
-static DEFAULT_TAGS: LazyLock<Value> = LazyLock::new(|| Value::Object(BTreeMap::new()));
-static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
-    vec![
-        Parameter {
-            keyword: "function",
-            kind: kind::BYTES,
-            required: true,
-            description: "The metric name to search.",
-            default: None,
-        },
-        Parameter {
-            keyword: "key",
-            kind: kind::BYTES,
-            required: true,
-            description: "The metric name to aggregate.",
-            default: None,
-        },
-        Parameter {
-            keyword: "tags",
-            kind: kind::OBJECT,
-            required: false,
-            description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
-            default: Some(&DEFAULT_TAGS),
-        },
-    ]
-});
-
 fn aggregate_metrics(
     metrics_storage: &MetricsStorage,
     function: &Bytes,
@@ -83,16 +54,24 @@ impl Function for AggregateVectorMetrics {
         )
     }
 
-    fn category(&self) -> &'static str {
-        Category::Metrics.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::FLOAT | kind::NULL
-    }
-
     fn parameters(&self) -> &'static [Parameter] {
-        &PARAMETERS
+        &[
+            Parameter {
+                keyword: "function",
+                kind: kind::BYTES,
+                required: true,
+            },
+            Parameter {
+                keyword: "key",
+                kind: kind::BYTES,
+                required: true,
+            },
+            Parameter {
+                keyword: "tags",
+                kind: kind::OBJECT,
+                required: false,
+            },
+        ]
     }
 
     fn examples(&self) -> &'static [Example] {
diff --git a/lib/vector-vrl-metrics/src/find_vector_metrics.rs b/lib/vector-vrl-metrics/src/find_vector_metrics.rs
index 4db90e4df..5ca5535a0 100644
--- a/lib/vector-vrl-metrics/src/find_vector_metrics.rs
+++ b/lib/vector-vrl-metrics/src/find_vector_metrics.rs
@@ -1,5 +1,4 @@
-use std::{collections::BTreeMap, sync::LazyLock};
-use vector_vrl_category::Category;
+use std::collections::BTreeMap;
 use vrl::prelude::expression::Expr;
 
 use vrl::prelude::*;
@@ -23,27 +22,6 @@ fn find_metrics(
     ))
 }
 
-static DEFAULT_TAGS: LazyLock<Value> = LazyLock::new(|| Value::Object(BTreeMap::new()));
-
-static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
-    vec![
-        Parameter {
-            keyword: "key",
-            kind: kind::BYTES,
-            required: true,
-            description: "The metric name to search.",
-            default: None,
-        },
-        Parameter {
-            keyword: "tags",
-            kind: kind::OBJECT,
-            required: false,
-            description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
-            default: Some(&DEFAULT_TAGS),
-        },
-    ]
-});
-
 #[derive(Clone, Copy, Debug)]
 pub struct FindVectorMetrics;
 
@@ -59,16 +37,19 @@ impl Function for FindVectorMetrics {
         )
     }
 
-    fn category(&self) -> &'static str {
-        Category::Metrics.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::ARRAY
-    }
-
     fn parameters(&self) -> &'static [Parameter] {
-        &PARAMETERS
+        &[
+            Parameter {
+                keyword: "key",
+                kind: kind::BYTES,
+                required: true,
+            },
+            Parameter {
+                keyword: "tags",
+                kind: kind::OBJECT,
+                required: false,
+            },
+        ]
     }
 
     fn examples(&self) -> &'static [Example] {
diff --git a/lib/vector-vrl-metrics/src/get_vector_metric.rs b/lib/vector-vrl-metrics/src/get_vector_metric.rs
index 21193b3e8..5f24705a9 100644
--- a/lib/vector-vrl-metrics/src/get_vector_metric.rs
+++ b/lib/vector-vrl-metrics/src/get_vector_metric.rs
@@ -1,6 +1,5 @@
-use std::{collections::BTreeMap, sync::LazyLock};
+use std::collections::BTreeMap;
 
-use vector_vrl_category::Category;
 use vrl::prelude::{expression::Expr, *};
 
 use crate::common::{
@@ -20,27 +19,6 @@ fn get_metric(
     Ok(value)
 }
 
-static DEFAULT_TAGS: LazyLock<Value> = LazyLock::new(|| Value::Object(BTreeMap::new()));
-
-static PARAMETERS: LazyLock<Vec<Parameter>> = LazyLock::new(|| {
-    vec![
-        Parameter {
-            keyword: "key",
-            kind: kind::BYTES,
-            required: true,
-            description: "The metric name to search.",
-            default: None,
-        },
-        Parameter {
-            keyword: "tags",
-            kind: kind::OBJECT,
-            required: false,
-            description: "Tags to filter the results on. Values in this object support wildcards ('*') to match on parts of the tag value.",
-            default: Some(&DEFAULT_TAGS),
-        },
-    ]
-});
-
 #[derive(Clone, Copy, Debug)]
 pub struct GetVectorMetric;
 
@@ -56,16 +34,19 @@ impl Function for GetVectorMetric {
         )
     }
 
-    fn category(&self) -> &'static str {
-        Category::Metrics.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::OBJECT | kind::NULL
-    }
-
     fn parameters(&self) -> &'static [Parameter] {
-        &PARAMETERS
+        &[
+            Parameter {
+                keyword: "key",
+                kind: kind::BYTES,
+                required: true,
+            },
+            Parameter {
+                keyword: "tags",
+                kind: kind::OBJECT,
+                required: false,
+            },
+        ]
     }
 
     fn examples(&self) -> &'static [Example] {
diff --git a/lib/vector-vrl/category/Cargo.toml b/lib/vector-vrl/category/Cargo.toml
deleted file mode 100644
index dafb938de..000000000
--- a/lib/vector-vrl/category/Cargo.toml
+++ /dev/null
@@ -1,10 +0,0 @@
-[package]
-name = "vector-vrl-category"
-version = "0.1.0"
-authors = ["Vector Contributors <vector@datadoghq.com>"]
-edition = "2024"
-publish = false
-license = "MPL-2.0"
-
-[dependencies]
-strum = { version = "0.27", features = ["derive"] }
diff --git a/lib/vector-vrl/category/src/lib.rs b/lib/vector-vrl/category/src/lib.rs
deleted file mode 100644
index bd1df2097..000000000
--- a/lib/vector-vrl/category/src/lib.rs
+++ /dev/null
@@ -1,16 +0,0 @@
-use strum::AsRefStr;
-
-/// Category classification for Vector-specific VRL functions.
-///
-/// This enum complements the categories defined in the VRL stdlib,
-/// providing Vector-specific categories for enrichment, metrics, and event functions.
-#[derive(Debug, Clone, Copy, AsRefStr)]
-#[strum(serialize_all = "PascalCase")]
-pub enum Category {
-    /// Enrichment table operations
-    Enrichment,
-    /// Event metadata and secret management
-    Event,
-    /// Internal Vector metrics operations
-    Metrics,
-}
diff --git a/lib/vector-vrl/functions/Cargo.toml b/lib/vector-vrl/functions/Cargo.toml
index 973c32e56..4e630b951 100644
--- a/lib/vector-vrl/functions/Cargo.toml
+++ b/lib/vector-vrl/functions/Cargo.toml
@@ -12,7 +12,6 @@ vrl.workspace = true
 enrichment = { path = "../../enrichment" }
 dnstap-parser = { path = "../../dnstap-parser", optional = true }
 vector-vrl-metrics = { path = "../../vector-vrl-metrics", optional = true }
-vector-vrl-category.workspace = true
 
 [features]
 default = []
diff --git a/lib/vector-vrl/functions/src/get_secret.rs b/lib/vector-vrl/functions/src/get_secret.rs
index 0f3a787d8..b643d3415 100644
--- a/lib/vector-vrl/functions/src/get_secret.rs
+++ b/lib/vector-vrl/functions/src/get_secret.rs
@@ -1,4 +1,3 @@
-use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 fn get_secret(ctx: &mut Context, key: Value) -> std::result::Result<Value, ExpressionError> {
@@ -22,21 +21,11 @@ impl Function for GetSecret {
         "Returns the value of the given secret from an event."
     }
 
-    fn category(&self) -> &'static str {
-        Category::Event.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::BYTES | kind::NULL
-    }
-
     fn parameters(&self) -> &'static [Parameter] {
         &[Parameter {
             keyword: "key",
             kind: kind::BYTES,
             required: true,
-            description: "The name of the secret.",
-            default: None,
         }]
     }
 
diff --git a/lib/vector-vrl/functions/src/remove_secret.rs b/lib/vector-vrl/functions/src/remove_secret.rs
index 47d5c2004..5bdc75153 100644
--- a/lib/vector-vrl/functions/src/remove_secret.rs
+++ b/lib/vector-vrl/functions/src/remove_secret.rs
@@ -1,4 +1,3 @@
-use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 fn remove_secret(ctx: &mut Context, key: Value) -> std::result::Result<Value, ExpressionError> {
@@ -19,21 +18,11 @@ impl Function for RemoveSecret {
         "Removes a secret from an event."
     }
 
-    fn category(&self) -> &'static str {
-        Category::Event.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::NULL
-    }
-
     fn parameters(&self) -> &'static [Parameter] {
         &[Parameter {
             keyword: "key",
             kind: kind::BYTES,
             required: true,
-            description: "The name of the secret to remove.",
-            default: None,
         }]
     }
 
diff --git a/lib/vector-vrl/functions/src/set_secret.rs b/lib/vector-vrl/functions/src/set_secret.rs
index 4fba86197..5a128b448 100644
--- a/lib/vector-vrl/functions/src/set_secret.rs
+++ b/lib/vector-vrl/functions/src/set_secret.rs
@@ -1,4 +1,3 @@
-use vector_vrl_category::Category;
 use vrl::prelude::*;
 
 fn set_secret(
@@ -26,29 +25,17 @@ impl Function for SetSecret {
         "Sets the given secret in the event."
     }
 
-    fn category(&self) -> &'static str {
-        Category::Event.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::NULL
-    }
-
     fn parameters(&self) -> &'static [Parameter] {
         &[
             Parameter {
                 keyword: "key",
                 kind: kind::BYTES,
                 required: true,
-                description: "The name of the secret.",
-                default: None,
             },
             Parameter {
                 keyword: "secret",
                 kind: kind::BYTES,
                 required: true,
-                description: "The secret value.",
-                default: None,
             },
         ]
     }
diff --git a/lib/vector-vrl/functions/src/set_semantic_meaning.rs b/lib/vector-vrl/functions/src/set_semantic_meaning.rs
index 13fe6239c..19dc0be34 100644
--- a/lib/vector-vrl/functions/src/set_semantic_meaning.rs
+++ b/lib/vector-vrl/functions/src/set_semantic_meaning.rs
@@ -3,7 +3,6 @@ use std::{
     ops::{Deref, DerefMut},
 };
 
-use vector_vrl_category::Category;
 use vrl::{
     diagnostic::Label,
     path::{OwnedTargetPath, PathPrefix},
@@ -47,29 +46,17 @@ impl Function for SetSemanticMeaning {
         "}
     }
 
-    fn category(&self) -> &'static str {
-        Category::Event.as_ref()
-    }
-
-    fn return_kind(&self) -> u16 {
-        kind::NULL
-    }
-
     fn parameters(&self) -> &'static [Parameter] {
         &[
             Parameter {
                 keyword: "target",
                 kind: kind::ANY,
                 required: true,
-                description: "The path of the value that is assigned a meaning.",
-                default: None,
             },
             Parameter {
                 keyword: "meaning",
                 kind: kind::BYTES,
                 required: true,
-                description: "The name of the meaning to assign.",
-                default: None,
             },
         ]
     }
diff --git a/src/config/compiler.rs b/src/config/compiler.rs
index f493f8c84..ecb90f7e3 100644
--- a/src/config/compiler.rs
+++ b/src/config/compiler.rs
@@ -36,7 +36,7 @@ pub fn compile(mut builder: ConfigBuilder) -> Result<(Config, Vec<String>), Vec<
         errors.extend(output_errors);
     }
 
-    if let Err(alpha_errors) = validation::check_values(&builder) {
+    if let Err(alpha_errors) = validation::check_buffer_utilization_ewma_alpha(&builder) {
         errors.extend(alpha_errors);
     }
 
diff --git a/src/config/validation.rs b/src/config/validation.rs
index 35519a93c..0b629ea90 100644
--- a/src/config/validation.rs
+++ b/src/config/validation.rs
@@ -11,28 +11,14 @@ use super::{
 };
 use crate::config::schema;
 
-/// Minimum value (exclusive) for EWMA alpha options.
+/// Minimum value (exclusive) for `utilization_ewma_alpha`.
 /// The alpha value must be strictly greater than this value.
 const EWMA_ALPHA_MIN: f64 = 0.0;
 
-/// Maximum value (exclusive) for EWMA alpha options.
+/// Maximum value (exclusive) for `utilization_ewma_alpha`.
 /// The alpha value must be strictly less than this value.
 const EWMA_ALPHA_MAX: f64 = 1.0;
 
-/// Validates an optional EWMA alpha value and returns an error message if invalid.
-/// Returns `None` if the value is `None` or valid, otherwise returns an error message.
-fn validate_ewma_alpha(alpha: Option<f64>, field_name: &str) -> Option<String> {
-    if let Some(alpha) = alpha
-        && !(alpha > EWMA_ALPHA_MIN && alpha < EWMA_ALPHA_MAX)
-    {
-        Some(format!(
-            "Global `{field_name}` must be between 0 and 1 exclusive (0 < alpha < 1), got {alpha}"
-        ))
-    } else {
-        None
-    }
-}
-
 /// Check that provide + topology config aren't present in the same builder, which is an error.
 pub fn check_provider(config: &ConfigBuilder) -> Result<(), Vec<String>> {
     if config.provider.is_some()
@@ -169,25 +155,17 @@ pub fn check_resources(config: &ConfigBuilder) -> Result<(), Vec<String>> {
     }
 }
 
-/// Validates that `*_ewma_alpha` values are within the valid range (0 < alpha < 1).
-pub fn check_values(config: &ConfigBuilder) -> Result<(), Vec<String>> {
-    let mut errors = Vec::new();
-
-    if let Some(error) = validate_ewma_alpha(
-        config.global.buffer_utilization_ewma_alpha,
-        "buffer_utilization_ewma_alpha",
-    ) {
-        errors.push(error);
-    }
-    if let Some(error) = validate_ewma_alpha(config.global.latency_ewma_alpha, "latency_ewma_alpha")
+/// Validates that `buffer_utilization_ewma_alpha` value is within the valid range (0 < alpha < 1)
+/// for the global configuration.
+pub fn check_buffer_utilization_ewma_alpha(config: &ConfigBuilder) -> Result<(), Vec<String>> {
+    if let Some(alpha) = config.global.buffer_utilization_ewma_alpha
+        && (alpha <= EWMA_ALPHA_MIN || alpha >= EWMA_ALPHA_MAX)
     {
-        errors.push(error);
-    }
-
-    if errors.is_empty() {
-        Ok(())
+        Err(vec![format!(
+            "Global `buffer_utilization_ewma_alpha` must be between 0 and 1 exclusive (0 < alpha < 1), got {alpha}"
+        )])
     } else {
-        Err(errors)
+        Ok(())
     }
 }
 
diff --git a/src/providers/http.rs b/src/providers/http.rs
index 975db2e17..c7dfa043e 100644
--- a/src/providers/http.rs
+++ b/src/providers/http.rs
@@ -10,7 +10,7 @@ use vector_lib::configurable::configurable_component;
 
 use super::BuildResult;
 use crate::{
-    config::{self, Format, ProxyConfig, interpolate, provider::ProviderConfig},
+    config::{self, Format, ProxyConfig, provider::ProviderConfig},
     http::HttpClient,
     signal,
     tls::{TlsConfig, TlsSettings},
@@ -57,9 +57,6 @@ pub struct HttpConfig {
     /// Which config format expected to be loaded
     #[configurable(derived)]
     config_format: Format,
-
-    /// Enable environment variable interpolation
-    interpolate_env: bool,
 }
 
 impl Default for HttpConfig {
@@ -71,7 +68,6 @@ impl Default for HttpConfig {
             tls_options: None,
             proxy: Default::default(),
             config_format: Format::default(),
-            interpolate_env: false,
         }
     }
 }
@@ -139,31 +135,12 @@ async fn http_request_to_config_builder(
     headers: &IndexMap<String, String>,
     proxy: &ProxyConfig,
     config_format: &Format,
-    interpolate_env: bool,
 ) -> BuildResult {
     let config_str = http_request(url, tls_options, headers, proxy)
         .await
         .map_err(|e| vec![e.to_owned()])?;
 
-    if !interpolate_env {
-        return config::load(config_str.chunk(), *config_format);
-    }
-
-    let env_vars = std::env::vars_os()
-        .map(|(k, v)| {
-            (
-                k.as_os_str().to_string_lossy().to_string(),
-                v.as_os_str().to_string_lossy().to_string(),
-            )
-        })
-        .collect::<std::collections::HashMap<String, String>>();
-
-    let config_str = interpolate(
-        std::str::from_utf8(&config_str).map_err(|e| vec![e.to_string()])?,
-        &env_vars,
-    )?;
-
-    config::load(config_str.as_bytes().chunk(), *config_format)
+    config::load(config_str.chunk(), *config_format)
 }
 
 /// Polls the HTTP endpoint after/every `poll_interval_secs`, returning a stream of `ConfigBuilder`.
@@ -174,7 +151,6 @@ fn poll_http(
     headers: IndexMap<String, String>,
     proxy: ProxyConfig,
     config_format: Format,
-    interpolate_env: bool,
 ) -> impl Stream<Item = signal::SignalTo> {
     let duration = time::Duration::from_secs(poll_interval_secs);
     let mut interval = time::interval_at(time::Instant::now() + duration, duration);
@@ -183,7 +159,7 @@ fn poll_http(
         loop {
             interval.tick().await;
 
-            match http_request_to_config_builder(&url, tls_options.as_ref(), &headers, &proxy, &config_format, interpolate_env).await {
+            match http_request_to_config_builder(&url, tls_options.as_ref(), &headers, &proxy, &config_format).await {
                 Ok(config_builder) => yield signal::SignalTo::ReloadFromConfigBuilder(config_builder),
                 Err(_) => {},
             };
@@ -215,7 +191,6 @@ impl ProviderConfig for HttpConfig {
             &request.headers,
             &proxy,
             &config_format,
-            self.interpolate_env,
         )
         .await?;
 
@@ -227,7 +202,6 @@ impl ProviderConfig for HttpConfig {
             request.headers.clone(),
             proxy.clone(),
             config_format,
-            self.interpolate_env,
         ));
 
         Ok(config_builder)
diff --git a/src/sinks/azure_logs_ingestion/config.rs b/src/sinks/azure_logs_ingestion/config.rs
new file mode 100644
index 000000000..5c1249363
--- /dev/null
+++ b/src/sinks/azure_logs_ingestion/config.rs
@@ -0,0 +1,163 @@
+
+use std::sync::Arc;
+
+use azure_core::auth::TokenCredential;
+use vector_lib::configurable::configurable_component;
+use vector_lib::schema;
+use vrl::value::Kind;
+
+use crate::{
+    http::{get_http_scheme_from_uri, HttpClient},
+    sinks::{
+        prelude::*,
+        util::{http::HttpStatusRetryLogic, RealtimeSizeBasedDefaultBatchSettings, UriSerde},
+    },
+};
+
+use super::{
+    service::{AzureLogsIngestionResponse, AzureLogsIngestionService},
+    sink::AzureLogsIngestionSink,
+};
+
+/// Max number of bytes in request body
+const MAX_BATCH_SIZE: usize = 30 * 1024 * 1024;
+
+// Log Ingestion API version
+// const API_VERSION: &str = "2023-01-01";
+
+/// Configuration for the `azure_logs_ingestion` sink.
+#[configurable_component(sink(
+    "azure_logs_ingestion",
+    "Publish log events to the Azure Logs Ingestion API."
+))]
+#[derive(Clone, Debug)]
+#[serde(deny_unknown_fields)]
+pub struct AzureLogsIngestionConfig {
+    /// The [Data collection endpoint URI][endpoint] associated with the Log Analytics workspace.
+    ///
+    /// [endpoint]: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview
+    #[configurable(metadata(docs::examples = "https://my-dce-5kyl.eastus-1.ingest.monitor.azure.com"))]
+    pub endpoint: String,
+
+    /// The [Data collection rule][dcr] for the Data collection endpoint.
+    ///
+    /// [dcr]: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview
+    #[configurable(metadata(docs::examples = "dcr-000a00a000a00000a000000aa000a0aa"))]
+    pub dcr: String,
+
+    /// The [Stream name][stream_name] for the Data collection rule.
+    ///
+    /// [stream_name]: https://learn.microsoft.com/en-us/azure/azure-monitor/logs/logs-ingestion-api-overview
+    #[configurable(metadata(docs::examples = "Custom-MyTable"))]
+    pub stream_name: String,
+
+    #[configurable(derived)]
+    #[serde(default, skip_serializing_if = "crate::serde::is_default")]
+    pub encoding: Transformer,
+
+    #[configurable(derived)]
+    #[serde(default)]
+    pub batch: BatchConfig<RealtimeSizeBasedDefaultBatchSettings>,
+
+    #[configurable(derived)]
+    #[serde(default)]
+    pub request: TowerRequestConfig,
+
+    #[configurable(derived)]
+    pub tls: Option<TlsConfig>,
+
+    #[configurable(derived)]
+    #[serde(
+        default,
+        deserialize_with = "crate::serde::bool_or_struct",
+        skip_serializing_if = "crate::serde::is_default"
+    )]
+    pub acknowledgements: AcknowledgementsConfig,
+}
+
+impl Default for AzureLogsIngestionConfig {
+    fn default() -> Self {
+        Self {
+            endpoint: Default::default(),
+            dcr: Default::default(),
+            stream_name: Default::default(),
+            encoding: Default::default(),
+            batch: Default::default(),
+            request: Default::default(),
+            tls: None,
+            acknowledgements: Default::default(),
+        }
+    }
+}
+
+impl AzureLogsIngestionConfig {
+
+    pub(super) async fn build_inner(
+        &self,
+        cx: SinkContext,
+        endpoint: UriSerde,
+    ) -> crate::Result<(VectorSink, Healthcheck)> {
+        let endpoint = endpoint.with_default_parts().uri;
+        let protocol = get_http_scheme_from_uri(&endpoint).to_string();
+
+        let batch_settings = self
+            .batch
+            .validate()?
+            .limit_max_bytes(MAX_BATCH_SIZE)?
+            .into_batcher_settings()?;
+
+        // TODO will need to change this as part of upstream 0.20.0
+        // https://github.com/Azure/azure-sdk-for-rust/blob/main/sdk/identity/azure_identity/CHANGELOG.md
+        let credential: Arc<dyn TokenCredential> = azure_identity::create_credential()?;
+
+        let tls_settings = TlsSettings::from_options(&self.tls)?;
+        let client = HttpClient::new(Some(tls_settings), &cx.proxy)?;
+
+        let service = AzureLogsIngestionService::new(
+            client,
+            endpoint,
+            credential,
+        )?;
+        let healthcheck = service.healthcheck();
+
+        let retry_logic =
+            HttpStatusRetryLogic::new(|res: &AzureLogsIngestionResponse| res.http_status);
+        let request_settings = self.request.into_settings();
+        let service = ServiceBuilder::new()
+            .settings(request_settings, retry_logic)
+            .service(service);
+
+        let sink = AzureLogsIngestionSink::new(
+            batch_settings,
+            self.encoding.clone(),
+            service,
+            protocol,
+        );
+
+        Ok((VectorSink::from_event_streamsink(sink), healthcheck))
+    }
+}
+
+impl_generate_config_from_default!(AzureLogsIngestionConfig);
+
+#[async_trait::async_trait]
+#[typetag::serde(name = "azure_logs_ingestion")]
+impl SinkConfig for AzureLogsIngestionConfig {
+    async fn build(&self, cx: SinkContext) -> crate::Result<(VectorSink, Healthcheck)> {
+        // https://my-dce-5kyl.eastus-1.ingest.monitor.azure.com/dataCollectionRules/dcr-000a00a000a00000a000000aa000a0aa/streams/Custom-MyTable?api-version=2023-01-01
+        // let endpoint = format!("{}/dataCollectionRules/{}/streams/{}", self.endpoint, self.dcr, self.stream_name).parse()?;
+        let endpoint = self.endpoint.parse()?;
+        self.build_inner(cx, endpoint).await
+    }
+
+    fn input(&self) -> Input {
+        let requirements =
+            schema::Requirement::empty().optional_meaning("timestamp", Kind::timestamp());
+
+        Input::log().with_schema_requirement(requirements)
+    }
+
+    fn acknowledgements(&self) -> &AcknowledgementsConfig {
+        &self.acknowledgements
+    }
+}
diff --git a/src/sinks/azure_logs_ingestion/mod.rs b/src/sinks/azure_logs_ingestion/mod.rs
new file mode 100644
index 000000000..dddcfc6fa
--- /dev/null
+++ b/src/sinks/azure_logs_ingestion/mod.rs
@@ -0,0 +1,13 @@
+//! The Azure Logs Ingestion [`vector_lib::sink::VectorSink`]
+//!
+//! This module contains the [`vector_lib::sink::VectorSink`] instance that is responsible for
+//! taking a stream of [`vector_lib::event::Event`] instances and forwarding them to the Azure
+//! Logs Ingestion API.
+
+mod config;
+mod service;
+mod sink;
+// #[cfg(test)]
+// mod tests;
+
+pub use config::AzureLogsIngestionConfig;
diff --git a/src/sinks/azure_logs_ingestion/service.rs b/src/sinks/azure_logs_ingestion/service.rs
new file mode 100644
index 000000000..9a7d99eca
--- /dev/null
+++ b/src/sinks/azure_logs_ingestion/service.rs
@@ -0,0 +1,192 @@
+use std::sync::LazyLock;
+use std::task::{Context, Poll};
+use std::sync::Arc;
+use futures::executor;
+
+use azure_core::auth::TokenCredential;
+
+use bytes::Bytes;
+use http::{
+    header::{self, HeaderMap},
+    HeaderValue, Request, StatusCode, Uri,
+};
+use hyper::Body;
+use tracing::Instrument;
+
+use crate::{http::HttpClient, sinks::prelude::*};
+
+// JSON content type of logs
+const CONTENT_TYPE: &str = "application/json";
+
+static CONTENT_TYPE_VALUE: LazyLock<HeaderValue> =
+    LazyLock::new(|| HeaderValue::from_static(CONTENT_TYPE));
+// static X_MS_CLIENT_REQUEST_ID_HEADER: LazyLock<HeaderName> =
+//     LazyLock::new(|| HeaderName::from_static("x-ms-client-request-id"));
+
+#[derive(Debug, Clone)]
+pub struct AzureLogsIngestionRequest {
+    pub body: Bytes,
+    pub finalizers: EventFinalizers,
+    pub metadata: RequestMetadata,
+}
+
+impl MetaDescriptive for AzureLogsIngestionRequest {
+    fn get_metadata(&self) -> &RequestMetadata {
+        &self.metadata
+    }
+
+    fn metadata_mut(&mut self) -> &mut RequestMetadata {
+        &mut self.metadata
+    }
+}
+
+impl Finalizable for AzureLogsIngestionRequest {
+    fn take_finalizers(&mut self) -> EventFinalizers {
+        self.finalizers.take_finalizers()
+    }
+}
+
+pub struct AzureLogsIngestionResponse {
+    pub http_status: StatusCode,
+    pub events_byte_size: GroupedCountByteSize,
+    pub raw_byte_size: usize,
+}
+
+impl DriverResponse for AzureLogsIngestionResponse {
+    fn event_status(&self) -> EventStatus {
+        match self.http_status.is_success() {
+            true => EventStatus::Delivered,
+            false => EventStatus::Rejected,
+        }
+    }
+
+    fn events_sent(&self) -> &GroupedCountByteSize {
+        &self.events_byte_size
+    }
+
+    fn bytes_sent(&self) -> Option<usize> {
+        Some(self.raw_byte_size)
+    }
+}
+
+/// `AzureLogsIngestionService` is a `Tower` service used to send logs to Azure.
+#[derive(Debug, Clone)]
+pub struct AzureLogsIngestionService {
+    client: HttpClient,
+    endpoint: Uri,
+    credential: Arc<dyn TokenCredential>,
+    default_headers: HeaderMap,
+}
+
+impl AzureLogsIngestionService {
+    /// Creates a new `AzureLogsIngestionService`.
+    pub fn new(
+        client: HttpClient,
+        endpoint: Uri,
+        credential: Arc<dyn TokenCredential>,
+    ) -> crate::Result<Self> {
+        // let mut parts = endpoint.into_parts();
+        // parts.path_and_query = Some(
+        //     format!("a9ee8e5b-ed0e-4980-9b9c-15e1f939db7f?api-version={API_VERSION}")
+        //         .parse()
+        //         .expect("query should never fail to parse"),
+        // );
+        // let endpoint = Uri::from_parts(parts)?;
+
+        let default_headers = {
+            let mut headers = HeaderMap::new();
+
+            headers.insert(header::CONTENT_TYPE, CONTENT_TYPE_VALUE.clone());
+            headers
+        };
+
+        Ok(Self {
+            client,
+            endpoint,
+            credential,
+            default_headers,
+        })
+    }
+
+    fn build_request(&self, body: Bytes) -> crate::Result<Request<Body>> {
+        let mut request = Request::post(&self.endpoint).body(Body::from(body))?;
+
+        // TODO: make this an option, for soverign clouds
+        let access_token = executor::block_on(self.credential
+            .get_token(&["https://monitor.azure.com/.default"]))
+            .expect("failed to get access token from credential");
+        
+        let bearer = format!("Bearer {}", access_token.token.secret());
+
+        *request.headers_mut() = self.default_headers.clone();
+        request
+            .headers_mut()
+            .insert(
+                header::AUTHORIZATION,
+                HeaderValue::from_str(&bearer).unwrap()
+            );
+
+        Ok(request)
+    }
+
+    pub fn healthcheck(&self) -> Healthcheck {
+        let mut client = self.client.clone();
+        let request = self.build_request(Bytes::from("[]"));
+        Box::pin(async move {
+            let request = request?;
+            let res = client.call(request).in_current_span().await?;
+
+            if res.status().is_server_error() {
+                return Err("Server returned a server error".into());
+            }
+
+            if res.status() == StatusCode::FORBIDDEN {
+                return Err("The service failed to authenticate the request. Verify that the workspace ID and connection key are valid".into());
+            }
+
+            if res.status() == StatusCode::NOT_FOUND {
+                return Err(
+                    "Either the URL provided is incorrect, or the request is too large".into(),
+                );
+            }
+
+            if res.status() == StatusCode::BAD_REQUEST {
+                return Err("The workspace has been closed or the request was invalid".into());
+            }
+
+            Ok(())
+        })
+    }
+}
+
+impl Service<AzureLogsIngestionRequest> for AzureLogsIngestionService {
+    type Response = AzureLogsIngestionResponse;
+    type Error = crate::Error;
+    type Future = BoxFuture<'static, Result<Self::Response, Self::Error>>;
+
+    // Emission of Error internal event is handled upstream by the caller.
+    fn poll_ready(&mut self, _cx: &mut Context) -> Poll<Result<(), Self::Error>> {
+        Poll::Ready(Ok(()))
+    }
+
+    // Emission of Error internal event is handled upstream by the caller.
+    fn call(&mut self, request: AzureLogsIngestionRequest) -> Self::Future {
+        let mut client = self.client.clone();
+        let http_request = self.build_request(request.body);
+        Box::pin(async move {
+            let http_request = http_request?;
+            let response = client.call(http_request).in_current_span().await?;
+            let response_status = response.status();
+            // let body_bytes: Bytes = hyper::body::to_bytes(response.into_body()).await.unwrap();
+            // let body_string: String = String::from_utf8(body_bytes.to_vec()).unwrap();
+            // println!("response: {}", body_string);
+            Ok(AzureLogsIngestionResponse {
+                http_status: response_status,
+                raw_byte_size: request.metadata.request_encoded_size(),
+                events_byte_size: request
+                    .metadata
+                    .into_events_estimated_json_encoded_byte_size(),
+            })
+        })
+    }
+}
diff --git a/src/sinks/azure_logs_ingestion/sink.rs b/src/sinks/azure_logs_ingestion/sink.rs
new file mode 100644
index 000000000..e86e1d27b
--- /dev/null
+++ b/src/sinks/azure_logs_ingestion/sink.rs
@@ -0,0 +1,151 @@
+use std::{fmt::Debug, io};
+
+use bytes::Bytes;
+use vector_lib::codecs::{encoding::Framer, CharacterDelimitedEncoder, JsonSerializerConfig};
+
+use crate::sinks::prelude::*;
+
+use super::service::AzureLogsIngestionRequest;
+
+pub struct AzureLogsIngestionSink<S> {
+    batch_settings: BatcherSettings,
+    encoding: JsonEncoding,
+    service: S,
+    protocol: String,
+}
+
+impl<S> AzureLogsIngestionSink<S>
+where
+    S: Service<AzureLogsIngestionRequest> + Send + 'static,
+    S::Future: Send + 'static,
+    S::Response: DriverResponse + Send + 'static,
+    S::Error: Debug + Into<crate::Error> + Send,
+{
+    pub fn new(
+        batch_settings: BatcherSettings,
+        transformer: Transformer,
+        service: S,
+        protocol: String,
+    ) -> Self {
+        Self {
+            batch_settings,
+            encoding: JsonEncoding::new(transformer),
+            service,
+            protocol,
+        }
+    }
+
+    async fn run_inner(self: Box<Self>, input: BoxStream<'_, Event>) -> Result<(), ()> {
+        input
+            .batched(self.batch_settings.as_byte_size_config())
+            .request_builder(
+                default_request_builder_concurrency_limit(),
+                AzureLogsIngestionRequestBuilder {
+                    encoding: self.encoding,
+                },
+            )
+            .filter_map(|request| async {
+                match request {
+                    Err(error) => {
+                        emit!(SinkRequestBuildError { error });
+                        None
+                    }
+                    Ok(req) => Some(req),
+                }
+            })
+            .into_driver(self.service)
+            .protocol(self.protocol.clone())
+            .run()
+            .await
+    }
+}
+
+#[async_trait::async_trait]
+impl<S> StreamSink<Event> for AzureLogsIngestionSink<S>
+where
+    S: Service<AzureLogsIngestionRequest> + Send + 'static,
+    S::Future: Send + 'static,
+    S::Response: DriverResponse + Send + 'static,
+    S::Error: Debug + Into<crate::Error> + Send,
+{
+    async fn run(
+        self: Box<Self>,
+        input: futures_util::stream::BoxStream<'_, Event>,
+    ) -> Result<(), ()> {
+        self.run_inner(input).await
+    }
+}
+
+/// Customized encoding specific to the Azure Logs Ingestion sink, as the API does not support full
+/// 9-digit nanosecond precision timestamps.
+#[derive(Clone, Debug)]
+pub(super) struct JsonEncoding {
+    encoder: (Transformer, Encoder<Framer>),
+}
+
+impl JsonEncoding {
+    pub fn new(transformer: Transformer) -> Self {
+        Self {
+            encoder: (
+                transformer,
+                Encoder::<Framer>::new(
+                    CharacterDelimitedEncoder::new(b',').into(),
+                    JsonSerializerConfig::default().build().into(),
+                ),
+            ),
+        }
+    }
+}
+
+impl crate::sinks::util::encoding::Encoder<Vec<Event>> for JsonEncoding {
+    fn encode_input(
+        &self,
+        input: Vec<Event>,
+        writer: &mut dyn io::Write,
+    ) -> io::Result<(usize, GroupedCountByteSize)> {
+        self.encoder.encode_input(input, writer)
+    }
+}
+
+struct AzureLogsIngestionRequestBuilder {
+    encoding: JsonEncoding,
+}
+
+impl RequestBuilder<Vec<Event>> for AzureLogsIngestionRequestBuilder {
+    type Metadata = EventFinalizers;
+    type Events = Vec<Event>;
+    type Encoder = JsonEncoding;
+    type Payload = Bytes;
+    type Request = AzureLogsIngestionRequest;
+    type Error = std::io::Error;
+
+    fn compression(&self) -> Compression {
+        Compression::None
+    }
+
+    fn encoder(&self) -> &Self::Encoder {
+        &self.encoding
+    }
+
+    fn split_input(
+        &self,
+        mut events: Vec<Event>,
+    ) -> (Self::Metadata, RequestMetadataBuilder, Self::Events) {
+        let finalizers = events.take_finalizers();
+        let builder = RequestMetadataBuilder::from_events(&events);
+        (finalizers, builder, events)
+    }
+
+    fn build_request(
+        &self,
+        finalizers: Self::Metadata,
+        request_metadata: RequestMetadata,
+        payload: EncodeResult<Self::Payload>,
+    ) -> Self::Request {
+        AzureLogsIngestionRequest {
+            body: payload.into_payload(),
+            finalizers,
+            metadata: request_metadata,
+        }
+    }
+}
diff --git a/src/sinks/clickhouse/arrow/parser.rs b/src/sinks/clickhouse/arrow/parser.rs
index ddafd0d0d..a13bd8234 100644
--- a/src/sinks/clickhouse/arrow/parser.rs
+++ b/src/sinks/clickhouse/arrow/parser.rs
@@ -1,18 +1,6 @@
 //! ClickHouse type parsing and conversion to Arrow types.
 
-use std::str::FromStr;
-
-use arrow::datatypes::{DataType, Field, Fields, TimeUnit};
-use itertools::Itertools;
-use nom::{
-    IResult, Parser,
-    bytes::complete::{tag, take_till, take_while1},
-    character::complete::{char, i8 as parse_i8, u8 as parse_u8, u32 as parse_u32},
-    combinator::{all_consuming, cut, opt},
-    error::{Error, ErrorKind},
-    multi::separated_list0,
-    sequence::{delimited, preceded, separated_pair, terminated},
-};
+use arrow::datatypes::{DataType, TimeUnit};
 
 const DECIMAL32_PRECISION: u8 = 9;
 const DECIMAL64_PRECISION: u8 = 18;
@@ -21,270 +9,264 @@ const DECIMAL256_PRECISION: u8 = 76;
 
 /// Represents a ClickHouse type with its modifiers and nested structure.
 #[derive(Debug, PartialEq, Clone)]
-pub enum ClickHouseType {
-    // Numeric types
-    Int8,
-    Int16,
-    Int32,
-    Int64,
-    UInt8,
-    UInt16,
-    UInt32,
-    UInt64,
-    Float32,
-    Float64,
-    Bool,
-
-    // Decimal with precision and scale
-    Decimal { precision: u8, scale: i8 },
-
-    // String types
-    String,
-    FixedString(u32),
-
-    // Date/time types
-    Date,
-    DateTime,
-    DateTime64 { precision: u8 },
-
-    // Wrapper types
-    Nullable(Box<ClickHouseType>),
-    LowCardinality(Box<ClickHouseType>),
-    Array(Box<ClickHouseType>),
-    Tuple(Vec<(Option<String>, ClickHouseType)>),
-    Map(Box<ClickHouseType>, Box<ClickHouseType>),
+pub enum ClickHouseType<'a> {
+    /// A primitive type like String, Int64, DateTime, etc.
+    Primitive(&'a str),
+    /// Nullable(T)
+    Nullable(Box<ClickHouseType<'a>>),
+    /// LowCardinality(T)
+    LowCardinality(Box<ClickHouseType<'a>>),
 }
 
-impl ClickHouseType {
+impl<'a> ClickHouseType<'a> {
     /// Returns true if this type or any of its nested types is Nullable.
     pub fn is_nullable(&self) -> bool {
         match self {
-            Self::Nullable(_) => true,
-            Self::LowCardinality(inner) => inner.is_nullable(),
+            ClickHouseType::Nullable(_) => true,
+            ClickHouseType::LowCardinality(inner) => inner.is_nullable(),
             _ => false,
         }
     }
 
-    /// Converts this ClickHouse type to an Arrow DataType.
-    /// Recursively handles nested types including Nullable/LowCardinality wrappers.
-    fn to_data_type(&self) -> Result<DataType, String> {
+    /// Returns the innermost base type, unwrapping all modifiers.
+    /// For example: LowCardinality(Nullable(String)) -> Primitive("String")
+    pub fn base_type(&self) -> &ClickHouseType<'a> {
         match self {
-            // Wrapper types - recurse to inner type
-            Self::Nullable(inner) | Self::LowCardinality(inner) => inner.to_data_type(),
-
-            // Numeric types
-            Self::Int8 => Ok(DataType::Int8),
-            Self::Int16 => Ok(DataType::Int16),
-            Self::Int32 => Ok(DataType::Int32),
-            Self::Int64 => Ok(DataType::Int64),
-            Self::UInt8 => Ok(DataType::UInt8),
-            Self::UInt16 => Ok(DataType::UInt16),
-            Self::UInt32 => Ok(DataType::UInt32),
-            Self::UInt64 => Ok(DataType::UInt64),
-            Self::Float32 => Ok(DataType::Float32),
-            Self::Float64 => Ok(DataType::Float64),
-            Self::Bool => Ok(DataType::Boolean),
-
-            // Decimal
-            Self::Decimal { precision, scale } => Ok(if *precision <= DECIMAL128_PRECISION {
-                DataType::Decimal128(*precision, *scale)
-            } else {
-                DataType::Decimal256(*precision, *scale)
-            }),
-
-            // String types
-            Self::String | Self::FixedString(_) => Ok(DataType::Utf8),
-
-            // Date/time types
-            Self::Date => Ok(DataType::Date32),
-            Self::DateTime => Ok(DataType::Timestamp(TimeUnit::Second, None)),
-            Self::DateTime64 { precision } => {
-                let unit = match precision {
-                    0 => TimeUnit::Second,
-                    1..=3 => TimeUnit::Millisecond,
-                    4..=6 => TimeUnit::Microsecond,
-                    7..=9 => TimeUnit::Nanosecond,
-                    _ => {
-                        return Err(format!(
-                            "Unsupported DateTime64 precision {}. Must be 0-9",
-                            precision
-                        ));
-                    }
-                };
-                Ok(DataType::Timestamp(unit, None))
-            }
-
-            // Container types
-            Self::Array(inner) => {
-                let (inner_arrow, inner_nullable) = inner.as_ref().try_into()?;
-                Ok(DataType::List(
-                    Field::new("item", inner_arrow, inner_nullable).into(),
-                ))
-            }
-            Self::Tuple(elements) => {
-                let fields: Vec<Field> = elements
-                    .iter()
-                    .enumerate()
-                    .map(|(i, (name_opt, elem))| {
-                        let (dt, nullable) = elem.try_into()?;
-                        let name = name_opt.clone().unwrap_or_else(|| format!("f{i}"));
-                        Ok::<_, String>(Field::new(name, dt, nullable))
-                    })
-                    .try_collect()?;
-                Ok(DataType::Struct(Fields::from(fields)))
-            }
-            Self::Map(key_type, value_type) => {
-                let (key_arrow, _): (DataType, bool) = key_type.as_ref().try_into()?;
-                if !matches!(key_arrow, DataType::Utf8) {
-                    return Err("Map keys must be String type.".to_string());
-                }
-                let (value_arrow, value_nullable) = value_type.as_ref().try_into()?;
-                let entries = DataType::Struct(Fields::from(vec![
-                    Field::new("keys", DataType::Utf8, false),
-                    Field::new("values", value_arrow, value_nullable),
-                ]));
-                Ok(DataType::Map(
-                    Field::new("entries", entries, false).into(),
-                    false,
-                ))
+            ClickHouseType::Nullable(inner) | ClickHouseType::LowCardinality(inner) => {
+                inner.base_type()
             }
+            _ => self,
         }
     }
 }
 
-impl TryFrom<&ClickHouseType> for (DataType, bool) {
-    type Error = String;
+/// Parses a ClickHouse type string into a structured representation.
+pub fn parse_ch_type(ty: &str) -> ClickHouseType<'_> {
+    let ty = ty.trim();
 
-    fn try_from(ch_type: &ClickHouseType) -> Result<Self, Self::Error> {
-        Ok((ch_type.to_data_type()?, ch_type.is_nullable()))
+    // Recursively strip and parse type modifiers
+    if let Some(inner) = strip_wrapper(ty, "Nullable") {
+        return ClickHouseType::Nullable(Box::new(parse_ch_type(inner)));
+    }
+    if let Some(inner) = strip_wrapper(ty, "LowCardinality") {
+        return ClickHouseType::LowCardinality(Box::new(parse_ch_type(inner)));
     }
+
+    // Base case: return primitive type for anything without modifiers
+    ClickHouseType::Primitive(ty)
 }
 
-/// Wraps a parser in parentheses with cut (no backtracking after open paren).
-fn parens<'a, O>(
-    inner: impl Parser<&'a str, Output = O, Error = nom::error::Error<&'a str>>,
-) -> impl Parser<&'a str, Output = O, Error = nom::error::Error<&'a str>> {
-    delimited(char('('), cut(inner), char(')'))
+/// Helper function to strip a wrapper from a type string.
+/// Returns the inner content if the type matches the wrapper pattern.
+fn strip_wrapper<'a>(ty: &'a str, wrapper_name: &str) -> Option<&'a str> {
+    ty.strip_prefix(wrapper_name)?
+        .trim_start()
+        .strip_prefix('(')?
+        .strip_suffix(')')
 }
 
-/// Parses an identifier (alphanumeric + underscore, at least one char).
-fn identifier(input: &str) -> IResult<&str, &str> {
-    take_while1(|c: char| c.is_alphanumeric() || c == '_')(input)
+/// Unwraps ClickHouse type modifiers like Nullable() and LowCardinality().
+/// Returns a tuple of (base_type, is_nullable).
+/// For example: "LowCardinality(Nullable(String))" -> ("String", true)
+pub fn unwrap_type_modifiers(ch_type: &str) -> (&str, bool) {
+    let parsed = parse_ch_type(ch_type);
+    let is_nullable = parsed.is_nullable();
+
+    match parsed.base_type() {
+        ClickHouseType::Primitive(base) => (base, is_nullable),
+        _ => (ch_type, is_nullable),
+    }
+}
+
+fn unsupported(ch_type: &str, kind: &str) -> String {
+    format!(
+        "{kind} type '{ch_type}' is not supported. \
+         ClickHouse {kind} types cannot be automatically converted to Arrow format."
+    )
 }
 
-/// Parses a single tuple element (either "Type" or "name Type").
-fn tuple_element(input: &str) -> IResult<&str, (Option<String>, ClickHouseType)> {
-    let (rest, name) = identifier(input)?;
-    match rest.strip_prefix(' ') {
-        Some(after_space) => {
-            let (rest, ty) = ch_type(after_space)?;
-            Ok((rest, (Some(name.to_owned()), ty)))
+/// Converts a ClickHouse type string to an Arrow DataType.
+/// Returns a tuple of (DataType, is_nullable).
+pub fn clickhouse_type_to_arrow(ch_type: &str) -> Result<(DataType, bool), String> {
+    let (base_type, is_nullable) = unwrap_type_modifiers(ch_type);
+    let (type_name, _) = extract_identifier(base_type);
+
+    let data_type = match type_name {
+        // Numeric
+        "Int8" => DataType::Int8,
+        "Int16" => DataType::Int16,
+        "Int32" => DataType::Int32,
+        "Int64" => DataType::Int64,
+        "UInt8" => DataType::UInt8,
+        "UInt16" => DataType::UInt16,
+        "UInt32" => DataType::UInt32,
+        "UInt64" => DataType::UInt64,
+        "Float32" => DataType::Float32,
+        "Float64" => DataType::Float64,
+        "Bool" => DataType::Boolean,
+        "Decimal" | "Decimal32" | "Decimal64" | "Decimal128" | "Decimal256" => {
+            parse_decimal_type(base_type)?
+        }
+
+        // Strings
+        "String" | "FixedString" => DataType::Utf8,
+
+        // Date and time types (timezones not currently handled, defaults to UTC)
+        "Date" | "Date32" => DataType::Date32,
+        "DateTime" => DataType::Timestamp(TimeUnit::Second, None),
+        "DateTime64" => parse_datetime64_precision(base_type)?,
+
+        // Unsupported
+        "Array" => return Err(unsupported(ch_type, "Array")),
+        "Tuple" => return Err(unsupported(ch_type, "Tuple")),
+        "Map" => return Err(unsupported(ch_type, "Map")),
+
+        // Unknown
+        _ => {
+            return Err(format!(
+                "Unknown ClickHouse type '{}'. This type cannot be automatically converted.",
+                type_name
+            ));
         }
-        None => {
-            // No space after identifier, so re-parse as a type
-            let (rest, ty) = ch_type(input)?;
-            Ok((rest, (None, ty)))
+    };
+
+    Ok((data_type, is_nullable))
+}
+
+/// Extracts an identifier from the start of a string.
+/// Returns (identifier, remaining_string).
+fn extract_identifier(input: &str) -> (&str, &str) {
+    for (i, c) in input.char_indices() {
+        if c.is_alphabetic() || c == '_' || (i > 0 && c.is_numeric()) {
+            continue;
         }
+        return (&input[..i], &input[i..]);
     }
+    (input, "")
 }
 
-/// Parses a complete ClickHouse type.
-///
-/// Nom parsers return `(rest, output)` where `rest` is the remaining unparsed input.
-/// For example, parsing `"Array(String)"`:
-///   - `identifier` consumes `"Array"`, returns `rest = "(String)"`, `name = "Array"`
-///   - The `"Array"` match arm then parses `rest` with `parens(ch_type)`
-fn ch_type(input: &str) -> IResult<&str, ClickHouseType> {
-    let (rest, name) = identifier(input)?;
-
-    match name {
-        // Wrapper types
-        "Nullable" => parens(ch_type)
-            .map(|t| ClickHouseType::Nullable(Box::new(t)))
-            .parse(rest),
-        "LowCardinality" => parens(ch_type)
-            .map(|t| ClickHouseType::LowCardinality(Box::new(t)))
-            .parse(rest),
-        "Array" => parens(ch_type)
-            .map(|t| ClickHouseType::Array(Box::new(t)))
-            .parse(rest),
-        "Map" => parens(separated_pair(ch_type, tag(", "), ch_type))
-            .map(|(k, v)| ClickHouseType::Map(Box::new(k), Box::new(v)))
-            .parse(rest),
-        "Tuple" => parens(separated_list0(tag(", "), tuple_element))
-            .map(ClickHouseType::Tuple)
-            .parse(rest),
-
-        // Numeric types
-        "Int8" => Ok((rest, ClickHouseType::Int8)),
-        "Int16" => Ok((rest, ClickHouseType::Int16)),
-        "Int32" => Ok((rest, ClickHouseType::Int32)),
-        "Int64" => Ok((rest, ClickHouseType::Int64)),
-        "UInt8" => Ok((rest, ClickHouseType::UInt8)),
-        "UInt16" => Ok((rest, ClickHouseType::UInt16)),
-        "UInt32" => Ok((rest, ClickHouseType::UInt32)),
-        "UInt64" => Ok((rest, ClickHouseType::UInt64)),
-        "Float32" => Ok((rest, ClickHouseType::Float32)),
-        "Float64" => Ok((rest, ClickHouseType::Float64)),
-        "Bool" => Ok((rest, ClickHouseType::Bool)),
-
-        // String types
-        "String" => Ok((rest, ClickHouseType::String)),
-        "FixedString" => parens(parse_u32)
-            .map(ClickHouseType::FixedString)
-            .parse(rest),
-
-        // Date/time types
-        "Date" | "Date32" => Ok((rest, ClickHouseType::Date)),
-        "DateTime" => Ok((rest, ClickHouseType::DateTime)),
-        "DateTime64" => {
-            let tz = delimited(char('\''), take_till(|c| c == '\''), char('\''));
-            parens(terminated(parse_u8, opt(preceded(tag(", "), tz))))
-                .map(|p| ClickHouseType::DateTime64 { precision: p })
-                .parse(rest)
+/// Parses comma-separated arguments from a parenthesized string.
+/// Input: "(arg1, arg2, arg3)" -> Output: Ok(vec!["arg1".to_string(), "arg2".to_string(), "arg3".to_string()])
+/// Returns an error if parentheses are malformed.
+fn parse_args(input: &str) -> Result<Vec<String>, String> {
+    let trimmed = input.trim();
+    if !trimmed.starts_with('(') || !trimmed.ends_with(')') {
+        return Err(format!(
+            "Expected parentheses around arguments in '{}'",
+            input
+        ));
+    }
+
+    let inner = trimmed[1..trimmed.len() - 1].trim();
+    if inner.is_empty() {
+        return Ok(vec![]);
+    }
+
+    // Split by comma, handling nested parentheses and quotes
+    let mut args = Vec::new();
+    let mut current_arg = String::new();
+    let mut depth = 0;
+    let mut in_quotes = false;
+
+    for c in inner.chars() {
+        match c {
+            '\'' if !in_quotes => in_quotes = true,
+            '\'' if in_quotes => in_quotes = false,
+            '(' if !in_quotes => depth += 1,
+            ')' if !in_quotes => depth -= 1,
+            ',' if depth == 0 && !in_quotes => {
+                args.push(current_arg.trim().to_string());
+                current_arg = String::new();
+                continue;
+            }
+            _ => {}
         }
+        current_arg.push(c);
+    }
 
-        // Decimal types
-        "Decimal" => parens(separated_pair(parse_u8, tag(", "), parse_i8))
-            .map(|(precision, scale)| ClickHouseType::Decimal { precision, scale })
-            .parse(rest),
-        "Decimal32" => parens(parse_i8)
-            .map(|scale| ClickHouseType::Decimal {
-                precision: DECIMAL32_PRECISION,
-                scale,
-            })
-            .parse(rest),
-        "Decimal64" => parens(parse_i8)
-            .map(|scale| ClickHouseType::Decimal {
-                precision: DECIMAL64_PRECISION,
-                scale,
-            })
-            .parse(rest),
-        "Decimal128" => parens(parse_i8)
-            .map(|scale| ClickHouseType::Decimal {
-                precision: DECIMAL128_PRECISION,
-                scale,
-            })
-            .parse(rest),
-        "Decimal256" => parens(parse_i8)
-            .map(|scale| ClickHouseType::Decimal {
-                precision: DECIMAL256_PRECISION,
-                scale,
+    if !current_arg.trim().is_empty() {
+        args.push(current_arg.trim().to_string());
+    }
+
+    Ok(args)
+}
+
+/// Parses ClickHouse Decimal types and returns the appropriate Arrow decimal type.
+/// ClickHouse formats:
+/// - Decimal(P, S) -> generic decimal with precision P and scale S
+/// - Decimal32(S) -> precision up to 9, scale S
+/// - Decimal64(S) -> precision up to 18, scale S
+/// - Decimal128(S) -> precision up to 38, scale S
+/// - Decimal256(S) -> precision up to 76, scale S
+///
+/// Uses metadata from ClickHouse's system.columns when available, otherwise falls back to parsing the type string.
+fn parse_decimal_type(ch_type: &str) -> Result<DataType, String> {
+    // Parse from type string
+    let (type_name, args_str) = extract_identifier(ch_type);
+
+    let result = parse_args(args_str).ok().and_then(|args| match type_name {
+        "Decimal" if args.len() == 2 => args[0].parse::<u8>().ok().zip(args[1].parse::<i8>().ok()),
+        "Decimal32" | "Decimal64" | "Decimal128" | "Decimal256" if args.len() == 1 => {
+            args[0].parse::<i8>().ok().map(|scale| {
+                let precision = match type_name {
+                    "Decimal32" => DECIMAL32_PRECISION,
+                    "Decimal64" => DECIMAL64_PRECISION,
+                    "Decimal128" => DECIMAL128_PRECISION,
+                    "Decimal256" => DECIMAL256_PRECISION,
+                    _ => unreachable!(),
+                };
+                (precision, scale)
             })
-            .parse(rest),
+        }
+        _ => None,
+    });
 
-        _ => Err(nom::Err::Error(Error::new(input, ErrorKind::Tag))),
-    }
+    result
+        .map(|(precision, scale)| {
+            if precision <= DECIMAL128_PRECISION {
+                DataType::Decimal128(precision, scale)
+            } else {
+                DataType::Decimal256(precision, scale)
+            }
+        })
+        .ok_or_else(|| format!("Could not parse Decimal type '{}'.", ch_type))
 }
 
-impl FromStr for ClickHouseType {
-    type Err = String;
+/// Parses DateTime64 precision and returns the appropriate Arrow timestamp type.
+/// DateTime64(0) -> Second
+/// DateTime64(3) -> Millisecond
+/// DateTime64(6) -> Microsecond
+/// DateTime64(9) -> Nanosecond
+///
+fn parse_datetime64_precision(ch_type: &str) -> Result<DataType, String> {
+    // Parse from type string
+    let (_type_name, args_str) = extract_identifier(ch_type);
+
+    let args = parse_args(args_str).map_err(|e| {
+        format!(
+            "Could not parse DateTime64 arguments from '{}': {}. Expected format: DateTime64(0-9) or DateTime64(0-9, 'timezone')",
+            ch_type, e
+        )
+    })?;
+
+    // DateTime64(precision) or DateTime64(precision, 'timezone')
+    if args.is_empty() {
+        return Err(format!(
+            "DateTime64 type '{}' has no precision argument. Expected format: DateTime64(0-9) or DateTime64(0-9, 'timezone')",
+            ch_type
+        ));
+    }
 
-    fn from_str(s: &str) -> Result<Self, Self::Err> {
-        all_consuming(ch_type)
-            .parse(s)
-            .map(|(_, parsed)| parsed)
-            .map_err(|e| format!("Failed to parse ClickHouse type '{s}': {e}"))
+    // Parse the precision (first argument)
+    match args[0].parse::<u8>() {
+        Ok(0) => Ok(DataType::Timestamp(TimeUnit::Second, None)),
+        Ok(1..=3) => Ok(DataType::Timestamp(TimeUnit::Millisecond, None)),
+        Ok(4..=6) => Ok(DataType::Timestamp(TimeUnit::Microsecond, None)),
+        Ok(7..=9) => Ok(DataType::Timestamp(TimeUnit::Nanosecond, None)),
+        _ => Err(format!(
+            "Unsupported DateTime64 precision in '{}'. Precision must be 0-9",
+            ch_type
+        )),
     }
 }
 
@@ -292,19 +274,33 @@ impl FromStr for ClickHouseType {
 mod tests {
     use super::*;
 
-    // Helper function for tests
-    fn convert_type(s: &str) -> Result<(DataType, bool), String> {
-        (&ClickHouseType::from_str(s)?).try_into()
+    // Helper function for tests that don't need metadata
+    fn convert_type_no_metadata(ch_type: &str) -> Result<(DataType, bool), String> {
+        clickhouse_type_to_arrow(ch_type)
     }
 
     #[test]
     fn test_clickhouse_type_mapping() {
-        assert_eq!(convert_type("String").unwrap(), (DataType::Utf8, false));
-        assert_eq!(convert_type("Int64").unwrap(), (DataType::Int64, false));
-        assert_eq!(convert_type("Float64").unwrap(), (DataType::Float64, false));
-        assert_eq!(convert_type("Bool").unwrap(), (DataType::Boolean, false));
         assert_eq!(
-            convert_type("DateTime").unwrap(),
+            convert_type_no_metadata("String").expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Utf8, false)
+        );
+        assert_eq!(
+            convert_type_no_metadata("Int64").expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Int64, false)
+        );
+        assert_eq!(
+            convert_type_no_metadata("Float64")
+                .expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Float64, false)
+        );
+        assert_eq!(
+            convert_type_no_metadata("Bool").expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Boolean, false)
+        );
+        assert_eq!(
+            convert_type_no_metadata("DateTime")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Second, None), false)
         );
     }
@@ -312,58 +308,85 @@ mod tests {
     #[test]
     fn test_datetime64_precision_mapping() {
         assert_eq!(
-            convert_type("DateTime64(0)").unwrap(),
+            convert_type_no_metadata("DateTime64(0)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Second, None), false)
         );
         assert_eq!(
-            convert_type("DateTime64(3)").unwrap(),
+            convert_type_no_metadata("DateTime64(3)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Millisecond, None), false)
         );
         assert_eq!(
-            convert_type("DateTime64(6)").unwrap(),
+            convert_type_no_metadata("DateTime64(6)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Microsecond, None), false)
         );
         assert_eq!(
-            convert_type("DateTime64(9)").unwrap(),
+            convert_type_no_metadata("DateTime64(9)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Nanosecond, None), false)
         );
-        // Test with timezones (ignored)
+        // Test with timezones
         assert_eq!(
-            convert_type("DateTime64(9, 'UTC')").unwrap(),
+            convert_type_no_metadata("DateTime64(9, 'UTC')")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Nanosecond, None), false)
         );
         assert_eq!(
-            convert_type("DateTime64(6, 'America/New_York')").unwrap(),
+            convert_type_no_metadata("DateTime64(6, 'UTC')")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Microsecond, None), false)
         );
-        // Edge cases
         assert_eq!(
-            convert_type("DateTime64(1)").unwrap(),
+            convert_type_no_metadata("DateTime64(9, 'America/New_York')")
+                .expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Timestamp(TimeUnit::Nanosecond, None), false)
+        );
+        // Test edge cases for precision ranges
+        assert_eq!(
+            convert_type_no_metadata("DateTime64(1)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Millisecond, None), false)
         );
         assert_eq!(
-            convert_type("DateTime64(4)").unwrap(),
+            convert_type_no_metadata("DateTime64(4)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Microsecond, None), false)
         );
         assert_eq!(
-            convert_type("DateTime64(7)").unwrap(),
+            convert_type_no_metadata("DateTime64(7)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Timestamp(TimeUnit::Nanosecond, None), false)
         );
     }
 
     #[test]
     fn test_nullable_type_mapping() {
-        assert_eq!(convert_type("String").unwrap(), (DataType::Utf8, false));
+        // Non-nullable types
+        assert_eq!(
+            convert_type_no_metadata("String").expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Utf8, false)
+        );
+        assert_eq!(
+            convert_type_no_metadata("Int64").expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Int64, false)
+        );
+
+        // Nullable types
         assert_eq!(
-            convert_type("Nullable(String)").unwrap(),
+            convert_type_no_metadata("Nullable(String)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Utf8, true)
         );
         assert_eq!(
-            convert_type("Nullable(Int64)").unwrap(),
+            convert_type_no_metadata("Nullable(Int64)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Int64, true)
         );
         assert_eq!(
-            convert_type("Nullable(Float64)").unwrap(),
+            convert_type_no_metadata("Nullable(Float64)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Float64, true)
         );
     }
@@ -371,286 +394,254 @@ mod tests {
     #[test]
     fn test_lowcardinality_type_mapping() {
         assert_eq!(
-            convert_type("LowCardinality(String)").unwrap(),
+            convert_type_no_metadata("LowCardinality(String)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Utf8, false)
         );
         assert_eq!(
-            convert_type("LowCardinality(FixedString(10))").unwrap(),
+            convert_type_no_metadata("LowCardinality(FixedString(10))")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Utf8, false)
         );
+        // Nullable + LowCardinality
         assert_eq!(
-            convert_type("LowCardinality(Nullable(String))").unwrap(),
+            convert_type_no_metadata("LowCardinality(Nullable(String))")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Utf8, true)
         );
     }
 
     #[test]
     fn test_decimal_type_mapping() {
-        // Decimal(P, S)
+        // Generic Decimal(P, S)
         assert_eq!(
-            convert_type("Decimal(10, 2)").unwrap(),
+            convert_type_no_metadata("Decimal(10, 2)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Decimal128(10, 2), false)
         );
         assert_eq!(
-            convert_type("Decimal(38, 6)").unwrap(),
+            convert_type_no_metadata("Decimal(38, 6)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Decimal128(38, 6), false)
         );
         assert_eq!(
-            convert_type("Decimal(50, 10)").unwrap(),
+            convert_type_no_metadata("Decimal(50, 10)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Decimal256(50, 10), false)
         );
 
-        // Decimal32(S) - precision 9
+        // Generic Decimal without spaces and with spaces
         assert_eq!(
-            convert_type("Decimal32(2)").unwrap(),
+            convert_type_no_metadata("Decimal(10,2)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Decimal128(10, 2), false)
+        );
+        assert_eq!(
+            convert_type_no_metadata("Decimal( 18 , 6 )")
+                .expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Decimal128(18, 6), false)
+        );
+
+        // Decimal32(S) - precision up to 9
+        assert_eq!(
+            convert_type_no_metadata("Decimal32(2)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Decimal128(9, 2), false)
         );
+        assert_eq!(
+            convert_type_no_metadata("Decimal32(4)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Decimal128(9, 4), false)
+        );
 
-        // Decimal64(S) - precision 18
+        // Decimal64(S) - precision up to 18
         assert_eq!(
-            convert_type("Decimal64(4)").unwrap(),
+            convert_type_no_metadata("Decimal64(4)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Decimal128(18, 4), false)
         );
+        assert_eq!(
+            convert_type_no_metadata("Decimal64(8)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
+            (DataType::Decimal128(18, 8), false)
+        );
 
-        // Decimal128(S) - precision 38
+        // Decimal128(S) - precision up to 38
         assert_eq!(
-            convert_type("Decimal128(10)").unwrap(),
+            convert_type_no_metadata("Decimal128(10)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Decimal128(38, 10), false)
         );
 
-        // Decimal256(S) - precision 76
+        // Decimal256(S) - precision up to 76
         assert_eq!(
-            convert_type("Decimal256(20)").unwrap(),
+            convert_type_no_metadata("Decimal256(20)")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Decimal256(76, 20), false)
         );
 
-        // Nullable
+        // With Nullable wrapper
         assert_eq!(
-            convert_type("Nullable(Decimal(18, 6))").unwrap(),
+            convert_type_no_metadata("Nullable(Decimal(18, 6))")
+                .expect("Failed to convert ClickHouse type to Arrow"),
             (DataType::Decimal128(18, 6), true)
         );
     }
 
     #[test]
-    fn test_array_type() {
-        let (data_type, is_nullable) = convert_type("Array(Int32)").unwrap();
-        assert!(!is_nullable);
-        match data_type {
-            DataType::List(field) => {
-                assert_eq!(field.data_type(), &DataType::Int32);
-                assert!(!field.is_nullable());
-            }
-            _ => panic!("Expected List type"),
-        }
-    }
-
-    #[test]
-    fn test_tuple_type() {
-        let (data_type, is_nullable) = convert_type("Tuple(String, Int64)").unwrap();
-        assert!(!is_nullable);
-        match data_type {
-            DataType::Struct(fields) => {
-                assert_eq!(fields.len(), 2);
-                assert_eq!(fields[0].data_type(), &DataType::Utf8);
-                assert_eq!(fields[1].data_type(), &DataType::Int64);
-            }
-            _ => panic!("Expected Struct type"),
-        }
-    }
-
-    #[test]
-    fn test_map_type() {
-        let (data_type, is_nullable) = convert_type("Map(String, Int64)").unwrap();
-        assert!(!is_nullable);
-        assert!(matches!(data_type, DataType::Map(_, _)));
+    fn test_extract_identifier() {
+        assert_eq!(extract_identifier("Decimal(10, 2)"), ("Decimal", "(10, 2)"));
+        assert_eq!(extract_identifier("DateTime64(3)"), ("DateTime64", "(3)"));
+        assert_eq!(extract_identifier("Int32"), ("Int32", ""));
+        assert_eq!(
+            extract_identifier("LowCardinality(String)"),
+            ("LowCardinality", "(String)")
+        );
+        assert_eq!(extract_identifier("Decimal128(10)"), ("Decimal128", "(10)"));
     }
 
     #[test]
-    fn test_unknown_type_fails() {
-        let result = convert_type("UnknownType");
-        assert!(result.is_err());
-    }
+    fn test_parse_args() {
+        // Simple cases
+        assert_eq!(
+            parse_args("(10, 2)").unwrap(),
+            vec!["10".to_string(), "2".to_string()]
+        );
+        assert_eq!(parse_args("(3)").unwrap(), vec!["3".to_string()]);
+        assert_eq!(parse_args("()").unwrap(), Vec::<String>::new());
 
-    #[test]
-    fn test_parse_primitives() {
-        assert_eq!("String".parse(), Ok(ClickHouseType::String));
-        assert_eq!("Int64".parse(), Ok(ClickHouseType::Int64));
+        // With spaces
         assert_eq!(
-            "DateTime64(3)".parse(),
-            Ok(ClickHouseType::DateTime64 { precision: 3 })
+            parse_args("( 10 , 2 )").unwrap(),
+            vec!["10".to_string(), "2".to_string()]
         );
-    }
 
-    #[test]
-    fn test_parse_nullable() {
+        // With nested parentheses
         assert_eq!(
-            "Nullable(String)".parse(),
-            Ok(ClickHouseType::Nullable(Box::new(ClickHouseType::String)))
+            parse_args("(Nullable(String))").unwrap(),
+            vec!["Nullable(String)".to_string()]
         );
         assert_eq!(
-            "Nullable(Int64)".parse(),
-            Ok(ClickHouseType::Nullable(Box::new(ClickHouseType::Int64)))
+            parse_args("(Array(Int32), String)").unwrap(),
+            vec!["Array(Int32)".to_string(), "String".to_string()]
         );
-    }
 
-    #[test]
-    fn test_parse_lowcardinality() {
+        // With quotes
         assert_eq!(
-            "LowCardinality(String)".parse(),
-            Ok(ClickHouseType::LowCardinality(Box::new(
-                ClickHouseType::String
-            )))
+            parse_args("(3, 'UTC')").unwrap(),
+            vec!["3".to_string(), "'UTC'".to_string()]
         );
         assert_eq!(
-            "LowCardinality(Nullable(String))".parse(),
-            Ok(ClickHouseType::LowCardinality(Box::new(
-                ClickHouseType::Nullable(Box::new(ClickHouseType::String))
-            )))
+            parse_args("(9, 'America/New_York')").unwrap(),
+            vec!["9".to_string(), "'America/New_York'".to_string()]
         );
-    }
 
-    #[test]
-    fn test_is_nullable() {
-        assert!(!ClickHouseType::from_str("String").unwrap().is_nullable());
-        assert!(
-            ClickHouseType::from_str("Nullable(String)")
-                .unwrap()
-                .is_nullable()
-        );
-        assert!(
-            ClickHouseType::from_str("LowCardinality(Nullable(String))")
-                .unwrap()
-                .is_nullable()
-        );
-        assert!(
-            !ClickHouseType::from_str("LowCardinality(String)")
-                .unwrap()
-                .is_nullable()
+        // Complex nested case
+        assert_eq!(
+            parse_args("(Tuple(Int32, String), Array(Float64))").unwrap(),
+            vec![
+                "Tuple(Int32, String)".to_string(),
+                "Array(Float64)".to_string()
+            ]
         );
+
+        // Error cases
+        assert!(parse_args("10, 2").is_err()); // Missing parentheses
+        assert!(parse_args("(10, 2").is_err()); // Missing closing paren
     }
 
     #[test]
-    fn test_array_type_parsing() {
-        let (dtype, nullable) = convert_type("Array(Int32)").unwrap();
-        assert!(matches!(dtype, DataType::List(_)));
-        assert!(!nullable);
-
-        // Nested array
-        let (dtype, _) = convert_type("Array(Array(String))").unwrap();
-        if let DataType::List(inner) = dtype {
-            assert!(matches!(inner.data_type(), DataType::List(_)));
-        } else {
-            panic!("Expected List type");
-        }
-
-        // Nullable array
-        let (_, nullable) = convert_type("Nullable(Array(Int64))").unwrap();
-        assert!(nullable);
+    fn test_array_type_not_supported() {
+        // Array types should return an error
+        let result = convert_type_no_metadata("Array(Int32)");
+        assert!(result.is_err());
+        let err = result.unwrap_err();
+        assert!(err.contains("Array type"));
+        assert!(err.contains("not supported"));
     }
 
     #[test]
-    fn test_tuple_type_parsing() {
-        let (dtype, _) = convert_type("Tuple(String, Int64)").unwrap();
-        if let DataType::Struct(fields) = dtype {
-            assert_eq!(fields.len(), 2);
-            assert_eq!(fields[0].name(), "f0");
-            assert_eq!(fields[1].name(), "f1");
-        } else {
-            panic!("Expected Struct type");
-        }
-
-        // Nested tuple
-        let (dtype, _) = convert_type("Tuple(Int32, Tuple(String, Float64))").unwrap();
-        if let DataType::Struct(fields) = dtype {
-            assert_eq!(fields.len(), 2);
-            assert!(matches!(fields[1].data_type(), DataType::Struct(_)));
-        } else {
-            panic!("Expected Struct type");
-        }
+    fn test_tuple_type_not_supported() {
+        // Tuple types should return an error
+        let result = convert_type_no_metadata("Tuple(String, Int64)");
+        assert!(result.is_err());
+        let err = result.unwrap_err();
+        assert!(err.contains("Tuple type"));
+        assert!(err.contains("not supported"));
     }
 
     #[test]
-    fn test_map_type_parsing() {
-        let (dtype, _) = convert_type("Map(String, Int64)").unwrap();
-        assert!(matches!(dtype, DataType::Map(_, _)));
-
-        // Map with complex value
-        let (dtype, _) = convert_type("Map(String, Array(Int32))").unwrap();
-        if let DataType::Map(entries, _) = dtype
-            && let DataType::Struct(fields) = entries.data_type()
-        {
-            assert!(matches!(fields[1].data_type(), DataType::List(_)));
-        }
+    fn test_map_type_not_supported() {
+        // Map types should return an error
+        let result = convert_type_no_metadata("Map(String, Int64)");
+        assert!(result.is_err());
+        let err = result.unwrap_err();
+        assert!(err.contains("Map type"));
+        assert!(err.contains("not supported"));
+    }
 
-        // Non-string key should error
-        let result = convert_type("Map(Int32, String)");
+    #[test]
+    fn test_unknown_type_fails() {
+        // Unknown types should return an error
+        let result = convert_type_no_metadata("UnknownType");
         assert!(result.is_err());
-        assert!(result.unwrap_err().contains("Map keys must be String"));
+        let err = result.unwrap_err();
+        assert!(err.contains("Unknown ClickHouse type"));
     }
 
     #[test]
-    fn test_complex_nested_types() {
-        // Array of tuples
-        let (dtype, _) = convert_type("Array(Tuple(String, Int64))").unwrap();
-        if let DataType::List(inner) = dtype {
-            assert!(matches!(inner.data_type(), DataType::Struct(_)));
-        } else {
-            panic!("Expected List type");
-        }
+    fn test_parse_ch_type_primitives() {
+        assert_eq!(parse_ch_type("String"), ClickHouseType::Primitive("String"));
+        assert_eq!(parse_ch_type("Int64"), ClickHouseType::Primitive("Int64"));
+        assert_eq!(
+            parse_ch_type("DateTime64(3)"),
+            ClickHouseType::Primitive("DateTime64(3)")
+        );
+    }
 
-        // Tuple with array and map
-        let (dtype, _) = convert_type("Tuple(Array(Int32), Map(String, Float64))").unwrap();
-        if let DataType::Struct(fields) = dtype {
-            assert_eq!(fields.len(), 2);
-            assert!(matches!(fields[0].data_type(), DataType::List(_)));
-            assert!(matches!(fields[1].data_type(), DataType::Map(_, _)));
-        } else {
-            panic!("Expected Struct type");
-        }
+    #[test]
+    fn test_parse_ch_type_nullable() {
+        assert_eq!(
+            parse_ch_type("Nullable(String)"),
+            ClickHouseType::Nullable(Box::new(ClickHouseType::Primitive("String")))
+        );
+        assert_eq!(
+            parse_ch_type("Nullable(Int64)"),
+            ClickHouseType::Nullable(Box::new(ClickHouseType::Primitive("Int64")))
+        );
+    }
 
-        // Map with tuple values
-        let (dtype, _) = convert_type("Map(String, Tuple(Int64, String))").unwrap();
-        if let DataType::Map(entries, _) = dtype
-            && let DataType::Struct(fields) = entries.data_type()
-        {
-            assert!(matches!(fields[1].data_type(), DataType::Struct(_)));
-        }
+    #[test]
+    fn test_parse_ch_type_lowcardinality() {
+        assert_eq!(
+            parse_ch_type("LowCardinality(String)"),
+            ClickHouseType::LowCardinality(Box::new(ClickHouseType::Primitive("String")))
+        );
+        assert_eq!(
+            parse_ch_type("LowCardinality(Nullable(String))"),
+            ClickHouseType::LowCardinality(Box::new(ClickHouseType::Nullable(Box::new(
+                ClickHouseType::Primitive("String")
+            ))))
+        );
     }
 
     #[test]
-    fn test_named_tuple_fields() {
-        let (dtype, _) = convert_type("Tuple(category String, tag String)").unwrap();
-        if let DataType::Struct(fields) = dtype {
-            assert_eq!(fields.len(), 2);
-            assert_eq!(fields[0].name(), "category");
-            assert_eq!(fields[1].name(), "tag");
-        } else {
-            panic!("Expected Struct type");
-        }
+    fn test_parse_ch_type_is_nullable() {
+        assert!(!parse_ch_type("String").is_nullable());
+        assert!(parse_ch_type("Nullable(String)").is_nullable());
+        assert!(parse_ch_type("LowCardinality(Nullable(String))").is_nullable());
+        assert!(!parse_ch_type("LowCardinality(String)").is_nullable());
+    }
 
-        // Array of named tuples
-        let (dtype, _) = convert_type("Array(Tuple(category String, tag String))").unwrap();
-        if let DataType::List(inner) = dtype {
-            if let DataType::Struct(fields) = inner.data_type() {
-                assert_eq!(fields[0].name(), "category");
-                assert_eq!(fields[1].name(), "tag");
-            } else {
-                panic!("Expected Struct type inside List");
-            }
-        } else {
-            panic!("Expected List type");
-        }
+    #[test]
+    fn test_parse_ch_type_base_type() {
+        let parsed = parse_ch_type("LowCardinality(Nullable(String))");
+        assert_eq!(parsed.base_type(), &ClickHouseType::Primitive("String"));
 
-        // Named tuple with complex types
-        let (dtype, _) =
-            convert_type("Tuple(items Array(Int32), metadata Map(String, String))").unwrap();
-        if let DataType::Struct(fields) = dtype {
-            assert_eq!(fields[0].name(), "items");
-            assert_eq!(fields[1].name(), "metadata");
-            assert!(matches!(fields[0].data_type(), DataType::List(_)));
-            assert!(matches!(fields[1].data_type(), DataType::Map(_, _)));
-        } else {
-            panic!("Expected Struct type");
-        }
+        let parsed = parse_ch_type("Nullable(Int64)");
+        assert_eq!(parsed.base_type(), &ClickHouseType::Primitive("Int64"));
+
+        let parsed = parse_ch_type("String");
+        assert_eq!(parsed.base_type(), &ClickHouseType::Primitive("String"));
     }
 }
diff --git a/src/sinks/clickhouse/arrow/schema.rs b/src/sinks/clickhouse/arrow/schema.rs
index 52f54d9b1..f2359ca5f 100644
--- a/src/sinks/clickhouse/arrow/schema.rs
+++ b/src/sinks/clickhouse/arrow/schema.rs
@@ -1,19 +1,15 @@
 //! Schema fetching and Arrow schema construction for ClickHouse tables.
 
-use std::str::FromStr;
-
 use arrow::datatypes::{Field, Schema};
 use async_trait::async_trait;
 use http::{Request, StatusCode};
 use hyper::Body;
-use itertools::Itertools;
 use serde::Deserialize;
-use url::form_urlencoded;
 use vector_lib::codecs::encoding::format::{ArrowEncodingError, SchemaProvider};
 
 use crate::http::{Auth, HttpClient};
 
-use super::parser::ClickHouseType;
+use super::parser::clickhouse_type_to_arrow;
 
 #[derive(Debug, Deserialize)]
 struct ColumnInfo {
@@ -22,16 +18,9 @@ struct ColumnInfo {
     column_type: String,
 }
 
-impl TryFrom<ColumnInfo> for Field {
-    type Error = Box<dyn std::error::Error + Send + Sync>;
-
-    fn try_from(column: ColumnInfo) -> Result<Self, Self::Error> {
-        let ch_type = ClickHouseType::from_str(&column.column_type)?;
-        let (dt, nullable) = (&ch_type)
-            .try_into()
-            .map_err(|e| format!("Failed to convert column '{}': {e}", column.name))?;
-        Ok(Field::new(column.name, dt, nullable))
-    }
+/// URL-encodes a string for use in HTTP query parameters.
+fn url_encode(s: &str) -> String {
+    percent_encoding::utf8_percent_encode(s, percent_encoding::NON_ALPHANUMERIC).to_string()
 }
 
 /// Fetches the schema for a ClickHouse table and converts it to an Arrow schema.
@@ -49,15 +38,14 @@ pub async fn fetch_table_schema(
                  FORMAT JSONEachRow";
 
     // Build URI with query and parameters
-    let query_string = form_urlencoded::Serializer::new(String::new())
-        .append_pair("query", query)
-        .append_pair("param_db", database)
-        .append_pair("param_tbl", table)
-        .finish();
-    let uri = format!("{endpoint}?{query_string}");
-    let mut request = Request::get(&uri)
-        .body(Body::empty())
-        .map_err(|e| format!("Failed to build request: {e}"))?;
+    let uri = format!(
+        "{}?query={}&param_db={}&param_tbl={}",
+        endpoint,
+        url_encode(query),
+        url_encode(database),
+        url_encode(table)
+    );
+    let mut request = Request::get(&uri).body(Body::empty()).unwrap();
 
     if let Some(auth) = auth {
         auth.apply(&mut request);
@@ -71,28 +59,40 @@ pub async fn fetch_table_schema(
                 .await?
                 .to_bytes();
             let body_str = String::from_utf8(body_bytes.into())
-                .map_err(|e| format!("Failed to parse response as UTF-8: {e}"))?;
+                .map_err(|e| format!("Failed to parse response as UTF-8: {}", e))?;
 
             parse_schema_from_response(&body_str)
         }
-        status => Err(format!("Failed to fetch schema from ClickHouse: HTTP {status}").into()),
+        status => Err(format!("Failed to fetch schema from ClickHouse: HTTP {}", status).into()),
     }
 }
 
 /// Parses the JSON response from ClickHouse and builds an Arrow schema.
 fn parse_schema_from_response(response: &str) -> crate::Result<Schema> {
-    let mut lines = response.lines().filter(|line| !line.is_empty()).peekable();
+    let mut columns: Vec<ColumnInfo> = Vec::new();
+
+    for line in response.lines() {
+        if line.trim().is_empty() {
+            continue;
+        }
 
-    if lines.peek().is_none() {
-        return Err("Table does not exist or has no columns".into());
+        let column: ColumnInfo = serde_json::from_str(line)
+            .map_err(|e| format!("Failed to parse column info: {}", e))?;
+        columns.push(column);
     }
 
-    lines
-        .map(|line| -> crate::Result<Field> {
-            serde_json::from_str::<ColumnInfo>(line)?.try_into()
-        })
-        .try_collect::<_, Vec<Field>, _>()
-        .map(Schema::new)
+    if columns.is_empty() {
+        return Err("No columns found in table schema".into());
+    }
+
+    let mut fields = Vec::new();
+    for column in columns {
+        let (arrow_type, nullable) = clickhouse_type_to_arrow(&column.column_type)
+            .map_err(|e| format!("Failed to convert column '{}': {}", column.name, e))?;
+        fields.push(Field::new(&column.name, arrow_type, nullable));
+    }
+
+    Ok(Schema::new(fields))
 }
 
 /// Schema provider implementation for ClickHouse tables.
diff --git a/src/sinks/clickhouse/integration_tests.rs b/src/sinks/clickhouse/integration_tests.rs
index a63e4a9e9..379859570 100644
--- a/src/sinks/clickhouse/integration_tests.rs
+++ b/src/sinks/clickhouse/integration_tests.rs
@@ -12,12 +12,11 @@ use futures::{
     stream,
 };
 use http::StatusCode;
-use ordered_float::NotNan;
 use serde::Deserialize;
 use serde_json::Value;
 use tokio::time::{Duration, timeout};
 use vector_lib::{
-    codecs::encoding::{ArrowStreamSerializerConfig, BatchSerializerConfig},
+    codecs::encoding::BatchSerializerConfig,
     event::{BatchNotifier, BatchStatus, BatchStatusReceiver, Event, LogEvent},
     lookup::PathPrefix,
 };
@@ -606,571 +605,3 @@ async fn insert_events_arrow_with_schema_fetching() {
         assert!(row.get("active").and_then(|v| v.as_bool()).is_some());
     }
 }
-
-#[tokio::test]
-async fn test_complex_types() {
-    trace_init();
-
-    let table = random_table_name();
-    let host = clickhouse_address();
-
-    let mut batch = BatchConfig::default();
-    batch.max_events = Some(3);
-
-    let arrow_config = ArrowStreamSerializerConfig {
-        allow_nullable_fields: true,
-        ..Default::default()
-    };
-
-    let config = ClickhouseConfig {
-        endpoint: host.parse().unwrap(),
-        table: table.clone().try_into().unwrap(),
-        compression: Compression::None,
-        format: crate::sinks::clickhouse::config::Format::ArrowStream,
-        batch_encoding: Some(BatchSerializerConfig::ArrowStream(arrow_config)),
-        batch,
-        request: TowerRequestConfig {
-            retry_attempts: 1,
-            ..Default::default()
-        },
-        ..Default::default()
-    };
-
-    let client = ClickhouseClient::new(host);
-
-    // Comprehensive schema with all complex types
-    client
-        .create_table(
-            &table,
-            "host String, timestamp DateTime64(3), message String, \
-             nested_int_array Array(Array(Int32)), \
-             nested_string_array Array(Array(String)), \
-             array_map Map(String, Array(String)), \
-             int_array_map Map(String, Array(Int64)), \
-             tuple_with_array Tuple(String, Array(Int32)), \
-             tuple_with_map Tuple(String, Map(String, Float64)), \
-             tuple_with_nested Tuple(String, Array(Int32), Map(String, Float64)), \
-             locations Array(Tuple(String, Float64, Float64)), \
-             tags_history Array(Map(String, String)), \
-             metrics_history Array(Map(String, Int32)), \
-             request_headers Map(String, String), \
-             response_metrics Tuple(Int32, Int64, Float64), \
-             tags Array(String), \
-             user_properties Map(String, Array(String)), \
-             array_with_nulls Array(Nullable(Int32)), \
-             array_with_named_tuple Array(Tuple(category String, tag String))",
-        )
-        .await;
-
-    let (sink, _hc) = config.build(SinkContext::default()).await.unwrap();
-
-    let mut events: Vec<Event> = Vec::new();
-
-    // Event 1: Comprehensive test with all complex types
-    let mut event1 = LogEvent::from("Comprehensive complex types test");
-    event1.insert("host", "host1.example.com");
-
-    // Nested arrays
-    event1.insert(
-        "nested_int_array",
-        vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Array(vec![
-                vector_lib::event::Value::Integer(1),
-                vector_lib::event::Value::Integer(2),
-            ]),
-            vector_lib::event::Value::Array(vec![
-                vector_lib::event::Value::Integer(3),
-                vector_lib::event::Value::Integer(4),
-            ]),
-        ]),
-    );
-    event1.insert(
-        "nested_string_array",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Bytes("a".into()),
-            vector_lib::event::Value::Bytes("b".into()),
-        ])]),
-    );
-
-    // Maps with arrays
-    let mut array_map = vector_lib::event::ObjectMap::new();
-    array_map.insert(
-        "fruits".into(),
-        vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Bytes("apple".into()),
-            vector_lib::event::Value::Bytes("banana".into()),
-        ]),
-    );
-    event1.insert("array_map", vector_lib::event::Value::Object(array_map));
-
-    let mut int_array_map = vector_lib::event::ObjectMap::new();
-    int_array_map.insert(
-        "scores".into(),
-        vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Integer(95),
-            vector_lib::event::Value::Integer(87),
-        ]),
-    );
-    event1.insert(
-        "int_array_map",
-        vector_lib::event::Value::Object(int_array_map),
-    );
-
-    // Tuples with complex types
-    let mut tuple_with_array = vector_lib::event::ObjectMap::new();
-    tuple_with_array.insert(
-        "f0".into(),
-        vector_lib::event::Value::Bytes("numbers".into()),
-    );
-    tuple_with_array.insert(
-        "f1".into(),
-        vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Integer(10),
-            vector_lib::event::Value::Integer(20),
-        ]),
-    );
-    event1.insert(
-        "tuple_with_array",
-        vector_lib::event::Value::Object(tuple_with_array),
-    );
-
-    let mut inner_map = vector_lib::event::ObjectMap::new();
-    inner_map.insert(
-        "temp".into(),
-        vector_lib::event::Value::Float(NotNan::new(22.5).unwrap()),
-    );
-    let mut tuple_with_map = vector_lib::event::ObjectMap::new();
-    tuple_with_map.insert(
-        "f0".into(),
-        vector_lib::event::Value::Bytes("metrics".into()),
-    );
-    tuple_with_map.insert("f1".into(), vector_lib::event::Value::Object(inner_map));
-    event1.insert(
-        "tuple_with_map",
-        vector_lib::event::Value::Object(tuple_with_map),
-    );
-
-    let mut inner_map2 = vector_lib::event::ObjectMap::new();
-    inner_map2.insert(
-        "avg".into(),
-        vector_lib::event::Value::Float(NotNan::new(95.5).unwrap()),
-    );
-    let mut tuple_complex = vector_lib::event::ObjectMap::new();
-    tuple_complex.insert(
-        "f0".into(),
-        vector_lib::event::Value::Bytes("results".into()),
-    );
-    tuple_complex.insert(
-        "f1".into(),
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(95)]),
-    );
-    tuple_complex.insert("f2".into(), vector_lib::event::Value::Object(inner_map2));
-    event1.insert(
-        "tuple_with_nested",
-        vector_lib::event::Value::Object(tuple_complex),
-    );
-
-    // Array of tuples
-    let mut loc1 = vector_lib::event::ObjectMap::new();
-    loc1.insert(
-        "f0".into(),
-        vector_lib::event::Value::Bytes("San Francisco".into()),
-    );
-    loc1.insert(
-        "f1".into(),
-        vector_lib::event::Value::Float(NotNan::new(37.7749).unwrap()),
-    );
-    loc1.insert(
-        "f2".into(),
-        vector_lib::event::Value::Float(NotNan::new(-122.4194).unwrap()),
-    );
-    event1.insert(
-        "locations",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(loc1)]),
-    );
-
-    // Array of maps
-    let mut tags1 = vector_lib::event::ObjectMap::new();
-    tags1.insert("env".into(), vector_lib::event::Value::Bytes("prod".into()));
-    event1.insert(
-        "tags_history",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(tags1)]),
-    );
-
-    let mut metrics1 = vector_lib::event::ObjectMap::new();
-    metrics1.insert("cpu".into(), vector_lib::event::Value::Integer(45));
-    event1.insert(
-        "metrics_history",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(metrics1)]),
-    );
-
-    // Structured log data
-    let mut headers = vector_lib::event::ObjectMap::new();
-    headers.insert(
-        "user-agent".into(),
-        vector_lib::event::Value::Bytes("Mozilla/5.0".into()),
-    );
-    event1.insert("request_headers", vector_lib::event::Value::Object(headers));
-
-    let mut metrics = vector_lib::event::ObjectMap::new();
-    metrics.insert("f0".into(), vector_lib::event::Value::Integer(200));
-    metrics.insert("f1".into(), vector_lib::event::Value::Integer(1234));
-    metrics.insert(
-        "f2".into(),
-        vector_lib::event::Value::Float(NotNan::new(0.145).unwrap()),
-    );
-    event1.insert(
-        "response_metrics",
-        vector_lib::event::Value::Object(metrics),
-    );
-
-    event1.insert(
-        "tags",
-        vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Bytes("api".into()),
-            vector_lib::event::Value::Bytes("v2".into()),
-        ]),
-    );
-
-    let mut user_props = vector_lib::event::ObjectMap::new();
-    user_props.insert(
-        "roles".into(),
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Bytes("admin".into())]),
-    );
-    event1.insert(
-        "user_properties",
-        vector_lib::event::Value::Object(user_props),
-    );
-
-    // Nullable array
-    event1.insert(
-        "array_with_nulls",
-        vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Integer(100),
-            vector_lib::event::Value::Integer(200),
-        ]),
-    );
-
-    // Named tuple array - tests that named fields work correctly
-    let mut named_tuple1 = vector_lib::event::ObjectMap::new();
-    named_tuple1.insert(
-        "category".into(),
-        vector_lib::event::Value::Bytes("priority".into()),
-    );
-    named_tuple1.insert("tag".into(), vector_lib::event::Value::Bytes("high".into()));
-
-    let mut named_tuple2 = vector_lib::event::ObjectMap::new();
-    named_tuple2.insert(
-        "category".into(),
-        vector_lib::event::Value::Bytes("environment".into()),
-    );
-    named_tuple2.insert(
-        "tag".into(),
-        vector_lib::event::Value::Bytes("production".into()),
-    );
-
-    event1.insert(
-        "array_with_named_tuple",
-        vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Object(named_tuple1),
-            vector_lib::event::Value::Object(named_tuple2),
-        ]),
-    );
-
-    events.push(event1.into());
-
-    // Event 2: Empty and edge cases
-    let mut event2 = LogEvent::from("Test empty collections");
-    event2.insert("host", "host2.example.com");
-    event2.insert("nested_int_array", vector_lib::event::Value::Array(vec![]));
-    event2.insert(
-        "nested_string_array",
-        vector_lib::event::Value::Array(vec![]),
-    );
-
-    let empty_map = vector_lib::event::ObjectMap::new();
-    event2.insert(
-        "array_map",
-        vector_lib::event::Value::Object(empty_map.clone()),
-    );
-    event2.insert(
-        "int_array_map",
-        vector_lib::event::Value::Object(empty_map.clone()),
-    );
-
-    let mut empty_tuple = vector_lib::event::ObjectMap::new();
-    empty_tuple.insert("f0".into(), vector_lib::event::Value::Bytes("empty".into()));
-    empty_tuple.insert("f1".into(), vector_lib::event::Value::Array(vec![]));
-    event2.insert(
-        "tuple_with_array",
-        vector_lib::event::Value::Object(empty_tuple),
-    );
-
-    let mut empty_tuple_map = vector_lib::event::ObjectMap::new();
-    empty_tuple_map.insert("f0".into(), vector_lib::event::Value::Bytes("empty".into()));
-    empty_tuple_map.insert(
-        "f1".into(),
-        vector_lib::event::Value::Object(empty_map.clone()),
-    );
-    event2.insert(
-        "tuple_with_map",
-        vector_lib::event::Value::Object(empty_tuple_map),
-    );
-
-    let mut empty_tuple_complex = vector_lib::event::ObjectMap::new();
-    empty_tuple_complex.insert("f0".into(), vector_lib::event::Value::Bytes("empty".into()));
-    empty_tuple_complex.insert("f1".into(), vector_lib::event::Value::Array(vec![]));
-    empty_tuple_complex.insert(
-        "f2".into(),
-        vector_lib::event::Value::Object(empty_map.clone()),
-    );
-    event2.insert(
-        "tuple_with_nested",
-        vector_lib::event::Value::Object(empty_tuple_complex),
-    );
-
-    event2.insert("locations", vector_lib::event::Value::Array(vec![]));
-    event2.insert("tags_history", vector_lib::event::Value::Array(vec![]));
-    event2.insert("metrics_history", vector_lib::event::Value::Array(vec![]));
-    event2.insert(
-        "request_headers",
-        vector_lib::event::Value::Object(empty_map.clone()),
-    );
-
-    let mut empty_metrics = vector_lib::event::ObjectMap::new();
-    empty_metrics.insert("f0".into(), vector_lib::event::Value::Integer(0));
-    empty_metrics.insert("f1".into(), vector_lib::event::Value::Integer(0));
-    empty_metrics.insert(
-        "f2".into(),
-        vector_lib::event::Value::Float(NotNan::new(0.0).unwrap()),
-    );
-    event2.insert(
-        "response_metrics",
-        vector_lib::event::Value::Object(empty_metrics),
-    );
-
-    event2.insert("tags", vector_lib::event::Value::Array(vec![]));
-    event2.insert(
-        "user_properties",
-        vector_lib::event::Value::Object(empty_map),
-    );
-    event2.insert("array_with_nulls", vector_lib::event::Value::Array(vec![]));
-    event2.insert(
-        "array_with_named_tuple",
-        vector_lib::event::Value::Array(vec![]),
-    );
-
-    events.push(event2.into());
-
-    // Event 3: More varied data
-    let mut event3 = LogEvent::from("Test varied data");
-    event3.insert("host", "host3.example.com");
-
-    event3.insert(
-        "nested_int_array",
-        vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Array(vec![]),
-            vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(99)]),
-        ]),
-    );
-    event3.insert(
-        "nested_string_array",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Array(vec![
-            vector_lib::event::Value::Bytes("test".into()),
-        ])]),
-    );
-
-    let mut map3 = vector_lib::event::ObjectMap::new();
-    map3.insert(
-        "colors".into(),
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Bytes("red".into())]),
-    );
-    event3.insert("array_map", vector_lib::event::Value::Object(map3));
-
-    let mut int_map3 = vector_lib::event::ObjectMap::new();
-    int_map3.insert(
-        "values".into(),
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(42)]),
-    );
-    event3.insert("int_array_map", vector_lib::event::Value::Object(int_map3));
-
-    let mut tuple3 = vector_lib::event::ObjectMap::new();
-    tuple3.insert("f0".into(), vector_lib::event::Value::Bytes("data".into()));
-    tuple3.insert(
-        "f1".into(),
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(5)]),
-    );
-    event3.insert("tuple_with_array", vector_lib::event::Value::Object(tuple3));
-
-    let mut map_inner = vector_lib::event::ObjectMap::new();
-    map_inner.insert(
-        "val".into(),
-        vector_lib::event::Value::Float(NotNan::new(1.0).unwrap()),
-    );
-    let mut tuple_map3 = vector_lib::event::ObjectMap::new();
-    tuple_map3.insert("f0".into(), vector_lib::event::Value::Bytes("test".into()));
-    tuple_map3.insert("f1".into(), vector_lib::event::Value::Object(map_inner));
-    event3.insert(
-        "tuple_with_map",
-        vector_lib::event::Value::Object(tuple_map3),
-    );
-
-    let mut map_inner2 = vector_lib::event::ObjectMap::new();
-    map_inner2.insert(
-        "x".into(),
-        vector_lib::event::Value::Float(NotNan::new(2.0).unwrap()),
-    );
-    let mut tuple_nested3 = vector_lib::event::ObjectMap::new();
-    tuple_nested3.insert("f0".into(), vector_lib::event::Value::Bytes("nest".into()));
-    tuple_nested3.insert(
-        "f1".into(),
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(1)]),
-    );
-    tuple_nested3.insert("f2".into(), vector_lib::event::Value::Object(map_inner2));
-    event3.insert(
-        "tuple_with_nested",
-        vector_lib::event::Value::Object(tuple_nested3),
-    );
-
-    let mut loc3 = vector_lib::event::ObjectMap::new();
-    loc3.insert("f0".into(), vector_lib::event::Value::Bytes("NYC".into()));
-    loc3.insert(
-        "f1".into(),
-        vector_lib::event::Value::Float(NotNan::new(40.7128).unwrap()),
-    );
-    loc3.insert(
-        "f2".into(),
-        vector_lib::event::Value::Float(NotNan::new(-74.0060).unwrap()),
-    );
-    event3.insert(
-        "locations",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(loc3)]),
-    );
-
-    let mut tags3 = vector_lib::event::ObjectMap::new();
-    tags3.insert("env".into(), vector_lib::event::Value::Bytes("dev".into()));
-    event3.insert(
-        "tags_history",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(tags3)]),
-    );
-
-    let mut metrics3 = vector_lib::event::ObjectMap::new();
-    metrics3.insert("cpu".into(), vector_lib::event::Value::Integer(60));
-    event3.insert(
-        "metrics_history",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(metrics3)]),
-    );
-
-    let mut headers3 = vector_lib::event::ObjectMap::new();
-    headers3.insert(
-        "content-type".into(),
-        vector_lib::event::Value::Bytes("application/json".into()),
-    );
-    event3.insert(
-        "request_headers",
-        vector_lib::event::Value::Object(headers3),
-    );
-
-    let mut metrics3_resp = vector_lib::event::ObjectMap::new();
-    metrics3_resp.insert("f0".into(), vector_lib::event::Value::Integer(404));
-    metrics3_resp.insert("f1".into(), vector_lib::event::Value::Integer(0));
-    metrics3_resp.insert(
-        "f2".into(),
-        vector_lib::event::Value::Float(NotNan::new(0.001).unwrap()),
-    );
-    event3.insert(
-        "response_metrics",
-        vector_lib::event::Value::Object(metrics3_resp),
-    );
-
-    event3.insert(
-        "tags",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Bytes("test".into())]),
-    );
-
-    let mut user_props3 = vector_lib::event::ObjectMap::new();
-    user_props3.insert(
-        "permissions".into(),
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Bytes("read".into())]),
-    );
-    event3.insert(
-        "user_properties",
-        vector_lib::event::Value::Object(user_props3),
-    );
-
-    event3.insert(
-        "array_with_nulls",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Integer(42)]),
-    );
-
-    // Named tuple with single element
-    let mut named_tuple3 = vector_lib::event::ObjectMap::new();
-    named_tuple3.insert(
-        "category".into(),
-        vector_lib::event::Value::Bytes("status".into()),
-    );
-    named_tuple3.insert(
-        "tag".into(),
-        vector_lib::event::Value::Bytes("active".into()),
-    );
-    event3.insert(
-        "array_with_named_tuple",
-        vector_lib::event::Value::Array(vec![vector_lib::event::Value::Object(named_tuple3)]),
-    );
-
-    events.push(event3.into());
-
-    run_and_assert_sink_compliance(sink, stream::iter(events), &SINK_TAGS).await;
-
-    let output = client.select_all(&table).await;
-    assert_eq!(3, output.rows);
-
-    // Verify event 1 - comprehensive data
-    let row1 = &output.data[0];
-    assert!(
-        row1.get("nested_int_array")
-            .and_then(|v| v.as_array())
-            .is_some()
-    );
-    assert!(row1.get("array_map").and_then(|v| v.as_object()).is_some());
-    // Tuples are returned as arrays from ClickHouse
-    assert!(
-        row1.get("tuple_with_array")
-            .and_then(|v| v.as_array())
-            .is_some()
-    );
-    assert!(row1.get("locations").and_then(|v| v.as_array()).is_some());
-    assert!(
-        row1.get("tags_history")
-            .and_then(|v| v.as_array())
-            .is_some()
-    );
-    assert!(
-        row1.get("request_headers")
-            .and_then(|v| v.as_object())
-            .is_some()
-    );
-    assert!(
-        row1.get("array_with_nulls")
-            .and_then(|v| v.as_array())
-            .is_some()
-    );
-
-    // Verify event 2 - empty collections
-    let row2 = &output.data[1];
-    let empty_nested = row2
-        .get("nested_int_array")
-        .and_then(|v| v.as_array())
-        .unwrap();
-    assert_eq!(0, empty_nested.len());
-    let empty_tags = row2.get("tags").and_then(|v| v.as_array()).unwrap();
-    assert_eq!(0, empty_tags.len());
-
-    // Verify event 3 - varied data
-    let row3 = &output.data[2];
-    let nested3 = row3
-        .get("nested_int_array")
-        .and_then(|v| v.as_array())
-        .unwrap();
-    assert_eq!(2, nested3.len());
-}
diff --git a/src/sinks/mod.rs b/src/sinks/mod.rs
index 75dcd683f..3a9c3c049 100644
--- a/src/sinks/mod.rs
+++ b/src/sinks/mod.rs
@@ -28,6 +28,8 @@ pub mod axiom;
 pub mod azure_blob;
 #[cfg(feature = "sinks-azure_blob")]
 pub mod azure_common;
+#[cfg(feature = "sinks-azure_logs_ingestion")]
+pub mod azure_logs_ingestion;
 #[cfg(feature = "sinks-azure_monitor_logs")]
 pub mod azure_monitor_logs;
 #[cfg(feature = "sinks-blackhole")]
diff --git a/src/sinks/prometheus/remote_write/config.rs b/src/sinks/prometheus/remote_write/config.rs
index d0dce3908..97560a777 100644
--- a/src/sinks/prometheus/remote_write/config.rs
+++ b/src/sinks/prometheus/remote_write/config.rs
@@ -178,14 +178,9 @@ impl SinkConfig for RemoteWriteConfig {
             None => None,
         };
 
-        let healthcheck_endpoint = match cx.healthcheck.uri {
-            Some(uri) => uri.uri,
-            None => endpoint.clone(),
-        };
-
         let healthcheck = healthcheck(
             client.clone(),
-            healthcheck_endpoint,
+            endpoint.clone(),
             self.compression,
             auth.clone(),
         )
diff --git a/src/sinks/util/encoding.rs b/src/sinks/util/encoding.rs
index 1dda331b3..9d0c383d8 100644
--- a/src/sinks/util/encoding.rs
+++ b/src/sinks/util/encoding.rs
@@ -125,7 +125,6 @@ impl Encoder<Vec<Event>> for (Transformer, vector_lib::codecs::BatchEncoder) {
         encoder
             .encode(transformed_events, &mut bytes)
             .map_err(|error| {
-                #[cfg(feature = "codecs-arrow")]
                 if let vector_lib::codecs::encoding::Error::SchemaConstraintViolation(
                     ref constraint_error,
                 ) = error
diff --git a/src/sources/host_metrics/filesystem.rs b/src/sources/host_metrics/filesystem.rs
index def1680d6..f6fb50ed5 100644
--- a/src/sources/host_metrics/filesystem.rs
+++ b/src/sources/host_metrics/filesystem.rs
@@ -2,8 +2,6 @@ use futures::StreamExt;
 use heim::units::information::byte;
 #[cfg(not(windows))]
 use heim::units::ratio::ratio;
-#[cfg(unix)]
-use nix::sys::statvfs::statvfs;
 use vector_lib::{configurable::configurable_component, metric_tags};
 
 use super::{FilterList, HostMetrics, default_all_devices, example_devices, filter_result};
@@ -130,32 +128,8 @@ impl HostMetrics {
                     output.gauge(
                         "filesystem_used_ratio",
                         usage.ratio().get::<ratio>() as f64,
-                        tags.clone(),
+                        tags,
                     );
-
-                    // inode metrics via a second statvfs call - heim's Usage wraps
-                    // libc::statvfs internally but doesn't expose inode fields
-                    // (f_files, f_ffree). the kernel caches statvfs for local
-                    // filesystems so the overhead is negligible, but network mounts
-                    // may pay a small extra cost.
-                    #[cfg(unix)]
-                    if let Ok(stat) = statvfs(partition.mount_point()) {
-                        let inodes_total = stat.files() as f64;
-                        let inodes_free = stat.files_free() as f64;
-                        let inodes_used = (inodes_total - inodes_free).max(0.0);
-                        let inodes_used_ratio = if inodes_total > 0.0 {
-                            inodes_used / inodes_total
-                        } else {
-                            0.0
-                        };
-
-                        output.gauge("filesystem_inodes_total", inodes_total, tags.clone());
-                        output.gauge("filesystem_inodes_free", inodes_free, tags.clone());
-                        output.gauge("filesystem_inodes_used", inodes_used, tags.clone());
-                        output.gauge("filesystem_inodes_used_ratio", inodes_used_ratio, tags);
-                    }
-                    #[cfg(windows)]
-                    drop(tags);
                 }
             }
             Err(error) => {
@@ -187,40 +161,17 @@ mod tests {
             .await;
         let metrics = buffer.metrics;
         assert!(!metrics.is_empty());
+        assert!(metrics.len().is_multiple_of(4));
         assert!(all_gauges(&metrics));
 
-        // Base metrics (these are always present)
-        let base_metrics = [
+        // There are exactly three filesystem_* names
+        for name in &[
             "filesystem_free_bytes",
             "filesystem_total_bytes",
             "filesystem_used_bytes",
             "filesystem_used_ratio",
-        ];
-
-        // Each filesystem should have all 4 base metrics
-        let num_filesystems = count_name(&metrics, "filesystem_free_bytes");
-        assert!(num_filesystems > 0);
-        for name in &base_metrics {
-            assert_eq!(count_name(&metrics, name), num_filesystems, "name={name}");
-        }
-
-        // Inode metrics are present for filesystems that support statvfs
-        // (some virtual filesystems like /proc, /sys might not)
-        let inode_metrics = [
-            "filesystem_inodes_total",
-            "filesystem_inodes_free",
-            "filesystem_inodes_used",
-            "filesystem_inodes_used_ratio",
-        ];
-        let num_inode_total = count_name(&metrics, "filesystem_inodes_total");
-        assert!(
-            num_inode_total > 0,
-            "Expected at least one filesystem to report inode metrics"
-        );
-
-        // For filesystems that report inodes, all 4 inode metrics should be present
-        for name in &inode_metrics {
-            assert_eq!(count_name(&metrics, name), num_inode_total, "name={name}");
+        ] {
+            assert_eq!(count_name(&metrics, name), metrics.len() / 4, "name={name}");
         }
 
         // They should all have "filesystem" and "mountpoint" tags
diff --git a/src/sources/websocket/source.rs b/src/sources/websocket/source.rs
index a195f2b48..cf3c75d40 100644
--- a/src/sources/websocket/source.rs
+++ b/src/sources/websocket/source.rs
@@ -1,6 +1,6 @@
 use std::pin::Pin;
 
-use chrono::{DateTime, Utc};
+use chrono::Utc;
 use futures::{Sink, Stream, StreamExt, pin_mut, sink::SinkExt};
 use snafu::Snafu;
 use tokio::time;
@@ -235,10 +235,9 @@ impl WebSocketSource {
                         kind,
                     });
 
-                    let now = Utc::now();
                     let events_with_meta = events.into_iter().map(|mut event| {
                         if let Event::Log(event) = &mut event {
-                            self.add_metadata(event, now);
+                            self.add_metadata(event);
                         }
                         event
                     });
@@ -256,10 +255,10 @@ impl WebSocketSource {
         }
     }
 
-    fn add_metadata(&self, event: &mut LogEvent, now: DateTime<Utc>) {
+    fn add_metadata(&self, event: &mut LogEvent) {
         self.params
             .log_namespace
-            .insert_standard_vector_source_metadata(event, WebSocketConfig::NAME, now);
+            .insert_standard_vector_source_metadata(event, WebSocketConfig::NAME, Utc::now());
     }
 
     async fn reconnect(
diff --git a/src/test_util/mock/sinks/completion.rs b/src/test_util/mock/sinks/completion.rs
deleted file mode 100644
index a390f33c9..000000000
--- a/src/test_util/mock/sinks/completion.rs
+++ /dev/null
@@ -1,95 +0,0 @@
-use std::sync::{Arc, Mutex};
-
-use async_trait::async_trait;
-use futures_util::{FutureExt, StreamExt, future, stream::BoxStream};
-use tokio::sync::oneshot::Sender;
-use vector_lib::{
-    config::{AcknowledgementsConfig, Input},
-    configurable::configurable_component,
-    event::Event,
-    sink::{StreamSink, VectorSink},
-};
-
-use crate::{
-    config::{SinkConfig, SinkContext},
-    sinks::Healthcheck,
-};
-
-/// Configuration for the `test_completion` sink.
-#[configurable_component(sink("test_completion", "Test (completion)."))]
-#[derive(Clone, Debug, Default)]
-pub struct CompletionSinkConfig {
-    #[serde(skip)]
-    expected: usize,
-
-    #[serde(skip)]
-    completion_tx: Arc<Mutex<Option<Sender<bool>>>>,
-}
-
-impl_generate_config_from_default!(CompletionSinkConfig);
-
-impl CompletionSinkConfig {
-    pub fn new(expected: usize, completion_tx: Sender<bool>) -> Self {
-        Self {
-            expected,
-            completion_tx: Arc::new(Mutex::new(Some(completion_tx))),
-        }
-    }
-}
-
-#[async_trait]
-#[typetag::serde(name = "test_completion")]
-impl SinkConfig for CompletionSinkConfig {
-    async fn build(&self, _cx: SinkContext) -> crate::Result<(VectorSink, Healthcheck)> {
-        let completion_tx = self
-            .completion_tx
-            .lock()
-            .expect("completion sink mutex poisoned")
-            .take();
-
-        let sink = CompletionSink {
-            remaining: self.expected,
-            completion_tx,
-        };
-        let healthcheck = future::ready(Ok(())).boxed();
-
-        Ok((VectorSink::from_event_streamsink(sink), healthcheck))
-    }
-
-    fn input(&self) -> Input {
-        Input::all()
-    }
-
-    fn acknowledgements(&self) -> &AcknowledgementsConfig {
-        &AcknowledgementsConfig::DEFAULT
-    }
-}
-
-struct CompletionSink {
-    remaining: usize,
-    completion_tx: Option<Sender<bool>>,
-}
-
-#[async_trait]
-impl StreamSink<Event> for CompletionSink {
-    async fn run(mut self: Box<Self>, mut input: BoxStream<'_, Event>) -> Result<(), ()> {
-        while let Some(event) = input.next().await {
-            drop(event);
-
-            if self.remaining > 0 {
-                self.remaining -= 1;
-                if self.remaining == 0
-                    && let Some(tx) = self.completion_tx.take()
-                {
-                    let _ = tx.send(true);
-                }
-            }
-        }
-
-        if let Some(tx) = self.completion_tx.take() {
-            let _ = tx.send(self.remaining == 0);
-        }
-
-        Ok(())
-    }
-}
diff --git a/src/test_util/mock/sinks/mod.rs b/src/test_util/mock/sinks/mod.rs
index 226391007..862d74036 100644
--- a/src/test_util/mock/sinks/mod.rs
+++ b/src/test_util/mock/sinks/mod.rs
@@ -4,9 +4,6 @@ pub use self::backpressure::BackpressureSinkConfig;
 mod basic;
 pub use self::basic::BasicSinkConfig;
 
-mod completion;
-pub use self::completion::CompletionSinkConfig;
-
 mod error;
 pub use self::error::ErrorSinkConfig;
 
diff --git a/src/test_util/mock/transforms/noop.rs b/src/test_util/mock/transforms/noop.rs
index 355480ddf..83506160c 100644
--- a/src/test_util/mock/transforms/noop.rs
+++ b/src/test_util/mock/transforms/noop.rs
@@ -1,7 +1,7 @@
-use std::{pin::Pin, time::Duration};
+use std::pin::Pin;
 
 use async_trait::async_trait;
-use futures_util::{Stream, StreamExt as _};
+use futures_util::Stream;
 use vector_lib::{
     config::{DataType, Input, TransformOutput},
     configurable::configurable_component,
@@ -19,31 +19,17 @@ use crate::config::{GenerateConfig, OutputId, TransformConfig, TransformContext}
 pub struct NoopTransformConfig {
     #[configurable(derived)]
     transform_type: TransformType,
-
-    /// Optional per-event/array delay, in milliseconds.
-    ///
-    /// This is intended for tests that need deterministic, non-zero component latency.
-    #[serde(default, skip_serializing_if = "Option::is_none")]
-    delay_ms: Option<u64>,
 }
 
 impl GenerateConfig for NoopTransformConfig {
     fn generate_config() -> toml::Value {
         toml::Value::try_from(&Self {
             transform_type: TransformType::Function,
-            delay_ms: None,
         })
         .unwrap()
     }
 }
 
-impl NoopTransformConfig {
-    pub fn with_delay_ms(mut self, delay_ms: u64) -> Self {
-        self.delay_ms = Some(delay_ms);
-        self
-    }
-}
-
 #[async_trait]
 #[typetag::serde(name = "test_noop")]
 impl TransformConfig for NoopTransformConfig {
@@ -66,55 +52,37 @@ impl TransformConfig for NoopTransformConfig {
     }
 
     async fn build(&self, _: &TransformContext) -> crate::Result<Transform> {
-        let delay = self.delay_ms.map(Duration::from_millis);
         match self.transform_type {
-            TransformType::Function => Ok(Transform::Function(Box::new(NoopTransform { delay }))),
-            TransformType::Synchronous => {
-                Ok(Transform::Synchronous(Box::new(NoopTransform { delay })))
-            }
-            TransformType::Task => Ok(Transform::Task(Box::new(NoopTransform { delay }))),
+            TransformType::Function => Ok(Transform::Function(Box::new(NoopTransform))),
+            TransformType::Synchronous => Ok(Transform::Synchronous(Box::new(NoopTransform))),
+            TransformType::Task => Ok(Transform::Task(Box::new(NoopTransform))),
         }
     }
 }
 
 impl From<TransformType> for NoopTransformConfig {
     fn from(transform_type: TransformType) -> Self {
-        Self {
-            transform_type,
-            delay_ms: None,
-        }
+        Self { transform_type }
     }
 }
 
 #[derive(Clone)]
-struct NoopTransform {
-    delay: Option<Duration>,
-}
+struct NoopTransform;
 
 impl FunctionTransform for NoopTransform {
     fn transform(&mut self, output: &mut OutputBuffer, event: Event) {
-        if let Some(delay) = self.delay {
-            std::thread::sleep(delay);
-        }
         output.push(event);
     }
 }
 
 impl<T> TaskTransform<T> for NoopTransform
 where
-    T: EventContainer + Send + 'static,
+    T: EventContainer + 'static,
 {
     fn transform(
         self: Box<Self>,
         task: Pin<Box<dyn futures_util::Stream<Item = T> + Send>>,
     ) -> Pin<Box<dyn Stream<Item = T> + Send>> {
-        if let Some(delay) = self.delay {
-            Box::pin(task.then(move |item| async move {
-                tokio::time::sleep(delay).await;
-                item
-            }))
-        } else {
-            Box::pin(task)
-        }
+        Box::pin(task)
     }
 }
diff --git a/src/top/cmd.rs b/src/top/cmd.rs
index 641006378..f63b2e313 100644
--- a/src/top/cmd.rs
+++ b/src/top/cmd.rs
@@ -1,16 +1,14 @@
-use std::collections::BTreeMap;
 use std::time::Duration;
 
 use chrono::Local;
 use futures_util::future::join_all;
-use regex::Regex;
 use tokio::sync::{mpsc, oneshot};
 use vector_lib::api_client::{Client, connect_subscription_client};
 
 use vector_lib::top::{
     dashboard::{init_dashboard, is_tty},
     metrics,
-    state::{self, ConnectionStatus, EventType, State},
+    state::{self, ConnectionStatus, EventType},
 };
 
 /// Delay (in milliseconds) before attempting to reconnect to the Vector API
@@ -55,20 +53,11 @@ pub async fn cmd(opts: &super::Opts) -> exitcode::ExitCode {
 pub async fn top(opts: &super::Opts, client: Client, dashboard_title: &str) -> exitcode::ExitCode {
     // Channel for updating state via event messages
     let (tx, rx) = tokio::sync::mpsc::channel(20);
-    let mut starting_state = State::new(BTreeMap::new());
-    starting_state.sort_state.column = opts.sort_field;
-    starting_state.sort_state.reverse = opts.sort_desc;
-    starting_state.filter_state.column = opts.filter_field;
-    starting_state.filter_state.pattern = opts
-        .filter_value
-        .as_deref()
-        .map(Regex::new)
-        .and_then(Result::ok);
-    let state_rx = state::updater(rx, starting_state).await;
+    let state_rx = state::updater(rx).await;
     // Channel for shutdown signal
     let (shutdown_tx, shutdown_rx) = oneshot::channel::<()>();
 
-    let connection = tokio::spawn(subscription(opts.clone(), client, tx.clone(), shutdown_tx));
+    let connection = tokio::spawn(subscription(opts.clone(), client, tx, shutdown_tx));
 
     // Initialize the dashboard
     match init_dashboard(
@@ -76,7 +65,6 @@ pub async fn top(opts: &super::Opts, client: Client, dashboard_title: &str) -> e
         opts.url().as_str(),
         opts.interval,
         opts.human_metrics,
-        tx,
         state_rx,
         shutdown_rx,
     )
diff --git a/src/top/mod.rs b/src/top/mod.rs
index 5fc101f7b..025596718 100644
--- a/src/top/mod.rs
+++ b/src/top/mod.rs
@@ -6,7 +6,6 @@ use glob::Pattern;
 
 pub use cmd::{cmd, top};
 use url::Url;
-use vector_lib::top::state::{FilterColumn, SortColumn};
 
 use crate::config::api::default_graphql_url;
 
@@ -35,24 +34,6 @@ pub struct Opts {
     /// Components IDs to observe (comma-separated; accepts glob patterns)
     #[arg(default_value = "*", value_delimiter(','), short = 'c', long)]
     components: Vec<Pattern>,
-
-    /// Field to sort values to by default (can be changed while running).
-    #[arg(short = 's', long)]
-    sort_field: Option<SortColumn>,
-
-    /// Sort descending instead of ascending.
-    #[arg(long, default_value_t = false)]
-    sort_desc: bool,
-
-    /// Field to filter values by default (can be changed while running).
-    #[arg(default_value = "id", long)]
-    filter_field: FilterColumn,
-
-    /// Filter to apply to the chosen field (ID by default).
-    ///
-    /// This accepts Regex patterns.
-    #[arg(short = 'f', long)]
-    filter_value: Option<String>,
 }
 
 impl Opts {
diff --git a/src/topology/builder.rs b/src/topology/builder.rs
index 0a8e7e2e9..ba19f1dc6 100644
--- a/src/topology/builder.rs
+++ b/src/topology/builder.rs
@@ -22,11 +22,10 @@ use vector_lib::{
         BufferType, WhenFull,
         topology::{
             builder::TopologyBuilder,
-            channel::{BufferReceiver, BufferSender, ChannelMetricMetadata, LimitedReceiver},
+            channel::{BufferReceiver, BufferSender, ChannelMetricMetadata},
         },
     },
     internal_event::{self, CountByteSize, EventsSent, InternalEventHandle as _, Registered},
-    latency::LatencyRecorder,
     schema::Definition,
     source_sender::{CHUNK_SIZE, SourceSenderItem},
     transform::update_runtime_schema_definition,
@@ -273,13 +272,34 @@ impl<'a> Builder<'a> {
             let mut schema_definitions = HashMap::with_capacity(source_outputs.len());
 
             for output in source_outputs.into_iter() {
-                let rx = builder.add_source_output(output.clone(), key.clone());
+                let mut rx = builder.add_source_output(output.clone(), key.clone());
 
-                let (fanout, control) = Fanout::new();
+                let (mut fanout, control) = Fanout::new();
                 let source_type = source.inner.get_component_name();
                 let source = Arc::new(key.clone());
 
-                let pump = run_source_output_pump(rx, fanout, source, source_type);
+                let pump = async move {
+                    debug!("Source pump starting.");
+
+                    while let Some(SourceSenderItem {
+                        events: mut array,
+                        send_reference,
+                    }) = rx.next().await
+                    {
+                        array.set_output_id(&source);
+                        array.set_source_type(source_type);
+                        fanout
+                            .send(array, Some(send_reference))
+                            .await
+                            .map_err(|e| {
+                                debug!("Source pump finished with an error.");
+                                TaskError::wrapped(e)
+                            })?;
+                    }
+
+                    debug!("Source pump finished normally.");
+                    Ok(TaskOutput::Source)
+                };
 
                 pumps.push(pump.instrument(span.clone()));
                 controls.insert(
@@ -516,7 +536,7 @@ impl<'a> Builder<'a> {
                 .insert(key.clone(), (input_tx, node.inputs.clone()));
 
             let (transform_task, transform_outputs) =
-                self.build_transform(transform, node, input_rx);
+                build_transform(transform, node, input_rx, &self.utilization_registry);
 
             self.outputs.extend(transform_outputs);
             self.tasks.insert(key.clone(), transform_task);
@@ -710,201 +730,6 @@ impl<'a> Builder<'a> {
             self.detach_triggers.insert(key.clone(), trigger);
         }
     }
-
-    fn build_transform(
-        &self,
-        transform: Transform,
-        node: TransformNode,
-        input_rx: BufferReceiver<EventArray>,
-    ) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
-        match transform {
-            // TODO: avoid the double boxing for function transforms here
-            Transform::Function(t) => self.build_sync_transform(Box::new(t), node, input_rx),
-            Transform::Synchronous(t) => self.build_sync_transform(t, node, input_rx),
-            Transform::Task(t) => self.build_task_transform(
-                t,
-                input_rx,
-                node.input_details.data_type(),
-                node.typetag,
-                &node.key,
-                &node.outputs,
-            ),
-        }
-    }
-
-    fn build_sync_transform(
-        &self,
-        t: Box<dyn SyncTransform>,
-        node: TransformNode,
-        input_rx: BufferReceiver<EventArray>,
-    ) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
-        let (outputs, controls) = TransformOutputs::new(node.outputs, &node.key);
-
-        let sender = self
-            .utilization_registry
-            .add_component(node.key.clone(), gauge!("utilization"));
-        let runner = Runner::new(
-            t,
-            input_rx,
-            sender,
-            node.input_details.data_type(),
-            outputs,
-            LatencyRecorder::new(self.config.global.latency_ewma_alpha),
-        );
-        let transform = if node.enable_concurrency {
-            runner.run_concurrently().boxed()
-        } else {
-            runner.run_inline().boxed()
-        };
-
-        let transform = async move {
-            debug!("Synchronous transform starting.");
-
-            match transform.await {
-                Ok(v) => {
-                    debug!("Synchronous transform finished normally.");
-                    Ok(v)
-                }
-                Err(e) => {
-                    debug!("Synchronous transform finished with an error.");
-                    Err(e)
-                }
-            }
-        };
-
-        let mut output_controls = HashMap::new();
-        for (name, control) in controls {
-            let id = name
-                .map(|name| OutputId::from((&node.key, name)))
-                .unwrap_or_else(|| OutputId::from(&node.key));
-            output_controls.insert(id, control);
-        }
-
-        let task = Task::new(node.key.clone(), node.typetag, transform);
-
-        (task, output_controls)
-    }
-
-    fn build_task_transform(
-        &self,
-        t: Box<dyn TaskTransform<EventArray>>,
-        input_rx: BufferReceiver<EventArray>,
-        input_type: DataType,
-        typetag: &str,
-        key: &ComponentKey,
-        outputs: &[TransformOutput],
-    ) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
-        let (mut fanout, control) = Fanout::new();
-
-        let sender = self
-            .utilization_registry
-            .add_component(key.clone(), gauge!("utilization"));
-        let input_rx = wrap(sender, key.clone(), input_rx.into_stream());
-
-        let events_received = register!(EventsReceived);
-        let filtered = input_rx
-            .filter(move |events| ready(filter_events_type(events, input_type)))
-            .inspect(move |events| {
-                events_received.emit(CountByteSize(
-                    events.len(),
-                    events.estimated_json_encoded_size_of(),
-                ))
-            });
-        let events_sent = register!(EventsSent::from(internal_event::Output(None)));
-        let output_id = Arc::new(OutputId {
-            component: key.clone(),
-            port: None,
-        });
-        let latency_recorder = LatencyRecorder::new(self.config.global.latency_ewma_alpha);
-
-        // Task transforms can only write to the default output, so only a single schema def map is needed
-        let schema_definition_map = outputs
-            .iter()
-            .find(|x| x.port.is_none())
-            .expect("output for default port required for task transforms")
-            .log_schema_definitions
-            .clone()
-            .into_iter()
-            .map(|(key, value)| (key, Arc::new(value)))
-            .collect();
-
-        let stream = t
-            .transform(Box::pin(filtered))
-            .map(move |mut events| {
-                for event in events.iter_events_mut() {
-                    update_runtime_schema_definition(event, &output_id, &schema_definition_map);
-                }
-                let now = Instant::now();
-                latency_recorder.on_send(&mut events, now);
-                (events, now)
-            })
-            .inspect(move |(events, _): &(EventArray, Instant)| {
-                events_sent.emit(CountByteSize(
-                    events.len(),
-                    events.estimated_json_encoded_size_of(),
-                ));
-            });
-        let transform = async move {
-            debug!("Task transform starting.");
-
-            match fanout.send_stream(stream).await {
-                Ok(()) => {
-                    debug!("Task transform finished normally.");
-                    Ok(TaskOutput::Transform)
-                }
-                Err(e) => {
-                    debug!("Task transform finished with an error.");
-                    Err(TaskError::wrapped(e))
-                }
-            }
-        }
-        .boxed();
-
-        let mut outputs = HashMap::new();
-        outputs.insert(OutputId::from(key), control);
-
-        let task = Task::new(key.clone(), typetag, transform);
-
-        (task, outputs)
-    }
-}
-
-async fn run_source_output_pump(
-    mut rx: LimitedReceiver<SourceSenderItem>,
-    mut fanout: Fanout,
-    source: Arc<ComponentKey>,
-    source_type: &'static str,
-) -> TaskResult {
-    debug!("Source pump starting.");
-
-    while let Some(SourceSenderItem {
-        events: mut array,
-        send_reference,
-    }) = rx.next().await
-    {
-        // Even though we have a `send_reference` timestamp above, that reference time is when
-        // the events were enqueued in the `SourceSender`, not when they were pulled out of the
-        // `rx` stream on this end. Since those times can be quite different (due to blocking
-        // inherent to the fanout send operation), we set the `last_transform_timestamp` to the
-        // current time instead to get an accurate reference for when the events started waiting
-        // for the first transform.
-        let now = Instant::now();
-        array.for_each_metadata_mut(|metadata| {
-            metadata.set_source_id(Arc::clone(&source));
-            metadata.set_source_type(source_type);
-            metadata.set_last_transform_timestamp(now);
-        });
-        fanout
-            .send(array, Some(send_reference))
-            .await
-            .map_err(|e| {
-                debug!("Source pump finished with an error.");
-                TaskError::wrapped(e)
-            })?;
-    }
-
-    debug!("Source pump finished normally.");
-    Ok(TaskOutput::Source)
 }
 
 pub async fn reload_enrichment_tables(config: &Config) {
@@ -1115,13 +940,80 @@ impl TransformNode {
     }
 }
 
+fn build_transform(
+    transform: Transform,
+    node: TransformNode,
+    input_rx: BufferReceiver<EventArray>,
+    utilization_registry: &UtilizationRegistry,
+) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
+    match transform {
+        // TODO: avoid the double boxing for function transforms here
+        Transform::Function(t) => {
+            build_sync_transform(Box::new(t), node, input_rx, utilization_registry)
+        }
+        Transform::Synchronous(t) => build_sync_transform(t, node, input_rx, utilization_registry),
+        Transform::Task(t) => build_task_transform(
+            t,
+            input_rx,
+            node.input_details.data_type(),
+            node.typetag,
+            &node.key,
+            &node.outputs,
+            utilization_registry,
+        ),
+    }
+}
+
+fn build_sync_transform(
+    t: Box<dyn SyncTransform>,
+    node: TransformNode,
+    input_rx: BufferReceiver<EventArray>,
+    utilization_registry: &UtilizationRegistry,
+) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
+    let (outputs, controls) = TransformOutputs::new(node.outputs, &node.key);
+
+    let sender = utilization_registry.add_component(node.key.clone(), gauge!("utilization"));
+    let runner = Runner::new(t, input_rx, sender, node.input_details.data_type(), outputs);
+    let transform = if node.enable_concurrency {
+        runner.run_concurrently().boxed()
+    } else {
+        runner.run_inline().boxed()
+    };
+
+    let transform = async move {
+        debug!("Synchronous transform starting.");
+
+        match transform.await {
+            Ok(v) => {
+                debug!("Synchronous transform finished normally.");
+                Ok(v)
+            }
+            Err(e) => {
+                debug!("Synchronous transform finished with an error.");
+                Err(e)
+            }
+        }
+    };
+
+    let mut output_controls = HashMap::new();
+    for (name, control) in controls {
+        let id = name
+            .map(|name| OutputId::from((&node.key, name)))
+            .unwrap_or_else(|| OutputId::from(&node.key));
+        output_controls.insert(id, control);
+    }
+
+    let task = Task::new(node.key.clone(), node.typetag, transform);
+
+    (task, output_controls)
+}
+
 struct Runner {
     transform: Box<dyn SyncTransform>,
     input_rx: Option<BufferReceiver<EventArray>>,
     input_type: DataType,
     outputs: TransformOutputs,
     timer_tx: UtilizationComponentSender,
-    latency_recorder: LatencyRecorder,
     events_received: Registered<EventsReceived>,
 }
 
@@ -1132,7 +1024,6 @@ impl Runner {
         timer_tx: UtilizationComponentSender,
         input_type: DataType,
         outputs: TransformOutputs,
-        latency_recorder: LatencyRecorder,
     ) -> Self {
         Self {
             transform,
@@ -1140,7 +1031,6 @@ impl Runner {
             input_type,
             outputs,
             timer_tx,
-            latency_recorder,
             events_received: register!(EventsReceived),
         }
     }
@@ -1156,8 +1046,6 @@ impl Runner {
 
     async fn send_outputs(&mut self, outputs_buf: &mut TransformOutputsBuf) -> crate::Result<()> {
         self.timer_tx.try_send_start_wait();
-        let now = Instant::now();
-        outputs_buf.for_each_array_mut(|array| self.latency_recorder.on_send(array, now));
         self.outputs.send(outputs_buf).await
     }
 
@@ -1207,7 +1095,8 @@ impl Runner {
 
                 result = in_flight.next(), if !in_flight.is_empty() => {
                     match result {
-                        Some(Ok(mut outputs_buf)) => {
+                        Some(Ok(outputs_buf)) => {
+                            let mut outputs_buf: TransformOutputsBuf = outputs_buf;
                             self.send_outputs(&mut outputs_buf).await
                                 .map_err(TaskError::wrapped)?;
                         }
@@ -1252,3 +1141,81 @@ impl Runner {
         Ok(TaskOutput::Transform)
     }
 }
+
+fn build_task_transform(
+    t: Box<dyn TaskTransform<EventArray>>,
+    input_rx: BufferReceiver<EventArray>,
+    input_type: DataType,
+    typetag: &str,
+    key: &ComponentKey,
+    outputs: &[TransformOutput],
+    utilization_registry: &UtilizationRegistry,
+) -> (Task, HashMap<OutputId, fanout::ControlChannel>) {
+    let (mut fanout, control) = Fanout::new();
+
+    let sender = utilization_registry.add_component(key.clone(), gauge!("utilization"));
+    let input_rx = wrap(sender, key.clone(), input_rx.into_stream());
+
+    let events_received = register!(EventsReceived);
+    let filtered = input_rx
+        .filter(move |events| ready(filter_events_type(events, input_type)))
+        .inspect(move |events| {
+            events_received.emit(CountByteSize(
+                events.len(),
+                events.estimated_json_encoded_size_of(),
+            ))
+        });
+    let events_sent = register!(EventsSent::from(internal_event::Output(None)));
+    let output_id = Arc::new(OutputId {
+        component: key.clone(),
+        port: None,
+    });
+
+    // Task transforms can only write to the default output, so only a single schema def map is needed
+    let schema_definition_map = outputs
+        .iter()
+        .find(|x| x.port.is_none())
+        .expect("output for default port required for task transforms")
+        .log_schema_definitions
+        .clone()
+        .into_iter()
+        .map(|(key, value)| (key, Arc::new(value)))
+        .collect();
+
+    let stream = t
+        .transform(Box::pin(filtered))
+        .map(move |mut events| {
+            for event in events.iter_events_mut() {
+                update_runtime_schema_definition(event, &output_id, &schema_definition_map);
+            }
+            (events, Instant::now())
+        })
+        .inspect(move |(events, _): &(EventArray, Instant)| {
+            events_sent.emit(CountByteSize(
+                events.len(),
+                events.estimated_json_encoded_size_of(),
+            ));
+        });
+    let transform = async move {
+        debug!("Task transform starting.");
+
+        match fanout.send_stream(stream).await {
+            Ok(()) => {
+                debug!("Task transform finished normally.");
+                Ok(TaskOutput::Transform)
+            }
+            Err(e) => {
+                debug!("Task transform finished with an error.");
+                Err(TaskError::wrapped(e))
+            }
+        }
+    }
+    .boxed();
+
+    let mut outputs = HashMap::new();
+    outputs.insert(OutputId::from(key), control);
+
+    let task = Task::new(key.clone(), typetag, transform);
+
+    (task, outputs)
+}
diff --git a/src/topology/test/compliance.rs b/src/topology/test/compliance.rs
index b54fa3af2..5f3783055 100644
--- a/src/topology/test/compliance.rs
+++ b/src/topology/test/compliance.rs
@@ -19,19 +19,6 @@ use crate::{
     topology::RunningTopology,
 };
 
-const TEST_SOURCE_COMPONENT_ID: &str = "in";
-const TEST_UPSTREAM_COMPONENT_ID: &str = "transform";
-const TEST_SOURCE_TYPE: &str = "unit_test";
-
-fn set_expected_source_metadata(event: &mut Event) {
-    event.set_source_id(Arc::new(ComponentKey::from(TEST_SOURCE_COMPONENT_ID)));
-    event.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
-    event.set_source_type(TEST_SOURCE_TYPE);
-    event
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
-}
-
 async fn create_topology(
     event: Event,
     transform_type: TransformType,
@@ -71,7 +58,11 @@ async fn test_function_transform_single_event() {
         let mut events = events.into_events().collect::<Vec<_>>();
         assert_eq!(events.len(), 1);
 
-        set_expected_source_metadata(&mut original_event);
+        original_event.set_source_id(Arc::new(ComponentKey::from("in")));
+        original_event.set_upstream_id(Arc::new(OutputId::from("transform")));
+        original_event
+            .metadata_mut()
+            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
         let event = events.remove(0);
         assert_eq!(original_event, event);
@@ -92,7 +83,11 @@ async fn test_sync_transform_single_event() {
         let mut events = events.into_events().collect::<Vec<_>>();
         assert_eq!(events.len(), 1);
 
-        set_expected_source_metadata(&mut original_event);
+        original_event.set_source_id(Arc::new(ComponentKey::from("in")));
+        original_event.set_upstream_id(Arc::new(OutputId::from("transform")));
+        original_event
+            .metadata_mut()
+            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
         let event = events.remove(0);
         assert_eq!(original_event, event);
@@ -112,7 +107,11 @@ async fn test_task_transform_single_event() {
         let mut events = events.into_events().collect::<Vec<_>>();
         assert_eq!(events.len(), 1);
 
-        set_expected_source_metadata(&mut original_event);
+        original_event.set_source_id(Arc::new(ComponentKey::from("in")));
+        original_event.set_upstream_id(Arc::new(OutputId::from("transform")));
+        original_event
+            .metadata_mut()
+            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
         let event = events.remove(0);
         assert_eq!(original_event, event);
diff --git a/src/topology/test/latency_metrics.rs b/src/topology/test/latency_metrics.rs
deleted file mode 100644
index 6b4dac1a4..000000000
--- a/src/topology/test/latency_metrics.rs
+++ /dev/null
@@ -1,147 +0,0 @@
-use std::time::Instant;
-use tokio::{
-    sync::oneshot,
-    time::{Duration, timeout},
-};
-use vector_lib::metrics::Controller;
-
-use crate::{
-    config::Config,
-    event::{Event, LogEvent, Metric, MetricValue},
-    test_util::{
-        mock::{
-            basic_source,
-            sinks::CompletionSinkConfig,
-            transforms::{NoopTransformConfig, TransformType},
-        },
-        start_topology, trace_init,
-    },
-};
-
-const EVENT_COUNT: usize = 100;
-const TRANSFORM_DELAY_MS: u64 = 10;
-const SOURCE_ID: &str = "latency_source";
-const TRANSFORM_ID: &str = "latency_delay";
-const TRANSFORM_TYPE: &str = "test_noop";
-const TRANSFORM_KIND: &str = "transform";
-const SINK_ID: &str = "latency_sink";
-
-struct LatencyTestRun {
-    metrics: Vec<Metric>,
-    elapsed_time: f64,
-}
-
-#[tokio::test]
-async fn component_latency_metrics_emitted() {
-    let run = run_latency_topology().await;
-
-    assert_histogram_count(
-        &run.metrics,
-        "component_latency_seconds",
-        has_component_tags,
-    );
-    assert_gauge_range(
-        &run.metrics,
-        "component_latency_mean_seconds",
-        has_component_tags,
-        TRANSFORM_DELAY_MS as f64 / 1000.0,
-        run.elapsed_time,
-    );
-}
-
-async fn run_latency_topology() -> LatencyTestRun {
-    trace_init();
-
-    let controller = Controller::get().expect("metrics controller");
-    controller.reset();
-
-    let (mut source_tx, source_config) = basic_source();
-    let transform_config =
-        NoopTransformConfig::from(TransformType::Task).with_delay_ms(TRANSFORM_DELAY_MS);
-    let (sink_done_tx, sink_done_rx) = oneshot::channel();
-    let sink_config = CompletionSinkConfig::new(EVENT_COUNT, sink_done_tx);
-
-    let mut config = Config::builder();
-    config.add_source(SOURCE_ID, source_config);
-    config.add_transform(TRANSFORM_ID, &[SOURCE_ID], transform_config);
-    config.add_sink(SINK_ID, &[TRANSFORM_ID], sink_config);
-
-    let start_time = Instant::now();
-    let (topology, _) = start_topology(config.build().unwrap(), false).await;
-
-    for idx in 0..EVENT_COUNT {
-        let event = Event::Log(LogEvent::from(format!("payload-{idx}")));
-        source_tx.send_event(event).await.unwrap();
-    }
-
-    drop(source_tx);
-
-    let completed = timeout(Duration::from_secs(5), sink_done_rx)
-        .await
-        .expect("timed out waiting for completion sink to finish")
-        .expect("completion sink sender dropped");
-    assert!(
-        completed,
-        "completion sink finished before receiving all events"
-    );
-
-    topology.stop().await;
-    let elapsed_time = start_time.elapsed().as_secs_f64();
-
-    LatencyTestRun {
-        metrics: controller.capture_metrics(),
-        elapsed_time,
-    }
-}
-
-fn assert_histogram_count(metrics: &[Metric], metric_name: &str, tags_match: fn(&Metric) -> bool) {
-    let histogram = metrics
-        .iter()
-        .find(|metric| metric.name() == metric_name && tags_match(metric))
-        .unwrap_or_else(|| panic!("{metric_name} histogram missing"));
-
-    match histogram.value() {
-        MetricValue::AggregatedHistogram { count, .. } => {
-            assert_eq!(
-                *count, EVENT_COUNT as u64,
-                "histogram count should match number of events"
-            );
-        }
-        other => panic!("expected aggregated histogram, got {other:?}"),
-    }
-}
-
-fn assert_gauge_range(
-    metrics: &[Metric],
-    metric_name: &str,
-    tags_match: fn(&Metric) -> bool,
-    expected_min: f64,
-    elapsed_time: f64,
-) {
-    let gauge = metrics
-        .iter()
-        .find(|metric| metric.name() == metric_name && tags_match(metric))
-        .unwrap_or_else(|| panic!("{metric_name} gauge missing"));
-
-    match gauge.value() {
-        MetricValue::Gauge { value } => {
-            assert!(
-                *value >= expected_min,
-                "expected mean latency to be >= {expected_min}, got {value}"
-            );
-            assert!(
-                *value < elapsed_time,
-                "expected mean latency ({value}) to be less than elapsed time ({elapsed_time})"
-            );
-        }
-        other => panic!("expected gauge metric, got {other:?}"),
-    }
-}
-
-fn has_component_tags(metric: &Metric) -> bool {
-    metric.tags().is_some_and(|tags| {
-        tags.get("component_id") == Some(TRANSFORM_ID)
-            && tags.get("component_type") == Some(TRANSFORM_TYPE)
-            && tags.get("component_kind") == Some(TRANSFORM_KIND)
-    })
-}
diff --git a/src/topology/test/mod.rs b/src/topology/test/mod.rs
index f4ccf9741..738a73125 100644
--- a/src/topology/test/mod.rs
+++ b/src/topology/test/mod.rs
@@ -39,7 +39,6 @@ mod crash;
 mod doesnt_reload;
 #[cfg(all(feature = "sources-http_server", feature = "sinks-http"))]
 mod end_to_end;
-mod latency_metrics;
 #[cfg(all(
     feature = "sources-prometheus",
     feature = "sinks-prometheus",
@@ -85,18 +84,6 @@ fn into_message_stream(array: SourceSenderItem) -> impl futures::Stream<Item = S
     stream::iter(array.events.into_events().map(into_message))
 }
 
-const TEST_UPSTREAM_COMPONENT_ID: &str = "test";
-const TEST_BASIC_SOURCE_TYPE: &str = "test_basic";
-
-fn set_expected_source_metadata(event: &mut Event, source_component_id: &str) {
-    event.set_source_id(Arc::new(ComponentKey::from(source_component_id)));
-    event.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
-    event.set_source_type(TEST_BASIC_SOURCE_TYPE);
-    event
-        .metadata_mut()
-        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
-}
-
 #[tokio::test]
 async fn topology_shutdown_while_active() {
     trace_init();
@@ -171,7 +158,11 @@ async fn topology_source_and_sink() {
 
     let res = out1.flat_map(into_event_stream).collect::<Vec<_>>().await;
 
-    set_expected_source_metadata(&mut event, "in1");
+    event.set_source_id(Arc::new(ComponentKey::from("in1")));
+    event.set_upstream_id(Arc::new(OutputId::from("test")));
+    event
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
     assert_eq!(vec![event], res);
 }
@@ -204,8 +195,18 @@ async fn topology_multiple_sources() {
 
     topology.stop().await;
 
-    set_expected_source_metadata(&mut event1, "in1");
-    set_expected_source_metadata(&mut event2, "in2");
+    event1.set_source_id(Arc::new(ComponentKey::from("in1")));
+    event2.set_source_id(Arc::new(ComponentKey::from("in2")));
+
+    event1.set_upstream_id(Arc::new(OutputId::from("test")));
+    event1
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+
+    event2.set_upstream_id(Arc::new(OutputId::from("test")));
+    event2
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
     assert_eq!(out_event1, Some(event1.into()));
     assert_eq!(out_event2, Some(event2.into()));
@@ -240,7 +241,12 @@ async fn topology_multiple_sinks() {
     let res2 = out2.flat_map(into_event_stream).collect::<Vec<_>>().await;
 
     // We should see that both sinks got the exact same event:
-    set_expected_source_metadata(&mut event, "in1");
+    event.set_source_id(Arc::new(ComponentKey::from("in1")));
+
+    event.set_upstream_id(Arc::new(OutputId::from("test")));
+    event
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
     let expected = vec![event];
     assert_eq!(expected, res1);
@@ -315,7 +321,12 @@ async fn topology_remove_one_source() {
     drop(in2);
     topology.stop().await;
 
-    set_expected_source_metadata(&mut event1, "in1");
+    event1.set_source_id(Arc::new(ComponentKey::from("in1")));
+
+    event1.set_upstream_id(Arc::new(OutputId::from("test")));
+    event1
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
     let res = h_out1.await.unwrap();
     assert_eq!(vec![event1], res);
@@ -354,7 +365,12 @@ async fn topology_remove_one_sink() {
     let res1 = out1.flat_map(into_event_stream).collect::<Vec<_>>().await;
     let res2 = out2.flat_map(into_event_stream).collect::<Vec<_>>().await;
 
-    set_expected_source_metadata(&mut event, "in1");
+    event.set_source_id(Arc::new(ComponentKey::from("in1")));
+
+    event.set_upstream_id(Arc::new(OutputId::from("test")));
+    event
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
     assert_eq!(vec![event], res1);
     assert_eq!(Vec::<Event>::new(), res2);
@@ -465,7 +481,11 @@ async fn topology_swap_source() {
     // as we've removed it from the topology prior to the sends.
     assert_eq!(Vec::<Event>::new(), res1);
 
-    set_expected_source_metadata(&mut event2, "in2");
+    event2.set_source_id(Arc::new(ComponentKey::from("in2")));
+    event2.set_upstream_id(Arc::new(OutputId::from("test")));
+    event2
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
     assert_eq!(vec![event2], res2);
 }
@@ -578,7 +598,11 @@ async fn topology_swap_sink() {
     // the new sink, which _was_ rebuilt:
     assert_eq!(Vec::<Event>::new(), res1);
 
-    set_expected_source_metadata(&mut event1, "in1");
+    event1.set_source_id(Arc::new(ComponentKey::from("in1")));
+    event1.set_upstream_id(Arc::new(OutputId::from("test")));
+    event1
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
     assert_eq!(vec![event1], res2);
 }
 
@@ -686,8 +710,17 @@ async fn topology_rebuild_connected() {
 
     let res = h_out1.await.unwrap();
 
-    set_expected_source_metadata(&mut event1, "in1");
-    set_expected_source_metadata(&mut event2, "in1");
+    event1.set_source_id(Arc::new(ComponentKey::from("in1")));
+    event2.set_source_id(Arc::new(ComponentKey::from("in1")));
+
+    event1.set_upstream_id(Arc::new(OutputId::from("test")));
+    event2.set_upstream_id(Arc::new(OutputId::from("test")));
+    event1
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
+    event2
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
     assert_eq!(vec![event1, event2], res);
 }
@@ -740,7 +773,11 @@ async fn topology_rebuild_connected_transform() {
     let res2 = h_out2.await.unwrap();
     assert_eq!(Vec::<Event>::new(), res1);
 
-    set_expected_source_metadata(&mut event, "in1");
+    event.set_source_id(Arc::new(ComponentKey::from("in1")));
+    event.set_upstream_id(Arc::new(OutputId::from("test")));
+    event
+        .metadata_mut()
+        .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
     assert_eq!(vec![event], res2);
 }
diff --git a/src/transforms/dedupe/config.rs b/src/transforms/dedupe/config.rs
index 5a4e728e7..2161c656f 100644
--- a/src/transforms/dedupe/config.rs
+++ b/src/transforms/dedupe/config.rs
@@ -104,20 +104,6 @@ mod tests {
         },
     };
 
-    const TEST_SOURCE_COMPONENT_ID: &str = "in";
-    const TEST_UPSTREAM_COMPONENT_ID: &str = "transform";
-    const TEST_SOURCE_TYPE: &str = "unit_test_stream";
-
-    fn set_expected_metadata(event: &mut Event) {
-        event.set_source_id(Arc::new(ComponentKey::from(TEST_SOURCE_COMPONENT_ID)));
-        event.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
-        event.set_source_type(TEST_SOURCE_TYPE);
-        // The schema definition is copied from the source for dedupe.
-        event
-            .metadata_mut()
-            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
-    }
-
     #[test]
     fn generate_config() {
         crate::test_util::test_generate_config::<DedupeConfig>();
@@ -195,7 +181,12 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event1
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event1);
 
             // Second event differs in matched field so should be output even though it
@@ -203,7 +194,12 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event2);
+            event2.set_source_id(Arc::new(ComponentKey::from("in")));
+            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event2
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event2);
 
             // Third event has the same value for "matched" as first event, so it should be dropped.
@@ -245,7 +241,12 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event1
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event1);
 
             // Second event has a different matched field name with the same value,
@@ -253,7 +254,12 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event2);
+            event2.set_source_id(Arc::new(ComponentKey::from("in")));
+            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event2
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event2);
 
             drop(tx);
@@ -298,7 +304,12 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event1
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event1);
 
             // Second event is the same just with different field order, so it
@@ -343,7 +354,13 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
+
+            // the schema definition is copied from the source for dedupe
+            event1
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event1);
 
             // Second event gets output because it's not a dupe. This causes the first
@@ -351,7 +368,12 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event2);
+            event2.set_source_id(Arc::new(ComponentKey::from("in")));
+            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event2
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
             assert_eq!(new_event, event2);
 
@@ -360,7 +382,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
             assert_eq!(new_event, event1);
 
             drop(tx);
@@ -410,7 +432,13 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
+
+            // the schema definition is copied from the source for dedupe
+            event1
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event1);
 
             // Second time the event gets dropped because it's a dupe.
@@ -422,7 +450,7 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
             assert_eq!(new_event, event1);
 
             drop(tx);
@@ -463,7 +491,12 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event1
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event1);
 
             // Second event should also get passed through even though the string
@@ -471,7 +504,12 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event2);
+            event2.set_source_id(Arc::new(ComponentKey::from("in")));
+            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event2
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event2);
 
             drop(tx);
@@ -516,7 +554,12 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event1
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event1);
 
             // Second event should also get passed through even though the string
@@ -524,7 +567,12 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event2);
+            event2.set_source_id(Arc::new(ComponentKey::from("in")));
+            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event2
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event2);
 
             drop(tx);
@@ -562,7 +610,12 @@ mod tests {
             tx.send(event1.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event1);
+            event1.set_source_id(Arc::new(ComponentKey::from("in")));
+            event1.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event1
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event1);
 
             // Second event should also get passed through as null is different than
@@ -570,7 +623,12 @@ mod tests {
             tx.send(event2.clone()).await.unwrap();
             let new_event = out.recv().await.unwrap();
 
-            set_expected_metadata(&mut event2);
+            event2.set_source_id(Arc::new(ComponentKey::from("in")));
+            event2.set_upstream_id(Arc::new(OutputId::from("transform")));
+            // the schema definition is copied from the source for dedupe
+            event2
+                .metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
             assert_eq!(new_event, event2);
 
             drop(tx);
diff --git a/src/transforms/filter.rs b/src/transforms/filter.rs
index 152681e9e..a2e8318b4 100644
--- a/src/transforms/filter.rs
+++ b/src/transforms/filter.rs
@@ -116,19 +116,6 @@ mod test {
         transforms::test::create_topology,
     };
 
-    const TEST_SOURCE_COMPONENT_ID: &str = "in";
-    const TEST_UPSTREAM_COMPONENT_ID: &str = "transform";
-    const TEST_SOURCE_TYPE: &str = "unit_test_stream";
-
-    fn set_expected_metadata(event: &mut Event) {
-        event.set_source_id(Arc::new(ComponentKey::from(TEST_SOURCE_COMPONENT_ID)));
-        event.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
-        event.set_source_type(TEST_SOURCE_TYPE);
-        event
-            .metadata_mut()
-            .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
-    }
-
     #[test]
     fn generate_config() {
         crate::test_util::test_generate_config::<super::FilterConfig>();
@@ -146,7 +133,10 @@ mod test {
             let mut log = Event::from(LogEvent::from("message"));
             tx.send(log.clone()).await.unwrap();
 
-            set_expected_metadata(&mut log);
+            log.set_source_id(Arc::new(ComponentKey::from("in")));
+            log.set_upstream_id(Arc::new(OutputId::from("transform")));
+            log.metadata_mut()
+                .set_schema_definition(&Arc::new(Definition::default_legacy_namespace()));
 
             assert_eq!(out.recv().await.unwrap(), log);
 
diff --git a/src/transforms/log_to_metric.rs b/src/transforms/log_to_metric.rs
index a9bb70790..a7fb90e1c 100644
--- a/src/transforms/log_to_metric.rs
+++ b/src/transforms/log_to_metric.rs
@@ -959,7 +959,6 @@ mod tests {
     use std::{sync::Arc, time::Duration};
 
     use chrono::{DateTime, Timelike, Utc, offset::TimeZone};
-    use similar_asserts::assert_eq;
     use tokio::sync::mpsc;
     use tokio_stream::wrappers::ReceiverStream;
     use vector_lib::{
@@ -979,11 +978,6 @@ mod tests {
         transforms::test::create_topology,
     };
 
-    const TEST_SOURCE_COMPONENT_ID: &str = "in";
-    const TEST_UPSTREAM_COMPONENT_ID: &str = "transform";
-    const TEST_SOURCE_TYPE: &str = "unit_test_stream";
-    const TEST_NAMESPACE: &str = "test_namespace";
-
     #[test]
     fn generate_config() {
         crate::test_util::test_generate_config::<LogToMetricConfig>();
@@ -1012,12 +1006,6 @@ mod tests {
         log
     }
 
-    fn set_test_source_metadata(metadata: &mut EventMetadata) {
-        metadata.set_upstream_id(Arc::new(OutputId::from(TEST_UPSTREAM_COMPONENT_ID)));
-        metadata.set_source_id(Arc::new(ComponentKey::from(TEST_SOURCE_COMPONENT_ID)));
-        metadata.set_source_type(TEST_SOURCE_TYPE);
-    }
-
     async fn do_transform(config: LogToMetricConfig, event: Event) -> Option<Event> {
         assert_transform_compliance(async move {
             let (tx, rx) = mpsc::channel(1);
@@ -1084,7 +1072,8 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
         let metric = do_transform(config, event).await.unwrap();
 
         assert_eq!(
@@ -1126,7 +1115,8 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1180,7 +1170,8 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1235,7 +1226,8 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let metric = do_transform(config, event).await.unwrap().into_metric();
         let tags = metric.tags().expect("Metric should have tags");
@@ -1359,7 +1351,8 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1414,7 +1407,8 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
         let metric = do_transform(config, event).await.unwrap();
 
         assert_eq!(
@@ -1454,7 +1448,8 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1495,7 +1490,8 @@ mod tests {
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
 
-        set_test_source_metadata(&mut metadata);
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1590,7 +1586,8 @@ mod tests {
 
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let output = do_transform_multiple_events(config, event, 2).await;
 
@@ -1655,7 +1652,8 @@ mod tests {
 
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let output = do_transform_multiple_events(config, event, 2).await;
 
@@ -1708,7 +1706,8 @@ mod tests {
                 ));
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1749,7 +1748,8 @@ mod tests {
 
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1791,7 +1791,8 @@ mod tests {
 
         // definitions aren't valid for metrics yet, it's just set to the default (anything).
         metadata.set_schema_definition(&Arc::new(Definition::any()));
-        set_test_source_metadata(&mut metadata);
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
 
         let metric = do_transform(config, event).await.unwrap();
 
@@ -1813,7 +1814,7 @@ mod tests {
     //  Metric Metadata Tests
     //
     fn create_log_event(json_str: &str) -> Event {
-        create_log_event_with_namespace(json_str, Some(TEST_NAMESPACE))
+        create_log_event_with_namespace(json_str, Some("test_namespace"))
     }
 
     fn create_log_event_with_namespace(json_str: &str, namespace: Option<&str>) -> Event {
@@ -1826,7 +1827,8 @@ mod tests {
         }
 
         let mut metadata = EventMetadata::default();
-        set_test_source_metadata(&mut metadata);
+        metadata.set_source_id(Arc::new(ComponentKey::from("in")));
+        metadata.set_upstream_id(Arc::new(OutputId::from("transform")));
 
         Event::Log(LogEvent::from_parts(log_value, metadata.clone()))
     }
@@ -1859,7 +1861,7 @@ mod tests {
                 MetricValue::Gauge { value: 990.0 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some(TEST_NAMESPACE))
+            .with_namespace(Some("test_namespace"))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -1936,7 +1938,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some(TEST_NAMESPACE))
+            .with_namespace(Some("test_namespace"))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -1995,7 +1997,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some(TEST_NAMESPACE))
+            .with_namespace(Some("test_namespace"))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -2054,7 +2056,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some(TEST_NAMESPACE))
+            .with_namespace(Some("test_namespace"))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -2115,7 +2117,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some(TEST_NAMESPACE))
+            .with_namespace(Some("test_namespace"))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -2152,7 +2154,7 @@ mod tests {
                 MetricValue::Counter { value: 10.0 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some(TEST_NAMESPACE))
+            .with_namespace(Some("test_namespace"))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
@@ -2191,7 +2193,7 @@ mod tests {
                 },
                 metric.metadata().clone(),
             )
-            .with_namespace(Some(TEST_NAMESPACE))
+            .with_namespace(Some("test_namespace"))
             .with_tags(Some(metric_tags!(
                 "env" => "test_env",
                 "host" => "localhost",
diff --git a/src/transforms/sample/transform.rs b/src/transforms/sample/transform.rs
index 74786a43f..20013c5e7 100644
--- a/src/transforms/sample/transform.rs
+++ b/src/transforms/sample/transform.rs
@@ -1,4 +1,4 @@
-use std::{collections::HashMap, fmt};
+use std::{borrow::Cow, collections::HashMap, fmt};
 
 use vector_lib::{
     config::LegacyKey,
@@ -7,7 +7,7 @@ use vector_lib::{
 
 use crate::{
     conditions::Condition,
-    event::{Event, Value},
+    event::Event,
     internal_events::SampleEventDiscarded,
     sinks::prelude::TemplateRenderingError,
     template::Template,
@@ -53,16 +53,16 @@ impl SampleMode {
         }
     }
 
-    fn increment(&mut self, group_by_key: Option<String>, value: Option<&Value>) -> bool {
+    fn increment(&mut self, group_by_key: &Option<String>, value: &Option<Cow<'_, str>>) -> bool {
         let threshold_exceeded = match self {
             Self::Rate { rate, counters } => {
-                let counter_value = counters.entry(group_by_key).or_default();
+                let counter_value = counters.entry(group_by_key.clone()).or_default();
                 let old_counter_value = *counter_value;
                 *counter_value += 1;
                 old_counter_value % *rate == 0
             }
             Self::Ratio { ratio, values, .. } => {
-                let value = values.entry(group_by_key).or_insert(1.0 - *ratio);
+                let value = values.entry(group_by_key.clone()).or_insert(1.0 - *ratio);
                 let increment: f64 = *value + *ratio;
                 *value = if increment >= 1.0 {
                     increment - 1.0
@@ -73,7 +73,7 @@ impl SampleMode {
             }
         };
         if let Some(value) = value {
-            self.hash_within_ratio(value.to_string_lossy().as_bytes())
+            self.hash_within_ratio(value.as_bytes())
         } else {
             threshold_exceeded
         }
@@ -159,36 +159,49 @@ impl FunctionTransform for Sample {
             }
         };
 
-        let value = self.key_field.as_ref().and_then(|key_field| match &event {
-            Event::Log(event) => event
-                .parse_path_and_get_value(key_field.as_str())
-                .ok()
-                .flatten(),
-            Event::Trace(event) => event
-                .parse_path_and_get_value(key_field.as_str())
-                .ok()
-                .flatten(),
-            Event::Metric(_) => panic!("component can never receive metric events"),
-        });
+        let value = self
+            .key_field
+            .as_ref()
+            .and_then(|key_field| match &event {
+                Event::Log(event) => event
+                    .parse_path_and_get_value(key_field.as_str())
+                    .ok()
+                    .flatten(),
+                Event::Trace(event) => event
+                    .parse_path_and_get_value(key_field.as_str())
+                    .ok()
+                    .flatten(),
+                Event::Metric(_) => panic!("component can never receive metric events"),
+            })
+            .map(|v| v.to_string_lossy());
 
         // Fetch actual field value if group_by option is set.
-        let group_by_key = self.group_by.as_ref().and_then(|group_by| {
-            match &event {
-                Event::Log(event) => group_by.render_string(event),
-                Event::Trace(event) => group_by.render_string(event),
-                Event::Metric(_) => panic!("component can never receive metric events"),
-            }
-            .map_err(|error| {
-                emit!(TemplateRenderingError {
-                    error,
-                    field: Some("group_by"),
-                    drop_event: false,
+        let group_by_key = self.group_by.as_ref().and_then(|group_by| match &event {
+            Event::Log(event) => group_by
+                .render_string(event)
+                .map_err(|error| {
+                    emit!(TemplateRenderingError {
+                        error,
+                        field: Some("group_by"),
+                        drop_event: false,
+                    })
                 })
-            })
-            .ok()
+                .ok(),
+            Event::Trace(event) => group_by
+                .render_string(event)
+                .map_err(|error| {
+                    emit!(TemplateRenderingError {
+                        error,
+                        field: Some("group_by"),
+                        drop_event: false,
+                    })
+                })
+                .ok(),
+            Event::Metric(_) => panic!("component can never receive metric events"),
         });
 
-        if self.rate.increment(group_by_key, value) {
+        let should_sample = self.rate.increment(&group_by_key, &value);
+        if should_sample {
             if let Some(path) = &self.sample_rate_key.path {
                 match event {
                     Event::Log(ref mut event) => {
diff --git a/vdev/Cargo.toml b/vdev/Cargo.toml
index ab7a68d0d..ea2e01533 100644
--- a/vdev/Cargo.toml
+++ b/vdev/Cargo.toml
@@ -43,7 +43,7 @@ toml.workspace = true
 toml_edit = "0.22"
 semver.workspace = true
 indoc.workspace = true
-git2 = { version = "0.20.4" }
+git2 = { version = "0.20.2" }
 cfg-if.workspace = true
 
 [package.metadata.binstall]
diff --git a/website/cue/reference/components/sinks/clickhouse.cue b/website/cue/reference/components/sinks/clickhouse.cue
index c0d80d144..1049cf521 100644
--- a/website/cue/reference/components/sinks/clickhouse.cue
+++ b/website/cue/reference/components/sinks/clickhouse.cue
@@ -142,6 +142,9 @@ components: sinks: clickhouse: {
 
 				The following ClickHouse column types are **not yet supported** by Vector's
 				ArrowStream implementation:
+				- `Array`
+				- `Tuple`
+				- `Map`
 				- `IPv4`
 				- `IPv6`
 
diff --git a/website/cue/reference/components/sinks/prometheus_remote_write.cue b/website/cue/reference/components/sinks/prometheus_remote_write.cue
index 5b9bb9267..92904077b 100644
--- a/website/cue/reference/components/sinks/prometheus_remote_write.cue
+++ b/website/cue/reference/components/sinks/prometheus_remote_write.cue
@@ -15,10 +15,7 @@ components: sinks: prometheus_remote_write: {
 	features: {
 		auto_generated:   true
 		acknowledgements: true
-		healthcheck: {
-			enabled:  true
-			uses_uri: true
-		}
+		healthcheck: enabled: true
 		send: {
 			batch: {
 				enabled:      true
diff --git a/website/cue/reference/components/sources/internal_metrics.cue b/website/cue/reference/components/sources/internal_metrics.cue
index 7aa7834cd..946544a70 100644
--- a/website/cue/reference/components/sources/internal_metrics.cue
+++ b/website/cue/reference/components/sources/internal_metrics.cue
@@ -273,28 +273,6 @@ components: sources: internal_metrics: {
 				reason: _reason
 			}
 		}
-		component_latency_seconds: {
-			description: """
-				The elapsed time, in fractional seconds, that an event spends in a single transform.
-
-				This includes both the time spent queued in the transforms input buffer and the time spent executing the transform itself.
-				"""
-			type:              "histogram"
-			default_namespace: "vector"
-			tags:              _internal_metrics_tags
-		}
-		component_latency_mean_seconds: {
-			description: """
-				The mean elapsed time, in fractional seconds, that an event spends in a single transform.
-
-				This includes both the time spent queued in the transforms input buffer and the time spent executing the transform itself.
-
-				This value is smoothed over time using an exponentially weighted moving average (EWMA).
-				"""
-			type:              "gauge"
-			default_namespace: "vector"
-			tags:              _internal_metrics_tags
-		}
 		buffer_byte_size: {
 			description:        "The number of bytes currently in the buffer."
 			type:               "gauge"
diff --git a/website/cue/reference/components/transforms.cue b/website/cue/reference/components/transforms.cue
index e06591510..6c907ecc7 100644
--- a/website/cue/reference/components/transforms.cue
+++ b/website/cue/reference/components/transforms.cue
@@ -15,13 +15,11 @@ components: transforms: [Name=string]: {
 	telemetry: metrics: {
 		component_discarded_events_total:     components.sources.internal_metrics.output.metrics.component_discarded_events_total
 		component_errors_total:               components.sources.internal_metrics.output.metrics.component_errors_total
-		component_latency_mean_seconds:       components.sources.internal_metrics.output.metrics.component_latency_mean_seconds
-		component_latency_seconds:            components.sources.internal_metrics.output.metrics.component_latency_seconds
-		component_received_event_bytes_total: components.sources.internal_metrics.output.metrics.component_received_event_bytes_total
 		component_received_events_count:      components.sources.internal_metrics.output.metrics.component_received_events_count
 		component_received_events_total:      components.sources.internal_metrics.output.metrics.component_received_events_total
-		component_sent_event_bytes_total:     components.sources.internal_metrics.output.metrics.component_sent_event_bytes_total
+		component_received_event_bytes_total: components.sources.internal_metrics.output.metrics.component_received_event_bytes_total
 		component_sent_events_total:          components.sources.internal_metrics.output.metrics.component_sent_events_total
+		component_sent_event_bytes_total:     components.sources.internal_metrics.output.metrics.component_sent_event_bytes_total
 		transform_buffer_max_byte_size:       components.sources.internal_metrics.output.metrics.transform_buffer_max_byte_size
 		transform_buffer_max_event_size:      components.sources.internal_metrics.output.metrics.transform_buffer_max_event_size
 		transform_buffer_max_size_bytes:      components.sources.internal_metrics.output.metrics.transform_buffer_max_size_bytes
diff --git a/website/cue/reference/generated/configuration.cue b/website/cue/reference/generated/configuration.cue
index 4c6678bde..6bf097aa7 100644
--- a/website/cue/reference/generated/configuration.cue
+++ b/website/cue/reference/generated/configuration.cue
@@ -706,11 +706,11 @@ generated: configuration: configuration: {
 			The alpha value for the exponential weighted moving average (EWMA) of source and transform
 			buffer utilization metrics.
 
-			This controls how quickly the `*_buffer_utilization_mean` gauges respond to new
-			observations. Values closer to 1.0 retain more of the previous value, leading to slower
-			adjustments. The default value of 0.9 is equivalent to a "half life" of 6-7 measurements.
+			This value specifies how much of the existing value is retained when each update is made.
+			Values closer to 1.0 result in the value adjusting slower to changes. The default value of
+			0.9 is equivalent to a "half life" of 6-7 measurements.
 
-			Must be between 0 and 1 exclusively (0 < alpha < 1).
+			Must be between 0 and 1 exclusive (0 < alpha < 1).
 			"""
 		required: false
 		type: float: {}
@@ -832,20 +832,6 @@ generated: configuration: configuration: {
 		required: false
 		type: float: {}
 	}
-	latency_ewma_alpha: {
-		description: """
-			The alpha value for the exponential weighted moving average (EWMA) of transform latency
-			metrics.
-
-			This controls how quickly the `component_latency_mean_seconds` gauge responds to new
-			observations. Values closer to 1.0 retain more of the previous value, leading to slower
-			adjustments. The default value of 0.9 is equivalent to a "half life" of 6-7 measurements.
-
-			Must be between 0 and 1 exclusively (0 < alpha < 1).
-			"""
-		required: false
-		type: float: {}
-	}
 	log_schema: {
 		common: false
 		description: """
diff --git a/website/package.json b/website/package.json
index fec13b066..81b4c64bd 100644
--- a/website/package.json
+++ b/website/package.json
@@ -2,9 +2,9 @@
   "type": "module",
   "private": true,
   "scripts": {
-    "typesense-index": "tsx scripts/typesense-index.ts",
-    "typesense-sync:preview": "NODE_ENV=preview tsx scripts/typesense-sync.ts",
-    "typesense-sync:production": "NODE_ENV=production tsx scripts/typesense-sync.ts",
+    "typesense-index": "ts-node scripts/typesense-index.ts",
+    "typesense-sync:preview": "NODE_ENV=preview ts-node scripts/typesense-sync.ts",
+    "typesense-sync:production": "NODE_ENV=production ts-node scripts/typesense-sync.ts",
     "config-examples": "node scripts/create-config-examples.js",
     "lighthouse": "lighthouse --output html --output-path ./reports/lighthouse.html --view https://vector.dev"
   },
@@ -54,10 +54,10 @@
     "tailwindcss": "^2.2.4",
     "tocbot": "^4.12.2",
     "topojson-client": "^3.1.0",
-    "tsx": "^4.21.0",
+    "ts-node": "^10.9.2",
     "typescript": "^5.9.3",
     "typesense": "^1.8.2",
-    "typesense-sync": "https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v2.0.0.tgz"
+    "typesense-sync": "https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v1.1.0.tgz"
   },
   "browserslist": [
     "since 2017-06"
diff --git a/website/scripts/typesense-index.ts b/website/scripts/typesense-index.ts
index 7b14f07d0..a80f1f343 100644
--- a/website/scripts/typesense-index.ts
+++ b/website/scripts/typesense-index.ts
@@ -1,15 +1,11 @@
 import chalk from "chalk";
-import * as cheerio from "cheerio";
+import cheerio from "cheerio";
 import { Element } from "domhandler";
 import dotEnv from "dotenv-defaults";
 import fs from "fs";
 import glob from "glob-promise";
 import path from "path";
 import crypto from "crypto";
-import { fileURLToPath } from "url";
-
-const __filename = fileURLToPath(import.meta.url);
-const __dirname = path.dirname(__filename);
 
 dotEnv.config();
 
diff --git a/website/scripts/typesense-sync.ts b/website/scripts/typesense-sync.ts
index 65f867de7..fd08850d1 100644
--- a/website/scripts/typesense-sync.ts
+++ b/website/scripts/typesense-sync.ts
@@ -1,19 +1,19 @@
-// Will be running in the CI/CD pipeline
-import process from 'node:process';
-import { typesenseSync } from 'typesense-sync';
-const configFilePath = './typesense.config.json';
+const { typesenseSync } = require('typesense-sync');
+const { saveSettings } = require('typesense-sync/settings');
+const tsConfig = require('../typesense.config.json');
 
-const config = {
-  configFilePath,
-  shouldSaveSettings: true,
-};
+const syncCollection = async () => {
+  const promises: Promise<any>[] = []
 
-typesenseSync(config)
-  .then(() => {
-    console.log('Typesense sync completed');
-  })
-  .catch((err) => {
-    console.error('An error occurred', err);
-    process.exit(1);
-  });
+  for (const collection of tsConfig.collections) {
+    console.log(`Updating collection ${collection.name}`)
+    promises.push(typesenseSync(collection.name, collection.file_path))
+  }
 
+  return await Promise.all(promises)
+}
+
+saveSettings()
+  .then(() => syncCollection())
+  .then(() => console.log('Typesense sync completed'))
+  .catch(error => console.log('An error occurred', error))
diff --git a/website/typesense.config.json b/website/typesense.config.json
index 89b40e66a..42b3acf11 100644
--- a/website/typesense.config.json
+++ b/website/typesense.config.json
@@ -15,9 +15,7 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string",
-            "stem_dictionary": "",
-            "store": true
+            "type": "string"
           },
           {
             "facet": false,
@@ -28,9 +26,7 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string",
-            "stem_dictionary": "",
-            "store": true
+            "type": "string"
           },
           {
             "facet": false,
@@ -41,9 +37,7 @@
             "optional": true,
             "sort": true,
             "stem": false,
-            "type": "int64",
-            "stem_dictionary": "",
-            "store": true
+            "type": "int64"
           },
           {
             "facet": false,
@@ -54,9 +48,7 @@
             "optional": true,
             "sort": false,
             "stem": false,
-              "type": "string",
-            "stem_dictionary": "",
-            "store": true
+            "type": "string"
           },
           {
             "facet": false,
@@ -67,9 +59,7 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string",
-            "stem_dictionary": "",
-            "store": true
+            "type": "string"
           },
           {
             "facet": false,
@@ -80,9 +70,7 @@
             "optional": true,
             "sort": true,
             "stem": false,
-            "type": "int64",
-            "stem_dictionary": "",
-            "store": true
+            "type": "int64"
           },
           {
             "facet": false,
@@ -93,9 +81,7 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string",
-            "stem_dictionary": "",
-            "store": true
+            "type": "string"
           },
           {
             "facet": false,
@@ -106,9 +92,7 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string[]",
-            "stem_dictionary": "",
-            "store": true
+            "type": "string[]"
           },
           {
             "facet": false,
@@ -119,9 +103,7 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string",
-            "stem_dictionary": "",
-            "store": true
+            "type": "string"
           },
           {
             "facet": false,
@@ -132,9 +114,7 @@
             "optional": true,
             "sort": false,
             "stem": false,
-            "type": "string[]",
-            "stem_dictionary": "",
-            "store": true
+            "type": "string[]"
           }
         ]
       },
diff --git a/website/yarn.lock b/website/yarn.lock
index fb7d5ede8..d5853b441 100644
--- a/website/yarn.lock
+++ b/website/yarn.lock
@@ -1066,6 +1066,13 @@
   resolved "https://registry.npmjs.org/@colors/colors/-/colors-1.6.0.tgz"
   integrity sha512-Ir+AOibqzrIsL6ajt3Rz3LskB7OiMVHqltZmspbW/TJuTVuyOMirVqAkjfY6JISiLHgyNqicAC8AyHHGzNd/dA==
 
+"@cspotcode/source-map-support@^0.8.0":
+  version "0.8.1"
+  resolved "https://registry.yarnpkg.com/@cspotcode/source-map-support/-/source-map-support-0.8.1.tgz#00629c35a688e05a88b1cda684fb9d5e73f000a1"
+  integrity sha512-IchNf6dN4tHoMFIn/7OE8LWZ19Y6q/67Bmf6vnGREv8RSbBVb9LPJxEcnwrcwX6ixSvaiGoomAUvu4YSxXrVgw==
+  dependencies:
+    "@jridgewell/trace-mapping" "0.3.9"
+
 "@dabh/diagnostics@^2.0.2":
   version "2.0.3"
   resolved "https://registry.npmjs.org/@dabh/diagnostics/-/diagnostics-2.0.3.tgz"
@@ -1167,136 +1174,6 @@
   resolved "https://registry.yarnpkg.com/@emotion/utils/-/utils-1.4.2.tgz#6df6c45881fcb1c412d6688a311a98b7f59c1b52"
   integrity sha512-3vLclRofFziIa3J2wDh9jjbkUz9qk5Vi3IZ/FSTKViB0k+ef0fPV7dYrUIugbgupYDx7v9ud/SjrtEP8Y4xLoA==
 
-"@esbuild/aix-ppc64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/aix-ppc64/-/aix-ppc64-0.27.3.tgz#815b39267f9bffd3407ea6c376ac32946e24f8d2"
-  integrity sha512-9fJMTNFTWZMh5qwrBItuziu834eOCUcEqymSH7pY+zoMVEZg3gcPuBNxH1EvfVYe9h0x/Ptw8KBzv7qxb7l8dg==
-
-"@esbuild/android-arm64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/android-arm64/-/android-arm64-0.27.3.tgz#19b882408829ad8e12b10aff2840711b2da361e8"
-  integrity sha512-YdghPYUmj/FX2SYKJ0OZxf+iaKgMsKHVPF1MAq/P8WirnSpCStzKJFjOjzsW0QQ7oIAiccHdcqjbHmJxRb/dmg==
-
-"@esbuild/android-arm@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/android-arm/-/android-arm-0.27.3.tgz#90be58de27915efa27b767fcbdb37a4470627d7b"
-  integrity sha512-i5D1hPY7GIQmXlXhs2w8AWHhenb00+GxjxRncS2ZM7YNVGNfaMxgzSGuO8o8SJzRc/oZwU2bcScvVERk03QhzA==
-
-"@esbuild/android-x64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/android-x64/-/android-x64-0.27.3.tgz#d7dcc976f16e01a9aaa2f9b938fbec7389f895ac"
-  integrity sha512-IN/0BNTkHtk8lkOM8JWAYFg4ORxBkZQf9zXiEOfERX/CzxW3Vg1ewAhU7QSWQpVIzTW+b8Xy+lGzdYXV6UZObQ==
-
-"@esbuild/darwin-arm64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/darwin-arm64/-/darwin-arm64-0.27.3.tgz#9f6cac72b3a8532298a6a4493ed639a8988e8abd"
-  integrity sha512-Re491k7ByTVRy0t3EKWajdLIr0gz2kKKfzafkth4Q8A5n1xTHrkqZgLLjFEHVD+AXdUGgQMq+Godfq45mGpCKg==
-
-"@esbuild/darwin-x64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/darwin-x64/-/darwin-x64-0.27.3.tgz#ac61d645faa37fd650340f1866b0812e1fb14d6a"
-  integrity sha512-vHk/hA7/1AckjGzRqi6wbo+jaShzRowYip6rt6q7VYEDX4LEy1pZfDpdxCBnGtl+A5zq8iXDcyuxwtv3hNtHFg==
-
-"@esbuild/freebsd-arm64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/freebsd-arm64/-/freebsd-arm64-0.27.3.tgz#b8625689d73cf1830fe58c39051acdc12474ea1b"
-  integrity sha512-ipTYM2fjt3kQAYOvo6vcxJx3nBYAzPjgTCk7QEgZG8AUO3ydUhvelmhrbOheMnGOlaSFUoHXB6un+A7q4ygY9w==
-
-"@esbuild/freebsd-x64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/freebsd-x64/-/freebsd-x64-0.27.3.tgz#07be7dd3c9d42fe0eccd2ab9f9ded780bc53bead"
-  integrity sha512-dDk0X87T7mI6U3K9VjWtHOXqwAMJBNN2r7bejDsc+j03SEjtD9HrOl8gVFByeM0aJksoUuUVU9TBaZa2rgj0oA==
-
-"@esbuild/linux-arm64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-arm64/-/linux-arm64-0.27.3.tgz#bf31918fe5c798586460d2b3d6c46ed2c01ca0b6"
-  integrity sha512-sZOuFz/xWnZ4KH3YfFrKCf1WyPZHakVzTiqji3WDc0BCl2kBwiJLCXpzLzUBLgmp4veFZdvN5ChW4Eq/8Fc2Fg==
-
-"@esbuild/linux-arm@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-arm/-/linux-arm-0.27.3.tgz#28493ee46abec1dc3f500223cd9f8d2df08f9d11"
-  integrity sha512-s6nPv2QkSupJwLYyfS+gwdirm0ukyTFNl3KTgZEAiJDd+iHZcbTPPcWCcRYH+WlNbwChgH2QkE9NSlNrMT8Gfw==
-
-"@esbuild/linux-ia32@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-ia32/-/linux-ia32-0.27.3.tgz#750752a8b30b43647402561eea764d0a41d0ee29"
-  integrity sha512-yGlQYjdxtLdh0a3jHjuwOrxQjOZYD/C9PfdbgJJF3TIZWnm/tMd/RcNiLngiu4iwcBAOezdnSLAwQDPqTmtTYg==
-
-"@esbuild/linux-loong64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-loong64/-/linux-loong64-0.27.3.tgz#a5a92813a04e71198c50f05adfaf18fc1e95b9ed"
-  integrity sha512-WO60Sn8ly3gtzhyjATDgieJNet/KqsDlX5nRC5Y3oTFcS1l0KWba+SEa9Ja1GfDqSF1z6hif/SkpQJbL63cgOA==
-
-"@esbuild/linux-mips64el@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-mips64el/-/linux-mips64el-0.27.3.tgz#deb45d7fd2d2161eadf1fbc593637ed766d50bb1"
-  integrity sha512-APsymYA6sGcZ4pD6k+UxbDjOFSvPWyZhjaiPyl/f79xKxwTnrn5QUnXR5prvetuaSMsb4jgeHewIDCIWljrSxw==
-
-"@esbuild/linux-ppc64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-ppc64/-/linux-ppc64-0.27.3.tgz#6f39ae0b8c4d3d2d61a65b26df79f6e12a1c3d78"
-  integrity sha512-eizBnTeBefojtDb9nSh4vvVQ3V9Qf9Df01PfawPcRzJH4gFSgrObw+LveUyDoKU3kxi5+9RJTCWlj4FjYXVPEA==
-
-"@esbuild/linux-riscv64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-riscv64/-/linux-riscv64-0.27.3.tgz#4c5c19c3916612ec8e3915187030b9df0b955c1d"
-  integrity sha512-3Emwh0r5wmfm3ssTWRQSyVhbOHvqegUDRd0WhmXKX2mkHJe1SFCMJhagUleMq+Uci34wLSipf8Lagt4LlpRFWQ==
-
-"@esbuild/linux-s390x@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-s390x/-/linux-s390x-0.27.3.tgz#9ed17b3198fa08ad5ccaa9e74f6c0aff7ad0156d"
-  integrity sha512-pBHUx9LzXWBc7MFIEEL0yD/ZVtNgLytvx60gES28GcWMqil8ElCYR4kvbV2BDqsHOvVDRrOxGySBM9Fcv744hw==
-
-"@esbuild/linux-x64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/linux-x64/-/linux-x64-0.27.3.tgz#12383dcbf71b7cf6513e58b4b08d95a710bf52a5"
-  integrity sha512-Czi8yzXUWIQYAtL/2y6vogER8pvcsOsk5cpwL4Gk5nJqH5UZiVByIY8Eorm5R13gq+DQKYg0+JyQoytLQas4dA==
-
-"@esbuild/netbsd-arm64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/netbsd-arm64/-/netbsd-arm64-0.27.3.tgz#dd0cb2fa543205fcd931df44f4786bfcce6df7d7"
-  integrity sha512-sDpk0RgmTCR/5HguIZa9n9u+HVKf40fbEUt+iTzSnCaGvY9kFP0YKBWZtJaraonFnqef5SlJ8/TiPAxzyS+UoA==
-
-"@esbuild/netbsd-x64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/netbsd-x64/-/netbsd-x64-0.27.3.tgz#028ad1807a8e03e155153b2d025b506c3787354b"
-  integrity sha512-P14lFKJl/DdaE00LItAukUdZO5iqNH7+PjoBm+fLQjtxfcfFE20Xf5CrLsmZdq5LFFZzb5JMZ9grUwvtVYzjiA==
-
-"@esbuild/openbsd-arm64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/openbsd-arm64/-/openbsd-arm64-0.27.3.tgz#e3c16ff3490c9b59b969fffca87f350ffc0e2af5"
-  integrity sha512-AIcMP77AvirGbRl/UZFTq5hjXK+2wC7qFRGoHSDrZ5v5b8DK/GYpXW3CPRL53NkvDqb9D+alBiC/dV0Fb7eJcw==
-
-"@esbuild/openbsd-x64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/openbsd-x64/-/openbsd-x64-0.27.3.tgz#c5a4693fcb03d1cbecbf8b422422468dfc0d2a8b"
-  integrity sha512-DnW2sRrBzA+YnE70LKqnM3P+z8vehfJWHXECbwBmH/CU51z6FiqTQTHFenPlHmo3a8UgpLyH3PT+87OViOh1AQ==
-
-"@esbuild/openharmony-arm64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/openharmony-arm64/-/openharmony-arm64-0.27.3.tgz#082082444f12db564a0775a41e1991c0e125055e"
-  integrity sha512-NinAEgr/etERPTsZJ7aEZQvvg/A6IsZG/LgZy+81wON2huV7SrK3e63dU0XhyZP4RKGyTm7aOgmQk0bGp0fy2g==
-
-"@esbuild/sunos-x64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/sunos-x64/-/sunos-x64-0.27.3.tgz#5ab036c53f929e8405c4e96e865a424160a1b537"
-  integrity sha512-PanZ+nEz+eWoBJ8/f8HKxTTD172SKwdXebZ0ndd953gt1HRBbhMsaNqjTyYLGLPdoWHy4zLU7bDVJztF5f3BHA==
-
-"@esbuild/win32-arm64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/win32-arm64/-/win32-arm64-0.27.3.tgz#38de700ef4b960a0045370c171794526e589862e"
-  integrity sha512-B2t59lWWYrbRDw/tjiWOuzSsFh1Y/E95ofKz7rIVYSQkUYBjfSgf6oeYPNWHToFRr2zx52JKApIcAS/D5TUBnA==
-
-"@esbuild/win32-ia32@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/win32-ia32/-/win32-ia32-0.27.3.tgz#451b93dc03ec5d4f38619e6cd64d9f9eff06f55c"
-  integrity sha512-QLKSFeXNS8+tHW7tZpMtjlNb7HKau0QDpwm49u0vUp9y1WOF+PEzkU84y9GqYaAVW8aH8f3GcBck26jh54cX4Q==
-
-"@esbuild/win32-x64@0.27.3":
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/@esbuild/win32-x64/-/win32-x64-0.27.3.tgz#0eaf705c941a218a43dba8e09f1df1d6cd2f1f17"
-  integrity sha512-4uJGhsxuptu3OcpVAzli+/gWusVGwZZHTlS63hh++ehExkVT8SgiEf7/uC/PclrPPkLhZqGgCTjd0VWLo6xMqA==
-
 "@fullhuman/postcss-purgecss@^4.0.3":
   version "4.1.3"
   resolved "https://registry.npmjs.org/@fullhuman/postcss-purgecss/-/postcss-purgecss-4.1.3.tgz"
@@ -1325,20 +1202,28 @@
     "@jridgewell/gen-mapping" "^0.3.5"
     "@jridgewell/trace-mapping" "^0.3.24"
 
-"@jridgewell/resolve-uri@^3.1.0":
+"@jridgewell/resolve-uri@^3.0.3", "@jridgewell/resolve-uri@^3.1.0":
   version "3.1.2"
   resolved "https://registry.npmjs.org/@jridgewell/resolve-uri/-/resolve-uri-3.1.2.tgz"
   integrity sha512-bRISgCIjP20/tbWSPWMEi54QVPRZExkuD9lJL+UIxUKtwVJA8wW1Trb1jMs1RFXo1CBTNZ/5hpC9QvmKWdopKw==
 
+"@jridgewell/sourcemap-codec@^1.4.10", "@jridgewell/sourcemap-codec@^1.5.0":
+  version "1.5.5"
+  resolved "https://registry.yarnpkg.com/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.5.tgz#6912b00d2c631c0d15ce1a7ab57cd657f2a8f8ba"
+  integrity sha512-cYQ9310grqxueWbl+WuIUIaiUaDcj7WOq5fVhEljNVgRfOUhY9fy2zTvfoqWsnebh8Sl70VScFbICvJnLKB0Og==
+
 "@jridgewell/sourcemap-codec@^1.4.14", "@jridgewell/sourcemap-codec@^1.4.15":
   version "1.5.0"
   resolved "https://registry.npmjs.org/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.0.tgz"
   integrity sha512-gv3ZRaISU3fjPAgNsriBRqGWQL6quFx04YMPW/zD8XMLsU32mhCCbfbO6KZFLjvYpCZ8zyDEgqsgf+PwPaM7GQ==
 
-"@jridgewell/sourcemap-codec@^1.5.0":
-  version "1.5.5"
-  resolved "https://registry.yarnpkg.com/@jridgewell/sourcemap-codec/-/sourcemap-codec-1.5.5.tgz#6912b00d2c631c0d15ce1a7ab57cd657f2a8f8ba"
-  integrity sha512-cYQ9310grqxueWbl+WuIUIaiUaDcj7WOq5fVhEljNVgRfOUhY9fy2zTvfoqWsnebh8Sl70VScFbICvJnLKB0Og==
+"@jridgewell/trace-mapping@0.3.9":
+  version "0.3.9"
+  resolved "https://registry.yarnpkg.com/@jridgewell/trace-mapping/-/trace-mapping-0.3.9.tgz#6534fd5933a53ba7cbf3a17615e273a0d1273ff9"
+  integrity sha512-3Belt6tdc8bPgAtbcmdtNJlirVoTmEb5e2gC94PnkwEW9jI6CAHUeoG85tjWP5WquqfavoMtMwiG4P926ZKKuQ==
+  dependencies:
+    "@jridgewell/resolve-uri" "^3.0.3"
+    "@jridgewell/sourcemap-codec" "^1.4.10"
 
 "@jridgewell/trace-mapping@^0.3.24", "@jridgewell/trace-mapping@^0.3.28":
   version "0.3.31"
@@ -1483,6 +1368,26 @@
     lodash.merge "^4.6.2"
     lodash.uniq "^4.5.0"
 
+"@tsconfig/node10@^1.0.7":
+  version "1.0.12"
+  resolved "https://registry.yarnpkg.com/@tsconfig/node10/-/node10-1.0.12.tgz#be57ceac1e4692b41be9de6be8c32a106636dba4"
+  integrity sha512-UCYBaeFvM11aU2y3YPZ//O5Rhj+xKyzy7mvcIoAjASbigy8mHMryP5cK7dgjlz2hWxh1g5pLw084E0a/wlUSFQ==
+
+"@tsconfig/node12@^1.0.7":
+  version "1.0.11"
+  resolved "https://registry.yarnpkg.com/@tsconfig/node12/-/node12-1.0.11.tgz#ee3def1f27d9ed66dac6e46a295cffb0152e058d"
+  integrity sha512-cqefuRsh12pWyGsIoBKJA9luFu3mRxCA+ORZvA4ktLSzIuCUtWVxGIuXigEwO5/ywWFMZ2QEGKWvkZG1zDMTag==
+
+"@tsconfig/node14@^1.0.0":
+  version "1.0.3"
+  resolved "https://registry.yarnpkg.com/@tsconfig/node14/-/node14-1.0.3.tgz#e4386316284f00b98435bf40f72f75a09dabf6c1"
+  integrity sha512-ysT8mhdixWK6Hw3i1V2AeRqZ5WfXg1G43mqoYlM2nc6388Fq5jcXyr5mRsqViLx/GJYdoL0bfXD8nmF+Zn/Iow==
+
+"@tsconfig/node16@^1.0.2":
+  version "1.0.4"
+  resolved "https://registry.yarnpkg.com/@tsconfig/node16/-/node16-1.0.4.tgz#0b92dcc0cc1c81f6f306a381f28e31b1a56536e9"
+  integrity sha512-vxhUy4J8lyeyinH7Azl1pdd43GJhZH/tP2weN8TntQblOY+A0XbT8DJk1/oCPuOOyg/Ja757rG0CgHcWC8OfMA==
+
 "@types/dotenv-defaults@^2.0.1":
   version "2.0.4"
   resolved "https://registry.npmjs.org/@types/dotenv-defaults/-/dotenv-defaults-2.0.4.tgz"
@@ -1574,11 +1479,23 @@ acorn-walk@^7.0.0:
   resolved "https://registry.npmjs.org/acorn-walk/-/acorn-walk-7.2.0.tgz"
   integrity sha512-OPdCF6GsMIP+Az+aWfAAOEt2/+iVDKE7oy6lJ098aoe59oAmK76qV6Gw60SbZ8jHuG2wH058GF4pLFbYamYrVA==
 
+acorn-walk@^8.1.1:
+  version "8.3.4"
+  resolved "https://registry.yarnpkg.com/acorn-walk/-/acorn-walk-8.3.4.tgz#794dd169c3977edf4ba4ea47583587c5866236b7"
+  integrity sha512-ueEepnujpqee2o5aIYnvHU6C0A42MNdsIDeqy5BydrkuC5R1ZuUFnm27EeFJGoEHJQgn3uleRvmTXaJgfXbt4g==
+  dependencies:
+    acorn "^8.11.0"
+
 acorn@^7.0.0:
   version "7.4.1"
   resolved "https://registry.npmjs.org/acorn/-/acorn-7.4.1.tgz"
   integrity sha512-nQyp0o1/mNdbTO1PO6kHkwSrmgZ0MT/jCCpNiwbUjGoRN4dlBhqJtoQuCnEOKzgTVwg0ZWiCoQy6SxMebQVh8A==
 
+acorn@^8.11.0, acorn@^8.4.1:
+  version "8.15.0"
+  resolved "https://registry.yarnpkg.com/acorn/-/acorn-8.15.0.tgz#a360898bc415edaac46c8241f6383975b930b816"
+  integrity sha512-NZyJarBfL7nWwIq+FDL6Zp/yHEhePMNnnJ0y3qfieCrmNvYct8uvtiV41UvlSe6apAfk0fY1FbWx+NwfmpvtTg==
+
 alpinejs@^2.5, alpinejs@^2.8.2:
   version "2.8.2"
   resolved "https://registry.npmjs.org/alpinejs/-/alpinejs-2.8.2.tgz"
@@ -1604,6 +1521,11 @@ anymatch@~3.1.2:
     normalize-path "^3.0.0"
     picomatch "^2.0.4"
 
+arg@^4.1.0:
+  version "4.1.3"
+  resolved "https://registry.npmjs.org/arg/-/arg-4.1.3.tgz"
+  integrity sha512-58S9QDqG0Xx27YwPSt9fJxivjYl432YCwfDMfZ+71RAqUrZef7LrKQZ3LHLOwCS4FLNBplP533Zx895SeOCHvA==
+
 arg@^5.0.1:
   version "5.0.2"
   resolved "https://registry.yarnpkg.com/arg/-/arg-5.0.2.tgz#c81433cc427c92c4dcf4865142dbca6f15acd59c"
@@ -1641,12 +1563,12 @@ autoprefixer@^10.2.5:
     postcss-value-parser "^4.2.0"
 
 axios@^1.6.0, axios@^1.8.4:
-  version "1.13.5"
-  resolved "https://registry.yarnpkg.com/axios/-/axios-1.13.5.tgz#5e464688fa127e11a660a2c49441c009f6567a43"
-  integrity sha512-cz4ur7Vb0xS4/KUN0tPWe44eqxrIu31me+fbang3ijiNscE129POzipJJA6zniq2C/Z6sJCjMimjS8Lc/GAs8Q==
+  version "1.13.2"
+  resolved "https://registry.yarnpkg.com/axios/-/axios-1.13.2.tgz#9ada120b7b5ab24509553ec3e40123521117f687"
+  integrity sha512-VPk9ebNqPcy5lRGuSlKx752IlDatOjT9paPlm8A7yOuW2Fbvp4X3JznJtT4f0GzGLLiWE9W8onz51SqLYwzGaA==
   dependencies:
-    follow-redirects "^1.15.11"
-    form-data "^4.0.5"
+    follow-redirects "^1.15.6"
+    form-data "^4.0.4"
     proxy-from-env "^1.1.0"
 
 babel-plugin-macros@^3.1.0:
@@ -2007,6 +1929,11 @@ cosmiconfig@^7.0.0, cosmiconfig@^7.0.1:
     path-type "^4.0.0"
     yaml "^1.10.0"
 
+create-require@^1.1.0:
+  version "1.1.1"
+  resolved "https://registry.npmjs.org/create-require/-/create-require-1.1.1.tgz"
+  integrity sha512-dcKFX3jn0MpIaXjisoRvexIJVEKzaq7z2rZKxf+MSr9TkdmHmsU4m2lcLojrj/FHl8mk5VxMmYA+ftRkP/3oKQ==
+
 crypto@^1.0.1:
   version "1.0.1"
   resolved "https://registry.npmjs.org/crypto/-/crypto-1.0.1.tgz"
@@ -2120,6 +2047,11 @@ didyoumean@^1.2.2:
   resolved "https://registry.npmjs.org/didyoumean/-/didyoumean-1.2.2.tgz"
   integrity sha512-gxtyfqMg7GKyhQmb056K7M3xszy/myH8w+B4RT+QXBQsvAOdc3XymqDDPHx1BgPgsdAA5SIifona89YtRATDzw==
 
+diff@^4.0.1:
+  version "4.0.2"
+  resolved "https://registry.npmjs.org/diff/-/diff-4.0.2.tgz"
+  integrity sha512-58lmxKSA4BNyLz+HHMUzlOEpg09FV+ev6ZMe3vJihgdxzgcwZ8VoEEPmALCZG9LmqfVoNMMKpttIYTVG6uDY7A==
+
 dir-glob@^3.0.1:
   version "3.0.1"
   resolved "https://registry.npmjs.org/dir-glob/-/dir-glob-3.0.1.tgz"
@@ -2280,38 +2212,6 @@ es-set-tostringtag@^2.1.0:
     has-tostringtag "^1.0.2"
     hasown "^2.0.2"
 
-esbuild@~0.27.0:
-  version "0.27.3"
-  resolved "https://registry.yarnpkg.com/esbuild/-/esbuild-0.27.3.tgz#5859ca8e70a3af956b26895ce4954d7e73bd27a8"
-  integrity sha512-8VwMnyGCONIs6cWue2IdpHxHnAjzxnw2Zr7MkVxB2vjmQ2ivqGFb4LEG3SMnv0Gb2F/G/2yA8zUaiL1gywDCCg==
-  optionalDependencies:
-    "@esbuild/aix-ppc64" "0.27.3"
-    "@esbuild/android-arm" "0.27.3"
-    "@esbuild/android-arm64" "0.27.3"
-    "@esbuild/android-x64" "0.27.3"
-    "@esbuild/darwin-arm64" "0.27.3"
-    "@esbuild/darwin-x64" "0.27.3"
-    "@esbuild/freebsd-arm64" "0.27.3"
-    "@esbuild/freebsd-x64" "0.27.3"
-    "@esbuild/linux-arm" "0.27.3"
-    "@esbuild/linux-arm64" "0.27.3"
-    "@esbuild/linux-ia32" "0.27.3"
-    "@esbuild/linux-loong64" "0.27.3"
-    "@esbuild/linux-mips64el" "0.27.3"
-    "@esbuild/linux-ppc64" "0.27.3"
-    "@esbuild/linux-riscv64" "0.27.3"
-    "@esbuild/linux-s390x" "0.27.3"
-    "@esbuild/linux-x64" "0.27.3"
-    "@esbuild/netbsd-arm64" "0.27.3"
-    "@esbuild/netbsd-x64" "0.27.3"
-    "@esbuild/openbsd-arm64" "0.27.3"
-    "@esbuild/openbsd-x64" "0.27.3"
-    "@esbuild/openharmony-arm64" "0.27.3"
-    "@esbuild/sunos-x64" "0.27.3"
-    "@esbuild/win32-arm64" "0.27.3"
-    "@esbuild/win32-ia32" "0.27.3"
-    "@esbuild/win32-x64" "0.27.3"
-
 escalade@^3.1.1, escalade@^3.2.0:
   version "3.2.0"
   resolved "https://registry.npmjs.org/escalade/-/escalade-3.2.0.tgz"
@@ -2382,15 +2282,15 @@ fn.name@1.x.x:
   resolved "https://registry.npmjs.org/fn.name/-/fn.name-1.1.0.tgz"
   integrity sha512-GRnmB5gPyJpAhTQdSZTSp9uaPSvl09KoYcMQtsB9rQoOmzs9dH6ffeccH+Z+cv6P68Hu5bC6JjRh4Ah/mHSNRw==
 
-follow-redirects@^1.15.11:
-  version "1.15.11"
-  resolved "https://registry.yarnpkg.com/follow-redirects/-/follow-redirects-1.15.11.tgz#777d73d72a92f8ec4d2e410eb47352a56b8e8340"
-  integrity sha512-deG2P0JfjrTxl50XGCDyfI97ZGVCxIpfKYmfyrQ54n5FO/0gfIES8C/Psl6kWVDolizcaaxZJnTS0QSMxvnsBQ==
+follow-redirects@^1.15.6:
+  version "1.15.6"
+  resolved "https://registry.npmjs.org/follow-redirects/-/follow-redirects-1.15.6.tgz"
+  integrity sha512-wWN62YITEaOpSK584EZXJafH1AGpO8RVgElfkuXbTOrPX4fIfOyEpW/CsiNd8JdYrAoOvafRTOEnvsO++qCqFA==
 
-form-data@^4.0.5:
-  version "4.0.5"
-  resolved "https://registry.yarnpkg.com/form-data/-/form-data-4.0.5.tgz#b49e48858045ff4cbf6b03e1805cebcad3679053"
-  integrity sha512-8RipRLol37bNs2bhoV67fiTEvdTrbMUYcFTiy3+wuuOnUog2QBHCZWXDRijWQfAkhBj2Uf5UnVaiWwA5vdd82w==
+form-data@^4.0.4:
+  version "4.0.4"
+  resolved "https://registry.npmjs.org/form-data/-/form-data-4.0.4.tgz"
+  integrity sha512-KrGhL9Q4zjj0kiUt5OO4Mr/A/jlI2jDYs5eHBpYHPcBEVSiipAvn2Ko2HnPe20rmcuuvMHNdZFp+4IlGTMF0Ow==
   dependencies:
     asynckit "^0.4.0"
     combined-stream "^1.0.8"
@@ -2442,11 +2342,6 @@ fsevents@~2.3.2:
   resolved "https://registry.npmjs.org/fsevents/-/fsevents-2.3.2.tgz"
   integrity sha512-xiqMQR4xAeHTuB9uWm+fFRcIOgKBMiOBP+eXiyT7jsgVCq1bkVygt00oASowB7EdtpOHaaPgKt812P9ab+DDKA==
 
-fsevents@~2.3.3:
-  version "2.3.3"
-  resolved "https://registry.yarnpkg.com/fsevents/-/fsevents-2.3.3.tgz#cac6407785d03675a2a5e1a5305c697b347d90d6"
-  integrity sha512-5xoDfX+fL7faATnagmWPpbFtwh/R77WmMMqqHGS65C3vvB0YHrgF+B1YmZ3441tMj5n63k0212XNoJwzlhffQw==
-
 function-bind@^1.1.2:
   version "1.1.2"
   resolved "https://registry.npmjs.org/function-bind/-/function-bind-1.1.2.tgz"
@@ -2491,13 +2386,6 @@ get-stdin@^8.0.0:
   resolved "https://registry.npmjs.org/get-stdin/-/get-stdin-8.0.0.tgz"
   integrity sha512-sY22aA6xchAzprjyqmSEQv4UbAAzRN0L2dQB0NlN5acTTK9Don6nhoc3eAbUnpZiCANAMfd/+40kVdKfFygohg==
 
-get-tsconfig@^4.7.5:
-  version "4.13.6"
-  resolved "https://registry.yarnpkg.com/get-tsconfig/-/get-tsconfig-4.13.6.tgz#2fbfda558a98a691a798f123afd95915badce876"
-  integrity sha512-shZT/QMiSHc/YBLxxOkMtgSid5HFoauqCE3/exfsEcwg1WkeqjG+V40yBbBrsD+jW2HDXcs28xOfcbm2jI8Ddw==
-  dependencies:
-    resolve-pkg-maps "^1.0.0"
-
 glob-parent@^5.1.2, glob-parent@~5.1.2:
   version "5.1.2"
   resolved "https://registry.npmjs.org/glob-parent/-/glob-parent-5.1.2.tgz"
@@ -2863,6 +2751,11 @@ make-dir@^2.1.0:
     pify "^4.0.1"
     semver "^5.6.0"
 
+make-error@^1.1.1:
+  version "1.3.6"
+  resolved "https://registry.npmjs.org/make-error/-/make-error-1.3.6.tgz"
+  integrity sha512-s8UhlNe7vPKomQhC1qFelMokr/Sc3AgNbso3n74mVPA5LTZwkB9NlXf4XPamLxJE8h0gh73rM94xvwRT2CVInw==
+
 math-intrinsics@^1.1.0:
   version "1.1.0"
   resolved "https://registry.npmjs.org/math-intrinsics/-/math-intrinsics-1.1.0.tgz"
@@ -3401,11 +3294,6 @@ resolve-from@^4.0.0:
   resolved "https://registry.npmjs.org/resolve-from/-/resolve-from-4.0.0.tgz"
   integrity sha512-pb/MYmXstAkysRFx8piNI1tGFNQIFA3vkE3Gq4EuA1dF6gHp/+vgZqsCGJapvy8N3Q+4o7FwvquPJcnZ7RYy4g==
 
-resolve-pkg-maps@^1.0.0:
-  version "1.0.0"
-  resolved "https://registry.yarnpkg.com/resolve-pkg-maps/-/resolve-pkg-maps-1.0.0.tgz#616b3dc2c57056b5588c31cdf4b3d64db133720f"
-  integrity sha512-seS2Tj26TBVOC2NIc2rOe2y2ZO7efxITtLZcGSOnHHNOQ7CkiUBfw0Iw2ck6xkIhPwLhKNLS8BO+hEpngQlqzw==
-
 resolve@^1.1.7, resolve@^1.14.2, resolve@^1.19.0, resolve@^1.20.0, resolve@^1.22.10:
   version "1.22.11"
   resolved "https://registry.yarnpkg.com/resolve/-/resolve-1.22.11.tgz#aad857ce1ffb8bfa9b0b1ac29f1156383f68c262"
@@ -3697,34 +3585,44 @@ ts-easing@^0.2.0:
   resolved "https://registry.npmjs.org/ts-easing/-/ts-easing-0.2.0.tgz"
   integrity sha512-Z86EW+fFFh/IFB1fqQ3/+7Zpf9t2ebOAxNI/V6Wo7r5gqiqtxmgTlQ1qbqQcjLKYeSHPTsEmvlJUDg/EuL0uHQ==
 
+ts-node@^10.9.2:
+  version "10.9.2"
+  resolved "https://registry.yarnpkg.com/ts-node/-/ts-node-10.9.2.tgz#70f021c9e185bccdca820e26dc413805c101c71f"
+  integrity sha512-f0FFpIdcHgn8zcPSbf1dRevwt047YMnaiJM3u2w2RewrB+fob/zePZcrOyQoLMMO7aBIddLcQIEK5dYjkLnGrQ==
+  dependencies:
+    "@cspotcode/source-map-support" "^0.8.0"
+    "@tsconfig/node10" "^1.0.7"
+    "@tsconfig/node12" "^1.0.7"
+    "@tsconfig/node14" "^1.0.0"
+    "@tsconfig/node16" "^1.0.2"
+    acorn "^8.4.1"
+    acorn-walk "^8.1.1"
+    arg "^4.1.0"
+    create-require "^1.1.0"
+    diff "^4.0.1"
+    make-error "^1.1.1"
+    v8-compile-cache-lib "^3.0.1"
+    yn "3.1.1"
+
 tslib@^2.1.0, tslib@^2.3.0, tslib@^2.6.2:
   version "2.8.1"
   resolved "https://registry.npmjs.org/tslib/-/tslib-2.8.1.tgz"
   integrity sha512-oJFu94HQb+KVduSUQL7wnpmqnfmLsOA/nAh6b6EH0wCEoK0/mPeXU6c3wKDV83MkOuHPRHtSXKKU99IBazS/2w==
 
-tsx@^4.21.0:
-  version "4.21.0"
-  resolved "https://registry.yarnpkg.com/tsx/-/tsx-4.21.0.tgz#32aa6cf17481e336f756195e6fe04dae3e6308b1"
-  integrity sha512-5C1sg4USs1lfG0GFb2RLXsdpXqBSEhAaA/0kPL01wxzpMqLILNxIxIOKiILz+cdg/pLnOUxFYOR5yhHU666wbw==
-  dependencies:
-    esbuild "~0.27.0"
-    get-tsconfig "^4.7.5"
-  optionalDependencies:
-    fsevents "~2.3.3"
-
 typescript@^5.9.3:
   version "5.9.3"
   resolved "https://registry.yarnpkg.com/typescript/-/typescript-5.9.3.tgz#5b4f59e15310ab17a216f5d6cf53ee476ede670f"
   integrity sha512-jl1vZzPDinLr9eUt3J/t7V6FgNEw9QjvBPdysz9KfQDD41fQrC2Y4vKQdiaUpFT4bXlb1RHhLpp8wtm6M5TgSw==
 
-"typesense-sync@https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v2.0.0.tgz":
-  version "2.0.0"
-  resolved "https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v2.0.0.tgz#eb267fc50527ed69ce272cef03ca61017bb20f9c"
+"typesense-sync@https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v1.1.0.tgz":
+  version "1.1.0"
+  resolved "https://s3.amazonaws.com/origin-static-assets/corp-node-packages/master/typesense-sync-v1.1.0.tgz"
+  integrity sha512-w1b1aWpiXHOF/VK9SpzsH1B4yVRcyn0EJGQDEoDL5a6R/iD3y+HvG93UGBKlwYCnjv3E5woxXOgYNIE0MqP2Kg==
   dependencies:
     "@babel/runtime" "^7.25.0"
+    crypto "^1.0.1"
     dotenv "^16.4.5"
-    lodash "^4.17.21"
-    typesense "^3.0.1"
+    typesense "^2.0.3"
     winston "^3.14.2"
     yargs "^17.7.2"
 
@@ -3736,10 +3634,10 @@ typesense@^1.8.2:
     axios "^1.6.0"
     loglevel "^1.8.1"
 
-typesense@^3.0.1:
-  version "3.0.1"
-  resolved "https://registry.yarnpkg.com/typesense/-/typesense-3.0.1.tgz#fbbfef3383d983fb58e97b78a236047843cc69ca"
-  integrity sha512-aRzuDQlwR7s2sWw+JiR3CufrMWpzH5UAJ4XlybYczD02QPy5jCsEQiueqUu0Wiai4zW/RGYRruF3XrdEXPgPJA==
+typesense@^2.0.3:
+  version "2.1.0"
+  resolved "https://registry.npmjs.org/typesense/-/typesense-2.1.0.tgz"
+  integrity sha512-a/IRTL+dRXlpRDU4UodyGj8hl5xBz3nKihVRd/KfSFAfFPGcpdX6lxIgwdXy3O6VLNNiEsN8YwIsPHQPVT0vNw==
   dependencies:
     axios "^1.8.4"
     loglevel "^1.8.1"
@@ -3813,6 +3711,11 @@ util@^0.10.3:
   dependencies:
     inherits "2.0.3"
 
+v8-compile-cache-lib@^3.0.1:
+  version "3.0.1"
+  resolved "https://registry.yarnpkg.com/v8-compile-cache-lib/-/v8-compile-cache-lib-3.0.1.tgz#6336e8d71965cb3d35a1bbb7868445a7c05264bf"
+  integrity sha512-wa7YjyUGfNZngI/vtK0UHAN+lgDCxBPCylVXGp0zu59Fz5aiGtNXaq3DhIov063MorB+VfufLh3JlF2KdTK3xg==
+
 whatwg-encoding@^3.1.1:
   version "3.1.1"
   resolved "https://registry.yarnpkg.com/whatwg-encoding/-/whatwg-encoding-3.1.1.tgz#d0f4ef769905d426e1688f3e34381a99b60b76e5"
@@ -3920,3 +3823,8 @@ yargs@^17.7.2:
     string-width "^4.2.3"
     y18n "^5.0.5"
     yargs-parser "^21.1.1"
+
+yn@3.1.1:
+  version "3.1.1"
+  resolved "https://registry.npmjs.org/yn/-/yn-3.1.1.tgz"
+  integrity sha512-Ux4ygGWsu2c7isFWe8Yu1YluJmqVhxqK2cLXNQA5AcC3QfbGNpM7fu0Y8b/z16pXLnFxZYvWhd3fhBY9DLmC6Q==
